<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Louaq - coding</title><description>注于多模态医学图像领域的研究者与爱好者。在这个日新月异的医学影像技术世界里，我希望通过这个小小的平台，与大家分享我对前沿论文的阅读心得、技术分析以及实践经验</description><link>https://louaq.github.io/</link><language>zh_CN</language><item><title>RCPS Rectified Contrastive Pseudo Supervision for Semi-Supervised Medical Image Segmentation</title><link>https://louaq.github.io/posts/rcps-rectified-contrastive-pseudo-supervision-for-semi-supervised-medical-image-segmentation/</link><guid isPermaLink="true">https://louaq.github.io/posts/rcps-rectified-contrastive-pseudo-supervision-for-semi-supervised-medical-image-segmentation/</guid><pubDate>Thu, 27 Nov 2025 12:00:00 GMT</pubDate><content:encoded>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;Medical image segmentation methods are generally designed as fully-supervised to guarantee model performance, which requires a significant amount of expert annotated samples that are high-cost and laborious. Semi-supervised image segmentation can alleviate the problem by utilizing a large number of unlabeled images along with limited labeled images. However, learning a robust representation from numerous unlabeled images remains challenging due to potential noise in pseudo labels and insufficient class separability in feature space, which undermines the performance of current semi-supervised segmentation approaches. To address the issues above, we propose a novel semi-supervised segmentation method named as Rectified Contrastive Pseudo Supervision (RCPS), which combines a rectified pseudo supervision and voxel-level contrastive learning to improve the effectiveness of semi-supervised segmentation. Particularly, we design a novel rectification strategy for the pseudo supervision method based on uncertainty estimation and consistency regularization to reduce the noise influence in pseudo labels. Furthermore, we introduce a bidirectional voxel contrastive loss in the network to ensure intra-class consistency and inter-class contrast in feature space, which increases class separability in the segmentation. The proposed RCPS segmentation method has been validated on two public datasets and an in-house clinical dataset. Experimental results reveal that the proposed
method yields better segmentation performance compared with the state-of-the-art methods in semi-supervised medical image segmentation.&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;医学图像分割方法通常设计为全监督，以保证模型性能，这需要大量的专家标注样本，这些样本成本高且费力。半监督图像分割可以通过利用大量未标记图像与有限的标记图像相结合来缓解这个问题。然而，由于伪标签中的潜在噪声和特征空间中类别可分性不足，从大量未标记图像中学习稳健的表示仍然具有挑战性，这削弱了当前半监督分割方法的性能。为了解决上述问题，我们提出了一种新的半监督分割方法，称为校正对比伪监督（RCPS），该方法结合了校正伪监督和体素级对比学习，以提高半监督分割的有效性。特别是，我们设计了一种新的基于不确定性估计和一致性正则化的伪监督方法校正策略，以减少伪标签中的噪声影响。此外，我们在网络中引入了双向体素对比损失，以确保特征空间中的类内一致性和类间对比，从而增加分割中的类别可分性。所提出的RCPS分割方法已在&lt;strong&gt;两个公共数据集&lt;/strong&gt;和&lt;strong&gt;一个内部临床数据集&lt;/strong&gt;上进行了验证。实验结果表明，与最新的半监督医学图像分割方法相比，所提出的方法在分割性能上表现更好。&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;医学图像分割的精准性对临床诊断和治疗至关重要，但现有方法多依赖全监督学习，需大量专家标注样本，而医学数据标注成本高、耗时长。半监督学习通过结合少量标注数据与大量未标注数据缓解该问题，但现有方法面临两大挑战：&lt;strong&gt;伪标签噪声和特征空间监督不足&lt;/strong&gt;。伪标签由模型预测生成，易受预测误差影响，导致监督不可靠；现有方法仅在标签空间提供监督，缺乏特征空间显式优化以提升类间分离性。为解决这些问题，本文提出Rectified Contrastive Pseudo Supervision（RCPS）方法，通过伪标签校正和体素级对比学习，提高半监督医学图像分割的鲁棒性和特征区分能力。&lt;/p&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;全监督学习主导&lt;/strong&gt;：医学图像分割依赖大量专家标注数据，U-Net及其变体（如Attention U-Net、Swin-Unet）为主流方法，但标注成本高、耗时，限制其应用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;半监督学习兴起&lt;/strong&gt;：通过少量标注数据结合大量无标注数据缓解标注压力，主流策略包括&lt;strong&gt;伪标签监督&lt;/strong&gt;（如Mean Teacher、Cross Pseudo Supervision）和&lt;strong&gt;一致性正则化&lt;/strong&gt;，但伪标签噪声和特征空间监督不足问题突出。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对比学习引入&lt;/strong&gt;：自监督对比学习（如MoCo、SimCLR）通过对齐增广视图提升特征判别性，近年扩展至体素级（如Pixel Contrastive Learning），但负样本采样策略和特征空间监督仍待优化。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6927b19f3203f7be00352a54.png&quot; alt=&quot;Snipaste_2025-11-27_10-03-24&quot; /&gt;&lt;/p&gt;
&lt;p&gt;该研究提出了一种名为**Rectified Contrastive Pseudo Supervision（RCPS，整流对比伪监督）**的半监督医学图像分割方法，旨在解决现有方法中伪标签噪声和特征空间类分离不足的问题。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;整体框架&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;RCPS基于U-Net架构，结合&lt;strong&gt;校正伪监督&lt;/strong&gt;（Rectified Pseudo Supervision）和&lt;strong&gt;双向体素对比学习&lt;/strong&gt;（Bidirectional Voxel Contrastive Learning），利用少量标注数据和大量无标注数据进行训练。&lt;/p&gt;
&lt;p&gt;整体损失函数包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;有监督损失&lt;/strong&gt;：交叉熵损失（Lce）+ Dice损失（LDice），用于标注数据的分割监督。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无监督损失&lt;/strong&gt;：整流伪监督损失（Lrp）+ 双向体素对比损失（Lbc），用于无标注数据的特征学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;核心创新模块&lt;/h3&gt;
&lt;p&gt;（1）校正伪监督（Rectified Pseudo Supervision） 针对伪标签噪声问题，通过&lt;strong&gt;不确定性估计&lt;/strong&gt;和&lt;strong&gt;一致性正则化&lt;/strong&gt;优化伪监督：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;伪监督生成&lt;/strong&gt;：对原始输入进行两种强度增强（亮度、对比度、高斯噪声），生成两个视图（xφ1, xφ2），并通过温度锐化（Temperature Sharpening）生成伪标签，计算交叉熵损失（Lp）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不确定性估计&lt;/strong&gt;：通过KL散度（Dkl）衡量模型预测的不确定性，动态调整伪标签权重，降低噪声影响：    ( L_{urp} = e^{-D_{kl}} L_p + D_{kl} ) - &lt;strong&gt;一致性正则化&lt;/strong&gt;：强制两个增强视图的预测结果一致，通过余弦距离（cosine distance）计算一致性损失（Lcr），最终整流伪监督损失为：    ( L_{rp} = L_{urp}(φ1, y) + L_{urp}(φ2, y) + L_{cr}(φ1, φ2) )&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(2) 双向体素对比学习（Bidirectional Voxel Contrastive Learning）为增强特征空间的类分离性，设计双向对比损失，拉近同类体素距离并推远异类体素：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;正样本对&lt;/strong&gt;：同一输入的两个增强视图中空间对应位置的体素特征（uφ1, uφ2）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;负样本对&lt;/strong&gt;：通过&lt;strong&gt;置信负采样策略&lt;/strong&gt;（Confident Negative Sampling），基于伪标签筛选高置信度的异类体素作为负样本（u−）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;双向InfoNCE损失&lt;/strong&gt;：同时计算uφ1→uφ2和uφ2→uφ1的对比损失，公式为：    ( L_{bc} = L_c(ψ1, ψ2) + L_c(ψ2, ψ1) )，其中( L_c )为InfoNCE损失。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;训练与优化&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;混合精度训练&lt;/strong&gt;和&lt;strong&gt;梯度检查点&lt;/strong&gt;：减少显存占用，加速训练。 - &lt;strong&gt;损失平衡&lt;/strong&gt;：通过超参数α和β平衡整流伪监督损失（Lrp）和双向对比损失（Lbc），整体无监督损失为：    ( ℓ_{unsup} = αL_{rp} + βL_{bc} )。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6927b3143203f7be00353f59.png&quot; alt=&quot;Snipaste_2025-11-27_10-09-44&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;h3&gt;数据集&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;数据集: LA Dataset (心房分割)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据来源&lt;/strong&gt;：2018年心房分割挑战赛数据集，含100例增强MRI扫描（各向同性分辨率0.625mm³）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据划分&lt;/strong&gt;：80例用于训练，20例用于验证，采用10%和20%标记数据的半监督设置。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;数据集: Pancreas-CTDataset (胰腺分割)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;数据来源&lt;/strong&gt;：NIH临床中心公开的82例3D腹部CT扫描，层内分辨率512×512，层间距1.5-2.5mm。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;预处理&lt;/strong&gt;：HU值归一化（窗位75，窗宽400），重采样至1mm³各向同性分辨率；数据划分62例训练，20例验证，同样采用10%和20%标记数据设置。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;数据集: TBI Dataset (创伤性脑损伤)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据来源&lt;/strong&gt;：华山医院100+例创伤性脑损伤患者的T1加权MRI，含42例标记数据和123例未标记数据，标注17个ROI。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预处理&lt;/strong&gt;：线性配准至MNI152模板，直方图均衡化；采用5折交叉验证评估。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;实验设置&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;网络架构&lt;/strong&gt;：基于3D U-Net，在第二个上采样块添加投影头用于对比损失计算。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练参数&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;优化器&lt;/strong&gt;：SGD（动量0.9，权重衰减1e-4），初始学习率1e-2，多项式衰减策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;超参数&lt;/strong&gt;：温度参数T=0.5，对比损失温度τ=0.1，损失权重α=0.1（LA）/0.2（Pancreas/TBI），β=0.1。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据增强&lt;/strong&gt;：随机网格畸变、亮度/对比度调整、高斯噪声。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估指标&lt;/strong&gt;：Dice相似系数（DSC）、95%豪斯多夫距离（HD95）、平均表面距离（ASD）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;实验结果&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;LA数据集&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;10%标记数据&lt;/strong&gt;：RCPS的DSC达90.12%，显著优于基线（79%）及其他SOTA方法（如UA-MT 86.3%、MC-Net+ 89.2%）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;20%标记数据&lt;/strong&gt;：DSC提升至91.21%，接近全监督上限（91.65%）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可视化结果&lt;/strong&gt;：RCPS对细小结构的分割更完整，边缘更贴合金标准。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6927b4843203f7be00355357.png&quot; alt=&quot;Snipaste_2025-11-27_10-15-45&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6927b4933203f7be00355407.png&quot; alt=&quot;Snipaste_2025-11-27_10-16-08&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pancreas-CT数据集&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;10%标记数据&lt;/strong&gt;：DSC从55%提升至76%，优于URPC（72.5%）和MC-Net+（74.3%）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;20%标记数据&lt;/strong&gt;：DSC达81.59%，接近全监督性能（83.89%）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;TBI数据集&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;多类别分割&lt;/strong&gt;：RCPS的平均DSC达71.88%，较全监督U-Net（61.64%）提升16%，优于先前方法（如TBI-GAN 68.3%）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键区域改善&lt;/strong&gt;：损伤区域的分割精度显著提升，减少漏检。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;实验（Ablation Experiments）​​&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6927b5103203f7be00355808.png&quot; alt=&quot;Snipaste_2025-11-27_10-18-03&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;核心组件有效性&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;校正伪监督（Lrp）&lt;/strong&gt;：单独使用提升DSC约5-7%；结合不确定性估计和一致性正则化效果最优。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;双向体素对比损失（Lbc）&lt;/strong&gt;：单独使用提升DSC约3-4%；双向计算比单向对比损失增益更高。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;负采样策略&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;置信负采样&lt;/strong&gt;：较随机采样提升DSC约4%，当负样本数N=100时性能接近N=400（节省50%显存）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;超参数影响&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;损失权重α/β&lt;/strong&gt;：α=0.2、β=0.1时在复杂任务（如TBI）中表现最佳；温度参数T=0.5可有效避免类别重叠。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;本文提出了一种名为Rectified Contrastive Pseudo Supervision（RCPS）的新型半监督医学图像分割方法，主要贡献包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;改进的伪监督策略&lt;/strong&gt;：设计了基于不确定性估计和一致性正则化的伪标签校正机制。通过对原始输入生成两种不同增强视图，利用KL散度进行不确定性估计（式5-6）和余弦距离一致性正则化（式7-8），有效降低伪标签噪声影响。实验表明，该策略使LA数据集在10%标记数据下的DSC从79%提升至90%以上（表I）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;双向体素对比学习&lt;/strong&gt;：提出基于InfoNCE损失的双向体素对比损失（式10-11），结合置信负采样策略（利用伪标签筛选不同类别体素），增强特征空间的类间分离性。在胰腺CT数据集上，该模块使10%标记数据下的DSC达到76%，显著优于现有方法（表II）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多场景验证&lt;/strong&gt;：在LA（MRI左心房分割）、胰腺CT和创伤性脑损伤（TBI）三个数据集上验证了方法有效性。TBI数据集的多类别分割任务中，DSC较全监督U-Net提升16%（从61.64%到71.88%），尤其改善了损伤区域的分割精度（图4-5）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;局限性与未来方向&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当前依赖人工设计的强度增强策略，未来将探索参数化数据增强方法&lt;/li&gt;
&lt;li&gt;未针对距离度量（如HD95）优化，计划引入距离相关损失函数&lt;/li&gt;
&lt;li&gt;需扩展至2D医学图像分割任务以验证通用性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该方法通过标签空间校正和特征空间监督的双重优化，在有限标记数据下实现了接近全监督的分割性能，为临床数据稀缺场景提供了有效解决方案。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;思考: 相对上一篇&lt;a&gt;文章&lt;/a&gt;,本文提出的改进伪监督和双向体素对比学习关注&lt;strong&gt;伪标签噪声和特征空间的监督&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content:encoded></item><item><title>Inconsistency-Aware Uncertainty Estimation for Semi-Supervised Medical Image Segmentation</title><link>https://louaq.github.io/posts/inconsistency-aware-uncertainty-estimation-for-semi-supervised-medical-image-segmentation/</link><guid isPermaLink="true">https://louaq.github.io/posts/inconsistency-aware-uncertainty-estimation-for-semi-supervised-medical-image-segmentation/</guid><pubDate>Tue, 25 Nov 2025 12:00:00 GMT</pubDate><content:encoded>&lt;blockquote&gt;
&lt;p&gt;理论性较强！！！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;In semi-supervised medical image segmentation,most previousworks drawon the common assumption that &lt;strong&gt;higher entropy means higher uncertainty&lt;/strong&gt;. In this paper, we investigate a novel method of estimating uncertainty. We observe that, when assigned different misclassification costs in a certain degree, if the segmentation result of a pixel becomes inconsistent, this pixel shows a relative uncertainty in its segmentation. Therefore, we present a new semi-supervised segmentation model, namely, conservative-radical network (CoraNet in short) based on our uncertainty estimation and separate self-training strategy. In particular, our CoraNet model consists of three major components: a conservative-radical module (CRM), a certain region segmentation network (C-SN), and an uncertain region segmentation network (UC-SN) that could be alternatively trained in an end-to-end manner. We have extensively evaluated our method on various segmentation tasks with publicly available benchmark datasets, including CT pancreas, MR endocardium, and MR multi-structures segmentation on the ACDC dataset. Compared with the current state of the art, our CoraNet has demonstrated superior performance. In addition, we have also analyzed its connection with and difference from conventional methods of uncertainty estimation in semi-supervised medical image segmentation.&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;在半监督医学图像分割中，之前的大多数研究都基于一个常见的假设，即更高的熵意味着更高的不确定性。在本文中，我们研究了一种新的不确定性估计方法。我们观察到，当在一定程度上分配不同的误分类成本时，如果一个像素的分割结果变得不一致，这个像素在其分割中表现出相对的不确定性。因此，我们提出了一种新的半监督分割模型，即基于我们不确定性估计和独立自训练策略的保守激进网络（简称CoraNet）。特别是，我们的CoraNet模型由三个主要组件组成：保守激进模块（CRM）、确定区域分割网络（C-SN）和不确定区域分割网络（UC-SN），它们可以以端到端的方式交替训练。我们在各种分割任务上对我们的方法进行了广泛评估，使用了公开可用的基准数据集，包括CT胰腺、MR心内膜和ACDC数据集上的MR多结构分割。与当前最先进的方法相比，我们的CoraNet表现出了卓越的性能。此外，我们还分析了其与传统半监督医学图像分割不确定性估计方法的联系和差异。&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;医学图像分割中，精确的&lt;strong&gt;器官和组织边界分割&lt;/strong&gt;是重要但具有挑战性的任务。传统深度学习分割模型多采用全监督学习，需大量人工标注数据，而医学图像标注耗时费力且易受主观因素影响，限制了模型在临床场景的部署。半监督学习可利用大量未标注图像缓解这一问题，但现有半监督分割方法多基于“&lt;strong&gt;熵值越高不确定性越大&lt;/strong&gt;”的假设估计不确定性，通过设定阈值区分可靠区域，存在两方面问题：一是初始伪标签预测易引入误差并传播；二是将确定性和不确定性区域输入同一网络，难以充分利用确定性区域信息且可能低估不确定性区域复杂性。因此，本文旨在从新视角估计不确定性并分别处理不同区域分割，提出保守-激进网络（CoraNet）框架。&lt;/p&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;监督学习主导&lt;/strong&gt;：基于深度学习的医学图像分割模型（如U-Net及其变体）依赖大量精确标注数据，在CT/MRI等模态取得高精度，但标注耗时且易受主观因素影响。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;半监督学习兴起&lt;/strong&gt;：通过少量标注数据与大量无标注数据结合，缓解标注压力，主流方法分为熵最小化（EM）和一致性正则化（CR）两类，如Mean Teacher、UA-MT等模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不确定性估计方法&lt;/strong&gt;：传统方法基于像素熵值或置信度阈值（如softmax输出）区分可靠区域，但依赖经验阈值，难以平衡精度与召回率。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/69265b753203f7be00337417.png&quot; alt=&quot;Snipaste_2025-11-26_09-43-28&quot; /&gt;&lt;/p&gt;
&lt;p&gt;本文提出的半监督医学图像分割模型为&lt;strong&gt;Conservative-Radical Network（CoraNet，保守-激进网络）&lt;/strong&gt;，其核心创新在于通过&lt;strong&gt;不一致性感知的不确定性估计&lt;/strong&gt;和&lt;strong&gt;分离自训练策略&lt;/strong&gt;，解决传统半监督分割中对标注数据依赖强、不确定性估计单一的问题。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;CoraNet模型结构&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;模型包含三个关键组件，以端到端交替训练方式协同工作：&lt;/p&gt;
&lt;h4&gt;1. &lt;strong&gt;保守-激进模块（Conservative-Radical Module, CRM）&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;功能&lt;/strong&gt;：通过引入不同误分类成本，生成像素级不确定性掩码，区分“确定区域”和“不确定区域”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实现&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;共享编码器（Encoder）与三个独立解码器（Decoder）：主解码器（D）、保守解码器（Dcon）、激进解码器（Drad）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;保守设置&lt;/strong&gt;：对背景类（class 0）赋予高误分类成本（权重α=5），倾向于少预测前景（如器官区域），避免假阳性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;激进设置&lt;/strong&gt;：对前景类（class 1）赋予高误分类成本（权重α=5），倾向于多预测前景，避免假阴性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不确定性定义&lt;/strong&gt;：通过保守与激进解码器输出的&lt;strong&gt;像素级异或（XOR）操作&lt;/strong&gt;，将预测不一致的区域标记为“不确定区域”，一致区域标记为“确定区域”（见图1）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2. &lt;strong&gt;确定区域分割网络（Certain Region Segmentation Network, C-SN）&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;功能&lt;/strong&gt;：利用确定区域的高置信度伪标签进行自训练，补充标注数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实现&lt;/strong&gt;：对确定区域的预测结果直接作为伪标签，通过交叉熵损失优化主解码器，强化模型对高置信区域的学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3. &lt;strong&gt;不确定区域分割网络（Uncertain Region Segmentation Network, UC-SN）&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;功能&lt;/strong&gt;：针对不确定区域（如边界、模糊区域）采用更鲁棒的标签分配策略，减少误差传播。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实现&lt;/strong&gt;：引入&lt;strong&gt;均值教师模型（Mean Teacher）&lt;/strong&gt;，通过学生模型（主解码器）与教师模型（权重滑动平均）的一致性约束（MSE损失），优化不确定区域的预测。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;数据集&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CT胰腺分割&lt;/strong&gt;：82个对比增强腹部CT容积数据，图像大小512×512×181至512×512×466体素，预处理包括强度归一化（[-100, 240]）、裁剪（192×240）和数据增强（旋转、缩放、翻转）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MR心内膜分割&lt;/strong&gt;：7,980张心脏MR图像（256×256像素），目标为左心室（LV）心内膜边界分割。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ACDC多结构分割&lt;/strong&gt;：100个 cine MR序列，需分割右心室（RV）、左心室（LV）和心肌（Myo）三类结构，图像统一resize为256×256像素。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型配置&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;骨干网络：2D实验采用U-Net，3D实验采用V-Net和ResNet-18。&lt;/li&gt;
&lt;li&gt;训练参数：Adam优化器（学习率0.001，β=(0.5, 0.999)），批大小4，α=5（错分成本权重），平衡损失权重1:1。&lt;/li&gt;
&lt;li&gt;评估指标：Dice相似系数（DSC）、精确率（Precision）、召回率（Recall）、Hausdorff距离（HD）、Jaccard指数、平均表面距离（ASD）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;二、主要实验内容&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;不确定性估计方法验证&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CRM模块可靠性&lt;/strong&gt;：对比传统基于softmax置信度的阈值法（阈值0.5/0.7/0.9），在初始预测阶段，CoraNet的关键成功指数（CSI）更高，且能更好平衡精确率与召回率，减少后续误差传播。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不同不确定性方法对比&lt;/strong&gt;：替换CRM为测试增强、MC Dropout、随机不确定性等方法，CoraNet在2D/3D胰腺分割任务上的DSC均显著优于对比方法（2D：67.01% vs 58.2-63.5%；3D：78.3% vs 72.1-76.5%）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分离自训练策略有效性&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;组件消融实验&lt;/strong&gt;：在CT胰腺分割上验证各模块作用：
&lt;ul&gt;
&lt;li&gt;仅使用主分割损失（seg）：DSC=58.2%&lt;/li&gt;
&lt;li&gt;seg+自训练（无掩码）：DSC=62.3%&lt;/li&gt;
&lt;li&gt;seg+自训练（带掩码）：DSC=64.1%&lt;/li&gt;
&lt;li&gt;seg+均值教师（带掩码）：DSC=65.7%&lt;/li&gt;
&lt;li&gt;完整模型（ours）：DSC=67.01%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不确定性区域动态变化&lt;/strong&gt;：训练过程中，不确定性区域从前景中心逐渐迁移至边界，验证模型对复杂区域的逐步学习能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与现有方法的对比&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CT胰腺分割&lt;/strong&gt;：在2D和3D设置下均优于当前SOTA方法（如UA-MT、UMCT、ADVNET），3D实验中DSC达78.3%（对比UMCT的77.6%），且训练时间更短（3小时 vs 24小时）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MR心内膜分割&lt;/strong&gt;：DSC=89.2%，显著高于均值教师（81.5%）和UA-MT（83.7%）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ACDC多类分割&lt;/strong&gt;：平均DSC=85.6%，超过伪标签法（76.3%）、VAT（78.5%）和均值教师（81.2%），接近全监督上限（87.3%）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;超参数敏感性分析&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;错分成本权重α&lt;/strong&gt;：在α=2/5/10中，α=5时性能最优（胰腺DSC=67.01%，心内膜DSC=89.2%），过大会引入噪声，过小则不确定性区域划分粗糙。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;三、实验结论&lt;/h3&gt;
&lt;p&gt;CoraNet通过基于成本敏感学习的不确定性估计（CRM模块）和分离自训练策略，在单类/多类医学图像分割任务中均实现了优于现有方法的性能，且保持了训练和推理效率。&lt;/p&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;在半监督医学图像分割中，如何估计和处理不确定性仍是一个关键问题。本文通过捕捉多成本敏感设置之间的不一致预测，提出了一种新的不确定性估计方法。该不确定性定义直接依赖分类输出，无需任何预定义的边界感知假设。基于此定义，本文还提出了一种分离自训练策略，对确定区域和不确定区域进行差异化处理。为实现端到端训练，本文开发了保守-激进模块（CRM）、确定区域分割网络（C-SN）和不确定区域分割网络（UC-SN）三个组件来训练半监督分割模型。通过在CT胰腺、MR心内膜和多类别ACDC分割等多种医学图像分割任务上的广泛评估，结果表明该方法在分割精度上优于其他相关基线方法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;思考：本文提出的基于不一致性不确定性评估的方法是一个新颖的思路，可以参考这个&lt;/p&gt;
&lt;/blockquote&gt;
</content:encoded></item><item><title>BraTS-GLI2024——治疗后胶质瘤分割</title><link>https://louaq.github.io/posts/brats2024-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/</link><guid isPermaLink="true">https://louaq.github.io/posts/brats2024-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/</guid><description>治疗后胶质瘤分割完整实现版本。</description><pubDate>Tue, 12 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;strong&gt;一、BraTS-GLI2024介绍&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;国际脑肿瘤分割(BraTS)挑战。自2012年以来，BraTS一直专注于为成人脑胶质瘤的描述生成基准环境和数据集。今年挑战的重点仍然是产生一个共同的基准环境，但与2023年类似，该数据集将大幅扩展到约4,500例，以解决额外的i)人口(例如撒哈拉以南非洲患者)，ii)肿瘤(例如脑膜瘤)，iii)临床问题(例如缺失数据)，以及iv)技术考虑(例如增强)。具体而言，BraTS 2024的重点是确定当前最先进的算法，用于(任务1)使用新数据集处理治疗后胶质瘤，(任务2)服务不足的撒哈拉以南非洲脑胶质瘤患者群体，(任务3)使用新数据集处理放疗脑膜瘤，(任务4)脑转移，(任务5)儿科脑肿瘤患者，(任务7和8)全局和局部缺失数据，(任务9)有用的增强技术，以及(任务10)使用新数据集处理病理学。值得注意的是，所有数据都是常规的临床获得的，脑肿瘤患者的多位点多参数磁共振成像(mpMRI)扫描。所有数据集的真实参考注释由神经放射学专家为训练、验证和测试数据集中的每个主题创建和批准，以定量评估参与算法的性能。   神经胶质瘤是成人中最常见的恶性原发性脑肿瘤，其中弥漫性神经胶质瘤最为常见。弥漫性神经胶质瘤的特点是其在中枢神经系统内的浸润性生长模式，由于其生物行为、预后和对治疗的反应多变，给治疗和监测带来了巨大挑战。弥漫性神经胶质瘤的治疗涉及针对肿瘤特征和患者健康状况的多模式方法，包括手术、放射治疗和全身治疗。MRI 仍然是弥漫性神经胶质瘤治疗后成像的黄金标准。它提供了有关肿瘤大小、位置和形态随时间变化的重要信息。弥漫性神经胶质瘤的治疗后成像是患者管理的基本组成部分，它决定了治疗的变化并与临床结果相关。脑肿瘤分割 (BraTS) 挑战赛旨在确定当前最先进的脑中枢神经系统肿瘤分割算法。训练和盲验证数据可用于构建和评估分割算法。盲法验证数据预测的评估指标将立即返回给参与者，并附上不断更新的排行榜。2024 年 BraTS 治疗后胶质瘤子挑战赛的目的是开发一种自动多室脑肿瘤分割算法，用于治疗后 MRI 上的高级别和低级别弥漫性胶质瘤。从该挑战赛中开发的数据和算法可用于创建客观评估残留肿瘤体积的工具，以进行治疗计划和结果预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;二、BraTS-GLI2024任务&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;肿瘤亚区域分割。注释包括增强组织（ET - 标签 3）、周围非增强 FLAIR 高信号（SNFH）- 标签 2）、非增强肿瘤核心（NETC - 标签 1）和切除腔（RC - 标签 4）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3cdaabd26c5dea4a928777dae57a19a1_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三、BraTS-GLI2024数据集&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;今年的 BraTS 挑战赛使用多机构常规治疗后临床获取的胶质瘤多参数 MRI (mpMRI) 扫描作为训练、验证和测试数据。数据来自七个不同的学术医疗中心，杜克大学，加州大学旧金山分校，密苏里大学哥伦比亚分校，加州大学圣地亚哥分校，海德堡大学医院，密歇根大学，印第安纳大学。专家神经放射学家为训练、验证和测试数据集中的每个受试者创建并批准肿瘤子区域的地面实况注释，以定量评估预测的肿瘤分割。所有 BraTS mpMRI 扫描均可作为 NIfTI 文件 (.nii.gz) 获得，并描述 a) 原始 (T1) 和 b) 对比后 T1 加权 (T1Gd)、c) T2 加权 (T2) 和 d) T2 液体衰减反转恢复 (FLAIR) 体积，并使用来自多个数据提供机构的不同临床方案和各种扫描仪获取。真实数据是在预处理后创建的，包括与同一解剖模板的联合配准、以相同分辨率 (1 mm3) 进行插值以及颅骨剥离。所有成像数据集均由一到四名评分员按照相同的注释协议手动注释，其注释均由经验丰富的神经放射学家批准。注释包括增强组织（ET - 标签 3）、周围非增强 FLAIR 高信号（SNFH）- 标签 2）、非增强肿瘤核心（NETC - 标签 1）和切除腔（RC - 标签 4）。数据下载：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;https://www.synapse.org/Synapse:syn59059776&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;考虑评估的子区域包括：ET 描述活动性肿瘤区域以及增强的结节区域。NETC 表示肿瘤内的坏死和囊肿。SNFH 包括水肿、浸润性肿瘤和治疗后变化。RC 包括近期和慢性切除腔，通常包含液体、血液、空气和/或蛋白质物质。肿瘤核心 (ET 加 NETC) 描述通常在手术过程中切除的内容。整个肿瘤 (ET 加 SNFH 加 NETC) 定义肿瘤的整个范围，包括肿瘤核心、浸润性肿瘤、肿瘤周围水肿和治疗相关变化。评价指标：病变骰子相似系数 (DSC)，它测量预测分割和地面真实分割之间的体素分割重叠，忽略真正的负体素。病变95% Hausdorff 距离 (HD95)，它测量预测分割和地面真实分割中心之间的距离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;四、技术路线&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1、根据固定阈值和形态学最大连通域分析得到大脑ROI区域，然后提取原始图像和标注图像的ROI。2、分析步骤1的ROI图像信息，得到图像平均大小是136x175x142，因此将图像缩放到固定大小160x160x160。3、图像预处理，对步骤2的原始图像进行像素值（1，99）截断，然后采用均值为0，方差为1的方式进行归一化处理。然后将数据分成训练集和验证集，对训练集做5倍数据增强处理。4、搭建VNet3d网络，使用AdamW优化器，学习率是0.001，batchsize是4，epoch是300，损失函数采用多分类的dice和交叉熵。5、训练结果和验证结果&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-df11b9a607030b10063b3776ea5c5f06_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5f06201f34e9ccd1d6a568f69066c444_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;6、验证集分割结果   左图是金标准结果，右图是预测分割结果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picx.zhimg.com/v2-772a08fb349aafe011903a23fa9a72b5_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-22f9323403582a110505bf641348f048_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9c2a1ae27705510554e52a95e142cfa7_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pica.zhimg.com/v2-58ebcbc0db271f4a1dae082b7a5dcd92_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picx.zhimg.com/v2-3e62926bfd194368a03cc3296711b455_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picx.zhimg.com/v2-27b119d7007bcd7d171201f3c93e4143_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-90d8240ba53aa5c24a8ef05b532af2d8_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;7、测试集分割结果&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e062d0eef5992c61950b7f44edca0119_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-34787a1bddd8ca065211f4605627c122_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pica.zhimg.com/v2-6be28a74936153edf7c6688714d82a7a_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0f4eefdf7f6a420aba946c24ca0f2b2c_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3b20ad70c62a32e07790444e55e77ed4_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picx.zhimg.com/v2-eaa010190a14e4605d023980f5a20141_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a25ab522c925af4455759d847b70e7f0_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-10858af719a3a72e615bd01577a21d2c_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pica.zhimg.com/v2-540c7563c132616191b403d34e21f48c_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8a152bddab7691f608059404063bbccf_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;转载自知乎：https://zhuanlan.zhihu.com/p/979775484&lt;/p&gt;
</content:encoded></item><item><title>Mutual learning with reliable pseudo label for semi-supervised medical image segmentation</title><link>https://louaq.github.io/posts/mutual-learning-with-reliable-pseudo-label-for-semi-supervised-medical-image-segmentation/</link><guid isPermaLink="true">https://louaq.github.io/posts/mutual-learning-with-reliable-pseudo-label-for-semi-supervised-medical-image-segmentation/</guid><pubDate>Thu, 27 Nov 2025 14:00:00 GMT</pubDate><content:encoded>&lt;blockquote&gt;
&lt;p&gt;Medical Image Analysis&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;Semi-supervised learning has garnered significant interest as a method to alleviate the burden of data
annotation. Recently, semi-supervised medical image segmentation has garnered significant interest that can
alleviate the burden of densely annotated data. Substantial advancements have been achieved by integrating
consistency-regularization and pseudo-labeling techniques. The quality of the pseudo-labels is crucial in this
regard. Unreliable pseudo-labeling can result in the introduction of noise, leading the model to converge to
suboptimal solutions. To address this issue, we propose learning from reliable pseudo-labels. In this paper,
we tackle two critical questions in learning from reliable pseudo-labels: which pseudo-labels are reliable and
how reliable are they? Specifically, we conduct a comparative analysis of two subnetworks to address both
challenges. Initially, we compare the prediction confidence of the two subnetworks. A higher confidence score
indicates a more reliable pseudo-label. Subsequently, we utilize intra-class similarity to assess the reliability
of the pseudo-labels to address the second challenge. The greater the intra-class similarity of the predicted
classes, the more reliable the pseudo-label. The subnetwork selectively incorporates knowledge imparted by
the other subnetwork model, contingent on the reliability of the pseudo labels. By reducing the introduction of
noise from unreliable pseudo-labels, we are able to improve the performance of segmentation. To demonstrate
the superiority of our approach, we conducted an extensive set of experiments on three datasets: Left Atrium,
Pancreas-CT and Brats-2019. The experimental results demonstrate that our approach achieves state-of-the-art
performance. Code is available at: https://github.com/Jiawei0o0/mutual-learning-with-reliable-pseudo-labels&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;半监督学习作为一种减轻数据标注负担的方法，已经引起了广泛关注。最近，半监督医学图像分割引起了极大的关注，它可以减轻密集标注数据的负担。通过结合一致性正则化和伪标签技术，已经取得了实质性进展。在这方面，伪标签的质量至关重要。不可靠的伪标签会导致噪声的引入，使模型收敛到次优解。为了解决这个问题，我们提出从可靠的伪标签中学习。在本文中，我们解决了从可靠伪标签中学习的两个关键问题：&lt;strong&gt;哪些伪标签是可靠&lt;/strong&gt;的以及&lt;strong&gt;它们有多可靠&lt;/strong&gt;？具体而言，我们针对这两个挑战进行了两个子网络的比较分析。首先，我们比较两个子网络的预测置信度。更高的置信度得分表明伪标签更可靠。随后，我们利用类内相似性来评估伪标签的可靠性，以解决第二个挑战。预测类别的类内相似性越高，伪标签就越可靠。子网络有选择地整合来自另一个子网络模型的知识，这取决于伪标签的可靠性。通过减少不可靠伪标签引入的噪声，我们能够提高分割的性能。为了展示我们方法的优越性，我们在三个数据集上进行了大量实验：左心房、胰腺-CT和Brats-2019。实验结果表明，我们的方法达到了最先进的性能。代码可在以下网址获取：https://github.com/Jiawei0o0/mutual-learning-with-reliable-pseudo-labels&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;医学图像分割是计算机辅助诊断系统的关键组成部分，但深度卷积神经网络（CNN）的优异性能依赖于大量像素级标注数据。医学图像的密集标注成本高、耗时长，半监督学习通过结合少量标注数据和大量未标注数据缓解这一问题，成为研究热点。现有半监督分割方法主要集成一致性正则化与伪标签技术，但伪标签质量对模型性能至关重要。不可靠的伪标签会引入噪声，导致模型收敛到次优解。传统方法通过固定阈值筛选高置信度伪标签，但阈值选择困难：低阈值可能保留噪声，高阈值会限制未标注数据利用并偏向多数类。因此，本文针对两个核心问题展开研究：如何识别可靠伪标签，以及如何量化其可靠性，旨在通过可靠伪标签的互学习减少噪声影响，提升半监督医学图像分割性能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6928fd4b3203f7be0039a421.png&quot; alt=&quot;Snipaste_2025-11-28_09-38-33&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;核心方向&lt;/strong&gt;：半监督医学图像分割，通过结合&lt;strong&gt;一致性正则化&lt;/strong&gt;与&lt;strong&gt;伪标签技术&lt;/strong&gt;，利用有限标注数据和大量无标注数据训练模型，缓解医学图像密集标注的高成本问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;主流方法&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;一致性正则化&lt;/strong&gt;：通过数据/特征/模型层面的扰动，强制模型输出一致预测（如UA-MT、URPC）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;伪标签技术&lt;/strong&gt;：生成高置信度伪标签指导无标注数据学习（如MC-Net、SS-Net）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;混合策略&lt;/strong&gt;：多子网络交叉监督融合上述两种方法（如CCT、AC-MT），提升性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6928fdeb3203f7be0039abc9.png&quot; alt=&quot;Snipaste_2025-11-28_09-41-21&quot; /&gt;&lt;/p&gt;
&lt;p&gt;本文提出了一种基于可靠伪标签的互学习半监督医学图像分割模型，主要创新点包括双子网互学习框架及可靠性评估机制两方面：&lt;/p&gt;
&lt;h3&gt;核心框架&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;两个子网络结构&lt;/strong&gt;     - 共享编码器+两个差异化解码器（采用不同上采样技术），生成两类伪标签   - 目标：解决传统伪标签方法中阈值筛选导致的噪声引入和类别偏见问题&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;互学习机制&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;两个解码器通过比较预测置信度实现双向知识蒸馏&lt;/li&gt;
&lt;li&gt;仅当对方子网预测置信度更高时才进行学习，避免噪声累积&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;可靠性评估方法&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;伪标签可靠性筛选（WR模块）&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;置信度比较&lt;/strong&gt;：计算两个子网的预测置信度矩阵（如式8）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动态掩码&lt;/strong&gt;：通过指示函数生成0-1掩码，仅保留高置信度伪标签（如式9）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势&lt;/strong&gt;：无需人工设定阈值，自适应过滤低质量伪标签&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;可靠性量化评估（HR模块）&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;类内语义一致性&lt;/strong&gt;：&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;基于伪标签生成类别原型（Class Prototype）&lt;/li&gt;
&lt;li&gt;计算像素特征与类别原型的余弦相似度&lt;/li&gt;
&lt;li&gt;通过MSE距离度量预测概率与特征相似度的一致性（如式13）&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;可靠性权重&lt;/strong&gt;：距离越小权重越高，降低不可靠伪标签的学习权重&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;损失函数设计&lt;/h3&gt;
&lt;p&gt;$$Loss = \gamma(L_{dice}^a + L_{dice}^b) + \beta(\lambda_{wr}^a\lambda_{hr}^aL_{ce}(y_b,y_p^a) + \lambda_{wr}^b\lambda_{hr}^bL_{ce}(y_a,y_p^b)) $$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;监督损失&lt;/strong&gt;：带标签数据的Dice损失&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无监督损失&lt;/strong&gt;：结合可靠性权重（WR×HR）的交叉熵损失&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动态平衡参数&lt;/strong&gt;：β采用时间依赖的高斯升温函数&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;数据集: &lt;strong&gt;Left Atrium (LA) 数据集&lt;/strong&gt;, &lt;strong&gt;Pancreas-CT 数据集&lt;/strong&gt;, &lt;strong&gt;Brats-2019 数据集&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;实现细节&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;网络架构&lt;/strong&gt;：采用V-Net作为 backbone，包含一个共享编码器和两个不同的解码器（使用不同的上采样技术）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练配置&lt;/strong&gt;：PyTorch框架，NVIDIA RTX 3090 GPU；优化器为SGD（初始学习率10⁻²，权重衰减10⁻⁴）；批大小4（2个标记样本+2个未标记样本）；训练迭代次数15k。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评价指标&lt;/strong&gt;：Dice系数、Jaccard系数、95% Hausdorff距离（95HD）、平均表面距离（ASD）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;对比实验&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;与现有半监督方法对比&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LA数据集&lt;/strong&gt;：在10%标记数据下，所提方法Dice系数达89.86%，较UA-MT（84.58%）、MC-Net+（88.39%）等方法显著提升；20%标记数据时Dice达91.02%，接近全监督V-Net（91.33%）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pancreas-CT数据集&lt;/strong&gt;：10%标记数据下Dice达75.93%，较URPC（73.53%）提升2.4%；20%标记数据时Dice达81.53%，较MC-Net+（79.37%）提升2.16%。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Brats-2019数据集&lt;/strong&gt;：10%标记数据下Dice达84.29%，较AC-MT（83.77%）提升0.52%；20%标记数据时Dice达85.47%，较AC-MT（84.63%）提升0.84%。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/692900053203f7be0039ba42.png&quot; alt=&quot;Snipaste_2025-11-28_09-50-14&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/692900073203f7be0039ba53.png&quot; alt=&quot;Snipaste_2025-11-28_09-50-04&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/692900143203f7be0039baa9.png&quot; alt=&quot;Snipaste_2025-11-28_09-50-33&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;实验（Ablation Experiments）​​&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;关键组件验证&lt;/strong&gt;：对比交叉熵损失（CE）、可靠伪标签选择（WR）、可靠性评估（HR）的组合效果。结果显示，同时使用WR和HR时性能最优（LA数据集Dice 89.86%，Pancreas-CT数据集Dice 75.93%）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可靠性度量方法对比&lt;/strong&gt;：对比特征相似度（Sim）、最大概率（Prob）、乘积（Multiply）、平均（Avg）等方法，所提基于MSE的度量方法效果最佳（LA数据集Dice 89.86%）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解码器数量扩展&lt;/strong&gt;：在LA数据集上，3个解码器性能略低于2个解码器；在Pancreas-CT数据集上，3个解码器在10%标记数据下Dice提升1.72%，但整体增益有限。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;阈值方法对比&lt;/strong&gt;：与基于阈值筛选高置信伪标签的方法相比，所提无阈值的互学习策略在LA数据集上Dice更高（89.86% vs 阈值0.5时的88.60%），且避免了阈值选择难题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;超参数敏感性分析&lt;/strong&gt;：超参数γ（平衡监督与无监督损失）在0.5时性能最优，过小将导致标记数据训练不足，过大则削弱互学习约束。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;本文针对半监督医学图像分割中伪标签质量影响模型性能的核心问题，提出了一种基于可靠伪标签的互学习框架，主要工作总结如下：&lt;/p&gt;
&lt;h3&gt;核心贡献&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;可靠伪标签学习框架&lt;/strong&gt;
提出双子网互学习机制解决两个关键问题：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;哪些伪标签可靠？&lt;/strong&gt; 通过比较两个子网的预测置信度（式8-9），选择置信度更高的伪标签作为监督信号，避免低质量预测误导模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;伪标签可靠性如何？&lt;/strong&gt; 引入类内语义一致性度量（式10-13），通过计算特征与类原型的余弦相似度及预测概率的MSE距离，量化伪标签可靠性并赋予权重。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;互学习策略&lt;/strong&gt;
两个子网采用不同上采样技术，仅在对方预测置信度更高时进行知识融合（图2），同时利用可靠性权重动态调整监督强度，减少噪声干扰。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实验验证&lt;/strong&gt;
在Left Atrium、Pancreas-CT和Brats-2019数据集上，10%/20%标注数据设置下均达到SOTA性能：
&lt;ul&gt;
&lt;li&gt;Left Atrium（10%标注）：Dice达89.86%（较基线提升10.33%），95HD降至6.91 voxels&lt;/li&gt;
&lt;li&gt;Pancreas-CT（10%标注）：Dice达75.93%（较基线提升21.99%）&lt;/li&gt;
&lt;li&gt;Brats-2019（10%标注）：Dice达84.29%（较SOTA AC-MT提升0.52%）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;关键发现&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;阈值无关筛选&lt;/strong&gt;：相比传统阈值筛选高置信伪标签，互学习机制可自适应区分可靠样本，缓解类别不平衡导致的偏见。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;类内一致性增益&lt;/strong&gt;：结合特征相似度与预测概率的可靠性度量（式13），较单一指标（概率/相似度）提升1.26%-1.66% Dice。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;子网数量影响&lt;/strong&gt;：双子网性能优于三子网（LA数据集Dice下降0.48%），表明过多子网可能引入冗余噪声。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;未来方向&lt;/h3&gt;
&lt;p&gt;计划整合类间差异性度量，通过联合类内相似度与类间区分度进一步优化伪标签质量评估，提升模型在低标注数据下的鲁棒性。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文提提出的伪标签互学习确定可靠的伪标签值得思考，同时还需要度量具体的可靠性，即&lt;strong&gt;哪些伪标签是可靠的和有多可靠&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content:encoded></item><item><title>Diff-IF Multi-modality image fusion via diffusion model with fusion knowledge prior</title><link>https://louaq.github.io/posts/diff-if-multi-modality-image-fusion-via-diffusion-model-with-fusion-knowledge-prior/</link><guid isPermaLink="true">https://louaq.github.io/posts/diff-if-multi-modality-image-fusion-via-diffusion-model-with-fusion-knowledge-prior/</guid><pubDate>Mon, 21 Jul 2025 16:20:00 GMT</pubDate><content:encoded>&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Multi-modality image fusion (MMIF)&lt;/strong&gt; aims to aggregate the complementary information from diverse source image domains. As the generative adversarial network-based methods have been the primary choice and demonstrated satisfactory fusion performance, they suffer from the unstable training and mode collapse. To tackle this challenge, we propose a novel diffusion model for MMIF incorporating fusion knowledge prior, termed as Diff-IF. Diff-IF proposes a trainable diffusion model paradigm for multi-modality image fusion, resolving the issue of lacking the ground truth for the diffusion model in image fusion tasks. It decomposes the diffusion-based image fusion method into conditional diffusion model and fusion knowledge prior with the targeted search to derive the prior distribution for the specific image fusion task. In particular, the forward diffusion process is guided by the fusion knowledge prior distribution through targeted search, while the reverse diffusion process is designed to generate high-quality fused images. Extensive experiments demonstrate that Diff-IF achieves outstanding performance, including exemplary visual preservation, and good preservation of weak textures, across various MMIF tasks such as infrared-visible image fusion and medical image fusion. The code will be available at https://github.com/XunpengYi/Diff-IF.&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;**多模态图像融合（MMIF）**旨在聚合来自不同源图像域的互补信息。尽管基于生成对抗网络的方法已成为主要选择并表现出令人满意的融合性能，它们仍面临不稳定训练和模式崩溃的问题。为了解决这一挑战，我们提出了一种新颖的结合融合知识先验的扩散模型用于MMIF，称为Diff-IF。Diff-IF提出了一种可训练的扩散模型范式用于多模态图像融合，解决了图像融合任务中扩散模型缺乏真实值的问题。它将基于扩散的图像融合方法分解为条件扩散模型和融合知识先验，通过目标搜索得出特定图像融合任务的先验分布。特别是，前向扩散过程通过目标搜索由融合知识先验分布引导，而反向扩散过程旨在生成高质量的融合图像。大量实验表明，Diff-IF在各种MMIF任务（如红外-可见光图像融合和医学图像融合）中实现了卓越的性能，包括出色的视觉保留和对弱纹理的良好保留。代码将在https://github.com/XunpengYi/Diff-IF提供。&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_16-21-48.png&quot; alt=&quot;Snipaste_2025-07-21_16-21-48&quot; /&gt;&lt;/p&gt;
&lt;p&gt;文章聚焦于&lt;strong&gt;多模态图像融合（MMIF）领域&lt;/strong&gt;，其研究背景源于现有方法的不足及扩散模型应用面临的挑战：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;MMIF的重要性&lt;/strong&gt;：由于单一图像传感器成像原理的局限，MMIF技术应运而生，旨在整合不同源图像的互补信息，生成高质量融合图像。经典的MMIF任务包括红外 - 可见光图像融合（IVF）和医学图像融合（MIF）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;现有方法的问题&lt;/strong&gt;：基于生成对抗网络（GAN）的方法虽在图像融合中取得了较好的效果，但存在训练不稳定和模式崩溃的问题，导致融合图像分布不合理、质量低，影响了其实用性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;扩散模型应用的挑战&lt;/strong&gt;：近年来，去噪扩散概率模型（DDPM）在图像生成等领域取得了巨大成功，但在MMIF中，由于缺乏真实标签，基线扩散模型无法直接实现图像融合。现有的基于预训练扩散模型的方法存在适应性差、效率低和设计繁琐等问题。 为解决上述问题，作者提出了一种具有融合知识先验的新型扩散模型Diff - IF，以提供更定制化、高效的MMIF解决方案。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;传统深度学习方法&lt;/strong&gt;：早期基于预训练自动编码器的融合策略广泛应用，如CSR、MedCNN等；后来出现基于CNN的端到端融合结构，如U2Fusion；此外，基于transformer、高级语义任务、图像配准与融合相互促进的技术也取得进展。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成式深度学习方法&lt;/strong&gt;：基于GAN的方法在图像融合任务中因能从分布进行融合，具有高质量和良好视觉感知的优势，但存在训练不稳定和模式崩溃问题；扩散模型在图像生成等领域表现出色，但在多模态图像融合中，现有方法存在设计繁琐、不针对特定任务等问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;h3&gt;模型原理&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型拆解&lt;/strong&gt;：Diff - IF将图像融合扩散模型分解为带有融合知识先验的条件扩散模型。利用有针对性的搜索技术，为扩散模型提供针对特定图像融合任务的最优先验分布。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;融合知识先验&lt;/strong&gt;：代表一般融合结果的通用分布，涵盖了各种融合策略的结果，源图像构成了先验分布的端点。在实际应用中，可以通过定义定制的子集融合先验分布来近似它。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;有针对性的搜索&lt;/strong&gt;：旨在在图像融合知识先验的高维流场中找到满足融合任务要求的子集先验分布。通过人工定义或度量评估来确定有针对性的搜索函数，以找到最优的分布解。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;模型结构&lt;/h3&gt;
&lt;h4&gt;正向扩散过程&lt;/h4&gt;
&lt;p&gt;从经过有针对性搜索得到的最优融合分布开始，逐步向数据中添加高斯噪声，直到数据分布接近标准高斯分布。该过程由融合知识先验分布通过有针对性的搜索进行引导。&lt;/p&gt;
&lt;h4&gt;反向扩散过程&lt;/h4&gt;
&lt;p&gt;从标准高斯分布开始，通过网络条件输入（如红外 - 可见光图像融合任务中的可见光图像和红外图像）进行采样，逐步去除噪声，生成符合融合知识先验概率分布的高质量融合图像。&lt;/p&gt;
&lt;h4&gt;网络结构&lt;/h4&gt;
&lt;p&gt;去噪网络$\epsilon_{\theta}$和细化网络$n_{\theta}$通过时间步$t$调制的网络实现，具体由时间步编码层和图像编码器 - 解码器组成。去噪网络使用SR3的骨干网络，细化网络修改了Restormer的骨干网络，记为R - 块，以实现双输入和有效耦合。&lt;/p&gt;
&lt;h3&gt;模型优势&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;训练稳定性&lt;/strong&gt;：与基于GAN的方法相比，Diff - IF的训练过程更加稳定，避免了模式崩溃的问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高质量融合结果&lt;/strong&gt;：通过融合知识先验和定制化的融合任务再训练，Diff - IF能够产生高质量的图像融合结果，具有出色的视觉保留、良好的弱纹理保留和强大的抗噪声能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定制化设计&lt;/strong&gt;：Diff - IF避免了无效的预训练模型参数和繁琐的手动融合设计，更适合多模态图像融合任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-00-30.png&quot; alt=&quot;Snipaste_2025-07-21_17-00-30&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-03-24.png&quot; alt=&quot;Snipaste_2025-07-21_17-03-24&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;红外 - 可见光图像融合（IVF）&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;实验细节&lt;/strong&gt;：在配备两块NVIDIA Geforce RTX 3090 GPU的机器上使用PyTorch进行实验。根据IVF任务特点设置超参数，训练图像裁剪为128×128，迭代次数40K，批量大小16，采用Adam优化器，学习率为1e - 4。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集和指标&lt;/strong&gt;：采用MSRS、RoadScene、LLVIP等公开数据集，还使用GUN数据集进行极端验证。评估指标包括互信息（MI）、差异相关性总和（SCD）、视觉信息保真度（VIF）、$Q_{AB/F}$和结构相似性指数（SSIM），指标值越高表示融合图像质量越好。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;融合知识先验和目标搜索&lt;/strong&gt;：融合知识先验包含TarDAL、DDFM等现有先进融合方法。采用简单通用样本搜索，以SSIM评估相似性、VIF评估视觉保真度、$Q_{AB/F}$评估纹理信息保留，根据任务需求设置权重。&lt;/li&gt;
&lt;li&gt;与SOTA方法比较
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定性比较&lt;/strong&gt;：在多个数据集上，Diff - IF相比其他方法具有明显优势，能保持可见光丰富一致的纹理和场景物理信息，突出红外信息的显著目标和弱热辐射信息，同时具备强大的红外噪声抑制能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定量比较&lt;/strong&gt;：Diff - IF在多个指标上取得最佳结果，VIF和SSIM表现良好，MI和$Q_{AB/F}$指标表明其能有效融合源图像信息，具有出色的纹理梯度表达能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;检测性能比较&lt;/strong&gt;：使用YOLOv8作为目标检测骨干网络，Diff - IF在LLVIP数据集上的目标检测任务中表现最优，展示了其在下游任务中的有效性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分割性能比较&lt;/strong&gt;：在FMB数据集上进行语义分割实验，Diff - IF的融合结果在分割得分和可视化方面表现出色，证明了其高质量的语义保留能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-05-44.png&quot; alt=&quot;Snipaste_2025-07-21_17-05-44&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-05-58.png&quot; alt=&quot;Snipaste_2025-07-21_17-05-58&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;医学图像融合（MIF）&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;实验细节&lt;/strong&gt;：实验设备和环境与IVF任务一致。根据MIF任务特点修改超参数，训练图像裁剪为128×128，迭代次数16K，批量大小16，采用Adam优化器，学习率为1e - 4。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集和其他配置&lt;/strong&gt;：采用哈佛大学医学图像数据集，包括MRI - CT、MRI - PET和MRI - SPECT图像。网络分别用672张图像训练、58张图像测试，评估指标、融合知识构建和目标搜索配置与IVF任务相同。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定性比较&lt;/strong&gt;：在MRI - CT图像中，Diff - IF能更好地保留源图像的纹理，突出高密度成像区域，同时有效保留软组织纹理信息，优于其他方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定量比较&lt;/strong&gt;：Diff - IF在大多数评估指标上优于其他先进方法，在信息保留、可视化和纹理保存方面表现出色，展示了其卓越的融合信息保存能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;实验（Ablation Experiments）​​&lt;/h2&gt;
&lt;p&gt;通过去除融合知识和目标搜索模块进行消融实验，使用与IVF和MIF任务相同的指标评估性能。结果表明，去除不同模块后，方法性能均不如Diff - IF，证明了融合知识和目标搜索的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-06-48.png&quot; alt=&quot;Snipaste_2025-07-21_17-06-48&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;基于扩散的多模态图像融合成本比较&lt;/h2&gt;
&lt;p&gt;在RTX 3090 GPU上比较Diff - IF和基于预训练扩散的图像融合方法（如DDFM）的资源消耗。结果显示，在960×1440尺寸图像上，DDFM因内存溢出无法运行，而Diff - IF可以处理。此外，Diff - IF在参数数量、计算量和运行时间方面具有明显优势。&lt;/p&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;作者提出Diff - IF模型为多模态图像融合提供了新的扩散模型范式，解决了缺乏真实标签的问题。该模型有以下优势：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;定制化适配&lt;/strong&gt;：与无条件扩散和基于分数的方法不同，Diff - IF更具定制性，避免了无效的预训练模型参数和繁琐的手动融合设计。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;融合效果好&lt;/strong&gt;：通过融合知识先验和目标搜索，Diff - IF在红外 - 可见光图像融合和医学图像融合任务中取得了出色的融合效果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;性能优越&lt;/strong&gt;：实验证明Diff - IF具有有效性、可靠性和优越性。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不过，基于扩散的图像融合方法计算消耗大、运行时间长，Diff - IF虽支持定制设计和加速采样，但仍有改进空间，后续可探索加速扩散过程，使其更适合图像融合任务。&lt;/p&gt;
</content:encoded></item><item><title>Brain tumor segmentation based on the dual-path network of multi-modal MRI images</title><link>https://louaq.github.io/posts/brain-tumor-segmentation-based-on-the-dual-path-network-of-multi-modal-mri-images/</link><guid isPermaLink="true">https://louaq.github.io/posts/brain-tumor-segmentation-based-on-the-dual-path-network-of-multi-modal-mri-images/</guid><pubDate>Sat, 19 Jul 2025 12:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;Because of the tumor with infiltrative growth, the &lt;strong&gt;glioma boundary is usually fused with the brain tissue&lt;/strong&gt;, which leads to the failure of accurately segmenting the brain tumor structure through single-modal images. The multi-modal ones are relatively complemented to the inherent heterogeneity and external boundary, which provide complementary features and outlines. Besides, it can retain the structural characteristics of brain diseases from multi angles. However, due to the particularity of multi-modal medical image sampling that increases uneven data density and dense structural vascular tumor mitosis, the glioma may have atypical boundary fuzzy and more noise. To solve this problem, in this paper, the dualpath network based on multi-modal feature fusion (MFF-DNet) is proposed. Firstly, the proposed network uses different kernels multiplexing methods to realize the combination of the large-scale perceptual domain and the non-linear mapping features, which effectively enhances the coherence of information flow. Then, the over-lapping frequency and the vanishing gradient phenomenon are reduced by the residual connection and the dense connection, which alleviate the mutual influence of multi-modal channels. Finally, a dual-path model based on the DenseNet network and the feature pyramid networks (FPN) is established to realize the fusion of low-level, middle-level, and high-level features. Besides, it increases the diversification of glioma non-linear structural features and improves the segmentation precision. A large number of ablation experiments show the effectiveness of the proposed model. The precision of the whole brain tumor and the core tumor can reach 0.92 and 0.90, respectively.&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;由于肿瘤呈浸润性生长，胶质瘤的边界通常与脑组织融合，这导致无法通过单模态图像准确分割脑肿瘤结构。多模态图像在固有异质性和外部边界方面具有相对互补性，提供了互补的特征和轮廓。此外，它可以从多个角度保留脑部疾病的结构特征。然而，由于多模态医学图像采样的特殊性，增加了不均匀的数据密度和密集的结构性血管肿瘤有丝分裂，胶质瘤可能具有非典型的边界模糊和更多的噪声。为了解决这个问题，本文提出了一种基于多模态特征融合的双路径网络（MFF-DNet）。首先，所提出的网络使用不同的核复用方法，实现大规模感知域与非线性映射特征的结合，有效增强信息流的一致性。然后，通过残差连接和密集连接减少重叠频率和梯度消失现象，缓解多模态通道的相互影响。最后，建立了基于DenseNet网络和特征金字塔网络（FPN）的双路径模型，实现低级、中级和高级特征的融合。此外，它增加了胶质瘤非线性结构特征的多样性，提高了分割精度。大量消融实验显示了所提出模型的有效性。整个脑肿瘤和核心肿瘤的精度分别可达0.92和0.90。&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;这篇文章聚焦脑肿瘤分割问题，其研究背景主要基于以下几点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;脑肿瘤特性与诊断需求&lt;/strong&gt;：神经胶质瘤是常见中枢神经系统疾病，分为高低不同等级，恶性程度与预后差异大。获取肿瘤位置及分级对治疗至关重要。磁共振成像（MRI）能提供多角度多模态图像，广泛用于脑疾病检测，但肿瘤浸润生长使边界与脑组织融合，单模态图像难以准确分割肿瘤结构。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;传统方法的局限性&lt;/strong&gt;：传统深度学习模型虽在脑肿瘤分割上有进展，如卷积神经网络（CNN）、全卷积网络（FCN）、Unet网络、DenseNet网络和特征金字塔网络（FPN）等，但存在诸多不足。如卷积过程丢失边界特征，上采样操作使特征图不完整；不同大小卷积核在采样时易出现重叠不均，导致梯度消失或爆炸；缺乏上下文信息和局部感受野特征，难以实现层间和层内特征融合，影响分类精度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;本文研究目的&lt;/strong&gt;：为克服上述网络的缺点，提高脑肿瘤分割精度，本文提出基于多模态特征融合的双路径网络（MFF - DNet）模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;多模态 MRI 应用广泛&lt;/strong&gt;：多模态 MRI 图像能从多角度保留脑疾病结构特征，在脑肿瘤检测中发挥重要作用，如 FLAIR、T1、T2 和 T1c 图像各具优势，为脑肿瘤检测和诊断提供了丰富信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习模型不断改进&lt;/strong&gt;：传统深度学习模型在脑肿瘤分割领域取得显著进展，如 CNN、FCN、Unet、DenseNet 和 FPN 等网络模型。这些模型在提取肿瘤特征和分割肿瘤方面各有特点，但也存在一些不足。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-07-51.png&quot; alt=&quot;Snipaste_2025-07-20_11-07-51&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-07-11.png&quot; alt=&quot;Snipaste_2025-07-20_11-07-11&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;创新点&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;不同大小的卷积核复用：
&lt;ul&gt;
&lt;li&gt;使用大卷积核（1×1@128）和小卷积核（1×1@32）训练多模态MRI图像和掩码图像。大卷积核具有大感受野和空间信息，小卷积核具有强非线性映射能力，二者结合可获得肿瘤的低级特征，减少训练参数数量和采样过程中的信息损失，增强信息流的有效性和连贯性。&lt;/li&gt;
&lt;li&gt;1×1卷积的两步操作可实现跨模态交互和多模态信息的集成特征。采用最大池化层，有效减少训练数据量和网络复杂度，增强模型泛化能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;残差连接和密集连接：
&lt;ul&gt;
&lt;li&gt;将单模态信息视为特征通道，通过密集连接结合多模态局部信息和全局特征，预测像素归属。例如，利用FLAIR图像和T1c图像的特点，结合T2图像获取核心肿瘤的非线性特征并对像素进行分类。&lt;/li&gt;
&lt;li&gt;对输入元素进行标准化处理，调整公式以实现层间标准化，激活特征的移动和缩放。使用ReLU激活函数使特征值落入线性区域，缓解过拟合导致的梯度消失问题；残差连接可自适应地校正大数据的复杂影响，细化网络。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;双路径网络：
&lt;ul&gt;
&lt;li&gt;结合DenseUnet和FPN构建双路径模型。DenseUnet将低级特征与中级特征融合，保留大量原始特征，减少参数数量，缓解梯度消失问题；FPN将中级信息与高级特征结合，突出肿瘤结构，增加胶质瘤非线性结构特征的多样性，提高分割精度。&lt;/li&gt;
&lt;li&gt;设置初始学习率为5 * 10⁻⁴，通过自适应运动估计优化器计算偏差校正后的学习率，最后使用softmax函数回归得到输出模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;模型工作流程&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;数据预处理：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;偏置场校正&lt;/strong&gt;：使用N4ITK方法校正MRI图像的偏置场，遍历所有像素计算类内方差，去除磁噪声干扰，确保同一MRI单模态序列的最终强度分布在相似范围内。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;区域生长算法&lt;/strong&gt;：通过比较像素的颜色和纹理特征，选择初始种子点，利用区域生长算法得到掩码图像，反映脑肿瘤的形状、外观以及与周围组织的关系，输入网络以快速掌握病变与周围组织的时空关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;差异图像（DI）&lt;/strong&gt;：计算MRI图像序列相邻特征的差异，去除与脑肿瘤结构识别无关的冗余区域，减少过拟合现象和泛化误差。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;特征提取与融合：
&lt;ul&gt;
&lt;li&gt;利用不同大小的卷积核复用获取肿瘤的低级特征，通过残差连接和密集连接调整多模态图像中的特征权重，减少重叠现象的影响。&lt;/li&gt;
&lt;li&gt;双路径网络将低级、中级和高级特征进行融合，增强模型对肿瘤特征的识别和分析能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型训练与优化&lt;/strong&gt;：使用BraTS数据集进行训练和测试，设置相关参数，通过自适应运动估计优化器调整学习率，使用softmax函数回归得到输出模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实验评估&lt;/strong&gt;：采用Dice、Sensitivity和Specificity指标评估模型的分割精度，通过消融实验从多模态图像、掩码图像和补丁大小三个方面分析模型的有效性，并与其他脑肿瘤分割算法进行比较。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据集&lt;/strong&gt;：使用&lt;strong&gt;BraTS 2015数据集&lt;/strong&gt;，包括220例高级别胶质瘤（HGG）和54例低级别胶质瘤（LGG），分为训练集和测试集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估指标&lt;/strong&gt;：使用&lt;strong&gt;Dice、Sensitivity和Specificity&lt;/strong&gt;指标评估模型的分割精度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-10-02.png&quot; alt=&quot;Snipaste_2025-07-20_11-10-02&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-10-14.png&quot; alt=&quot;Snipaste_2025-07-20_11-10-14&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;与其他算法比较&lt;/strong&gt;：与其他脑肿瘤分割算法相比，MFF - DNet模型的全肿瘤和核心肿瘤精度分别可达0.92和0.90，在精度、灵敏度和特异性方面表现更优。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;实验（Ablation Experiments）​​&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;消融实验分析：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;多模态图像&lt;/strong&gt;：单模态T1c图像分割精度较高，基于T1c图像测试其他双模态和多模态图像的精度，多模态FLAIR、T1c和T2图像的全肿瘤和核心肿瘤精度可达0.92和0.90。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;掩码图像&lt;/strong&gt;：掩码图像提供位置特征，可克服小卷积核导致的信息流不连贯问题，提高分割精度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图像补丁&lt;/strong&gt;：不同大小的补丁对分割性能影响较小，验证了模型的鲁棒性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-11-56.png&quot; alt=&quot;Snipaste_2025-07-20_11-11-56&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;作者提出基于&lt;strong&gt;多模态MRI脑肿瘤图像的双路径MFF - DNet模型&lt;/strong&gt;，该模型融合低、中、高层特征，增加了胶质瘤非线性结构特征的多样性，提高了分割精度；通过不同内核复用实现了更广泛的感受野和非线性映射能力的结合，解决了边界特征图不完整问题；利用残差连接和密集连接，获得更准确的生理组织和软组织对比结构特征，克服了MRI脑肿瘤图像重叠不均问题。消融实验验证了模型三个创新点在BraTS数据集上的有效性，表明其较现有脑肿瘤分割方法有明显优势。后续工作将聚焦于&lt;strong&gt;特征权重自适应调整和多模态特征的进一步融合&lt;/strong&gt;。&lt;/p&gt;
</content:encoded></item><item><title>FPL+ Filtered Pseudo Label-Based Unsupervised Cross-Modality Adaptation for 3D Medical Image Segmentation</title><link>https://louaq.github.io/posts/fpl-filtered-pseudo-label-based-unsupervised-cross-modality-adaptation-for-3d-medical-image-segmentation/</link><guid isPermaLink="true">https://louaq.github.io/posts/fpl-filtered-pseudo-label-based-unsupervised-cross-modality-adaptation-for-3d-medical-image-segmentation/</guid><pubDate>Fri, 18 Jul 2025 16:27:00 GMT</pubDate><content:encoded>&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;Adapting a medical image segmentation model to a new domain is important for improving its cross-domain transferability, and due to the expensive annotation process, Unsupervised Domain Adaptation (UDA) is appeal-
ing where only unlabeled images are needed for the adaptation. Existing UDA methods are mainly based on image or feature alignment with adversarial training for regularization, and they are limited by insufficient supervision in the target domain. In this paper, we propose an enhanced Filtered Pseudo Label (FPL+)-based UDA method for 3D medical image segmentation. It first uses cross-domain data augmentation to translate labeled images in the source domain to a dual-domain training set consisting of a pseudo source-domain set andapseudo target-domain set. To leverage the dual-domain augmented images to train a pseudo label generator, domain-specific batch normalization layers are used to deal with the domain shift while learning the
domain-invariant structure features, generating high-quality pseudo labels for target-domain images. We then combine labeled source-domain images and target-domain images with pseudo labels to train a final segmentor, where image-level weighting based on uncertainty estimation and pixel-level weighting based on dual-domain consensus are proposed to mitigate the adverse effect of noisy pseudo labels. Experiments on three public multi-modal datasets for Vestibular Schwannoma, brain tumor and whole heart segmentation show that our method surpassed ten state-of-the-art UDA methods, and it even achieved better results than fully supervised learning in the target domain in some cases.&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;将医学图像分割模型适应到新的领域对于提高其跨领域的可迁移性至关重要&lt;/strong&gt;，由于标注过程昂贵，无监督领域适应（UDA）显得极具吸引力，因为只需要未标注的图像进行适应。现有的UDA方法主要基于图像或特征对齐，通过对抗训练进行正则化，但在目标领域中由于监督不足而受到限制。本文提出了一种增强的基于过滤伪标签（FPL+）的UDA方法，用于3D医学图像分割。首先，使用跨领域数据增强将源领域的标注图像转换为由伪源领域集和伪目标领域集组成的双域训练集。为了利用双域增强图像训练伪标签生成器，使用特定领域的批归一化层来处理领域偏移，同时学习领域不变的结构特征，为目标领域图像生成高质量的伪标签。随后，我们结合带有伪标签的源领域标注图像和目标领域图像训练最终分割器，其中基于不确定性估计的图像级加权和基于双域一致性的像素级加权被提出以减轻噪声伪标签的不利影响。在三个公共多模态数据集上的实验，涉及前庭神经鞘瘤、脑肿瘤和全心分割，显示我们的方法超越了十种最先进的UDA方法，甚至在某些情况下在目标领域中取得了比完全监督学习更好的结果。&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;这篇文章聚焦于3D医学图像跨模态分割模型的无监督域适应方法研究，其研究背景主要源于以下两方面问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;跨模态性能差异大&lt;/strong&gt;：医学图像存在多种模态，不同模态间有显著的域差距，如前庭神经鞘瘤分割中的增强T1（ceT1）和高分辨率T2（hrT2）磁共振成像。用一种模态训练的模型在其他模态图像上表现差，且为每种模态手动标注医学图像耗时费力，直接应用训练模型或分别训练新模型都不现实。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;现有方法局限性&lt;/strong&gt;：早期域适应方法需在源域和目标域进行标注，无监督域适应（UDA）虽有潜力，但现有UDA方法多基于图像或特征对齐与对抗训练正则化，主要针对2D医学图像分割，在3D医学图像分割上性能有限。此外，伪标签在UDA中的应用研究较少，因其受源域和目标域间显著域偏移影响，难以生成可靠伪标签，且直接使用含大量噪声的伪标签会误导目标域分割模型训练。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;深度学习在医学图像分割取得进展&lt;/strong&gt;：如脑胶质瘤和前庭神经鞘瘤分割算法性能接近手动分割，但不同模态医学图像间存在显著领域差距，模型跨模态表现不佳。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;领域适应方法涌现&lt;/strong&gt;：早期领域适应方法需源域和一定程度的目标域标注，半监督DA利用少量标注和大量未标注图像进行适应。无监督领域适应（UDA）成为热门，现有方法主要基于图像或特征对齐，结合对抗训练进行正则化。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;伪标签学习受关注&lt;/strong&gt;：伪标签广泛用于训练标注不足或弱标注的分割模型，但在UDA中的应用研究较少。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;p&gt;本文提出了一种基于&lt;strong&gt;增强过滤伪标签&lt;/strong&gt;（FPL+）的**无监督域自适应（UDA）**方法用于3D医学图像分割，以下是该模型的主要构成和关键步骤：&lt;/p&gt;
&lt;h3&gt;1. 跨域数据增强（Cross - Domain Data Augmentation, CDDA）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：为了缓解源域和目标域之间的域差距，生成更多可用于训练的样本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;具体操作&lt;/strong&gt;：利用图像风格转换器$T_t$和辅助目标风格转换器$T_{at}$将标记的源域图像$X_{s_i}$转换为两个伪目标域图像$X_{s\rightarrow t_i}$和$X_{s\rightarrow at_i}$，再将它们分别通过$T_s$转换回两个伪源域图像$X_{s&apos;&lt;em&gt;i}$和$X&lt;/em&gt;{s&apos;&apos;&lt;em&gt;i}$。这样每个标记的源域图像可以得到四个增强图像，且它们共享相同的分割标签$Y&lt;/em&gt;{s_i}$。增强后的源域训练集$D_{ss} = {X_{s_i}, X_{s&apos;&lt;em&gt;i}, X&lt;/em&gt;{s&apos;&apos;&lt;em&gt;i}}$和伪目标域训练集$D&lt;/em&gt;{st} = {X_{s\rightarrow t_i}, X_{s\rightarrow at_i}}$用于训练伪标签生成器。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练损失&lt;/strong&gt;：图像转换器$T_s$和$T_t$基于CycleGAN实现，训练涉及两个对抗损失$L_{t_{gan}}$、$L_{s_{gan}}$和一个循环一致性损失$L_{cyc}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. 双域伪标签生成器（Dual - Domain pseudo label Generator, DDG）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：学习增强后的双域图像，为目标域训练集提供高质量的伪标签。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;具体操作&lt;/strong&gt;：使用双域批量归一化（Dual - BN）层处理域偏移，同时学习域不变的结构特征。在某一层中，从源域提取的特征由源域BN层归一化，从目标域提取的特征由目标域BN层归一化。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失函数&lt;/strong&gt;：使用Dice损失$L_{dice}$在$D_{ss}$和$D_{st}$上训练$DDG$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. 伪标签过滤（Pseudo Label Filtering）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;目的：抑制不可靠的伪标签，提高最终分割器的训练效果。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;基于尺寸感知不确定性估计的图像级加权&lt;/strong&gt;：使用蒙特卡罗（MC）Dropout进行不确定性估计，计算每个目标域图像的方差图$V_j$，并根据估计的不确定区域大小$\eta_j$对图像级不确定性$v_j$进行归一化，得到图像级不确定性$u_j$，进而计算图像级权重$w_j$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于双域一致性的像素级加权&lt;/strong&gt;：将目标域图像$X_{t_j}$通过$T_s$转换为伪源域图像$X_{t\rightarrow s_j}$，并使用$DDG$得到另一个伪标签$\hat{Y}&lt;em&gt;{t\rightarrow s_j}$。通过比较$\hat{Y}&lt;/em&gt;{t\rightarrow s_j}$和$\hat{Y}&lt;em&gt;{t_j}$，将一致和不一致的区域分别视为可靠和不可靠的预测，定义像素级权重图$M_j$。最后将图像级权重$w_j$和像素级权重图$M_j$组合成单一权重图$A_j = M_j \cdot w_j$，并使用加权Dice损失$L&lt;/em&gt;{w - dice}$进行训练。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. 最终分割器学习（Final Segmentor Learning）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：结合源域标记图像和带有伪标签的目标域图像进行联合训练，学习域不变特征，提高在目标域的分割性能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;具体操作&lt;/strong&gt;：最终分割器$S$采用与伪标签生成器$G$相同的架构，基于双域批量归一化层设计。其训练损失结合了源域的Dice损失和目标域的加权Dice损失。为了加速训练，最终分割器$S$使用伪标签生成器$G$的权重进行初始化。在测试阶段，直接使用训练好的最终分割器$S$进行推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-50-38.png&quot; alt=&quot;Snipaste_2025-07-18_16-50-38&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;前庭神经鞘瘤分割数据集（Vestibular Schwannoma Segmentation Dataset）&lt;/strong&gt;：使用公开的包含242名患者的3D MRI图像数据集，有对比增强T1加权（ceT1）和高分辨率T2加权（hrT2）两种模态。将其随机分为200个训练样本、14个验证样本和28个测试样本，进行双向适应实验。实验前对图像进行裁剪和归一化预处理。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;脑肿瘤分割数据集（BraTS Dataset）&lt;/strong&gt;：采用2020年多模态脑肿瘤分割挑战赛数据集，使用T2和FLAIR图像进行双向适应，目标是分割整个肿瘤。从官方训练集中选取样本，部分用于验证和测试。对图像进行强度归一化和去除无肿瘤切片的预处理。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;心脏分割数据集（MMWHS Dataset）&lt;/strong&gt;：该数据集包含20个3D CT扫描和20个3D MRI扫描，分割目标包括升主动脉（AA）、左心房血腔（LAC）、左心室血腔（LVC）和左心室心肌（MYO）。指定MRI为源域，CT为目标域，划分训练、验证和测试集，并对图像进行裁剪和归一化。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;实现细节&lt;/strong&gt;：伪标签生成器G和最终分割器S基于改进的2.5D网络实现，添加额外的BN层用于双域批量归一化。使用Adam优化器训练，G训练200个周期，S初始化后训练100个周期。设置不同的补丁大小和批量大小，按照CycleGAN实现训练图像翻译器Ts和Tt。蒙特卡罗dropout的超参数K设为5，熵阈值e设为0.2。使用PyTorch 1.8.1在NVIDIA GeForce RTX 2080Ti GPU上实现实验，通过Dice分数和平均对称表面距离（ASSD）定量评估分割性能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;将FPL+与十种最先进的无监督域适应（UDA）方法进行比较，同时还与“w/o DA”（直接应用源域训练的模型到目标域）、“labeled target”（使用目标域全注释图像训练）和“strong upbound”（使用源域和目标域的标注图像训练双域分割网络）进行对比。&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;前庭神经鞘瘤分割结果&lt;/strong&gt;：“w/o DA”方法在两个方向上的平均Dice分数极低，表明两种模态间存在显著的域差距。所有UDA方法均有改进，FPL+在两个方向上的平均Dice分数分别达到82.92%和91.98%，显著高于其他方法，甚至在某些情况下优于“labeled target”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;脑胶质瘤分割结果&lt;/strong&gt;：在“FLAIR到T2”和“T2到FLAIR”两个方向上，FPL+的平均Dice分数和ASSD均优于其他现有UDA方法，在“T2到FLAIR”方向上超过“labeled target”，略低于“strong upper bound”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;心脏分割结果&lt;/strong&gt;：“w/o DA”的Dice分数远低于“labeled target”，表明MR和CT模态间存在明显的域差距。FPL+的平均Dice分数和ASSD分别为73.70%和2.61mm，显著优于现有UDA方法，能更准确地分割心脏子结构。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-53-40.png&quot; alt=&quot;Snipaste_2025-07-18_16-53-40&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-53-57.png&quot; alt=&quot;Snipaste_2025-07-18_16-53-57&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;实验（Ablation Experiments）​​&lt;/h2&gt;
&lt;p&gt;在VS数据集上对双域伪标签生成器（DDG）和最终分割器S进行全面的消融实验，验证FPL+各组件的有效性，并将伪标签过滤方法与几种现有的抗噪声学习方法进行比较。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;CDDA和Dual - BN对DDG的有效性&lt;/strong&gt;：以仅使用Ds→t训练伪标签生成器为基线，实验表明结合Ds→t和Ds训练且不使用Dual - BN时性能有所提升，引入Dual - BN进一步提高性能，使用CDDA生成的双域增强图像结合Dual - BN能获得最高的平均Dice分数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最终分割器训练的消融实验&lt;/strong&gt;：以标准监督学习使用DDG生成的目标域伪标签为基线，逐步引入添加源域标注图像、使用Dual - BN、从G初始化S、图像级加权和像素级加权等组件。结果显示每个组件都能有效提高分割性能，结合图像级和像素级加权训练最终分割器能获得最佳效果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与其他伪标签学习方法的比较&lt;/strong&gt;：将FPL+训练S的策略与Co - teaching、GCE Loss和TriNet三种最先进的从噪声标签学习的方法进行比较，结果表明FPL+的性能优于这些方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;超参数的有效性&lt;/strong&gt;：研究了图像级加权的熵图阈值e和辅助翻译器训练周期数对性能的影响。结果显示e设置为0.2时性能最佳，辅助翻译器训练到200个周期时，伪标签生成器的性能达到峰值。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-54-32.png&quot; alt=&quot;Snipaste_2025-07-18_16-54-32&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-54-52.png&quot; alt=&quot;Snipaste_2025-07-18_16-54-52&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;作者提出了基于&lt;strong&gt;增强过滤伪标签（FPL+）的无监督跨模态适应方法&lt;/strong&gt;用于3D医学图像分割，并得出以下结论：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;方法有效性&lt;/strong&gt;：通过跨域数据增强和图像、像素级加权等策略，有效对齐源域和目标域图像，筛选可靠伪标签，提升了模型跨域转移能力。在三个公共多模态数据集上的实验表明，该方法超越了十种最先进的无监督域适应（UDA）方法，在某些情况下甚至优于目标域的全监督学习。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法局限性&lt;/strong&gt;：方法涉及训练伪标签生成器和最终分割器两个步骤，增加了一定复杂性；因GPU内存和图像翻译伪影问题，难以实现3D图像的端到端生成和分割；要求源域和目标域的分割目标可见且拓扑相似。未来，该方法有望应用于其他分割任务。&lt;/li&gt;
&lt;/ol&gt;
</content:encoded></item><item><title>Automatic Brain Segmentation for PET/MR Dual-Modal Images Through a Cross-Fusion Mechanism</title><link>https://louaq.github.io/posts/automatic-brain-segmentation-for-petmr-dual-modal-images-through-a-cross-fusion-mechanism/</link><guid isPermaLink="true">https://louaq.github.io/posts/automatic-brain-segmentation-for-petmr-dual-modal-images-through-a-cross-fusion-mechanism/</guid><description>不同脑区和组织的精确分割通常是神经科学中检测和诊断各种神经系统疾病的前提。</description><pubDate>Thu, 10 Jul 2025 17:23:00 GMT</pubDate><content:encoded>&lt;p&gt;作者单位&lt;/p&gt;
&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;The precise segmentation of different &lt;strong&gt;brain regions and tissues&lt;/strong&gt; is usually a prerequisite for the detection and diagnosis of various neurological disorders in neuroscience. Considering the abundance of functional and structural dual-modality information for &lt;strong&gt;positron emission tomography/magnetic resonance&lt;/strong&gt; (PET/MR) images, we propose a novel 3D whole-brain segmentation network with a cross-fusion mechanism introduced to obtain 45 brain regions. Specifically, the network processes PET and MR images simultaneously, employing UX-Net and a cross-fusion block for feature extraction and fusion in the encoder. We test our method by comparing it with other deep learning-based methods, including 3DUXNET, Swin-UNETR, UNETR, nnFormer, UNet3D, NestedUNet, ResUNet, and VNet. The experimental results demonstrate that the proposed method achieves better segmentation performance in terms of both visual and quantitative evaluation metrics and achieves more precise segmentation in three views while preserving fine details. In particular, the proposed method achieves superior quantitative results, with a Dice coefficient of 85.73% ± 0.01%, a Jaccard index of 76.68% ± 0.02%, a sensitivity of 85.00% ± 0.01%, a precision of 83.26% ± 0.03% and a Hausdorff distance (HD) of 4.4885 ± 14.85%. Moreover, the distribution and correlation of the SUV in the volume of interest (VOI) are also evaluated (PCC &amp;gt; 0.9), indicating consistency with the ground truth and the superiority of the proposed method. In future work, we will utilize our whole-brain segmentation method in clinical practice to assist doctors in accurately diagnosing and treating brain diseases.&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;不同脑区和组织的精确分割通常是神经科学中检测和诊断各种神经系统疾病的前提。考虑到正电子发射断层扫描/磁共振（PET/MR）图像的功能和结构双模态信息的丰富性，我们提出了一种新颖的三维全脑分割网络，通过引入交叉融合机制来获得45个脑区。具体而言，该网络同时处理PET和MR图像，在编码器中采用UX-Net和交叉融合块进行特征提取和融合。我们通过与其他基于深度学习的方法进行比较来测试我们的方法，包括3DUXNET、Swin-UNETR、UNETR、nnFormer、UNet3D、NestedUNet、ResUNet和VNet。实验结果表明，所提出的方法在视觉和定量评估指标方面实现了更好的分割性能，并在三个视图中实现了更精确的分割，同时保留了细节。特别是，所提出的方法在量化结果方面表现优异，Dice系数为85.73% ± 0.01%，Jaccard指数为76.68% ± 0.02%，敏感性为85.00% ± 0.01%，精确度为83.26% ± 0.03%，Hausdorff距离（HD）为4.4885 ± 14.85%。此外，还评估了感兴趣区域（VOI）中SUV的分布和相关性（PCC &amp;gt; 0.9），表明与真实值的一致性以及所提出方法的优越性。在未来的工作中，我们将利用我们的全脑分割方法在临床实践中帮助医生准确诊断和治疗脑疾病。&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;本文聚焦于脑区分割研究，旨在解决现有方法的局限，其研究背景如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;脑区分割的重要性&lt;/strong&gt;：精准的脑区分割对神经科学研究和临床诊断意义重大，不同脑区的体积、表面积和形态与帕金森病、阿尔茨海默病等多种神经系统疾病相关。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PET/MR成像系统的优势&lt;/strong&gt;：正电子发射断层扫描/磁共振（PET/MR）集成成像系统结合了PET代谢成像和MR高分辨率成像的优点，是诊断脑部疾病的有效工具。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;现有脑区分割方法的不足&lt;/strong&gt;：手动分割脑医学图像耗时费力，且结果易受个体差异和主观因素影响；传统自动分割方法依赖手动特征工程，对图像质量和噪声敏感，对解剖变异的鲁棒性差；现有的深度学习方法多基于单模态医学图像分割，部分融合双模态的方法分割的脑区较少，融合方式简单，缺乏深度和全面的整合。 因此，本文提出一种基于交叉融合机制的自动脑分割方法，充分利用PET和MR医学图像，结合两者的功能和结构信息，以实现更精确、全面的脑区自动分割。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：可分为&lt;strong&gt;阈值和统计方法&lt;/strong&gt;、&lt;strong&gt;图像处理和数学模型方法&lt;/strong&gt;、&lt;strong&gt;基于图谱的方法&lt;/strong&gt;，但依赖手动特征工程，对图像质量和噪声敏感，对解剖变异鲁棒性差。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习方法&lt;/strong&gt;：基于高分辨率MR图像或代谢成像与低分辨率PET的单模态分割方法较多，部分方法开始融合PET和MR双模态信息，但存在分割脑区少、融合方法简单的问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-51-30.png&quot; alt=&quot;Snipaste_2025-07-10_20-51-30&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-51-36.png&quot; alt=&quot;Snipaste_2025-07-10_20-51-36&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;网络整体结构&lt;/h3&gt;
&lt;p&gt;模型采用经典的U形分割结构，由编码器（encoder）和解码器（decoder）组成。PET和MR图像同时作为输入，通过编码器中的特征提取模块（UX - Net block）和融合模块（cross - fusion block）进行特征提取与融合，最后由解码器完成脑分割任务。&lt;/p&gt;
&lt;h3&gt;编码器部分&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;UX-Net Block：作为特征提取器，是网络的骨干部分。该模块采用分层变压器（hierarchical transformer）的思想，使用大规模深度卷积（large-scale depthwise convolution）在不增加计算成本的情况下获取局部信息，适合分层变压器结构并保留了卷积神经网络（CNN）的归纳偏置等优点。
&lt;ul&gt;
&lt;li&gt;首先通过大卷积核的投影层提取图像块特征。&lt;/li&gt;
&lt;li&gt;分四个阶段提取不同层次的特征，每个阶段包含两个7×7×7的大规模深度卷积和两个1×1×1的逐点卷积，以丰富特征表示并减少通道冗余。&lt;/li&gt;
&lt;li&gt;每个阶段使用2×2×2的标准卷积模块将特征分辨率降低1/2，并应用层归一化（layer normalization）和高斯误差线性单元（GELU）作为激活函数。&lt;/li&gt;
&lt;li&gt;最后通过包含两个批量归一化的3×3×3卷积层的残差块稳定提取的特征。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cross - Fusion Block：主要用于融合经过残差块处理后的PET和MR图像在四个阶段提取的特征，由两个交叉注意力块（cross - attention blocks）组成。
&lt;ul&gt;
&lt;li&gt;利用多通道交叉注意力机制（multichannel cross - attention mechanism）在两个不同来源的特征之间建立复杂关联，实现全面的特征交叉融合。&lt;/li&gt;
&lt;li&gt;具体过程是先计算查询（query）、键（key）和值（value），得到两个特征的注意力分布，实现全局关联，再通过卷积层进一步处理输出特征。&lt;/li&gt;
&lt;li&gt;通过两次交叉注意力计算（CA1和CA2）并拼接结果，最后通过残差连接得到最终的交叉融合结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;解码器部分&lt;/h3&gt;
&lt;p&gt;编码器各阶段生成的多尺度输出通过跳跃连接（skip connections）连接到解码器，形成U形网络用于下游分割任务。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;解码器将各阶段特征上采样并与前一阶段的特征拼接，经过残差块处理。&lt;/li&gt;
&lt;li&gt;网络的原始输入和编码器的输出按从上到下顺序设置为[e0, e1, e2, e3, e4]，解码器的输出按从上到下顺序设置为[d1, d2, d3, d4]。&lt;/li&gt;
&lt;li&gt;以第3个解码器层为例，先对第4个解码器层的输出进行上采样，然后与编码器第2层的输出拼接，经过残差块得到该层输出。&lt;/li&gt;
&lt;li&gt;最后将解码器的输出输入到包含1×1×1卷积层和softmax激活函数的残差块中，预测分割概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;损失函数&lt;/h3&gt;
&lt;p&gt;结合了Dice系数损失（Dice coefﬁcient loss）和交叉熵损失（cross - entropy loss）构建混合损失函数，以充分利用两种损失函数的优点，增强模型在图像分割任务中的性能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dice系数损失用于衡量两个样本之间的相似度，常用于图像分割任务。&lt;/li&gt;
&lt;li&gt;交叉熵损失是图像分割中广泛使用的损失函数。&lt;/li&gt;
&lt;li&gt;总损失L是Dice系数损失Ldice和交叉熵损失Lce的加权和，权重为$ω$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;数据集&lt;/strong&gt;：使用了110名受试者的18F - FDG PET/MR脑图像，所有数据在3.0T的PET/MRI集成扫描仪上采集。利用FreeSurfer工具获取45个脑区作为真实标签（GT），并对PET和MR图像进行配准和独热编码处理。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;训练实现
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据预处理&lt;/strong&gt;：对输入图像的像素值进行最小 - 最大归一化，并裁剪前景以消除背景。同时，引入正负样本的随机裁剪，获取96×96×96的图像块。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据增强&lt;/strong&gt;：采用随机缩放、随机翻转和随机强度调整等技术增加数据多样性，缓解过拟合问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练设置&lt;/strong&gt;：在PyTorch框架、Windows 10系统和NVIDIA GeForce RTX 3090 24GB GPU上进行训练。训练250个轮次，批次大小设为1，学习率为1e - 4，使用AdamW动态调整训练损失，每次训练迭代输入2个96×96×96的图像块，损失权重ω设为1。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对比方法&lt;/strong&gt;：将提出的方法与四种基于Transformer的模型（3DUXNET、SwinUNETR、UNETR、nnFormer）和四种基于CNN的模型（UNet3D、NestedUNet、ResUNet、VNet）进行对比。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估指标&lt;/strong&gt;：采用五种评估指标，包括Dice相似度、Jaccard系数、精确率、灵敏度和Hausdorff距离（HD），全面评估分割性能&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-55-55.png&quot; alt=&quot;Snipaste_2025-07-10_20-55-55&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-56-01.png&quot; alt=&quot;Snipaste_2025-07-10_20-56-01&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-56-24.png&quot; alt=&quot;Snipaste_2025-07-10_20-56-24&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;整体定量评估&lt;/strong&gt;：可视化全脑分割结果，提出的方法在三个视图上的分割结果更接近GT，能完整分割45个脑区并保留细节。定量结果显示，该方法在各项指标上表现优越，相比最佳对比方法有显著提升。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特定脑区评估&lt;/strong&gt;：分析特定脑区（如壳核、海马体、尾状核和右 inferior - 侧脑室）的分割结果，提出的方法能更好地保留边缘细节，与GT的一致性更高。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;一致性和相关性分析&lt;/strong&gt;：分析PET图像分割结果在不同感兴趣体积（VOI）区域的标准摄取值（SUV）分布和相关性，提出的方法的SUV分布与GT最一致，相关系数PCC较高。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;临床定量质量评估&lt;/strong&gt;：计算肿瘤与背景比和临床容忍率，提出的方法临床容忍率最低，分割性能显著优于其他方法。&lt;/li&gt;
&lt;li&gt;额外数据评估和验证
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;额外数据测试&lt;/strong&gt;：引入40例PET和MR数据进行测试，提出的方法在额外数据集上仍保持优越性能，证明了模型的鲁棒性和通用性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;噪声数据测试&lt;/strong&gt;：向测试集的PET图像中添加不同水平的噪声，提出的方法在噪声环境下仍能准确分割脑区，表现出较强的鲁棒性，而NestedUNet受噪声影响最大。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;实验（Ablation Experiments）​​&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;消融研究和融合模块分析
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;消融分析&lt;/strong&gt;：验证了UX - Net块和交叉融合块在模型分割效果中的关键作用，替换这两个模块会导致定量指标下降。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;交叉融合块有效性分析&lt;/strong&gt;：交叉融合机制在Dice分数、Jaccard系数和精确率方面取得最佳定量结果，分割性能出色。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-57-20.png&quot; alt=&quot;Snipaste_2025-07-10_20-57-20&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;作者提出一种基于&lt;strong&gt;交叉融合机制&lt;/strong&gt;的自动脑分割方法，将&lt;strong&gt;PET和MR多模态信息&lt;/strong&gt;融合用于精确脑分割。实验在PET/MR数据集上开展，并与8种深度学习方法对比，从视觉、定量和临床三方面评估分割结果，证明该方法的优越性。具体而言，此方法取得了优异的定量结果，Dice得分85.73% ± 0.01%，Jaccard指数76.68% ± 0.02%等，SUV相关性评估也显示其优于其他方法，临床误差容忍率不超5%。该方法实现了精确的全脑分割，对脑部疾病的临床诊断和分析有益。未来，作者将探索该方法在其他模态图像处理任务中的应用。&lt;/p&gt;
</content:encoded></item><item><title>Flexible Fusion Network for Multi-Modal Brain Tumor Segmentation</title><link>https://louaq.github.io/posts/flexible-fusion-network-for-multi-modal-brain-tumor-segmentation/</link><guid isPermaLink="true">https://louaq.github.io/posts/flexible-fusion-network-for-multi-modal-brain-tumor-segmentation/</guid><description>自动化脑肿瘤分割对于辅助脑疾病诊断和评估疾病进展至关重要。</description><pubDate>Tue, 08 Jul 2025 17:38:00 GMT</pubDate><content:encoded>&lt;blockquote&gt;
&lt;p&gt;南京理工大学、东南大学&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;Automated brain tumor segmentation is crucial for aiding brain disease diagnosis and evaluating disease progress. Currently, &lt;strong&gt;magnetic resonance imaging&lt;/strong&gt; (MRI) is a routinely adopted approach in the field of brain tumor segmentation that can provide different modality images. It is critical to leverage multi-modal images to boost brain tumor segmentation performance. Existing works commonly concentrate on generating a shared representation by fusing multi-modal data, while few methods take into account modality-specific characteristics. Besides, how to efficiently fuse arbitrary numbers of modalities is still a difficult task. In this study, we present a flexible fusion network (termed F2Net) for multi-modal brain tumor segmentation, which can flexibly fuse arbitrary numbers of multi-modal information to explore complementary information while maintaining the specific characteristics of each modality. Our F2Net is based on the encoder-decoder structure, which utilizes two Transformer-based feature learning streams and a cross-modal shared learning network to extract individual and shared feature representations. To effectively integrate the knowledge from the multi-modality data, we propose a cross-modal feature enhanced module (CFM) and a multi-modal collaboration module (MCM), which aims at fusing the multi-modal features into the shared learning network and incorporating the features from encoders into the shared decoder, respectively. Extensive experimental results on multiple benchmark datasets demonstrate the effectiveness of our F2Net over other state-of-the-art segmentation methods.&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;自动化脑肿瘤分割对于辅助脑疾病诊断和评估疾病进展至关重要。目前，磁共振成像（MRI）是脑肿瘤分割领域常用的方法，可以提供不同的模态图像。利用多模态图像提升脑肿瘤分割性能是关键。现有研究通常关注通过融合多模态数据生成共享表示，而很少有方法考虑模态特异性特征。此外，如何有效地融合任意数量的模态仍是一个困难的任务。在本研究中，我们提出了一种灵活的融合网络（称为F2Net）用于多模态脑肿瘤分割，该网络可以灵活地融合任意数量的多模态信息，以探索互补信息，同时保持每种模态的特异性特征。我们的F2Net基于编码器-解码器结构，利用两个基于Transformer的特征学习流和一个跨模态共享学习网络来提取个体和共享特征表示。为了有效整合多模态数据的知识，我们提出了一个跨模态特征增强模块（CFM）和一个多模态协作模块（MCM），分别旨在将多模态特征融合到共享学习网络中，并将编码器中的特征整合到共享解码器中。多个基准数据集上的广泛实验结果证明了我们的F2Net在其他最新分割方法上的有效性。&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;本文聚焦多模态脑肿瘤分割，旨在提升分割性能，其研究背景如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;临床需求&lt;/strong&gt;：脑肿瘤是全球致命疾病之一，及时检测对临床评估和治疗策略至关重要。磁共振成像（MRI）是分析脑肿瘤的常用工具，包含&lt;strong&gt;T1、T2、T1CE和FLAIR四种成像模态&lt;/strong&gt;，各模态能提供大脑结构的独特信息和肿瘤不同子区域的互补信息。但自动脑肿瘤分割面临肿瘤大小、形状和位置多样性大等挑战。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;现有方法局限&lt;/strong&gt;：卷积神经网络（CNNs）在医学分割任务中取得显著成功，如U - Net及其变体，但现有多模态脑肿瘤分割方法存在不足。早期融合策略不能有效保留各模态特征和探索模态间联系；晚期融合模型虽提取独立特征，但未充分利用模态特征提升分割性能；部分方法学习融合信息时忽略了模态编码器特征的重要性。因此，如何有效融合多模态数据，同时挖掘共享信息和捕捉模态特定特征以获得良好的分割结果仍是挑战。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_09-56-38.png&quot; alt=&quot;Snipaste_2025-07-09_09-56-38&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;医学图像分割&lt;/strong&gt;：CNN 算法广泛应用，U - Net 及其变体表现出色，且已拓展到 3D 分割；Transformer 用于计算机视觉任务，部分结合 Transformer 的模型在医学图像分割中取得进展。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;脑肿瘤分割&lt;/strong&gt;：从生成概率模型转向神经网络模型，包括 2D 和 3D CNN 模型，部分模型结合 Transformer 提升性能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模态学习&lt;/strong&gt;：多模态数据融合受关注，有多种融合算法，如独立深度学习流、模态感知模块、共享特征提取等方法。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_09-58-59.png&quot; alt=&quot;Snipaste_2025-07-09_09-58-59&quot; /&gt;&lt;/p&gt;
&lt;p&gt;F2Net主要包含两部分：两个&lt;strong&gt;基于Transformer的特定学习网络&lt;/strong&gt;和一个&lt;strong&gt;跨模态共享学习网络&lt;/strong&gt;。多模态数据首先输入到两个基于Transformer的特定学习网络中学习多级特征，然后通过提出的跨模态特征增强模块（CFM）对多级特征表示进行融合，融合后的特征以逐层策略通过共享学习网络。同时，使用跳跃连接将编码器路径的原始多级特征表示合并到相应的解码器中。此外，还提出了多模态协作模块（MCM），有选择地将特定编码器和共享编码器的特征集成到共享解码器中，以提供更丰富的信息，提高特征表示能力。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于Transformer的特定学习网络
&lt;ul&gt;
&lt;li&gt;以基于Transformer的结构作为骨干网络，如PVTv2，它能捕捉长距离依赖关系。输入图像被划分为不重叠的块，将块特征投影到C维空间，并添加位置嵌入后输入到具有空间缩减注意力（SA）层的基于Transformer的编码器中。&lt;/li&gt;
&lt;li&gt;采用渐进收缩策略生成多尺度特征，得到两组多尺度的四个特征F1和F2。&lt;/li&gt;
&lt;li&gt;解码器路径使用跳跃连接将编码器的特征合并到解码器中，通过简单的级联块（“Cas. Block”）组合分层特征。每个解码器都有监督信号，有助于最终分割结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;跨模态共享学习网络
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;跨模态特征增强模块（CFM）&lt;/strong&gt;：为有效利用不同模态的独特特征而提出。首先将两个模态特定编码器的特征进行拼接，经过3×3卷积层和Sigmoid激活函数生成归一化图wi，作为特征级注意力权重。然后将两个特征与wi相乘，得到增强的特征表示。接着对增强的特征图进行求和、平滑处理，并与原始特征拼接，最后结合上一层的上下文信息。CFM能够有效捕捉多模态之间的互补信息，减少背景噪声。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模态协作模块（MCM）&lt;/strong&gt;：在跨模态共享学习网络的解码器中使用。将模态特定编码器的特征与共享编码器的特征相乘，再与共享编码器的特征拼接，经过一系列操作得到增强的特征表示。还将模态特定编码器的特征协作相乘，与前面的增强特征拼接，最终得到协作特征。MCM能够利用多模态数据的互补信息，提高脑肿瘤分割结果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解码器路径&lt;/strong&gt;：利用融合后的特征fMCM1、fMCM2、fMCM3进行解码，通过跳跃连接将编码器的分层特征合并到共享解码器中，最终输出作为最终分割结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失函数&lt;/strong&gt;：采用混合损失评估预测结果与真实标签之间的差异，混合损失包括加权Dice损失和交叉熵（CE）损失。F2Net的总损失函数由共享学习网络的损失、两个模态特定解码器的损失组成。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;模型优势&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;灵活性&lt;/strong&gt;：能够灵活融合任意数量的多模态信息，探索互补信息的同时保留每个模态的特定特征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;有效性&lt;/strong&gt;：在多个基准数据集上的实验结果表明，F2Net优于其他最先进的分割方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;创新性&lt;/strong&gt;：提出的CFM和MCM模块有效提高了特征表示能力和分割性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;数据集&lt;/strong&gt;：在三个数据集上评估F2Net，分别是&lt;strong&gt;BraTS 2019、BraTS 2020和ISLES 2015 SISS&lt;/strong&gt;。其中，BraTS 2019有335个多模态磁共振扫描，BraTS 2020有369个多模态磁共振扫描，每个受试者都有T1、T1CE、T2和FLAIR四种模态及对应的分割掩码；ISLES 2015 SISS有28个训练病例，包含T1、T1CE、T2和扩散加权成像（DWI）四种模态。实验采用体积轴向平面的2D切片，对输入图像进行归一化和边界裁剪，并将每个2D切片裁剪为224×224大小。同时，将每个数据集中的所有受试者随机分为80%的训练数据和20%的测试数据。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;实现细节&lt;/strong&gt;：使用在ImageNet - 1K上预训练的PVTv2作为F2Net的骨干网络，在一块NVIDIA RTX 2080Ti GPU上训练。采用SGD优化器，初始学习率为0.01，并按照“Poly”策略降低学习率，批大小设置为12，训练100个epoch。从BraTS 2019和BraTS 2020数据集中采用四种模态训练网络，从ISLES 2015数据集中选择三种模态训练分割方法。代码使用&lt;strong&gt;PyTorch和MindSpore&lt;/strong&gt;实现，对于比较方法，除TranSiam外，均使用发布的源代码进行训练，TranSiam则按照原论文设置进行复现。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;评估指标&lt;/strong&gt;：使用四个指标评估所有分割方法的有效性，分别是骰子分数（Dice）、交并比（IoU）、95%豪斯多夫距离（HD95）和敏感度。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;定量结果&lt;/strong&gt;：选择七种最先进的方法进行比较实验，包括单模态和多模态分割方法。结果表明，F2Net在这些数据集上的分割性能均高于其他比较方法，其关键优势在于能有效利用多模态数据间的相关性和互补信息，提升多模态脑肿瘤分割性能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定性比较&lt;/strong&gt;：可视化结果显示，F2Net取得了良好进展，与其他方法相比，漏分割组织更少。单模态方法往往无法分割不同类型的脑肿瘤，因为它们未考虑各模态的特征和模态间的关系；与其他多模态分割方法相比，由于提出的CFM和MCM，F2Net能取得更准确的结果和更清晰的边缘。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-03-44.png&quot; alt=&quot;Snipaste_2025-07-09_10-03-44&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-03-50.png&quot; alt=&quot;Snipaste_2025-07-09_10-03-50&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-04-31.png&quot; alt=&quot;Snipaste_2025-07-09_10-04-31&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;实验（Ablation Experiments）​​&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;CFM的有效性&lt;/strong&gt;：通过两个基线实验研究CFM的重要性，结果表明CFM能提高多模态融合的表征能力，提升分割性能，引入MCM时也能观察到性能提升。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MCM的有效性&lt;/strong&gt;：基于一个基线实验研究MCM的益处，结果显示引入MCM组件能有效利用各模态特定编码器提取的特征，准确定位和分割真实肿瘤区域，在四个评估指标上均有更好表现。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;灵活融合策略的有效性&lt;/strong&gt;：在BraTS 2020数据集上设计多个消融实验，结果表明使用共享学习网络能获得更好性能，引入CFM和MCM时融合策略能挖掘丰富模式并产生高性能；使用四个完整模态时，分割精度相对更好，说明该融合策略能有效融合多模态数据，探索共享信息和模态特定特征。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-05-33.png&quot; alt=&quot;Snipaste_2025-07-09_10-05-33&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-05-42.png&quot; alt=&quot;Snipaste_2025-07-09_10-05-42&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了用于&lt;strong&gt;多模态脑肿瘤分割&lt;/strong&gt;的灵活融合网络F2Net，并得出以下结论：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;性能优越&lt;/strong&gt;：F2Net能灵活融合任意数量的多模态信息，在三个数据集上的实验表明，相比其他先进的分割方法，它能有效利用多模态数据的相关性和互补信息，取得更高的分割性能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模块有效&lt;/strong&gt;：交叉模态特征增强模块（CFM）和多模态协作模块（MCM）可有效捕捉多模态之间的互补信息，抑制背景噪声，提升特征表示能力，从而提高分割性能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略可行&lt;/strong&gt;：灵活融合策略能高效融合多模态数据，挖掘丰富模式，明确探索共享信息和模态特定特征，使用更多模态时分割结果更好。不过，该模型在边界模糊和大病变情况下效果欠佳，后续需研究更轻量级骨干网络和自适应多模态融合方法。&lt;/li&gt;
&lt;/ol&gt;
</content:encoded></item><item><title>BTSegDiff Brain tumor segmentation based on multimodal MRI Dynamically guided diffusion probability model</title><link>https://louaq.github.io/posts/btsegdiff-brain-tumor-segmentation-based-on-multimodal-mri-dynamically-guided-diffusion-probability-model/</link><guid isPermaLink="true">https://louaq.github.io/posts/btsegdiff-brain-tumor-segmentation-based-on-multimodal-mri-dynamically-guided-diffusion-probability-model/</guid><pubDate>Sun, 06 Jul 2025 20:30:00 GMT</pubDate><content:encoded>&lt;blockquote&gt;
&lt;p&gt;针对论文，有以下疑问：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Introduction第4段中第6句和第7句，有表述重复的情况，仅有method和approach两个单词不同&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-50-26.png&quot; alt=&quot;Snipaste_2025-07-06_20-50-26&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在 &lt;strong&gt;2.1. MRI brain tumor segmentation&lt;/strong&gt; 第2段中，出现MR images的表述，但是全文中只出现过MRI images（核磁共振图像）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-54-08.png&quot; alt=&quot;Snipaste_2025-07-06_20-54-08&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在 &lt;strong&gt;4.4. Uncertainty measure based sampling&lt;/strong&gt;  第6段中，表述：&lt;strong&gt;sUncertainty measure&lt;/strong&gt; 不理解&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-57-19.png&quot; alt=&quot;Snipaste_2025-07-06_20-57-19&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;In the treatment of brain tumors, accurate diagnosis and treatment heavily rely on reliable &lt;strong&gt;brain tumor segmentation&lt;/strong&gt;, where multimodal &lt;strong&gt;Magnetic Resonance Imaging&lt;/strong&gt; (MRI) plays a pivotal role by providing valuable
complementary information. This integration significantly enhances the performance of brain tumor segmen­
tation. However, due to the uneven grayscale distribution, irregular shapes, and significant size variations in
brain tumor images, this task remains highly challenging. In order to overcome these obstacles, we have introduced a novel framework for automated segmentation of brain tumors that leverages the diverse infor­
mation from multi-modal MRI scans. Our proposed method is named BTSegDiff and it is based on a Diffusion
Probability Model (DPM). First, we designed a dynamic conditional guidance module consisting of an encoder.
This encoder is used to extract information from multimodal MRI images and guide the DPM in generating accurate and realistic segmentation masks. During the guidance process, we need to fuse the diffused generated features with the extracted multimodal features. However, diffusion process itself introduces a significant amount of Gaussian noise, which can affect the fusion results. Therefore, we designed a Fourier domain feature fusion module to transfer this fusion process to Euclidean space and reduce the impact of high-frequency noise on fusion. Lastly, we have taken into account that the DPM, as a generative model, produces non-unique results with each sampling. In the meticulous field of medicine, this is highly detrimental. Therefore, we have designed a Stepwise Uncertainty Sampling module based on Monte Carlo uncertainty calculation to generate unique out­ comes and enhance segmentation accuracy simultaneously. To validate the effectiveness of our approach, we perform a validation on the popular BraTs2020 and BraTS2021 benchmarks. The experimental results show that our method outperforms many existing brain tumor segmentation methods. Our code is available at https://github.com/jaceqin/BTSegDiff.&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;在脑肿瘤的治疗中，准确的诊断和治疗严重依赖于可靠的脑肿瘤分割，而多模态磁共振成像（MRI）通过提供有价值的补充信息发挥关键作用。这种整合显著提高了脑肿瘤分割的性能。然而，由于脑肿瘤图像中灰度分布不均、形状不规则以及大小差异显著，这项任务仍然极具挑战性。为了克服这些障碍，我们引入了一种利用多模态MRI扫描的多样信息进行脑肿瘤自动分割的新框架。我们提出的方法名为BTSegDiff，基于扩散概率模型（DPM）。首先，我们设计了一个动态条件指导模块，该模块由编码器组成。编码器用于从多模态MRI图像中提取信息，并指导DPM生成准确和逼真的分割掩码。在指导过程中，我们需要将扩散生成的特征与提取的多模态特征进行融合。然而，扩散过程本身引入了大量的高斯噪声，这可能影响融合结果。因此，我们设计了一个傅里叶域特征融合模块，将该融合过程转移到欧几里得空间，并减少高频噪声对融合的影响。最后，我们考虑到DPM作为生成模型，每次采样产生的结果都不唯一。在医学这个细致的领域中，这非常不利。因此，我们设计了一个基于蒙特卡洛不确定性计算的逐步不确定性采样模块，以生成独特结果并同时提高分割精度。为了验证我们方法的有效性，我们在流行的BraTs2020和BraTS2021基准上进行了验证。实验结果表明，我们的方法优于许多现有的脑肿瘤分割方法。我们的代码可在https://github.com/jaceqin/BTSegDiff。&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;本文聚焦于&lt;strong&gt;脑肿瘤分割研究&lt;/strong&gt;，其背景源于脑肿瘤发病率上升，准确分割对诊断治疗至关重要。传统手动分割耗人力且依赖医生经验，计算机辅助自动分割方法具有重要临床价值，但面临诸多挑战。 早期基于区域生成、边缘检测和阈值分割的自动分割方法虽有进步，但需手动调参，难以处理多模态图像。深度学习发展使基于其的医学图像分割方法展现出强大特征学习能力，U - Net 提升了脑肿瘤分割性能，Transform 能更好学习全局信息。然而，这些方法仍存在不足，如 CNN 难以学习全局特征，Transform 需为不同数据集设计合适架构。 扩散概率模型（DPM）在生成任务和医学成像领域有一定成效，但现有基于 DPM 的脑肿瘤分割方法忽视了多模态信息提取和模型不确定性定量分析。为解决这些问题，作者提出基于 DPM 的脑肿瘤分割框架 BTSegDiff，旨在利用多模态 MRI 信息引导 DPM 生成高质量分割结果，减少噪声影响，解决 DPM 结果不唯一问题，提升分割准确性和稳定性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-39-04.png&quot; alt=&quot;Snipaste_2025-07-06_20-39-04&quot; /&gt;&lt;/p&gt;
&lt;p&gt;​                                                    Different modalities in the multimodal MRI dataset&lt;/p&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：早期基于区域生成、边缘检测和阈值分割的自动化方法，相比手动方法提高了脑肿瘤分割效率，具有广泛适用性和低成本的优点，但仍需手动调整参数，且难以处理多模态图像。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习方法&lt;/strong&gt;：基于深度学习的医学图像分割方法特征学习能力强。U - Net改进了脑肿瘤分割性能，但学习脑肿瘤图像全局特征的能力不足；Transform能捕捉长距离依赖关系，在脑肿瘤分割领域表现出色，但需为每个数据集专门设计合适的模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;扩散概率模型&lt;/strong&gt;：扩散概率模型（DPM）在生成任务和医学成像中展现出有效性，现有基于DPM的方法在脑肿瘤分割领域超越了Transform，但忽略了多模态信息提取和模型不确定性的定量分析。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-41-38.png&quot; alt=&quot;Snipaste_2025-07-06_20-41-38&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;整体架构&lt;/h3&gt;
&lt;p&gt;使用基于&lt;strong&gt;ResNet&lt;/strong&gt;修改的EC和EU编码器对原始图像和当前估计图像进行编码，结合多级残差连接且无批量归一化层，使用组归一化层。每个编码块先进行卷积层下采样，再连接由两个卷积层、组归一化和SiLU激活组成的残差块。解码器Du和Dm基于U - Net，Dm由两个含注意力层的残差块组成，Du先连接三个残差块接收Dm信息，再连接五个下采样卷积层，每层后接三个残差块，部分残差块后有注意力层，编码器各层与解码器同层有跳跃连接。&lt;/p&gt;
&lt;h3&gt;多模态动态条件引导&lt;/h3&gt;
&lt;p&gt;以往采用固定图像引导扩散概率模型，但脑组织结构复杂，MRI图像存在偏差场效应和噪声，正常与患病组织灰度相似，固定输入图像会影响网络学习能力。因此，该模块将当前步骤的分割信息与原始图像的分割信息融合，公式为$A = LN (m_{xt})⨁LN (m_{I})$ ，其中⨁表示逐元素相加，LN ( • )表示归一化。不过当前步骤存在高斯噪声会影响融合结果，为此提出傅里叶域特征融合模块来抑制噪声影响。&lt;/p&gt;
&lt;h3&gt;傅里叶域特征融合&lt;/h3&gt;
&lt;p&gt;受相关文献启发，该模块将当前特征图和条件输入特征图转换到傅里叶空间以减轻噪声影响。对两个特征图进行二维快速傅里叶变换（2D FFT），与参数化权重图逐元素相乘，增强后的特征图逐元素相加并通过sigmoid激活函数生成傅里叶空间的亲和图，公式为$M = Sigmoid ( ( F [ m_{xI,t} ] ⊗W ) ⨁(F [m_{I}] ⊗W ) )$ ，其中⊙表示逐元素相乘，F [ • ]表示2D FFT，m为输入特征图，W为参数化权重图。最后通过二维逆快速傅里叶变换（2D IFFT）将亲和图转换回欧几里得空间，公式为$\tilde{m} = F ^{-1}[M ]$ 。&lt;/p&gt;
&lt;h3&gt;基于不确定性度量的采样&lt;/h3&gt;
&lt;p&gt;扩散模型是生成模型，传统生成任务中多次采样结果不确定，但脑肿瘤分割区域固定。该模块发现时间步越接近$x_0$，预测图像越准确、不确定性越低。借鉴MC Dropout和Diff - UNet计算不确定性，公式为$U_{t} = -X_{t} log(X_{t})$ ，其中$X_{t} = \frac{1}{S} \sum_{1}^{S} X_{S_{t}}$ ，S为每个扩散步骤生成的结果数量。结合不确定性和预测步数得到最终分割结果，权重公式为$w_{t} = e^{sigmoid ( \frac{t}{T} )}×(1 - U_{t})$ ，最终预测结果$Y = \sum_{t = 1}^{T} w_{t} × X_{t}$ ，其中t为当前预测步骤，T为总预测步数。&lt;/p&gt;
&lt;h3&gt;训练&lt;/h3&gt;
&lt;p&gt;训练过程依据DPM设置，扩散步数T设为1000。每次迭代随机获取图像I及其真实二值分割图G，从均匀分布中采样迭代步数t，从标准分布中采样噪声$\epsilon$ 。损失函数为$Loss = E_{x_0,\epsilon,t} [ | \epsilon - \epsilon_{\theta} ( \sqrt{\alpha_{t}} x_0 + \epsilon \sqrt{ (1 - \alpha_{t}) }, I, t ) |^2 + \frac{1}{2} \sum( x_{I,t} - G )^2 ]$ ，训练时设$x_0 = G$ 。采样过程为随机过程，保存每步生成的$x_{I,t}$ 用于基于不确定性度量的采样，最终生成分割掩码。&lt;/p&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;数据集:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;脑肿瘤数据集（BraTs2020 - 2021）&lt;/strong&gt;：BraTs2020和BraTs2021数据集分别包含369和1251名患者的训练数据。将训练集按轴向切片，每个患者数据切成155片，选取第80 - 120片，打乱后划分训练集和测试集。BraTs2020使用了来自340 - 1200名患者脑肿瘤MR图像的13940个训练数据，测试集为来自29名患者的1189个MR图像；BraTs2021使用了49200个训练数据，测试集为来自51名患者的2091个MR图像。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;甲状腺结节数据集（DDTI）&lt;/strong&gt;：是一个甲状腺超声图像的开放数据库，包含99例和134张图像。将XML文件转换为JPG格式后，最终数据集包含2878个训练对和613个测试对，对图像大小进行了标准化处理。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;青光眼数据集（REFUGE - 2）&lt;/strong&gt;：由REFUGE竞赛提供，包含1200对RGB眼底图像，训练集和测试集按7:3比例划分。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;与现有方法对比&lt;/strong&gt;：在BraTs2020 - 2021数据集上，与多种脑肿瘤分割方法进行对比。结果表明，该方法在Dice系数、HD95和Jaccard指数等指标上优于传统方法和基于扩散概率模型的最新方法。例如，相较于EnsemDiff，Dice系数提高了5.87% - 6.95%，Jaccard指数提高了6.92% - 8.74%。同时，在Sensitivity和Specificity指标上也表现出色。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;在其他数据集上的验证&lt;/strong&gt;：在REFUGE - 2和DDTI数据集上进行对比。在REFUGE - 2数据集上，该方法在Dice系数和Jaccard指数上优于TransUNet等方法；在DDTI数据集上，虽然该方法在部分指标上优于大多数方法，但由于超声图像的特点，部分特征像素在融合过程中被忽略，导致最终结果未达预期。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-46-18.png&quot; alt=&quot;Snipaste_2025-07-06_20-46-18&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-46-24.png&quot; alt=&quot;Snipaste_2025-07-06_20-46-24&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-46-38.png&quot; alt=&quot;Snipaste_2025-07-06_20-46-38&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;实验（Ablation Experiments）​​&lt;/h2&gt;
&lt;p&gt;对多模态动态条件引导（MDCG）、傅里叶域特征融合（FFF）和基于不确定性测量的采样（UMS）三个关键组件进行消融实验。结果显示，传统DPM不适合脑肿瘤分割，MDCG有效解决了肿瘤位置信息缺失的问题，FFF进一步提高了分割性能，UMS使分割结果更稳定。同时，确定了UMS中最优的S值为3。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-47-36.png&quot; alt=&quot;Snipaste_2025-07-06_20-47-36&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-47-40.png&quot; alt=&quot;Snipaste_2025-07-06_20-47-40&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;作者提出基于扩散概率模型（DPM）的脑肿瘤分割框架，得出以下结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;利用多模态MRI引导DPM生成高质量分割结果，在傅里叶域进行特征融合操作，降低DPM高频噪声对融合的影响。&lt;/li&gt;
&lt;li&gt;考虑DPM生成结果的不确定性对脑肿瘤分割性能的影响，结合各步骤的不确定性生成最终分割结果，在多个数据集上验证了方法的有效性。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;不足：指出研究存在的问题，如DPM采样过程慢，脑肿瘤数据集切片会丢失肿瘤原始空间位置信息，影响空间体积准确性。后续将采用新加速策略，考虑肿瘤体素信息，将方法应用于3D领域，使分割结果更可靠。&lt;/p&gt;
&lt;/blockquote&gt;
</content:encoded></item><item><title>BraTS2023-MEN 数据集介绍</title><link>https://louaq.github.io/posts/brats2023-men-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/</link><guid isPermaLink="true">https://louaq.github.io/posts/brats2023-men-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/</guid><description>BraTS2023-MEN数据集处理。</description><pubDate>Sat, 05 Jul 2025 10:24:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/662644135&quot;&gt;内容链接&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;数据集信息&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BraTS2023-MEN&lt;/strong&gt;(Brain Tumor Segmentation 2023 Meningioma Challenge) 是 BraTS2023 五个分割子任务中之一，与 BraTS 常规分割脑胶质瘤不同，该子任务目标是从多模态 MR 图像 (mpMRI) 中分割&lt;strong&gt;脑膜瘤&lt;/strong&gt;。该数据集在 23 年 5 月份放出合计 6 个中心的 1650 例数据，其中有标注的训练集 1000 例，每例提供四种序列 MR 的输入图像（t1w, t1c, t2w, t2f）以及脑膜瘤的分割结果，标注内容主要包括非增强肿瘤核心（NETC）、周围非增强的FLAIR高信号（SNFH）和增强型肿瘤（ET）。验证集提供图像但没有标注，可以在官网提交验证，而测试集数据不公开。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;脑膜瘤&lt;/strong&gt;是成人最常见的原发性颅内肿瘤，大多数脑膜瘤（约80%）是世界卫生组织（WHO）1级良性肿瘤，通常可以通过观察、手术切除和/或放射治疗来良好控制。然而，高级别的脑膜瘤（WHO 2级和3级）与显著更高的发病率和死亡率相关且容易复发。和 BraTS23 其它分割任务一样，数据所有标签和数据都经过了预处理，这包括与统一的解剖模板对齐、调整到相同的分辨率（1 mm³）并进行了颅骨剥离。&lt;/p&gt;
&lt;h2&gt;数据集元信息&lt;/h2&gt;
&lt;p&gt;数据集所有图像的 spacing 和 size 都已经被预处理到一致。&lt;/p&gt;
&lt;p&gt;二维切片个数：620,000（基于 1000 例训练集统计 155000 × 4）&lt;/p&gt;
&lt;h2&gt;可视化&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-94086c2af9de4a1a4a3246aa225a8e2f_r.jpg&quot; alt=&quot;图片描述&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;文件结构&lt;/h2&gt;
&lt;p&gt;官方文件结构如下，包含两个主要目录：ASNR-MICCAI-BraTS2023-MEN-Challenge-TrainingData 和 ASNR-MICCAI-BraTS2023-MEN-Challenge-ValidationData，分别代表训练数据和验证数据。&lt;/p&gt;
&lt;h2&gt;作者及机构&lt;/h2&gt;
&lt;p&gt;Evan Calabrese (放射医学部，神经放射学分部，杜克大学医学中心，美国)&lt;/p&gt;
&lt;p&gt;Dominic Labella (放射治疗学部，杜克大学医学中心，美国)&lt;/p&gt;
&lt;p&gt;转载自知乎：https://zhuanlan.zhihu.com/p/662644135&lt;/p&gt;
</content:encoded></item><item><title>Asymmetric Adaptive Heterogeneous Network for Multi-Modality Medical Image Segmentation</title><link>https://louaq.github.io/posts/asymmetric-adaptive-heterogeneous-network-for-multi-modality-medical-image-segmentation/</link><guid isPermaLink="true">https://louaq.github.io/posts/asymmetric-adaptive-heterogeneous-network-for-multi-modality-medical-image-segmentation/</guid><description>现有的多模态医学图像分割研究往往在不加区分地聚合所有模态，并采用多个对称编码器或解码器进行特征提取和融合时。</description><pubDate>Fri, 04 Jul 2025 10:24:00 GMT</pubDate><content:encoded>&lt;p&gt;重庆邮电大学、第三军医大学、重庆医科大学第二附属医院&lt;/p&gt;
&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;Existing studies of &lt;strong&gt;multi-modality medical image segmentation&lt;/strong&gt; tend to aggregate all modalities without discrimination and employ multiple symmetric encoders or decoders for feature extraction and fusion. They often over-look the different contributions to visual representation and intelligent decisions among multi-modality images. Motivated by this discovery, this paper proposes an asymmetric adaptive heterogeneous network for multi-modality image feature extraction with modality discrimination and adaptive fusion. For feature extraction, it uses a heterogeneous two-stream asymmetric feature-bridging network to extract complementary features from auxiliary multi-modality and leading single-modality images, respectively. For feature adaptive fusion, the proposed Transformer-CNN Feature Alignment and Fusion (T-CFAF) module enhances the leading single-modality information, and the Cross-Modality Heterogeneous Graph Fusion (CMHGF) module further fuses multi-modality features at a high-level semantic layer adaptively. Comparative evaluation with ten segmentation models on six datasets demonstrates significant efficiency gains as well as highly competitive segmentation accuracy. ((Our code is publicly available at https://github.com/joker-527/AAHN)&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;现有的多模态医学图像分割研究往往在不加区分地聚合所有模态，并采用多个对称编码器或解码器进行特征提取和融合时，忽视了多模态图像在视觉表示和智能决策中的不同贡献。受此发现的启发，本文提出了一种用于多模态图像特征提取的&lt;strong&gt;非对称自适应异构网络&lt;/strong&gt;，该网络具有模态区分和自适应融合功能。在特征提取方面，使用异构双流非对称特征桥接网络分别从辅助多模态和主要单模态图像中提取互补特征。对于特征自适应融合，提出的Transformer-CNN特征对齐与融合（T-CFAF）模块增强了主要单模态信息，而跨模态异构图融合（CMHGF）模块进一步在高级语义层自适应地融合多模态特征。与六个数据集上的十个分割模型进行比较评估，结果显示显著的效率提升以及极具竞争力的分割准确性。（我们的代码在https://github.com/joker-527/AAHN公开提供）&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-28-08.png&quot; alt=&quot;Snipaste_2025-07-05_16-28-08&quot; /&gt;&lt;/p&gt;
&lt;p&gt;多模态医学图像能从多个角度提供互补信息，整合不同成像模态可提升成像数据的敏感性、特异性和空间分辨率。然而，现有多模态医学图像分割研究存在两个关键问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;特征提取问题&lt;/strong&gt;：现有方法倾向于无差别聚合所有模态，采用对称编码器或解码器进行特征提取，忽略了不同模态图像对视觉表示和智能决策的不同贡献，难以有效提取多模态图像中独特且互补的特征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征融合问题&lt;/strong&gt;：常见的特征融合方法是在通道维度上进行初始拼接或求和，然后输入特定融合模块，这种方式可能导致性能瓶颈，无法实现多模态特征的自适应融合，还可能造成独特特征的丢失。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;为解决这些问题，本文提出了一种用于多模态医学图像特征提取的非对称自适应异构网络，旨在实现模态区分和自适应融合，以有效管理多模态特征的异质性，显著提高分割结果。&lt;/p&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;多模态特征提取&lt;/strong&gt;：常见方法有集中提取（如3D AGSE - VNet、MBANet、Swin UNETR）和单独提取（如ME - Net、MAML、NestedFormer、A2FSeg），但大多基于对称特征提取，未考虑模态关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征融合&lt;/strong&gt;：现有方法常采用初始拼接或求和，再通过特定融合模块处理，如IVD - Net、Wang等人和Zhou等人的方法，但可能导致性能瓶颈，且简单使用CNN或Transformer会损失独特特征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CNN和Transformer结合&lt;/strong&gt;：多种方法将两者结合用于医学图像处理，但常忽略不同模态图像特征的异质性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图神经网络应用&lt;/strong&gt;：用于分割的图结构模型主要关注单模态，难以处理多模态图像的异质性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-31-41.png&quot; alt=&quot;Snipaste_2025-07-05_16-31-41&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;整体架构&lt;/h3&gt;
&lt;p&gt;模型的整体架构包括一个非对称双分支特征桥接编码器、Transformer - CNN 特征对齐与融合（T - CFAF）模块、跨模态异质图融合（CMHGF）模块和解码器，主要流程为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;特征提取&lt;/strong&gt;：使用非对称双分支特征桥接编码器从辅助多模态和主导单模态图像中分别提取互补特征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征融合&lt;/strong&gt;：T - CFAF 模块增强主导单模态信息，CMHGF 模块在高层语义层自适应地融合多模态特征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图像分割&lt;/strong&gt;：解码器通过跳跃连接逐步对齐来自 CNN 分支的低层次和高层次特征，实现最终的分割。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;主要组件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;非对称双分支特征桥接编码器：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;多模态 Transformer 分支&lt;/strong&gt;：具有强大的全局上下文信息建模能力，能够更好地捕捉辅助多模态特征。该分支包括嵌入层和骨干结构。嵌入层采用传统卷积结构，可保留位置信息并提取详细特征；骨干结构是 CNN 和 ViT 的混合组合，结合了两者的优势，能够保留 CNN 的准确位置信息和 Transformers 的全局上下文建模能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;单模态 CNN 分支&lt;/strong&gt;：利用 CNN 保留局部细节的能力处理主导单模态图像。主要由多个残差块（RES）和 T - CFAF 模块组成，通过平均池化形成特征金字塔结构，用于提取多尺度特征。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer - CNN 特征对齐与融合（T-CFAF）模块&lt;/strong&gt;：为了增强 CNN 分支上主导单模态信息，该模块将多模态特征图和单模态特征图在通道维度上拼接，通过计算通道注意力和跨模态注意力，调整通道权重，补充多模态信息，增强主导 CNN 特征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;跨模态异构图融合（CMHGF）模块&lt;/strong&gt;：用于缓解不同模态之间的特征异质性，实现跨模态特征的自适应融合。该模块将特征图空间投影到图空间，通过图卷积操作更新节点特征，构建异质图，计算节点间的相似度，更新节点特征向量，最终实现多模态特征的融合。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;数据集：在六个数据集上进行实验，涵盖不同医学场景，为模型性能评估提供多样数据支持。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hecktor21&lt;/strong&gt;：来自2021年Aicrowd在MICCAI发布的HECKTOR挑战，包含325名患者的18F - FDG PET和CT扫描，其中224例有标注。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prostate158&lt;/strong&gt;：包含158个前列腺MRI，专家标注，有T2加权和扩散加权图像及表观扩散系数图，目标区域为前列腺周围区（PZ）和过渡区（TZ）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BraTS2019和BraTS2023&lt;/strong&gt;：来自2019年和2023年脑肿瘤分割挑战，分别有335和1251个标注的MRI图像，每个病例有四种模态，标注区域包括肿瘤的三个子区域。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CHAOS&lt;/strong&gt;：来自联合（CT - MR）健康腹部器官分割挑战的任务5，旨在从20个不同序列的MRI数据集中分割腹部器官。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BraTS2024&lt;/strong&gt;：专注于治疗后神经胶质瘤的自动多区域脑肿瘤分割，使用治疗后的MRI，评估子区域包括非增强肿瘤核心等。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实现细节&lt;/strong&gt;：使用Pytorch - 1.7.1实现，在单张24GB NVIDIA GeForce RTX 3090 GPU上从零开始训练300个epoch，批量大小为16，输入图像调整为128×128。采用SGD优化器，初始学习率0.001并使用动态学习率策略，结合交叉熵损失和Dice损失训练网络。根据器官和病变可见性直观选择单模态分支作为CNN分支的输入。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-35-31.png&quot; alt=&quot;Snipaste_2025-07-05_16-35-31&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;头颈部肿瘤分割（Hecktor21数据集）&lt;/strong&gt;：该方法在HD95指标上取得最低分2.0793，Dice系数达到最高0.8242。箱线图显示其在Dice和HD95得分上表现更优且稳定，可视化结果表明该方法分割结果接近真实情况，对小目标体积也有良好处理能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;前列腺分割（Prostate158数据集）&lt;/strong&gt;：在PZ和TZ区域的Dice得分表现更好，PZ区域Dice得分为0.7833，TZ区域为0.8921，相比其他方法有显著提升。在HD95指标上虽未都达到最佳，但与最佳方法差距小，整体表现最优，且具有稳定性。可视化结果显示该方法在PZ区域过分割和欠分割问题较少，3D视角结果更接近真实情况。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;脑肿瘤分割（BraTS2019数据集）&lt;/strong&gt;：在WT和TC区域Dice得分最高，分别为0.9280和0.8764，ET区域Dice得分仅次于MAML模型。HD95指标上，ET区域得分最低，WT和TC区域与最佳方法差距不显著，整体平均HD95得分最低。箱线图显示Dice和HD95得分分布更集中，异常值少。可视化结果表明该方法在脑肿瘤和水肿区域分割更准确。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;腹部器官分割（CHAOS数据集）&lt;/strong&gt;：在所有评估器官上均优于其他方法，肝脏、右肾、左肾和脾脏的Dice系数分别为0.9206、0.9350、0.9077和0.9208，HD95也表现良好。箱线图显示该方法在两个指标下箱体高度最短，变异性最低，异常值最少，稳定性高。可视化结果表明该方法在不同形状和大小的腹部器官分割上表现更好。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;脑肿瘤分割（BraTS2023数据集）&lt;/strong&gt;：在WT、TC和ET区域的Dice和HD95指标上均表现优异，Dice得分分别为0.9371、0.9054和0.8629，HD95值分别为2.7786mm、3.3692mm和2.3758mm。箱线图显示WT区域Dice得分和三个区域HD95得分分布更集中，TC和ET区域Dice得分异常值少。可视化结果表明该方法在脑肿瘤和水肿区域分割更准确，具有良好的泛化能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;脑肿瘤分割（BraTS2024数据集）&lt;/strong&gt;：除NETC区域的HD95外，在所有评估肿瘤区域表现显著提升，在TC、WT、SNFH和RC区域Dice得分最高，HD95得分较低。可视化结果表明该方法在不同子区域分割更准确。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在线评估结果（BraTS2023验证集）&lt;/strong&gt;：与竞争方法相比，该方法在TC区域取得有竞争力的结果，Dice为0.860，HD95为15.110。该方法属于研究导向方法，注重探索不同模态的异构表示和融合，虽一些竞争方法计算能力和后处理技术强，但该方法在整体分割性能和泛化能力上表现出色。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-36-26.png&quot; alt=&quot;Snipaste_2025-07-05_16-36-26&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;实验（Ablation Experiments）​​&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;各模块有效性&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;T - CFAF模块&lt;/strong&gt;：移除该模块会导致Dice得分和HD95距离显著下降，添加CA和CMA可提高平均Dice得分，降低HD95距离。可视化结果表明添加T - CFAF后模型分割更接近真实情况。该模块有效补充多模态信息，促进特征提取，提高整体分割性能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CMHGF模块&lt;/strong&gt;：用通道连接结构替代该模块会使平均Dice得分下降，HD95距离增加。在BraTS2019数据集上的详细实验表明，CMHGF模块比CMA模块更能有效整合跨模态特征，增强分割准确性，跳跃连接和GNN结构对模型性能提升起关键作用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;不同单模态分支比较&lt;/strong&gt;：不同主要模态输入会导致不同结果，但该模型在大多数情况下与无模态区分的方法相比，能产生最高分割准确率。不同模态贡献不同，如头颈部肿瘤分割中高分辨率CT图像局部信息可提高准确性，脑肿瘤分割中T2图像因能清晰描绘病变而表现更好，前列腺和腹部器官分割中不同主要模态结果无明显规律。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-37-31.png&quot; alt=&quot;Snipaste_2025-07-05_16-37-31&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-38-07.png&quot; alt=&quot;Snipaste_2025-07-05_16-38-07&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;作者提出了一种用于多模态医学图像分割的非对称异构网络，得出以下结论：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创新性模块有效：特征桥接的Transformer - CNN非对称双编码器分支可高效提取辅助多模态和主导单模态的互补特征；T - CFAF模块促进多模态信息集成到单模态分支；CMHGF模块利用图结构自适应融合双编码器的跨模态特征。&lt;/li&gt;
&lt;li&gt;性能优越：实验结果表明，该方法显著优于现有的多模态分割方法，在多个数据集上的各项指标表现出色。&lt;/li&gt;
&lt;li&gt;具有通用性：该模型可作为通用的多模态研究框架，单模态编码分支易于替换，未来可进一步研究基于文本-图像的多模态模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;不足：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(1) 参数数量和模型复杂度&lt;/strong&gt;：该方法的参数数量和计算量较大，每批次大小为1时，有4.1137亿个参数和941.001 GFLOPs，相比其他方法规模更大。这一问题导致受限于GPU内存，只能使用二维网络。尽管采用的非对称特征编码结构以及T - CFAF和CMHGF特征对齐融合模块能有效学习多模态信息并取得优于现有先进技术的结果，但大量的参数限制了模型在处理能力或内存受限的实际场景中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(2) 处理不完整模态数据能力&lt;/strong&gt;：该方法主要聚焦于对齐的多模态医学图像分割。然而在实际临床环境中，由于设备限制或患者状况等原因，可能会出现模态缺失的情况。未来研究需要解决在模态数据不完整的情况下进行有效分割的问题，即开发出能够适应模态缺失情况，并在关键模态信息缺失时仍能保持高分割精度的鲁棒算法。&lt;/p&gt;
&lt;/blockquote&gt;
</content:encoded></item><item><title>BraTs数据集处理及python读取.nii文件</title><link>https://louaq.github.io/posts/brats2019%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86%E5%8F%8Apython%E8%AF%BB%E5%8F%96nii%E6%96%87%E4%BB%B6/</link><guid isPermaLink="true">https://louaq.github.io/posts/brats2019%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86%E5%8F%8Apython%E8%AF%BB%E5%8F%96nii%E6%96%87%E4%BB%B6/</guid><description>BraTs数据集处理。</description><pubDate>Fri, 04 Jul 2025 10:24:00 GMT</pubDate><content:encoded>&lt;h2&gt;数据集&lt;/h2&gt;
&lt;p&gt;BraTS 是MICCAI脑肿瘤分割比赛的数据集，BraTs 2018中的训练集( training set) 有285个病例
每个病例有四个模态(t1、t2、flair、t1ce)，需要分割三个部分：whole tumor(WT), enhance tumor(ET), and tumor core(TC)，相当于三个label。
每例病例中包含4种模态的MRI序列和1个seg文件,所有序列尺寸全部为(240, 240, 155)，如下图：&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6c1124180c7b582959148d148a04f54c.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;MR图像属性信息：（240x240x155）一个MR序列有155张图片，每张图片的大小为240x240&lt;/p&gt;
&lt;h2&gt;HGG、LGG&lt;/h2&gt;
&lt;p&gt;BraTs训练集( training set) 又划分为HGG和LGG,他们分别表示:&lt;br /&gt;
&lt;strong&gt;HGG :高级别胶质瘤（WHO3～4级）&lt;/strong&gt; 为低分化胶质瘤；这类肿瘤为恶性肿瘤，患者预后较差。&lt;br /&gt;
&lt;strong&gt;LGG :低级别胶质瘤（WHO1～2级）&lt;/strong&gt; 为分化良好的胶质瘤；虽然这类肿瘤在生物上并不属于良性肿瘤，但是患者的预后相对较好。
&lt;strong&gt;Domain shift 域转移&lt;/strong&gt;&lt;br /&gt;
即试图将学习的模型应用于与训练数据(源域)分布不同的测试数据(目标域)上，其性能会下降。由于医学图像多模态，数据域偏移的情况更加自然和严重。如下图所示，不同的医学图像（核磁共振（MR)成像和计算机断层扫描（CT））可以看到两幅图像的心脏区域在视觉上明显不同。毫无疑问的是，网络在MR图像上进行训练后不能用于CT图像上的测试。&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/361793728e77ba4f81e9fa2a4b26c188.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;多模态&lt;/h2&gt;
&lt;p&gt;多模态通俗地讲就是一个东西可以有多种不同的形态表现，个人理解四个模态就相当于channels。BraTs数据集中有t1、t2、t1ce、flair四个模态，而在医学界，把像 t1、t2、t1ce、flair这样的称为序列,一个病例的可以有多个序列,每个序列由许多切片组成, 此外,获得每种序列的方式不同,例如t1、t2是由于测量电磁波的物理量不同而产生的两种不同的序列,再例如t1ce序列要在做MR之前往血液打造影剂.&lt;/p&gt;
&lt;h2&gt;nii格式数据&lt;/h2&gt;
&lt;p&gt;标准NIFTI图像的扩展名是.nii，也包含了头文件及图像资料，NIFTI格式也可使用独立的图像文件（.img）和头文件（.hdr）。&lt;br /&gt;
nii图像为三维图像，进行切片后分别表示&lt;br /&gt;
矢量面Median sagittal section (人体从前向后切开)&lt;br /&gt;
冠状面coronal section (人体沿左向右切开)&lt;br /&gt;
水平面Transverse section**&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1c71bfc511b80e8dbecddb1063ae4cc7.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;读取nii文件方法&lt;/h3&gt;
&lt;h3&gt;1.NII文件可以直接用软件&lt;strong&gt;ITK-SNAP&lt;/strong&gt; 打开&lt;/h3&gt;
&lt;h3&gt;2.利用代码进行解析&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;2.1 安装torchio&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install torchio
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;读取文件并显示图片(以下仅读取t1)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import torchio as tio
t1_path = &apos;BraTS19_2013_2_1_t1.nii.gz&apos;
t1_img = tio.ScalarImage(t1_path)
t1_img.plot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;t1显示如下：&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/3565edad611614d765c5f18680583840.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.2 安装nibabel：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install nibabel
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;读取文件并显示图片(以下仅读取t1)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import nibabel as nib
import matplotlib.pyplot as plt
import numpy as np
img = nib.load(&apos;BraTS19_2013_2_1_flair.nii.gz&apos;)
print(img.shape)        # shape(240, 240, 155)
print(img.header[&apos;db_name&apos;]) 
width, height, queue = img.dataobj.shape    # 由文件本身维度确定，可能是3维，也可能是4维 
#print(&quot;width&quot;,width)  # 240
#print(&quot;height&quot;,height) # 240
#print(&quot;queue&quot;,queue)   # 155
nib.viewers.OrthoSlicer3D(img.dataobj).show()

num = 1
for i in range(0,queue,10):
 
    img_arr = img.dataobj[:,:,i]
    plt.subplot(5,4,num)
    plt.imshow(img_arr,cmap=&apos;gray&apos;)
    num +=1
 
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;显示图片如下：&lt;em&gt;（如果是黑色，鼠标拖动一下图片或者动一下滚轮，可能是坐标在原点）&lt;/em&gt;&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/383a27fe5b998e62881b627bcc20ed64.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ae90cda8e929faa17e89aed401044ca8.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;两种方式好像第二种会显示的的是3D的，更多态化一点，光是看一下数据长什么样子的话，两种方式都能用&lt;/p&gt;
&lt;h3&gt;T1加权、T2加权&lt;/h3&gt;
&lt;p&gt;MRI扫描方式可以简单的划分为常规扫描和功能扫描两大类。常规扫描主要反映解剖形态；功能扫描则以不同方式反映人体新陈代谢、血液流动等功能信息。&lt;br /&gt;
&lt;strong&gt;T1加权（T1 Weighted）和T2加权（T2 Weighted）是最常用，也是最基础的常规扫描。几乎所有的临床MRI检查都会包含T1加权和T2加权扫描。这里的“加权”，就是突出的意思。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;T1看结构&lt;/strong&gt;
T1图像的整体感官跟“临床图像”的“习惯配色风格”非常接近,你看白质是白的,灰质是灰的,脑脊液是黑的.所以T1图像就可以看出各种断层解剖图.于是“T1看解剖结构”的说法就这么来了.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;T2看病变&lt;/strong&gt;
T2信号跟水含量有关(而Flair是结合水)很多病灶的T2信号要强于周围的正常组织.而很多病变都伴随组织水肿.从下图中可以看到,非常白的是水肿,然后比较白的那块影影约约的就是病变(红色)的地方了.&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/76f9af39dca5f18ef8a0cd485dd9a11d.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;p&gt;如下图所示，左：T1加权图像 右：T2加权图像&lt;br /&gt;
T1加权突出显示解剖结构，T2加权则能够突出显示病灶&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c060f70ebac1fd8d70fab3cea10a690b.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;T1CE&lt;/h3&gt;
&lt;p&gt;t1ce序列要在做MR之前往血液打造影剂(颜料),&lt;strong&gt;亮的地方血供丰富&lt;/strong&gt;,强化明显说明血流很丰富,什么地方需要血流很快呢?&lt;strong&gt;肿瘤嘛~它们不断分裂需要大量的营养.&lt;/strong&gt;&lt;br /&gt;
下图所示, &lt;strong&gt;蓝色区域是增强瘤(enhancing tumor),它环绕的里面绿色那些是坏疽(ju)(necrotic components),坏疽就是细胞坏死然后液化.所以这些坏疽还被被称为非增强瘤(non-enhancing tumor).&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/7e6911952c9ee327e5eb42e35dde0493.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;Flair&lt;/h3&gt;
&lt;p&gt;FLAIR序列是核磁共振(MR)的一种常用的序列，全称是液体衰减反转回复序列，也称为水抑制成像技术. 通俗地说,它是压水像。在该序列上，脑脊液呈现低信号(暗一些)，实质性病灶和含有结合水的病灶显示为明显的高信号(亮一些)&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/803d8fd6791236b331a58ff404df5820.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;分割部分WT、ET、TC&lt;/h3&gt;
&lt;p&gt;下图是Brats的数据集的一个序列的病例[颜色标签和上面那些图无关],我通过ITK-SNAP导入flair序列以及对应的分割标签, 需要分割有三个部分,分别是WT、ET、TC.&lt;br /&gt;
&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/fbd1e73394ba9ccd7edf12beb79e0660.png&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;br /&gt;
&lt;strong&gt;数据集规定, 图中绿色为浮肿区域(ED,peritumoral edema) (标签2)、黄色为增强肿瘤区域(ET,enhancing tumor)(标签4)、红色为坏疽(NET,non-enhancing tumor)(标签1)、背景(标签0)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;然后这些标签合并为3个嵌套的子区域:&lt;br /&gt;
&lt;strong&gt;whole tumor (WT) --------包含所有labels（红+黄+绿）&lt;br /&gt;
tumor core (TC) --------- 红色+黄色label&lt;br /&gt;
enhancing tumor (ET) -------黄色label&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;即 WT = ED + ET + NET&lt;br /&gt;
TC = ET+NET&lt;br /&gt;
ET&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;转载自CSDN：https://blog.csdn.net/qq_42740834/article/details/124473611&lt;/p&gt;
</content:encoded></item><item><title>A Simple and Robust Framework for Cross-Modality Medical Image Segmentation applied to Vision Transformers</title><link>https://louaq.github.io/posts/a-simple-and-robust-framework-for-cross-modality-medical-image-segmentation-applied-to-vision-transformers/</link><guid isPermaLink="true">https://louaq.github.io/posts/a-simple-and-robust-framework-for-cross-modality-medical-image-segmentation-applied-to-vision-transformers/</guid><pubDate>Wed, 28 May 2025 15:47:00 GMT</pubDate><content:encoded>&lt;p&gt;Centre des Mat´eriaux、Centre de Mise en Forme des Mat´eriaux、Centre de Morphologie Math´ematique&lt;/p&gt;
&lt;h2&gt;摘要&lt;/h2&gt;
&lt;p&gt;When it comes to clinical images, automatic segmentation has a wide variety of applications and a considerable
diversity of input domains, such as different types of Magnetic Resonance Images and Computerized Tomography scans. This heterogeneity is a challenge for cross-modality algorithms that should equally perform independently of the input image type fed to them. Often, segmentation models are trained using a single modality, preventing generalization to other types of input data without resorting to transfer learning techniques. Furthermore, the multi-modal or cross-modality architectures proposed in the literature frequently require registered images, which are not easy to collect in clinical environments, or need additional process-
ing steps, such as synthetic image generation. In this work, we propose a simple framework to achieve fair image segmentation ofmultiple modalities using a single conditional model that adapts its normalization layers based on the input type, trained with non-registered interleaved mixed data. We show that our framework outperforms other cross-modality segmentation methods, when applied to the same 3D UNet baseline model, on the Multi-Modality Whole Heart Segmentation Challenge. Furthermore, we define the Conditional Vision Transformer encoder, based on the proposed cross-modality framework, and we show that it brings significant improvements to the resulting segmentation, up to 6.87% ofDice accuracy, with respect to its baseline ref-
erence. The code to reproduce our experiments and the trained model weights are publicly available at https://github.com/matteo-bastico/MI-Seg.&lt;/p&gt;
&lt;h2&gt;翻译&lt;/h2&gt;
&lt;p&gt;对于临床图像，&lt;strong&gt;自动分割&lt;/strong&gt;具有广泛的应用和多样的输入域，例如不同类型的磁共振图像和计算机断层扫描。这种异质性对跨模态算法提出了挑战，因为它们应该在不考虑输入图像类型的情况下同样有效地运行。通常，分割模型使用单一模态进行训练，无法在不借助迁移学习技术的情况下泛化到其他类型的输入数据。此外，文献中提出的多模态或跨模态架构通常需要已配准的图像，而这些图像在临床环境中难以收集，或者需要额外的处理步骤，如合成图像生成。在这项工作中，我们提出了一个简单的框架，通过一个单一的条件模型实现多模态图像的公平分割，该模型根据输入类型调整其归一化层，并使用未配准的交错混合数据进行训练。我们展示了在相同的3D UNet基线模型上，我们的框架在多模态全心脏分割挑战中优于其他跨模态分割方法。此外，我们基于所提出的跨模态框架定义了条件视觉Transformer编码器，并展示了它对最终分割带来了显著的改进，与基线参考相比，Dice准确率提高了最多6.87%。复现我们实验的代码和训练好的模型权重已公开在https://github.com/matteo-bastico/MI-Seg。&lt;/p&gt;
&lt;h2&gt;研究背景&lt;/h2&gt;
&lt;p&gt;医学图像分割是&lt;strong&gt;深度学习和人工智能&lt;/strong&gt;领域的研究热点，但当前的算法存在一些问题，这构成了本文的研究背景：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;单模态训练局限性&lt;/strong&gt;：现有算法通常在单一医学成像模态上训练，如T1 - 或T2加权磁共振成像（MRI）或计算机断层扫描（CT），在测试不同训练图像时易受数据可变性影响。数据可变性源于成像方法、扫描仪、采集设置和患者个体差异等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模态和跨模态方法的不足&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;多模态方法&lt;/strong&gt;：需堆叠不同类型图像生成组合输入，或利用多模态生成合成图像，但都需要配准的医学图像，而采集同一患者的多张图像受资源和时间限制。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;跨模态方法&lt;/strong&gt;：如使用辅助模态改进目标模态分割，采用微调或迁移学习，但未充分利用跨模态信息。联合训练在域转移显著时难直接学习共同特征，基于合成图像生成的技术增加了计算复杂性，不利于实时应用。 因此，本文旨在提出一种通用框架，实现高质量跨模态分割，同时避免模型开销和训练时对配准临床图像的需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;研究现状&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;模型架构&lt;/strong&gt;：近年来，多种深度学习架构被提出用于医学图像分割，如 UNet 及其变体，以及基于 Vision Transformers（ViT）的模型，如 TransUNET、UNETR 和 Swin - UNETR 等，性能表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多模态学习&lt;/strong&gt;：多模态学习在医学影像领域有应用，包括基于&lt;strong&gt;生成对抗网络&lt;/strong&gt;（&lt;strong&gt;GAN&lt;/strong&gt;）的合成图像生成和多模态图像分割，部分研究将二者结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;跨模态分割&lt;/strong&gt;：提出了一些跨模态医学图像分割技术，如使用辅助模态提升目标模态分割性能、联合训练、基于特征提取器的方法等。&lt;/p&gt;
&lt;h2&gt;提出的模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_15-58-11.png&quot; alt=&quot;Snipaste_2025-05-28_15-58-11&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-00-23.png&quot; alt=&quot;Snipaste_2025-05-28_16-00-23&quot; /&gt;&lt;/p&gt;
&lt;p&gt;论文提出了一个简单的框架和基于该框架的条件Vision Transformers（C - ViT）编码器，用于跨模态医学图像分割，具体如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;跨模态分割框架&lt;/strong&gt;：该框架适用于任何编码器 - 解码器架构，旨在实现高质量的跨模态分割，且无需在分割模型上增加额外开销，也不需要注册临床图像进行训练。具体做法是将不同领域的医学图像直接输入到一个单一的模态条件模型中，该模型通过自适应其编码器归一化层来生成所需的分割结果。自适应基于条件实例归一化（CIN），模型可以端到端地进行训练。通过随机混合多种模态的不同数据（即交错混合训练方式），避免了先前的合成图像风格迁移。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;条件视觉变换器（C - ViT）编码器&lt;/strong&gt;：基于上述提出的框架，正式定义了C - ViT编码器架构，用于构建与模态无关的基于ViT的图像分割或分类模型。C - ViT编码器有两个子层，即多头自注意力机制（MSA）和多层感知机（MLP），在每个子层之前都使用CIN替换了传统的层归一化（LN）。C - ViT编码器可以推广到Swin - 变压器模型中。 实验结果表明，该框架和C - ViT编码器在多模态全心脏分割（MM - WHS）2017挑战赛数据集上，相较于其他方法，显著提高了分割准确性，同时降低了训练和推理的复杂性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;实验（Compared with SOTA）&lt;/h2&gt;
&lt;p&gt;数据集：&lt;strong&gt;Multi-modality Whole Heart Segmentation Challenge 2017 (MM-WHS 2017) dataset&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-03-16.png&quot; alt=&quot;Snipaste_2025-05-28_16-03-16&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-03-23.png&quot; alt=&quot;Snipaste_2025-05-28_16-03-23&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据和方法&lt;/strong&gt;：在心脏子结构分割的定量比较中，对比了微调、联合训练、X形架构、Zhang等人基于合成图像生成的在线训练方法以及Li等人的知识蒸馏跨模态分割技术。微调是先使用辅助模态训练模型，再将知识迁移到目标模态；联合训练是在不同批次中交替使用两种模态同时训练基线模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果分析&lt;/strong&gt;：微调、联合训练和X形架构对分割精度的提升有限，因为它们没有充分利用跨模态信息。基于GAN的在线合成方法（有无相互知识蒸馏）能显著提高目标模态分割的平均精度，最高达3.06%，但会给模型带来显著开销，限制实时应用。本文方法在降低训练和推理复杂度的同时，显著提高了分割精度，平均Dice系数提升了0.65%，单个心脏子结构（如右心室）最高提升2.71%，不过升主动脉和肺动脉的性能略有下降，可能是手动分割范围与测试工具裁剪不一致导致。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;实验（Ablation Experiments）​​&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定量结果&lt;/strong&gt;：与基线模型、微调及联合训练的结果相比，本文框架将心脏子结构的平均Dice系数提高了4%，全心分割（WHS）的准确率提高了6.87%。基于ViT的条件模型在验证集上表现优于基于UNet的模型，但在测试集上性能略差，可能是因为Transformer通常需要更多数据进行更精细的泛化。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定性结果&lt;/strong&gt;：使用C - ViT时，与其他方法相比有明显改进，如在某些切片的分割结果中能对真实标注（GT）进行细化，3D分割中C - ViT的结果没有真实标注区域外的误报。且该框架在交叉验证中表现稳健。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-04-21.png&quot; alt=&quot;Snipaste_2025-05-28_16-04-21&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;p&gt;作者提出一种简单框架，用单跨模态条件模型和交错混合数据训练，以减少模型开销和对配准数据的需求，实现不同医学图像的分割。结论如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于条件实例归一化（CIN）定义通用条件模型，可应用于所有先进医学图像分割架构。&lt;/li&gt;
&lt;li&gt;开发新的条件视觉变压器（C - ViT）编码器，用于创建基于ViT的条件模型。&lt;/li&gt;
&lt;li&gt;该框架在心脏子结构分割的多模态公共数据集上达到了跨模态医学图像分割的新水平，不仅利用辅助模态帮助目标模态分割，也能对辅助模态高质量分割。&lt;/li&gt;
&lt;li&gt;未来可在更多模态数据集测试，以无监督域适应方式扩展框架，实现单标注模态训练并适应无标注域。&lt;/li&gt;
&lt;/ol&gt;
</content:encoded></item></channel></rss>