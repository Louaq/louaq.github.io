<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Dual Attention Encoder with Joint Preservation for Medical Image Segmentation</title>
      <link href="/post/dual-attention-encoder-with-joint-preservation-for-medical-image-segmentation/"/>
      <url>/post/dual-attention-encoder-with-joint-preservation-for-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<p>作者单位</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Rolling-Unet Revitalizing MLP’s Ability to Efficiently Extract Long-Distance Dependencies for Medical Image Segmentation</title>
      <link href="/post/rolling-unet-revitalizing-mlp-s-ability-to-efficiently-extract-long-distance-dependencies-for-medical-image-segmentation/"/>
      <url>/post/rolling-unet-revitalizing-mlp-s-ability-to-efficiently-extract-long-distance-dependencies-for-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Medical image segmentation</strong> methods based on deep learning network are mainly divided into CNN and Transformer. However, CNN struggles to capture long-distance dependencies, while Transformer suffers from high computational complexity and poor local feature learning. To efficiently extract and fuse local features and long-range dependencies, this paper proposes Rolling-Unet, which is a CNN model combined with MLP. Specifically, we propose the core R-MLP module, which is responsible for learning the long-distance dependency in a single direction of the whole image. By controlling and combining R-MLP modules in different directions, OR-MLP and DOR-MLP modules are formed to capture long-distance dependencies in multiple directions. Further, Lo2 block is proposed to encode both local context information and long-distance dependencies without excessive<br>computational burden. Lo2 block has the same parameter size and computational complexity as a 3×3 convolution. The experimental results on four public datasets show that Rolling-Unet achieves superior performance compared to the state-of-<br>the-art methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>基于深度学习网络的医学图像分割方法主要分为CNN和transformer。然而，CNN很难捕获长距离依赖关系，而transformer则存在计算复杂度高和局部特征学习能力差的问题。为了有效地提取和融合局部特征和远程依赖关系，本文提出了一种结合MLP的CNN模型rollling - unet。具体来说，我们提出了核心R-MLP模块，该模块负责学习整个图像在单一方向上的长距离依赖关系。通过对不同方向的R-MLP模块进行控制和组合，形成OR-MLP和DOR-MLP模块，以捕获多方向的远程依赖关系。此外，在不增加计算负担的情况下，提出了Lo2块对本地上下文信息和远程依赖关系进行编码。Lo2块具有与3×3卷积相同的参数大小和计算复杂度。在四个公共数据集上的实验结果表明，RollingUnet的性能优于当前的方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>这篇文章聚焦于医疗图像分割领域，旨在解决现有方法在提取和融合局部特征与长距离依赖关系方面的不足，具体研究背景如下： </p><ol><li><strong>CNN的局限性</strong>：基于卷积神经网络（CNN）的医疗图像分割方法虽有发展，如U-Net及其变体，但卷积操作的固有局部性使其难以学习清晰的全局和远程语义信息。 </li><li><strong>Transformer的问题</strong>：受自然语言处理中Transformer成功的启发，研究者将其引入视觉领域，但它需要大量训练数据，计算复杂度高，且在捕捉局部特征方面表现不佳，如Vision Transformer和Swin Transformer等。 </li><li><strong>CNN与Transformer结合的不足</strong>：一些方法尝试结合CNN和Transformer，但仍无法很好地平衡性能和计算成本。</li><li><strong>MLP的困境</strong>：多层感知器（MLP）理论上是通用逼近器，但计算量大、易过拟合，输入扁平化限制分辨率，虽有改进工作，但在医疗图像分割领域应用较少，且现有模型难以兼顾局部和全局特征。 因此，文章提出Rolling-Unet，结合CNN和MLP，以有效提取和融合局部特征与长距离依赖关系，实现更准确的医疗图像分割。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>CNN 方法</strong>：以 U-Net 为代表，后续有 UNet++、Att - UNet 等改进模型，通过引入注意力机制、图像金字塔等技术提升性能，但受卷积操作局部性限制，<strong>难以学习全局和远程语义信息</strong>。</li><li><strong>Transformer 方法</strong>：如 Vision Transformer、Swin Transformer 等被引入医学图像领域，能捕捉远程依赖，但<strong>计算复杂度高，对训练数据量要求大</strong>，且在捕捉局部特征方面表现不佳。</li><li><strong>CNN 与 Transformer 结合方法</strong>：如 MedT、UCTransNet 等，尝试融合二者优势，但仍难以平衡性能和计算成本。</li><li><strong>MLP 方法</strong>：MLP - Mixer 复兴了 MLP 在图像任务中的应用，后续工作引入局部先验，但大多仅具备局部感受野，在医学图像领域基于 MLP 的分割模型较少。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-04-47.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-04-47.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-04-47"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-46.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-46.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-10-46"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-10-52"></p><h3 id="核心模块"><a href="#核心模块" class="headerlink" title="核心模块"></a>核心模块</h3><ol><li><strong>R-MLP模块</strong>：负责学习整个图像在单个方向上的长距离依赖关系。对特征矩阵中每个通道层的特征图沿同一方向进行滚动操作（包括移位和裁剪两步），然后在每个空间位置索引处进行通道投影以编码长距离依赖。该操作初步降低了MLP对位置信息的敏感性，使用权重共享进一步减少了这种敏感性。</li><li><strong>OR-MLP模块</strong>：通过先沿宽度方向应用R - MLP，再沿高度方向应用R - MLP，形成正交滚动MLP模块，能够捕获多个方向的远程依赖关系。</li><li><strong>DOR-MLP模块</strong>：将两个互补的OR - MLP模块并行化，可捕获宽度、高度、正对角线和负对角线四个方向的长距离依赖关系。</li><li><strong>Lo2块</strong>：由DOR - MLP模块和深度可分离卷积（DSC）模块并行组成，能够同时提取图像的局部上下文信息和长距离依赖关系，且参数和计算量与3×3卷积处于同一水平。</li><li><strong>Feature Incentive Block (特征激励块)</strong>：用于编码器的第4层和瓶颈层，主要对特征和通道数量变化进行编码。在编码器第4层采用GELU激活函数和LayerNorm；在解码器第4层，由卷积块、RELU激活函数和BatchNorm组成。</li></ol><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>Rolling-Unet采用U-Net的U形框架，包括<strong>编码器-解码器结构</strong>、<strong>瓶颈层</strong>和<strong>跳跃连接</strong>。编码器 解码器有四个下采样和上采样阶段，分别通过最大池化和双线性插值实现。前三层包含标准的3×3卷积块，第四层和瓶颈层使用特征激励块和Lo2块。跳跃连接通过相加融合相同尺度的特征。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：</p><p>ISIC 2018：用于皮肤病诊断的图像数据集，包含多种皮肤病的图像和相应的标签</p><p>BUSI：乳腺超声图像</p><p>CHASEDB1：眼底血管分割</p><p>GlaS：结直肠腺体组织的分割任务</p></blockquote><ul><li><strong>对比方法</strong>：将Rolling - Unet与其他先进方法进行对比，包括基于CNN的U - Net、UNet++、Att - Unet、DconnNet；基于Transformer的UCTransNet、MedT；基于MLP的UNeXt。</li><li><strong>评估指标</strong>：采用交并比（IoU）、F1分数和95%豪斯多夫距离（HD95）作为评估指标。</li><li><strong>实验结果</strong>：Rolling - Unet在所有数据集上均优于其他方法。在BUSI和ISIC 2018数据集上优势显著，能更有效地提取远程依赖关系以提升分割性能。在ISIC 2018上改变图像大小的实验进一步验证了这一点，只有Rolling - Unet和UNeXt在图像尺寸增大时性能保持稳定。在GlaS和CHASEDB1数据集上，虽无方法取得显著优势，但Rolling - Unet表现最佳且标准差小。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-17-07"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><p>在ISIC 2018数据集（图像大小为512）上进行消融实验，以研究各因素对模型性能的影响。</p><ul><li><strong>Lo2模块分析</strong>：Lo2块由DOR - MLP和DSC模块并行组成。实验表明，无论DSC模块是否存在，R - MLP、OR - MLP和DOR - MLP的性能逐步提升，证明了所提模块捕获长距离依赖的有效性，且与DSC模块结合可进一步提升性能，说明融合远程依赖和局部上下文信息至关重要。</li><li><strong>R - MLP作用验证</strong>：将Rolling - Unet中的R - MLP替换为常规MLP，模型失去捕获长距离依赖的能力，性能显著下降。</li><li><strong>模块组合方式探究</strong>：对比DOR - MLP和DSC的不同组合方式（先执行DOR - MLP再执行DSC、先执行DSC再执行DOR - MLP、并行连接），结果表明并行连接效果最佳，说明提取局部特征和远程依赖的顺序不重要，同时提取后融合效果最好。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-45.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-45.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-17-45"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出的Rolling-Unet模型能在不增加计算成本的情况下捕获长距离依赖关系，且性能优于现有方法。具体结论如下： </p><ol><li>多方向的远程依赖并非严格意义上的全局感受野，是MLP的一种折中，但R - MLP模块灵活，组合使用可捕获大规模区域甚至全局特征，未来将深入探索。 </li><li>在四个不同数据集上，Rolling - Unet在初级和次级模型中表现最佳，尤其在BUSI和ISIC 2018数据集上优势显著，能有效提取目标轮廓，提升分割性能。 </li><li>消融实验表明，融合远程依赖和局部上下文信息至关重要，同时提取并融合二者效果最佳。未来，作者还将研究其在三维医学图像分割及其他图像任务中的潜力。</li></ol>]]></content>
      
      
      <categories>
          
          <category> medical image segmentation </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>C-CAM Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image</title>
      <link href="/post/c-cam-causal-cam-for-weakly-supervised-semantic-segmentation-on-medical-image/"/>
      <url>/post/c-cam-causal-cam-for-weakly-supervised-semantic-segmentation-on-medical-image/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Recently, many excellent weakly supervised semantic segmentation (WSSS) works are proposed based on class activation mapping (CAM). However, there are few works that consider the characteristics ofmedical images. In this paper, we find that there are mainly two challenges of medical images in WSSS: i) the boundary of object foreground and background is not clear; ii) the co-occurrence phenomenon is very severe in training stage. We thus propose a Causal CAM (C-CAM) method to overcome the above challenges. Our method is motivated by two cause-effect chains including category-causality chain and anatomy-<br>causality chain. The category-causality chain represents the image content (cause) affects the category (effect). The anatomy-causality chain represents the anatomical structure (cause) affects the organ segmentation (effect). Extensive experiments were conducted on three public medical image data sets. Our C-CAM generates the best pseudo masks with the DSC of 77.26%, 80.34% and 78.15% on ProMRI, ACDC and CHAOS compared with other CAM-like methods. The pseudo masks ofC-CAM are further used to improve the segmentation performance for organ segmentation tasks. Our C-CAM achieves DSC of 83.83% on<br>ProMRI and DSC of87.54% on ACDC, which outperforms state-of-the-art WSSS methods. Our code is available at <a href="https://github.com/Tian-lab/C-CAM">https://github.com/Tian-lab/C-CAM</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>近年来，人们提出了许多基于<strong>类激活映射</strong>的弱监督语义分割(WSSS)方法。然而，很少有工作考虑到医学图像的特点。在本文中，我们发现医学图像在WSSS中主要存在两个挑战:1)<strong>目标前景和背景的边界不清晰</strong>;Ii)<strong>训练阶段共现现象非常严重</strong>。因此，我们提出了一种因果CAM (C-CAM)方法来克服上述挑战。我们的方法是由两个因果链驱动的，包括范畴因果链和解剖因果链。范畴-因果链表示图像内容(因)影响范畴(果)。解剖-因果链表示解剖结构(因)影响器官分割(果)。在三个公共医学图像数据集上进行了大量的实验。与其他类cam方法相比，我们的C-CAM在ProMRI、ACDC和CHAOS上的DSC分别为77.26%、80.34%和78.15%，生成的伪掩膜效果最好。进一步利用c - cam的伪掩膜来提高器官分割任务的分割性能。我们的C-CAM在ProMRI上的DSC为83.83%，在ACDC上的DSC为87.54%，优于最先进的WSSS方法。我们的代码可在<a href="https://github.com/Tian-lab/C-CAM%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Tian-lab/C-CAM上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦医学图像弱监督语义分割（WSSS），其研究背景主要源于以下方面：</p><ul><li><strong>语义分割现状</strong>：深度学习推动语义分割广泛研究，传统范式依赖大量像素级标注数据，但获取此类标注耗时且成本高，因此WSSS应运而生。其中，图像级标签获取最易却也最具挑战性，现有基于类激活映射（CAM）的WSSS方法多针对自然图像。</li><li><strong>医学图像挑战</strong>：与自然图像相比，医学图像在基于图像级标签的WSSS中存在两大挑战。一是前景与背景边界模糊，使CAM模型难以准确分类；二是训练阶段共现现象严重，不同器官常同时出现在同一图像中，仅依靠图像级标签，CAM模型难以激活正确的共现器官。</li><li><strong>现有方法不足</strong>：多数基于CAM的WSSS方法未考虑医学图像的上述特性，无法在医学图像上取得良好效果。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：基于类激活映射（CAM）的方法是主流，多数聚焦于自然图像，利用图像级标签等弱标注，常见流程为生成种子区域、细化种子生成伪掩码、用伪掩码训练分割模型。</li><li><strong>解剖先验</strong>：在图像分割中融入先验知识可提升性能，医学图像的解剖先验更具影响力，但现有方法需专业知识或复杂模型。</li><li><strong>计算机视觉中的因果关系</strong>：因果关系在计算机视觉任务中应用广泛，有助于提供更好的学习和可解释模型，但在医学图像弱监督语义分割中应用较少。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-28-49.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-28-49.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-16_13-28-49"></p><ul><li><p><strong>全局采样模块（Global Sampling Module）</strong>：将训练图像输入纯CAM（P - CAM）模型生成粗分割掩码，该模块最终输出包含类别和解剖信息的全局上下文图$M_{GC}$。</p></li><li><p>因果模块（Causality Module）</p><p>：基于两条因果链设计，分别为类别因果链和解剖因果链。</p><ul><li><strong>类别因果链（Category - Causality Chain）</strong>：将粗分割掩码和全局上下文图输入重塑层，通过两个卷积层投影到同一空间，计算类别感知注意力向量$A_{category}$，最终得到图像特定的类别因果图$M_{c}$。</li><li><strong>解剖因果链（Anatomy - Causality Chain）</strong>：设计一个0&#x2F;1指示器表示医学图像的解剖信息，计算解剖因果图$M_{S}$，将其与仅包含类别因果的显著图$CAM_{cc}$相乘得到最终显著图$CAM_{ac}$，进而生成伪分割掩码$S_{pseudo}$</li></ul></li></ul><h3 id="模型优势"><a href="#模型优势" class="headerlink" title="模型优势"></a>模型优势</h3><ul><li><strong>引入因果关系</strong>：C-CAM是首个将<strong>因果关系</strong>引入医学图像弱监督语义分割的方法，生成的伪分割掩码边界更清晰、形状更准确。</li><li><strong>解决关键问题</strong>：类别因果链缓解了边界模糊问题，解剖因果链解决了共现问题。</li><li><strong>实验效果好</strong>：在三个公共医学图像数据集（ProMRI、ACDC和CHAOS）上的实验表明，C-CAM生成的伪掩码在DSC指标上表现优异，训练的分割网络<strong>U-Net</strong>达到了最先进的性能。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：ProMRI、ACDC、CHAOS</p></blockquote><ol><li><strong>与其他CAM类方法比较</strong>：将C - CAM与Grad - CAM、Grad - CAM++等CAM类定位方法比较，使用相同的训练基线模型，测试所有背景阈值，展示不同方法伪掩码的最佳DSC结果。结果显示，C - CAM在三个医学图像数据集上生成的伪掩码性能最佳，在CHAOS的所有类别上表现良好。</li><li><strong>参数敏感性实验</strong>：评估背景阈值对生成伪分割掩码的影响，比较几种不同的CAM类方法。多数CAM类方法对背景阈值敏感，而C - CAM在背景阈值范围为0.3 - 0.9时，显著性图的DSC能稳定在较高值，表明其对背景阈值的鲁棒性。</li><li><strong>显著性图可视化</strong>：直观展示C - CAM的优势。结合类别因果关系，C - CAM能解决模糊边界问题，在ProMRI和ACDC数据集上，其显著性图的前景和背景边界清晰；借助解剖因果关系，能显著缓解共现问题，且错误激活的无关背景区域更少。</li><li><strong>与其他WSSS方法比较</strong>：用生成的伪分割掩码在全监督下训练U - Net模型，将测试数据的最终分割结果与其他先进的WSSS方法比较。在ProMRI数据集上，对于整个前列腺，C - CAM的DSC最高（83.83%），标准差最低（5.14%），在平均表面距离（ASD）和平均绝对距离（MAD）指标上也表现最佳；在ACDC数据集上，C - CAM在所有三个指标上均取得最佳性能。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-38-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-38-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-16_13-38-40"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ol><li><strong>消融实验</strong>：分析C - CAM各模块的作用。结果表明，与P - CAM相比，类别因果关系和解剖因果关系都提高了三个数据集上伪掩码的准确性。解剖因果关系在ProMRI上提升2.43%，在ACDC上提升1.79%，在CHAOS多标签分割任务中提升显著（18.3%）；结合类别因果关系后，ProMRI、ACDC和CHAOS数据集的DSC分别进一步提升4.22%、3.46%和5.41%；再训练一个亲和模型后，三个数据集上生成的伪分割掩码的DSC分别达到77.26%、80.34%和78.15%。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-37-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-37-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-16_13-37-24"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于医<strong>学图像弱监督语义分割</strong>（<strong>WSSS</strong>）的因果类激活映射（C-CAM）方法，得出以下结论：</p><ol><li><strong>方法有效性</strong>：C-CAM集成类别因果链和解剖因果链生成准确的伪分割掩码，能缓解前景与背景边界模糊问题，解决器官共现问题，生成的显著图边界清晰，符合解剖学知识。 </li><li><strong>性能优越性</strong>：C-CAM在ProMRI、ACDC和CHAOS数据集上优于六种先进的类CAM方法；用其伪掩码训练的U-Net分割网络在ProMRI和ACDC数据集上达到了先进水平。 </li><li><strong>局限性与展望</strong>：C-CAM难以分割形状复杂的物体，未来可结合少量强标签和大量弱标签，提供更准确的类别和解剖信息。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation</title>
      <link href="/post/pixel-wise-reclassification-with-prototypes-for-enhancing-weakly-supervised-semantic-segmentation/"/>
      <url>/post/pixel-wise-reclassification-with-prototypes-for-enhancing-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Refining the seed region to obtain finely annotated <strong>pseudo masks</strong> for training a segmentation model is a crucial step in the multi-stage weakly supervised semantic segmentation (WSSS) framework. One of the most popular refinement methods, IRN, extends seed regions towards the edges in the image. However, we observed that, due to the lack of guidance from semantic information, IRN’s refinement may lead the generation of partially erroneous refinement directions. To address this issue, we leverage prototypes<br>to recover the overlooked category semantic information in the refinement stage. We propose a prototype-based pseudo mask reclassification post-processing (PtReCl) to correct misclassified pixels in the pseudo masks, generating refined pseudo masks with more accurate coverage. Experimental evaluations demonstrate that our post-processing approach brings improvements in both pseudo mask quality and segmentation results on PASCAL VOC and MS COCO datasets, achieving state-of-the-art performance on VOC.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>在多阶段弱监督语义分割(WSSS)框架中，对种子区域进行细化以获得精细标注的<strong>伪掩膜</strong>用于训练分割模型是至关重要的一步。最流行的细化方法之一是IRN，它将种子区域向图像的边缘扩展。然而，我们观察到，由于缺乏语义信息的指导，IRN的细化可能导致部分错误的细化方向的产生。为了解决这个问题，我们利用原型来恢复细化阶段中被忽略的类别语义信息。我们提出了一种基于原型的伪掩码重分类后处理(PtReCl)来纠正伪掩膜中的错误分类像素，生成更精确覆盖的精细伪掩膜。实验评估表明，我们的后处理方法改善了<strong>PASCAL VOC</strong>和<strong>MS COCO</strong>数据集的伪掩码质量和分割结果，实现了最先进的VOC性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法在生成伪掩码时存在的问题，具体研究背景如下：</p><ul><li><strong>WSSS的目标与流程</strong>：WSSS旨在利用图像级标注数据集完成像素级分类任务，以降低数据标注成本。当前主流方法遵循三阶段流程，其中生成高质量伪掩码对最终分割模型的性能至关重要。 </li><li><strong>现有方法的局限性</strong>：最常用的细化方法IRN在细化种子区域时，因缺乏语义信息指导，可能导致部分错误的细化方向，产生大量错误的伪掩码。</li><li><strong>原型学习的潜力</strong>：近年来，研究发现原型学习可助力语义分割，它能从少量类样本中归纳特定类别的特征，实现特征的像素级分类，还能保留更多非学习参数以预测多样特征。 </li><li><strong>本文的研究动机</strong>：基于上述背景，作者提出基于<strong>原型的伪掩码重分类后处理方法</strong>（<strong>PtReCl</strong>），利用原型的类别区分性恢复伪掩码中误分类的像素，以提高伪掩码质量和分割性能。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多阶段WSSS框架</strong>：主流方法分三步，先训练分类模型生成种子区域，再用细化方法生成伪掩码，最后用伪掩码训练全监督语义分割模型。</li><li><strong>CAM方法</strong>：解决CAM作为种子区域时前景覆盖不足问题，如采用擦除、对抗学习、利用ViT上下文建模等方法。</li><li><strong>细化方法</strong>：主要分为利用显著性检测和随机游走与语义亲和两类，部分方法还借助Transformer中的注意力矩阵。</li><li><strong>原型学习</strong>：在语义分割中，部分研究将原型用于对比学习或自监督学习，部分用原型替换分类器结构。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>多阶段弱监督语义分割框架中，细化种子区域以获得精细注释的伪掩码是训练分割模型的关键步骤。现有流行的细化方法IRN在细化过程中缺乏语义信息的引导，可能导致部分错误的细化方向。为解决这一问题，作者提出了<strong>PtReCl</strong>方法。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-21-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-21-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-21-22"></p><h3 id="模型流程"><a href="#模型流程" class="headerlink" title="模型流程"></a>模型流程</h3><ol><li><strong>种子区域获取</strong>：利用原始的类激活映射（Class Activation Maps, CAM）方法获取种子区域。训练分类模型后，丢弃分类器中的全局平均池化（Global Average Pooling, GAP）层，直接在原始特征图上进行预测，忽略负预测分数并归一化生成CAM。</li><li><strong>伪掩码生成</strong>：使用IRN方法对种子区域进行细化，生成伪掩码。</li><li><strong>伪掩码恢复网络</strong>：参考Deeplab的结构构建伪分割网络，以伪掩码作为像素级注释，通过空洞空间金字塔池化层（Atrous Spatial Pyramid Pooling, ASPP）提取图像特征并获得像素级预测结果。引入标签条件策略（Label Conditioning strategy），根据图像级类别注释保留相关通道，丢弃无关通道，以减轻无关通道对后续原型准确性的影响。</li><li><strong>前景 - 背景原型获取</strong>：依次遍历训练集图像，使用骨干网络提取特征。对于伪掩码中每个类别的前景区域，收集其对应特征到前景特征集；对于非该类别区域，收集其对应特征到背景特征集。使用余弦距离作为度量，采用K - means聚类方法为每个类别获取多个前景和背景原型。</li><li><strong>多原型像素级重新分类</strong>：使用伪掩码恢复网络的骨干提取图像特征，利用特定类别的前景和背景原型对像素特征的语义信息进行重新分类。计算每个位置与前景 - 背景原型的余弦相似度，对相似度进行降序排序，选择前m个距离参与像素分类计算，生成像素级重新分类图。</li><li><strong>重新细化</strong>：将重新分类图替换IRN中的CAM，再次使用IRN进行细化，增强其边缘信息，得到后处理的伪掩码。</li><li><strong>全监督语义分割</strong>：使用后处理的伪掩码训练全监督语义分割模型，如DeeplabV2和UperNet - Swin。</li></ol><h3 id="模型贡献"><a href="#模型贡献" class="headerlink" title="模型贡献"></a>模型贡献</h3><ul><li><strong>解决分类错误</strong>：提出PtReCl后处理方法，利用原型的类别区分性，通过前景 - 背景特征恢复伪掩码中误分类的像素。</li><li><strong>多原型分类</strong>：设计多原型像素级分类方法，利用伪分割网络重建伪掩码并通过聚类方法获取原型，缓解不同类别有效原型数量的差异，获得准确的重新分类图。</li><li><strong>实验验证</strong>：在PASCAL VOC和MS COCO数据集上进行了广泛实验，结果表明PtReCl方法能有效提高伪掩码的准确性，从而提升分割性能，在VOC数据集上取得了最先进的结果。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>PASCAL VOC 2012</strong>、<strong>MS COCO 2014</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-25-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-25-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-25-57"></p><ul><li><strong>伪掩码增强</strong>：与一些先进的WSSS方法相比，经PtReCl处理后的伪掩码在VOC上提升了8.4%，在COCO上提升了3.4%，在VOC上取得了最佳性能，在COCO上也有出色表现。</li><li><strong>分割性能提升</strong>：在使用DeepLab作为全监督分割方法的VOC实验中，PtReCl在两种常用预训练ResNet101骨干网络下均取得了最先进的结果。在基于Transformer的分割方法中，使用UperNet - Swin作为骨干网络时，PtReCl也达到了最先进的性能。在COCO上，尽管受噪声影响，PtReCl仍优于除AMN和LPCAM外的其他方法，与基线IRN相比，在验证集上提升了2.2%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-26-36.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-26-36.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-26-36"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>有效性验证</strong>：PtReCl在VOC和COCO上分别将伪掩码的mIoU提高了8.4%和3.4%。通过对比不使用原型和使用不同数量原型时的像素级分类结果，验证了多原型像素级分类方法的有效性，当M设为[10, 15, 20]时，重分类图的mIoU最高可达70%。</li><li><strong>原型数量影响</strong>：研究了调整每个类别的原型数量K（范围从2到30）对重分类效果的影响。重分类图的mIoU随原型数量增加先上升后稳定，最终将类中心数量设为20。在10到30的范围内，重分类图的mIoU波动仅在1%以内，表明在合理范围内改变原型数量对重分类效果影响不大。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-20.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-20.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-28-20"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-28-28"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-28-33"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者指出广泛使用的<strong>WSSS</strong>方法IRN在细化策略上存在局限，它在不考虑特定像素级语义信息的情况下将种子区域向图像边缘扩展，导致部分错误细化。基于现有的WSSS三阶段框架，作者引入了基于原型的重分类后处理方法，以纠正伪掩码中的像素错误分类，得到更精确的后处理伪掩码。 通过在<strong>VOC和COCO</strong>数据集上的大量实验，结果表明该后处理阶段有效提高了伪掩码的质量和分割模型的性能，在VOC数据集上取得了最先进的成果。 </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>弱监督语义分割</title>
      <link href="/post/20250412-ruo-jian-du-yu-yi-fen-ge/"/>
      <url>/post/20250412-ruo-jian-du-yu-yi-fen-ge/</url>
      
        <content type="html"><![CDATA[<p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-29-41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-29-41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-29-41"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-30-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-30-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-30-22"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-31-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-31-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-31-13"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-32-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-32-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-32-28"></p><p>CAM：图像中的那些像素点对类别的响应比较高（粗糙的伪标签）</p><p>种子区域：粗糙的伪标签中<strong>置信度比较高的区域</strong>，比如某一个区域认为就是猫或狗</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-37-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-37-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-37-57"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-40-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-40-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-40-23"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-42-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-42-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-42-58"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-16-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-16-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_20-16-09"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-24-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-24-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_20-24-05"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/knowledge-transfer-with-simulated-inter-image-erasing-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/knowledge-transfer-with-simulated-inter-image-erasing-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>南京理工大学、地平线机器人</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Though <strong>adversarial erasing</strong> has prevailed in <strong>weakly supervised semantic segmentation</strong> to help activate integral object regions, existing approaches still suffer from the dilemma of under-activation and over-expansion due to the difficulty in determining when to stop erasing. In this paper, we propose a Knowledge Transfer with Simulated Inter-Image Erasing (KTSE) approach for weakly supervised semantic segmentation to alleviate the above problem. In contrast to existing erasing-based methods that remove the discriminative part for more object discovery, we propose a simulated inter-image erasing scenario to weaken the original activation by introducing extra object information. Then, object knowledge is transferred from the anchor image to the consequent less activated localization map to strengthen network localization ability. Considering the adopted bidirectional alignment will also weaken the anchor image activation if appropriate constraints are missing, we propose a self-supervised regularization module to maintain the reliable activation in discriminative regions and improve the inter-class object boundary recognition for complex images with multiple categories of objects. In addition, we resort to intra-image erasing and propose a multi-granularity alignment module to gently enlarge the object activation to boost the object knowledge transfer. Extensive experiments and ablation studies on PASCAL VOC 2012 and COCO datasets demonstrate the superiority of our proposed approach. Source codes and models are available at <a href="https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE">https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>尽管<strong>对抗擦除</strong>在弱监督语义分割中很流行，以帮助激活完整的目标区域，但由于难以确定何时停止擦除，现有的方法仍然存在<strong>激活不足和过度扩展</strong>的困境。在本文中，我们提出了一种基于模拟图像间擦除(KTSE)的弱监督语义分割方法来缓解上述问题。与现有的基于擦除的方法不同，我们提出了一种模拟图像间擦除场景，通过引入额外的目标信息来削弱原始激活。然后，将目标知识从锚点图像转移到随后激活程度较低的定位图中，以增强网络定位能力。考虑到如果缺少适当的约束条件，所采用的双向对齐也会削弱锚点图像的激活，我们提出了一种自监督正则化模块，以保持在判别区域的可靠激活，并改善具有多类别物体的复杂图像的类间物体边界识别。此外，我们采用图像内擦除的方法，并提出了一种多粒度对齐模块来温和地放大目标激活，以促进目标知识的转移。在PASCAL VOC 2012和COCO数据集上进行的大量实验和烧蚀研究证明了我们提出的方法的优越性。源代码和模型可在<a href="https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割在深度学习时代取得了巨大进展，广泛应用于<strong>自动驾驶和图像编辑等领域</strong>。但深度学习模型训练依赖大量标注图像，收集精确的像素级标注耗时耗力。因此，弱监督学习作为减轻标注负担的方向，受到众多研究者关注。 本文聚焦于图像级标签监督下的弱监督语义分割（WSSS）。当前WSSS通常遵循将图像标签转换为像素级粗标签、细化伪标签、用细化标签训练最终分割模型的三步流程。在分割标签生成方面，类激活图（CAM）技术是目标定位的主流范式，但朴素CAM只能突出对象最具判别性的区域，激活小且稀疏，导致对象挖掘不完整。 为解决这一问题，许多工作致力于扩展CAM激活以生成高质量伪标签，其中对抗擦除是主流方法之一。然而，现有的基于对抗擦除的方法难以确定何时停止擦除，过度擦除会导致过度扩展，擦除不足则会导致激活不足。因此，本文提出了一种基于模拟图像间擦除的知识转移（KTSE）方法，以缓解上述问题。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：以图像级标签作为弱监督进行语义分割是热门方向，通常采用<strong>类激活图（CAM）技术定位目标对象</strong>，将图像级标签转化为像素级标注。为扩大CAM激活区域，研究者采用了多种方法，如扩张卷积、对比学习、自监督学习、利用跨图像信息等。</li><li><strong>基于擦除的方法</strong>：通过掩盖训练图像中的区域，迫使网络寻找其他相关部分，以扩大CAM激活区域。其中，对抗擦除方法通过掩盖最具判别性的区域，展现出更有潜力的激活扩展效果。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><h3 id="网络结构图"><a href="#网络结构图" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-25-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-25-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-11_09-25-53"></p><p>本文提出了一种用于弱监督语义分割的模拟图像间擦除知识转移（Knowledge Transfer with Simulated Inter - Image Erasing，KTSE）方法，以缓解现有基于对抗擦除方法的过度扩展和激活不足问题。以下是该模型的详细介绍： </p><h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p>训练一个带有给定图像级弱标签的分类网络，由骨干特征提取器和池化分类头组成。模拟图像间擦除场景，通过拼接配对图像引入额外对象信息，将对象知识从锚图像转移到后续激活较低的定位图，以增强网络的对象定位能力。同时，提出了自监督正则化模块和多粒度对齐模块。</p><h3 id="具体模块"><a href="#具体模块" class="headerlink" title="具体模块"></a>具体模块</h3><ol><li><strong>类激活图（CAM）生成</strong>    <ul><li><strong>网络架构调整</strong>：参考ACoL移除最终全连接层，将骨干网络输出通道设置为C + 1（C为前景类别数，1为背景），直接从类感知CAM特征F生成对象定位图。    </li><li><strong>CAM计算</strong>：对于每个前景类c，将注意力图Fc输入ReLU层，然后归一化到0到1的范围，即$A ^{c} &#x3D;\frac {ReLU\left ( F^{c} \right )}{\max \left ( F^{c} \right )}$。   </li><li><strong>池化头与损失函数</strong>：采用门控金字塔池化（GPP）层作为最终池化头，使用多标签软边缘损失训练分类网络，损失函数为$\mathcal {L} _{cls}&#x3D;-\frac {1}{C} \sum _{ c&#x3D;1}^{C} y^{c} \log \sigma \left (q^{c}\right )+\left (1-y^{c}\right ) \log \left [1-\sigma \left (q^{c}\right )\right ]$，其中$\sigma (·)$是sigmoid函数，$y^{c}$是第c类的图像级标签。</li></ul></li><li><strong>模拟图像间擦除（SIE）</strong>    <ul><li><strong>场景设计</strong>：与现有擦除方法不同，通过拼接锚图像和配对图像创建更大的合成图像，将锚图像视为被擦除的图像。引入额外对象信息使锚图像中突出的对象区域减少，然后将锚图像的对象知识转移到激活较低的合成图像锚部分，以增强网络的对象定位能力。   </li><li><strong>知识转移损失</strong>：损失函数为$\mathcal {L} <em>{kt} &#x3D; ReLU \left ( \hat {F</em>{a} } - \hat {F_{s} } \right )$，其中$\hat {F_{a} } &#x3D;CFE\left ( F_{a}, y \right )$，$\hat {F_{s} } &#x3D;CFE\left ( F_{s}, y \right )$，$F_{a}$和$F_{s}$分别表示锚分支和模拟分支的CAM特征，CFE表示类特征提取。</li></ul></li><li><strong>自监督正则化（SSR）</strong>    <ul><li><strong>问题提出</strong>：由于知识转移是双向的，学习模拟分支的稀疏激活会削弱锚分支的对象挖掘。为保持锚分支在判别区域的可靠激活，提出自监督正则化模块。   </li><li><strong>伪标签生成</strong>：使用两个阈值$\beta_{h} &#x3D; 0.3$和$\beta_{l} &#x3D; 0.15$定位置信前景和背景，生成伪标签$\hat {Y} _{i,j}$。   </li><li><strong>损失函数</strong>：使用交叉熵损失$\mathcal {L} _{ce}$直接监督CAM特征的学习，同时设计类间损失$\mathcal {L} _{inter}$鼓励复杂图像中多个前景类的激活一致性，以提高类间对象边界的识别能力。</li></ul></li><li><strong>多粒度对齐（MGA）</strong>   <ul><li><strong>问题提出</strong>：自监督正则化模块虽能促进知识转移，但网络对象定位能力的提升受锚CAM质量限制，因此采用传统的图像内擦除并提出多粒度对齐模块。  </li><li><strong>全局对齐</strong>：将锚特征$F_{a}$和掩码特征$F_{m}$输入类特征提取模块，采用全局平均池化（GAP）操作获得每个分支的最终类置信度，设计图像级全局对齐损失$\mathcal {L} _{global}$。   </li><li><strong>局部对齐</strong>：利用像素级局部激活对齐将擦除图像中新发现的对象信息转移到锚分支，损失函数为$\mathcal {L} <em>{local}&#x3D; ReLU\left ( \hat {F</em>{m}} - \hat {F_{a}} \right )$。</li></ul></li></ol><h3 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h3><p>整体训练损失为$\mathcal {L}&#x3D;\mathcal {L} <em>{cls} + \mathcal {L}</em>{kt} + \mathcal {L}<em>{global} + \mathcal {L}</em>{local} + \mathcal {L}_{ce} + \lambda <em>{inter}\mathcal {L}</em>{inter}$，其中$\lambda _{inter} &#x3D; 0.005$是控制类间损失权重的超参数。 </p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：<strong>PASCAL VOC 2012 (20+1)、COCO 2014 (80+1)</strong></p></blockquote><ul><li><strong>伪掩膜的准确性</strong>：KTSE方法的分割种子mIoU达到67.0%，比IRN的基线提高了18.7%，超过了当前最优方法FPR 3.2%；经过IRN进一步细化后，生成的伪掩码mIoU达到73.8%，超过了先前的SOTA方法AEFT和ACR超过1.5%。</li><li><strong>PASCAL VOC 2012分割图的准确性</strong>：使用VGG骨干网络时，KTSE方法在验证集和测试集上的性能分别为67.3%和67.0%，优于仅使用图像级标签的其他现有技术方法，并且与许多依赖显著性图的方法具有竞争力；使用ResNet骨干网络时，验证集和测试集的结果分别提高到73.0%和72.9%，优于最近的SOTA方法，例如在测试集上比OCR和ACR高约1%。</li><li><strong>COCO分割图的准确性</strong>：使用VGG骨干网络时，KTSE方法的mIoU达到37.2%，远优于仅使用图像级标签的先前方法，例如比CONTA高13.5% mIoU，并且与具有额外显著性指导的先前SOTA方法具有竞争力；使用ResNet骨干网络时，KTSE方法达到了45.9% mIoU的最佳结果，分别比ACR和BECO高0.6%和0.8% mIoU。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-33-16.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-33-16.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-11_09-33-16"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>逐元素组件分析</strong>：通过实验验证了KTSE方法中各个组件对提高伪掩码质量的贡献。模拟图像间擦除（SIE）模块可将分割种子的准确率从基线的54.2%提高到57.1%；自监督正则化（SSR）模块可使mIoU达到57.4%；SIE和SSR的组合可将性能显著提升至63.8% mIoU；多粒度对齐（MGA）模块最终使伪掩码的mIoU达到67.0%。</li><li><strong>MGA与先前基于擦除的方法比较</strong>：MGA模块采用来自锚分支的软类置信度知识来指导掩码图像的温和激活扩展。实验表明，CAM特征的全局对齐可将基线从54.2%提高到58.7%，优于ACoL中的刚性分类指导和AEFT中采用的GPP特征对齐；加上像素级局部对齐后，MGA模块最终将性能提高到60.2%，证明了其温和对齐策略相对于先前基于擦除方法的优势。</li><li><strong>SIE现象分析</strong>：基于经典对抗擦除方法的观察，引入额外的判别性目标信息会使原始高激活区域变得不那么具有判别性并降低激活度，通过从原始CAM学习可以增强这种减弱的激活。当网络学会增加对拼接图像中不那么具有判别性区域的关注时，也会学会激活原始图像中不那么具有判别性的目标区域以定位更多目标。</li><li><strong>SIE与数据增强比较</strong>：虽然像CutMix这样的数据增强方法也会改变锚图像并导致激活扰动，但它们不能保证像SIE那样使拼接图像的锚部分激活减弱（可能导致过度扩展）。SIE的新颖之处在于构建了模拟图像间擦除场景，通过从锚分支的目标知识中学习来提高后续激活减弱的注意力图，从而增强网络的定位能力。实验表明，KTSE方法在VOC和COCO数据集上的性能显著优于基于CutMix的数据增强方法CDA。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于弱监督语义分割的<strong>知识转移</strong>与<strong>模拟图像间擦除</strong>（<strong>KTSE</strong>）方法，并得出以下结论：</p><ol><li>与现有对抗擦除方法不同，KTSE模拟图像间擦除场景，添加额外对象信息，增强网络目标定位能力，缓解过扩展问题。 </li><li>提出自监督正则化模块，维持判别区域可靠激活，提升复杂图像类间目标边界识别能力。 </li><li>提出多粒度对齐模块，通过图像级全局对齐和像素级局部对齐扩大目标激活，促进知识转移。</li><li>在PASCAL VOC 2012和COCO数据集上的大量实验和消融研究表明，KTSE方法优于现有方法，能有效缓解过扩展和激活不足问题。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SFC Shared Feature Calibration in Weakly Supervised Semantic Segmentation</title>
      <link href="/post/sfc-shared-feature-calibration-in-weakly-supervised-semantic-segmentation/"/>
      <url>/post/sfc-shared-feature-calibration-in-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Image-level weakly supervised semantic segmentation has received increasing attention due to its low annotation cost.<br>Existing methods mainly rely on <strong>Class Activation Mapping (CAM)</strong> to obtain pseudo-labels for training semantic segmentation models. In this work, we are the first to demonstrate that long-tailed distribution in training data can cause the CAM calculated through classifier weights over-activated for head classes and under-activated for tail classes due to the shared features among head- and tail- classes. This degrades pseudo-label quality and further influences final semantic segmentation performance. To address this issue, we propose a Shared Feature Calibration (SFC) method for CAM generation. Specifically, we leverage the class prototypes that carry positive shared features and propose a Multi-Scaled Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the CAMs generated through classifier weights and class prototypes during training. The MSDW loss counterbalances over-activation and under-activation by calibrating the shared features in head-&#x2F;tail-class classifier weights. Experimental results show that our SFC significantly improves CAM boundaries and achieves new state-of-the-art performances. The project is available at <a href="https://github.com/Barrett-python/SFC">https://github.com/Barrett-python/SFC</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>图像级弱监督语义分割因其标注成本低而受到越来越多的关注。现有方法主要依靠类激活映射(Class Activation Mapping, CAM)获取伪标签，用于训练语义分割模型。在这项工作中，我们首次证明了训练数据中的长尾分布会导致通过分类器权重计算的CAM由于头类和尾类之间的共享特征而对头类过度激活而对尾类激活不足。这降低了伪标签的质量，并进一步影响最终的语义分割性能。为了解决这一问题，我们提出了一种用于CAM生成的共享特征校准(SFC)方法。具体来说，我们利用带有正共享特征的类原型，并提出了多尺度分布加权(MSDW)一致性损失，以缩小训练期间通过分类器权重和类原型生成的cam之间的差距。MSDW损失通过校准头&#x2F;尾类分类器权重中的共享特征来平衡过度激活和欠激活。实验结果表明，我们的SFC显著改善了CAM边界，实现了新的最先进的性能。该项目可在<a href="https://github.com/Barrett-python/SFC%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Barrett-python/SFC上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p><strong>图像级弱监督语义分割（WSSS）<strong>因标注成本低而备受关注，现有方法多依赖</strong>类激活映射</strong>（CAM）生成伪标签来训练语义分割模型。然而，研究发现WSSS的训练数据呈长尾分布，这使得头类和尾类之间的共享特征成分在头类分类器权重中倾向于为正，在尾类分类器权重中倾向于为负。 具体而言，头类权重接收的正梯度多于负梯度，尾类权重则相反。这导致包含共享特征的像素被头类分类器权重激活，而包含尾类特征的像素未被尾类权重激活，使得通过分类器权重计算的CAM对头部类过度激活，对尾部类激活不足，进而降低了伪标签的质量，影响了最终的WSSS性能。 目前，尚未有工作针对长尾分布训练数据导致的过激活和欠激活问题进行研究。因此，本文旨在分析该问题产生的原因，并提出共享特征校准（SFC）方法来解决这一问题，以提高CAM质量和WSSS性能。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：伪标签生成基于注意力映射，关键在于生成高质量的<strong>类激活映射</strong>（CAM）。已有方法采用启发式策略、自监督学习、对比学习等生成CAM，还使用<strong>CRF、IRN等方法对初始映射进行细化</strong>。此外，视觉-语言预训练成为解决下游视觉 - 语言任务（包括WSSS）的流行方法。</li><li><strong>分类中的共享特征</strong>：分类是语义分割的上游任务，现有方法多提取判别性部分特征进行分类，避免共享特征影响分类性能。而WSSS不能仅依赖判别性特征构建完整CAM，部分方法冻结预训练编码器的若干层以避免遗忘非判别性特征。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_13-57-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_13-57-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_13-57-44"></p><p>本文提出了一种名为<strong>共享特征校准</strong>（<strong>Shared Feature Calibration，SFC</strong>）的方法，用于解决弱监督语义分割（Weakly Supervised Semantic Segmentation，WSSS）中因训练数据<strong>长尾分布</strong>导致的类激活映射（Class Activation Mapping，CAM）过激活和欠激活问题。以下是该模型的详细介绍：</p><ol><li><strong>模型背景</strong>：图像级弱监督语义分割因标注成本低而受到广泛关注。现有方法主要依赖CAM获取伪标签来训练语义分割模型，但训练数据的长尾分布会使头类别的CAM过激活，尾类别的CAM欠激活，从而降低伪标签质量，影响最终分割性能。 </li><li><strong>模型结构</strong>    - <strong>图像库重采样（Image Bank Re-sampling，IBR）</strong>：维护一个图像库，存储每个前景类别的最新图像。在训练时，从图像库中均匀采样图像并与原始训练批次拼接，以增加尾类别样本的采样频率，有效校准尾类别分类器权重中的共享特征。   <ul><li><strong>多尺度分布加权一致性损失（Multi-Scaled Distribution-Weighted，MSDW）</strong>：提出两个分布加权一致性损失$L_{P_{DW}}$和$L_{W_{DW}}$，分别用于缩小原型CAM和分类器权重CAM之间的差距，以及缩小原始图像和下采样图像的分类器权重CAM之间的差距。通过计算缩放分布系数$DC_c$对一致性损失进行重新加权，使总需求较高的类别分配更高的一致性损失。</li></ul></li><li><strong>模型推理</strong>：最终的CAM通过将分类器权重CAM和原型CAM相加得到，即$(M_{final})<em>{\tilde{c}} &#x3D; (M</em>{W}(F, W, I))<em>{\tilde{c}} + (M</em>{P}(F, P))_{\tilde{c}}$，共同解决过激活和欠激活问题。 </li><li><strong>实验结果</strong>：在PASCAL VOC 2012和MS COCO 2014两个基准数据集上进行实验，结果表明SFC方法显著提高了CAM的边界质量，在弱监督语义分割任务中取得了新的最先进性能。 综上所述，SFC方法通过图像库重采样和多尺度分布加权一致性损失，有效解决了长尾分布下共享特征导致的CAM过激活和欠激活问题，提高了弱监督语义分割的性能。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：PASCAL VOC 2021、MS COCO 2014</p></blockquote><ul><li><strong>伪标签质量比较</strong>：评估伪标签生成过程中中间和最终结果的质量。比较分类模型生成的初始CAM、经过CRF和IRN后处理的CAM。实验结果表明，SFC生成的CAM明显优于以往方法，在PASCAL VOC数据集上，SFC的CAM比现有方法高2.6%；经过CRF处理后mIoU达到69.4%，再经过IRN处理后mIoU提高到73.7%，比AMN方法高1.5%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-01-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-01-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-01-54"></p><ul><li><strong>弱监督语义分割性能比较</strong>：将经过CRF和IRN后处理的伪掩码作为真实标签，以全监督方式训练语义分割模型。在PASCAL VOC 2012的验证集和测试集上，使用ImageNet预训练骨干网络，SFC方法的mIoU分别达到71.2%和72.5%，优于仅使用图像级标签或同时使用图像级标签和显著性图的其他弱监督语义分割方法。在MS COCO 2014验证集上，使用ResNet101骨干网络，SFC方法的mIoU达到46.8%，比AMN方法高2.1%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-02-00.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-02-00.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-02-00"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-03-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-03-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-03-21"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>SFC组件有效性验证</strong>：验证图像库重采样（IBR）和多尺度分布加权（MSDW）一致性损失（包括LP DW和LW DW）的有效性。结果表明，IBR可提高分类器权重CAM的mIoU，增加尾部类别的采样频率能提升LMSDW的有效性；LW DW可增强LP DW带来的性能提升，但单独使用LW DW无法校准下采样特征空间中的共享特征，性能会显著下降。</li><li><strong>DC系数有效性研究</strong>：研究式（6）中DC系数的有效性。结果显示，DC系数能有效调整每个类别的一致性损失权重，带来显著的性能提升。</li><li><strong>CAM组合性能比较</strong>：比较推理时单独使用MW、MP和组合使用Mfinal的性能。结果表明，组合使用Mfinal的性能最高，说明在SFC中用MP补充MW效果更好。</li><li><strong>不同类别集性能增益分析</strong>：分析有无DC系数时不同类别集的平均性能增益。结果显示，使用DC系数时，头部和尾部类别能获得更多的mIoU增益。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-04-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-04-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-04-01"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者得出以下结论：</p><ol><li>首次指出在长尾场景下，<strong>头类和尾类的共享特征</strong>会使头类的分类器权重生成的类激活映射（CAM）扩大，尾类的CAM缩小，导致伪标签质量下降，影响弱监督语义分割（WSSS）最终性能。</li><li>提出共享特征校准（SFC）方法，通过图像库重采样（IBR）和多尺度分布加权（MSDW）一致性损失，平衡不同分类器权重中的共享特征比例，避免共享特征导致的<strong>过激活和欠激活</strong>问题。</li><li>实验表明，SFC显著改善了CAM边界，在<strong>Pascal VOC 2012</strong>和<strong>MS COCO 2014</strong>数据集上仅使用图像级标签就取得了新的最优WSSS性能，为提高图像级弱监督语义分割中CAM的准确性提供了新视角，未来将探索其他可能的解决方案。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WeakCLIP Adapting CLIP for Weakly-Supervised Semantic Segmentation</title>
      <link href="/post/weakclip-adapting-clip-for-weakly-supervised-semantic/"/>
      <url>/post/weakclip-adapting-clip-for-weakly-supervised-semantic/</url>
      
        <content type="html"><![CDATA[<p>华中科技大学、西北工业大学</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Contrastive language and image pre-training (CLIP) achieves great success in various computer vision tasks and also presents<br>an opportune avenue for enhancing weakly-supervised image understanding with its large-scale pre-trained knowledge. As<br>an effective way to reduce the reliance on pixel-level human-annotated labels, weakly-supervised semantic segmentation<br>(WSSS) aims to refine the class activation map (CAM) and produce high-quality pseudo masks. Weakly-supervised semantic<br>segmentation (WSSS)aims to refine the class activationmap(CAM)as pseudo masks, but heavily relies on inductive biases like<br>hand-crafted priors and digital image processing methods. For the vision-language pre-trained model, i.e. CLIP, we propose a<br>novel text-to-pixel matching paradigm forWSSS.However, directly applying CLIP toWSSS is challenging due to three critical<br>problems: (1) the task gap between contrastive pre-training and WSSS CAM refinement, (2) lacking text-to-pixel modeling to<br>fully utilize the pre-trained knowledge, and (3) the insufficient details owning to the 1&#x2F;16 down-sampling resolution ofViT. Thus,<br>we proposeWeakCLIP to address the problems and leverage the pre-trained knowledge from CLIP toWSSS. Specifically, we<br>first address the task gap by proposing a pyramid adapter and learnable prompts to extract WSSS-specific representation. We<br>then design a co-attention matching module to model text-to-pixel relationships. Finally, the pyramid adapter and text-guided<br>decoder are introduced to gather multi-level information and integrate it with text guidance hierarchically.WeakCLIP provides<br>an effective and parameter-efficient way to transfer CLIP knowledge to refine CAM. Extensive experiments demonstrate that<br>WeakCLIP achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 74.0% mIoU on the val set of<br>PASCAL VOC 2012 and 46.1% mIoU on the val set of COCO 2014. The source code and model checkpoints are released<br>at <a href="https://github.com/hustvl/WeakCLIP">https://github.com/hustvl/WeakCLIP</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>对比语言和图像预训练(CLIP)在各种计算机视觉任务中取得了巨大的成功，并且利用其大规模的预训练知识为增强弱监督图像理解提供了一个很好的途径。弱监督语义分割(WSSS)是一种减少对像素级人工标注标签依赖的有效方法，其目的是细化类激活图(CAM)并生成高质量的伪掩膜。弱监督语义分割(WSSS)旨在将类激活图(CAM)细化为伪掩膜，但严重依赖于手工制作先验和数字图像处理方法等归纳偏差。对于视觉语言预训练模型，即CLIP，我们提出了一种新的文本到像素的wsss匹配范式。然而，直接应用CLIP toWSSS是具有挑战性的，因为存在三个关键问题:(1)对比预训练与WSSSCAM细化之间的任务差距;(2)缺乏文本到像素的建模以充分利用预训练的知识;(3)由于vit的下采样分辨率为1 16，细节不足。因此，我们提出了weakclip来解决问题，并利用CLIP toWSSS的预训练知识。具体来说，我们首先通过提出金字塔适配器和可学习的提示词符来提取特定于wss的表示来解决任务差距。然后，我们设计了一个共同关注匹配模块来模拟文本到像素的关系。最后，引入金字塔适配器和文本引导解码器，实现多级信息采集，并与文本引导分层集成。WeakCLIP提供了一种有效的、参数高效的方法来传递CLIP知识以改进CAM。大量的实验表明，WeakCLIP在标准基准测试上达到了最先进的WSSS性能，即在PASCAL VOC 2012的val集上达到了74.0%的mIoU，在COCO 2014的val集上达到了46.1%的mIoU。源代码和模型检查点在<a href="https://github.com/hustvl/WeakCLIP%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/hustvl/WeakCLIP上发布。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）任务，旨在解决当前方法在处理类激活图（CAM）种子时面临的问题，具体研究背景如下： </p><ul><li><strong>WSSS的重要性与挑战</strong>：语义分割中像素级标注耗时费力，限制了实际应用。WSSS利用弱监督信息生成伪像素级分割，可减轻标注负担，但仅使用图像级标签的WSSS是该领域最具挑战性的方向。</li><li><strong>现有CAM细化方法的局限性</strong>：现有方法多依赖手工先验和改进的数字图像处理算法来细化CAM，这些方法存在归纳偏差，限制了性能和鲁棒性。 </li><li><strong>CLIP的潜力与应用挑战</strong>：CLIP在计算机视觉任务中取得了巨大成功，为WSSS带来了新的机遇。然而，直接将CLIP应用于WSSS存在任务差距、缺乏文本到像素建模以及细节不足等问题。 基于以上背景，作者提出了WeakCLIP方法，旨在利用CLIP的预训练知识，通过文本到像素匹配范式解决WSSS中的关键问题，提高伪掩码的质量，从而推动WSSS的发展。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：为减轻像素级标注负担，出现多种基于不同弱监督信息（如<strong>边界框、涂鸦、点、图像级标签</strong>）的算法。其中，基于<strong>图像级标签</strong>的WSSS最具挑战性，常使用类激活图（CAM）定位目标，但原始CAM噪声大、易出错，已有多种方法对其进行优化。</li><li><strong>大规模预训练模型</strong>：大规模预训练模型在各领域广泛应用，如CLIP通过对比学习在大量图像-文本对上预训练，展现出强大的知识迁移能力。已有研究尝试将CLIP应用于WSSS，如CLIMS引入辅助损失，CLIP - ES利用文本提示和GradCAM提升CAM质量。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-44-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-44-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-09_08-44-06"></p><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>本文提出了一种名为<strong>WeakCLIP的弱监督语义分割（WSSS）方法</strong>，旨在利用预训练的CLIP模型知识来改进WSSS网络的类激活图（CAM）细化过程。以下是WeakCLIP模型的详细介绍： </p><ol><li><p><strong>文本到像素匹配范式</strong>：与以往基于CLIP的WSSS方法不同，WeakCLIP提出了<strong>文本到像素匹配</strong>范式，以在像素级别查询相似度。具体而言，输入图像通过CLIP预训练的ViT - B网络提取多层特征图，经过投影层后，定义文本到像素匹配操作，得到文本到像素匹配的嵌入。 </p></li><li><p><strong>WeakCLIP框架</strong> </p><ul><li><strong>可学习提示（Learnable Prompt）</strong>：受CoOp和CLIP - Adapter启发，提出可学习嵌入作为自适应提示。将类文本标记并嵌入为类文本嵌入，与随机初始化的可学习嵌入拼接，作为文本编码器的输入，最终投影得到文本嵌入。  </li><li><strong>金字塔适配器（Pyramid Adapter）</strong>：为解决CLIP视觉编码器专注于整体图像内容以及低分辨率问题，提出金字塔适配器。它独立于CLIP图像编码器，对不同分辨率的特征图进行处理，通过上采样和下采样操作，生成不同分辨率的特征，有效融合低级细节和高级表示。   </li><li><strong>协同注意力匹配模块（Co - attention Matching）</strong>：为充分利用CLIP预训练知识，提出协同注意力匹配模块，用于建模双向文本到像素匹配。该模块使用两个交叉注意力模块分别建模文本到像素和像素到文本的关系，并通过残差连接更新文本和图像嵌入，最后进行文本到图像匹配得到协同注意力匹配的嵌入。  </li><li><strong>文本引导解码器（Text - Guided Decoder）</strong>：为解决CLIP ViT - B的分辨率限制问题，引入文本引导解码器。将协同注意力匹配的嵌入插值到与适配器输出特征对应的大小，与适配器输出特征拼接后进行解码，得到分割预测。    </li><li><strong>WSSS损失（WSSS Losses）</strong>：采用DSRG中使用的WSSS损失，包括平衡种子损失和边界损失。平衡种子损失计算分割预测与CAM种子之间的加权交叉熵损失；边界损失先使用条件随机场（CRF）处理分割预测以细化对象边界，然后计算CRF细化结果与分割预测之间的Kullback - Leibler散度损失。</li></ul></li><li><p><strong>伪掩码生成和再训练</strong>：使用训练好的WeakCLIP网络生成高质量的伪掩码。当推理结果中的类别不在图像级标签中时，将其标记为未知标签。最后，使用生成的伪掩码进行全监督分割，采用DeepLabv1网络架构，并尝试使用更先进的基于ViT的分割方法进行再训练。 实验结果表明，WeakCLIP在PASCAL VOC 2012和COCO 2014数据集上取得了优于以往WSSS方法的结果，证明了该方法的有效性和高效性。</p></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-49-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-49-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-09_08-49-54"></p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>PASCAL VOC 2012、COCO 2014</strong></p><ul><li><strong>ASCAL VOC 2012</strong>：在CAM监督方面，WeakCLIP与MCTformer相同，但低于ViT - PCM；在伪掩码质量上，比基线MCTformer提高了8.1%，比AMN提高了5.0%。使用精炼后的伪掩码训练DeepLabV1网络，WeakCLIP在验证集和测试集上的mIoU分别达到74.0%和73.8%，优于其他仅使用图像级监督的方法，以及部分使用额外显著图监督或边界框监督的方法。使用基于ViT的再训练基准（Segmenter和SegFormer）可进一步提升分割结果，混合ViT再训练的WeakCLIP表现最佳。</li><li><strong>COCO 2014</strong>：WeakCLIP在验证集上的mIoU达到46.1%，比基线MCTformer提高了4.1%，优于其他仅使用图像级监督的方法。使用SegFormer和MiT - B2骨干进行再训练，WeakCLIP在COCO 2014验证集上取得最佳性能。</li></ul><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ol><li><ul><li><strong>组件改进</strong>：协同注意力匹配模块将验证集mIoU提高到67.4%；可学习提示将其提高到68.9%；金字塔适配器将性能提升到70.3%；文本引导解码器将验证集mIoU进一步提升到72.6%。</li><li><strong>可学习嵌入数量</strong>：可学习嵌入数量为8时性能最佳。</li><li><strong>可学习温度初始值</strong>：协同注意力匹配中可学习温度初始值为1e - 1时性能最佳。</li></ul></li></ol><h2 id="其他实验（Other-Experiments）-1st-place-medal"><a href="#其他实验（Other-Experiments）-1st-place-medal" class="headerlink" title="其他实验（Other Experiments）:1st_place_medal:"></a>其他实验（Other Experiments）:1st_place_medal:</h2><ol><li><strong>逐类语义分割结果</strong>：在PASCAL VOC 2012的验证集和测试集以及COCO 2014的验证集上，将WeakCLIP与基线MCTformer进行逐类分割结果比较，WeakCLIP在大多数类别中表现更优。</li><li><strong>可视化分析</strong><ul><li>比较MCTformer和WeakCLIP生成的伪掩码，WeakCLIP生成的语义信息更准确、精确，能识别出MCTformer遗漏或识别不准确的对象位置。</li><li>在PASCAL VOC 2012验证集上再训练后的分割结果可视化显示，WeakCLIP对室内和室外场景都能实现准确分割。</li></ul></li><li><strong>参数效率分析</strong>：与MCTformer相比，WeakCLIP仅训练12.4%的参数，训练帧率（FPS）快4.3倍，节省68.4%的GPU内存。</li><li><strong>不同CLIP骨干实验</strong>：使用不同CLIP骨干进行实验，结果表明WeakCLIP - ResNet101性能优于WeakCLIP - ResNet50，WeakCLIP - ViT - B表现最佳。</li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了名为<strong>WeakCLIP</strong>的新方案，旨在利用预训练CLIP模型的知识来增强弱监督语义分割（WSSS）网络的**类激活图（CAM）**细化过程。该框架采用了新的文本到像素匹配范式，有效解决了将CLIP集成到WSSS中存在的三个关键问题。在广泛使用的PASCAL VOC 2012和COCO 2014数据集上的实验结果表明，与以往的WSSS方法相比，WeakCLIP取得了显著改进。引入利用大规模视觉语言预训练的WeakCLIP范式，有望推动WSSS问题的解决。未来，作者计划探索更先进的大规模CLIP，以提升WSSS的像素级理解能力。 </p>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/all-pairs-consistency-learning-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/all-pairs-consistency-learning-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>澳大利亚国立大学、OpenNLPLab、上海人工智能实验室、厦门大学、OPPO研究院</p><p>::: tip</p><p>启发</p><p>:::</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>In this work, we propose a new <strong>transformer-based regularization</strong> to better localize objects for <strong>Weakly supervised semantic segmentation (WSSS)</strong>. In image-level WSSS, Class Activation Map (CAM) is adopted to generate object localization as pseudo segmentation labels. To address the partial activation issue of the CAMs, consistency regularization is employed to maintain activation intensity invariance across various image augmentations. However, such methods ignore pair-wise relations among regions within each CAM, which capture context and should also be invariant across image views. To this end, we propose a new<br>all-pairs consistency regularization (ACR). Given a pair of augmented views, our approach regularizes the activation intensities between a pair of augmented views, while also ensuring that the affinity across regions within each view remains consistent. We adopt vision transformers as the self-attention mechanism naturally embeds pair-wise affinity. This enables us to simply regularize the distance between the attention matrices of augmented image pairs. Additionally, we introduce a novel class-wise localization<br>method that leverages the gradients ofthe class token. Our method can be seamlessly integrated into existing WSSS methods using transformers without modifying the architectures. We evaluate our method on PASCAL VOC and MS COCO datasets. Our method produces noticeably better class localization maps (67.3% mIoU on PASCAL VOC train), resulting in superior WSSS performances.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>在这项工作中，我们提出了一种新的<strong>基于变换的正则化方法</strong>来更好地定位弱监督语义分割(WSSS)的对象。在图像级WSSS中，采用类激活图(Class Activation Map, CAM)生成目标定位作为伪分割标签。为了解决cam的部分激活问题，采用一致性正则化方法在不同的图像增强中保持激活强度的不变性。然而，这些方法忽略了每个CAM内区域之间的成对关系，这种关系捕获上下文，并且应该在图像视图之间保持不变。为此，我们提出了一种新的全对一致性正则化(ACR)。给定一对增强视图，我们的方法规范了一对增强视图之间的激活强度，同时还确保每个视图中跨区域的亲和性保持一致。我们采用视觉Transformer作为自注意力机制机制，自然嵌入成对的亲和力。这使我们能够简单地正则化增广图像对的注意矩阵之间的距离。此外，我们引入了一种新的类智能定位方法，利用类标记的梯度。我们的方法可以使用Transformer无缝集成到现有的WSSS方法中，而无需修改体系结构。我们在PASCAL VOC和MS COCO数据集上评估了我们的方法。我们的方法产生了明显更好的类定位图(在PASCAL VOC训练上有67.3%的mIoU)，从而获得了卓越的WSSS性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法的局限性，具体研究背景如下：</p><ul><li><strong>WSSS的意义与挑战</strong>：WSSS旨在利用图像级标签、点、涂鸦和边界框等弱标签，缓解像素级标注的繁琐和高成本问题。其中，图像级WSSS仅使用类别标签监督像素级预测，尤为具有挑战性。 </li><li><strong>现有方法的不足</strong>：现有图像级WSSS方法通常依赖基于卷积神经网络的类激活图（CAM）生成伪分割标签，但CAM存在激活不完整和不准确的问题，这是由于图像标签和像素级分割监督之间的差距导致的。</li><li><strong>一致性正则化的局限</strong>：现有工作使用增强不变一致性来改进CAM，考虑了区域激活一致性，但忽略了跨视图的成对一致性，即区域亲和性一致性。激活一致性只能发现新视图中的激活，无法解决未激活区域和背景噪声问题。</li><li><strong>本文的研究动机</strong>：鉴于亲和性是上下文编码的一种方式，且上下文对像素级预测至关重要，本文提出全对一致性正则化（ACR）方法，同时强制区域激活一致性和区域亲和性一致性，以提高WSSS的性能。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><p><strong>弱监督语义分割方法多样</strong>：采用图像级标签、涂鸦、点和边界框等弱标签，避免像素级标注的繁琐。图像级弱监督语义分割常依赖类激活图（CAM）生成伪分割标签，且有多种方法对其进行优化。</p></li><li><p><strong>一致性正则化受关注</strong>：不同类型的一致性被提出用于优化初始种子，如CAM一致性、特征一致性等。</p></li><li><p><strong>亲和性学习细化</strong>：成对亲和性常被用于优化初始种子，在CNN和Transformer时代都有相关研究。</p></li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-44-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-44-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-44-09"></p><p>本文提出了一种名为<strong>全对一致性正则化</strong>（<strong>All-pairs Consistency Regularization，ACR</strong>）的模型，用于弱监督语义分割（Weakly Supervised Semantic Segmentation，WSSS）任务。</p><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>在图像级弱监督语义分割中，<strong>类激活图</strong>（<strong>Class Activation Map，CAM</strong>）常被用于生成目标定位作为伪分割标签，但存在部分激活问题。现有方法采用一致性正则化来保持不同图像增强下的激活强度不变性，但忽略了每个CAM内区域之间的成对关系。因此，ACR模型旨在同时确保区域激活一致性和区域亲和性一致性，以更好地定位目标。</p><h3 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h3><ul><li><strong>基于视觉Transformer</strong>：选择视觉Transformer作为基础模型，因为其自注意力机制可以自然地编码区域之间的依赖关系，适合建模两种一致性，且无需引入额外模块。</li><li>注意力一致性正则化<ul><li><strong>区域激活一致性</strong>：通过比较原始图像和增强图像的类到块注意力（class-to-patch attention），计算两者之间的ℓ1损失，以鼓励网络生成对变换不变的目标定位。</li><li><strong>区域亲和性一致性</strong>：比较原始图像和增强图像的块到块注意力（patch-to-patch attention），计算两者之间的ℓ1损失，以鼓励图像区域之间的成对关系对变换不变。</li><li><strong>变换逆操作</strong>：为了解决图像增强后注意力矩阵空间顺序不一致的问题，引入变换逆操作，恢复注意力矩阵的原始空间顺序，以便直接计算两个注意力矩阵之间的距离。</li></ul></li><li>基于梯度的Transformer类定位图生成<ul><li><strong>梯度计算</strong>：通过反向传播分类分数，计算类到块注意力的类特定梯度，去除负值并重塑为h×w的图，得到类定位图。</li><li><strong>亲和性细化</strong>：利用学习到的块间亲和性对激活图进行细化，结合区域激活一致性和区域亲和性一致性，生成最终的类定位图。</li></ul></li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>PASCAL VOC、MS COCO</strong></p><h4 id="MS-COCO"><a href="#MS-COCO" class="headerlink" title="MS COCO"></a>MS COCO</h4><p>表1展示了在MS COCO上的分割结果。本文方法实现了45%的分割平均交并比（mIoU），明显优于现有方法。值得注意的是，该结果不依赖任何额外的显著性信息，但超过了所有先前的WSSS方法，包括使用显著性信息的方法。MS COCO是一个更大的数据集，有更多语义类别和包含多个对象的复杂图像。这一结果表明，显著性信息可能会阻碍WSSS方法在复杂场景中的扩展性，因此本文方法未纳入显著性信息。该结果证明了全对一致性正则化（ACR）能够在具有挑战性的场景中生成可靠的类别定位图。MS COCO的每类结果报告在补充材料中。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-50-42.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-50-42.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-50-42"></p><h4 id="PASCAL-VOC"><a href="#PASCAL-VOC" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h4><ul><li><strong>种子性能</strong>：表2报告了类别定位图的mIoU，包括有无亲和性细化的性能。结果显示，即使没有亲和性细化，ACR*仍优于大多数现有的非显著性方法（mIoU为59.4%）。本文的ACR显著改善了初始种子，证明了所提出的ACR的有效性。在没有显著性信息辅助的情况下，先前最佳方法[66]也采用变压器亲和性来细化种子，而ACR比其高出5.2%。图5展示了定性结果。此外，图6展示了在包含多个对象的复杂场景中的种子，ACR学习到精确的亲和性，有助于形成具有精确边界的完整对象形状。</li><li><strong>伪标签性能</strong>：表2的最后一列显示了伪分割标签的性能。遵循常见做法，采用PSA [2]将激活图（种子）处理为像素级伪分割标签。实验发现PSA容易受到误报样本（即过度激活）的影响。为避免过度激活，使用ACR*训练PSA网络，然后训练好的PSA网络将细化ACR种子（67.3%）为伪标签。结果表明，本文方法显著改善了伪标签。</li><li><strong>语义分割性能</strong>：表3展示了在PASCAL VOC上的语义分割结果。ACR在验证集和测试集上分别取得了71.2%和70.9%的有竞争力的结果，优于先前的非显著性方法。图7显示，使用本文伪标签训练的分割模型可以产生准确和完整的预测。PASCAL VOC的每类结果报告在补充材料中。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-51-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-51-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-51-40"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><p>本文提出在分类训练期间同时正则化区域激活和区域亲和性。表4对这两个正则化项进行了消融实验。首先观察到，即使在基线模型中，区域亲和性也能显著提高种子质量，这验证了视觉变压器的上下文编码能力。通过引入这两个正则化项，发现它们分别对性能有显著提升。同时使用两个正则化项取得了最优结果，与普通变压器基线（51.1%）相比，整体mIoU提高了15.8%，证明了ACR的有效性。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-53-12.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-53-12.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-53-12"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种名为<strong>全对一致性正则化（ACR）的训练框架</strong>，用于从变压器生成更好的类定位图。研究结果表明，ACR在分类训练中利用区域激活一致性和区域亲和一致性，通过变压器的自注意力机制同时规范这两种一致性。仅使用一个类令牌，ACR就能学习精确的对象定位和准确的成对亲和性，以提取对象范围。其类定位图显著优于先前方法，在PASCAL VOC和MS COCO数据集上取得了最先进的性能。此外，ACR可以无缝集成到视觉变压器网络中，无需额外修改，这有助于其他基于变压器的任务。因此，作者认为ACR是一种简单而有效的方法，能够为弱监督语义分割提供更好的初始种子。 </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Self-supervised vision transformers for semantic segmentation</title>
      <link href="/post/self-supervised-vision-transformers-for-semantic-segmentation/"/>
      <url>/post/self-supervised-vision-transformers-for-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Semantic segmentation is a fundamental task in computer vision and it is a building block of many other vision applications. Nevertheless, semantic segmentation annotations are extremely expensive to collect, so using pre-training to alleviate the need for a large number of labeled samples is appealing. Recently, <strong>self-supervised</strong> <strong>learning (SSL)</strong> has shown effectiveness in extracting strong representations and has been widely applied to a variety of downstream tasks. However, most works perform sub-optimally in semantic segmentation because they ignore the specific properties of segmentation: (i) the need of pixel level fine-grained understanding; (ii) with the assistance of global context understanding; (iii) both of the above achieve with the dense self-supervisory signal. Based on these key factors, we introduce a systematic self-supervised pre-training framework for semantic segmentation, which consists of a hierarchical encoder–decoder architecture MEVT for generating high-resolution features with global contextual information propagation and a self-supervised training strategy for learning fine-grained semantic features. In our study, our framework shows competitive performance compared with other main self-supervised pre-training methods for semantic segmentation on <strong>COCO-Stuff, ADE20K, PASCAL VOC, and Cityscapes</strong> datasets. e.g., MEVT achieves the advantage in linear probing by +1.3 mIoU on PASCAL VOC.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>语义分割是计算机视觉的基础任务，也是众多视觉应用的基础模块。但由于语义分割标注的获取成本极高，通过预训练减少对大量标注样本的依赖显得尤为重要。近年来，自监督学习（SSL）在特征提取方面展现出显著成效，已被广泛应用于各类下游任务。然而，现有方法在语义分割任务中效果欠佳，主要原因在于忽视了该任务的三个核心特性：(i) 需要像素级的细粒度理解；(ii) 需要结合全局上下文信息；(iii) 必须通过密集的自监督信号同时实现上述两个目标。基于这些关键要素，我们开发了系统的自监督预训练框架，包含以下创新：采用 MEVT 分层编码器-解码器架构生成具有全局上下文传播能力的高分辨率特征，以及专门设计的自监督训练策略用于学习细粒度语义特征。实验表明，在 COCO-Stuff、ADE20K、PASCAL VOC 和 Cityscapes 等数据集上，我们的框架相比其他主流自监督预训练方法展现出竞争优势。典型例证如：MEVT 在 PASCAL VOC 的线性探测任务中实现了 1.3 mIoU 的性能提升。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割是计算机视觉的基础任务，在自动驾驶、机器人操作等领域应用广泛，但标注成本高昂，多数语义分割数据集规模远小于分类数据集。因此，利用大规模无标签数据进行预训练以减少对大量标注样本的依赖成为潜在解决方案。 </p><p>近年来，<strong>自监督学习（SSL）及其在视觉Transformer</strong>上的应用在计算机视觉领域取得显著进展，能帮助网络学习通用视觉表示，降低对大规模标注数据的需求。然而，多数自监督学习方法在语义分割任务中表现欠佳，原因在于它们忽略了语义分割的特定属性：需要像素级的细粒度理解、借助全局上下文理解，且要通过密集的自监督信号实现上述两点。 基于这些问题，本文作者探索一种适用于语义分割的自监督预训练方法，提出了一个系统的自监督预训练框架，旨在生成具有全局上下文信息传播的高分辨率特征，并学习细粒度的语义特征，以提升语义分割任务的性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>自监督学习</strong>：提出多种预训练任务，如色彩化、图像修复等，对比学习在下游视觉任务表现良好，基于掩码图像建模的方法取得了不错的成果，部分研究还改进了训练目标和架构。</li><li><strong>密集预测预训练</strong>：利用自监督学习进行密集预测预训练，一些方法聚焦实例级&#x2F;原型对应，部分引入像素&#x2F;区域级自监督预训练方法。</li><li><strong>视觉Transformer</strong>：ViT将Transformer应用于图像识别，Swin Transformer引入卷积风格窗口计算，部分工作构建多分辨率特征图用于密集输出。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-00-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-00-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-14_21-00-59"></p><p>本文提出了一种用于<strong>语义分割的自监督预训练框架</strong>，该框架在网络架构和自监督目标方面都有创新，核心模型是多尺度编码器 - 解码器视觉变压器（Multi-scale Encoder–Decoder Vision Transformer, MEVT），以下是详细介绍： </p><ol><li><strong>模型架构</strong>    <ul><li><strong>多尺度解码器</strong>：为了获得高分辨率的细粒度特征，MEVT在预训练架构中引入了多尺度解码器，并对编码器和解码器进行联合预训练。解码器由全局注意力阶段（Stage 5）和局部注意力阶段（Stage 6）组成，通过“Patch Unmerging”层对特征图进行上采样，同时引入跳跃连接（skip-connections），促进训练过程中浅层的梯度传播。    </li><li><strong>混合注意力机制</strong>：为了融合全局和局部上下文信息，MEVT采用了一种简单而有效的混合注意力策略。在浅层（Stage 1、2和6）使用Swin Transformer的窗口注意力块来处理局部信息，在深层（Stage 3、4和5）使用全局自注意力（ViT块）来增强全局上下文信息的传播。</li></ul></li><li><strong>自监督预训练策略</strong>：MEVT使用图像级自蒸馏损失（来自DINO）对全局平均池化（GAP）特征进行预训练。将图像的两个增强视图分别输入到教师网络和学生网络中，通过最小化交叉熵损失将知识从教师网络蒸馏到学生网络。教师网络通过指数移动平均（EMA）更新。在Stage 4和Stage 6的输出处分别应用自蒸馏损失，并对两个损失项进行等权重加权，以确保编码器和解码器网络得到充分的预训练。</li><li><strong>模型优势</strong></li></ol><ul><li><strong>性能表现</strong>：在多个语义分割数据集（COCO - Stuff、ADE20K、PASCAL VOC和Cityscapes）上的实验结果表明，MEVT在各种设置（线性探测、微调、低样本学习）下均优于大多数现有方法。例如，在PASCAL VOC上，MEVT在线性探测中比其他方法提高了+1.3 mIoU。   </li><li><strong>特征学习</strong>：通过实验和消融研究，证明了MEVT能够学习到具有细粒度和全局上下文感知能力的视觉表示，适用于具有挑战性的语义分割任务。例如，在定性结果中，MEVT在复杂环境中识别小物体的能力优于其他基线方法。</li></ul><h2 id="实验（Compared-with-SOTA）-1st-place-medal"><a href="#实验（Compared-with-SOTA）-1st-place-medal" class="headerlink" title="实验（Compared with SOTA）:1st_place_medal:"></a>实验（Compared with SOTA）:1st_place_medal:</h2><p><strong>数据集</strong>：在ImageNet上进行300个epoch的预训练。</p><ul><li><strong>线性探测结果</strong>：在COCO - Stuff、ADE20K、Cityscapes和PASCAL等数据集上，MEVT在大多数数据集上优于所有基线方法。例如，在具有挑战性的Cityscapes数据集上，MEVT比基于Transformer的DINO方法高出8.4 mIoU，比采用Swin - T的MOBY方法高出2.6 mIoU。</li><li><strong>端到端微调结果</strong>：在ADE20K数据集上，使用线性头时，MEVT比之前最好的方法iBOT高出2.4 mIoU。</li><li><strong>低样本微调结果</strong>：在不同比例标记的ADE20K图像上，MEVT在各种监督水平下均优于现有方法，表明其在实际场景中能实现更高效的语义分割。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-14_21-04-37"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-14_21-04-43"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>预训练解码器</strong>：将解码器纳入预训练框架显著提高了线性探测mIoU（+4.9）和微调mIoU（+2.2）。</li><li><strong>解码器深度</strong>：默认使用两个块的解码器，从一个块增加到两个块可提高线性探测和微调性能，增加到三个块时性能下降。</li><li><strong>多尺度融合</strong>：MEVT在低分辨率阶段使用全局自注意力进行多尺度信息融合，比仅依赖窗口注意力的Swin - T + W.A.Dec.在线性探测和微调上分别高出2.9 mIoU和2.8 mIoU。</li><li><strong>跳跃连接</strong>：添加两个跳跃连接时性能最佳，线性探测mIoU从67.8提高到71.5。</li><li><strong>位置编码</strong>：成对相对位置偏差的效果优于其他位置偏差，线性探测mIoU提高了4.3。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种<strong>自监督预训练方法</strong>，用于促进下游<strong>语义分割任务</strong>，得出以下结论：</p><ol><li><strong>方法创新</strong>：该方法在神经网络架构和自监督目标方面均有创新，构建了包含多尺度编码器-解码器架构MEVT和自监督训练策略的框架。</li><li><strong>性能优越</strong>：此框架简单且强大，在<strong>COCO-Stuff、ADE20K、PASCAL VOC和Cityscapes</strong>四个常用数据集的多种语义分割和低样本评估指标上达到了最优性能。 </li><li><strong>应用前景</strong>：<strong>作者希望该简单框架能推动无标签或少量标签语义分割的广泛应用，减少对大量高质量标注数据的依赖。</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation</title>
      <link href="/post/a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation/"/>
      <url>/post/a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="南京信息工程大学、青海师范大学、澳门大学、中国科学院-100"><a href="#南京信息工程大学、青海师范大学、澳门大学、中国科学院-100" class="headerlink" title="南京信息工程大学、青海师范大学、澳门大学、中国科学院  :100:"></a>南京信息工程大学、青海师范大学、澳门大学、中国科学院  :100:</h2><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a><strong>摘要：</strong></h2><blockquote><p><strong>Few-shot semantic segmentation (FSS)</strong> aims to generate a model for segmenting novel classes using a limited number of annotated samples. Previous FSS methods have shown sensitivity to background noise due to inherent bias, attention bias, and spatial-aware bias. In this study, we propose a <strong>Transformer-Based Adaptive Prototype Matching Network</strong> to establish robust matching relationships by improving the semantic and spatial perception of query features. The model includes three modules: <strong>target enhancement module (TEM)</strong>, <strong>dual constraint aggregation module (DCAM)</strong>, and <strong>dual classification module (DCM)</strong>. In particular, TEM mitigates inherent bias by exploring the relevance of multi-scale local context to enhance foreground features. Then, DCAM addresses attention bias through the dual semantic-aware attention mechanism to strengthen constraints. Finally, the DCM module decouples the segmentation task into semantic alignment and spatial alignment to alleviate spatial-aware bias. Extensive experiments on <strong>PASCAL-5i</strong> and <strong>COCO-20i</strong> confirm the effectiveness of our approach.</p></blockquote><h2 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a><strong>翻译：</strong></h2><blockquote><p>Few-shot语义分割（FSS）旨在通过少量的标注样本为新的类别生成一个分割模型。以往的FSS方法由于固有偏差、注意力偏差和空间感知偏差，往往对背景噪声过于敏感。在本研究中，我们提出了一种基于Transformer的自适应原型匹配网络，通过增强查询特征的语义和空间感知能力，建立更为稳定的匹配关系。该模型包含三个模块：目标增强模块（TEM）、双重约束聚合模块（DCAM）和双重分类模块（DCM）。其中，TEM通过探索多尺度局部上下文的相关性，增强前景特征，从而减轻固有的偏差。接着，DCAM通过双重语义感知注意力机制解决了注意力偏差问题，强化了约束效果。最后，DCM模块将分割任务拆解为语义对齐和空间对齐，帮助缓解空间感知偏差。我们在PASCAL-5i和COCO-20i数据集上进行了大量实验，验证了该方法的有效性。</p></blockquote><h2 id="研究背景："><a href="#研究背景：" class="headerlink" title="研究背景："></a><strong>研究背景：</strong></h2><p>近年来，由于深度学习在计算机视觉领域的快速发展，所以传统的语义分割取得了飞速进步。在这种情况下，少样本分割(few-shot segmentation, FSS)被提出用于模拟有限数据和多类别的真实世界场景。</p><p>FSS遵循元学习框架，执行过程分特征提取、匹配和分类三个阶段。现有FSS模型虽有成果，但受背景干扰，存在三方面问题：一是特征提取阶段，预训练骨干网络有固有偏差，易优先提取无关特征；二是特征匹配阶段，注意力机制在目标类别内差异大时，会导致注意力偏差；三是分类阶段，现有方法多依赖语义相关性，忽略空间信息，产生空间感知偏差。</p><p>基于上述问题，作者提出一种基于Transformer的自适应原型匹配网络，通过在模型执行的三个阶段进行策略性和高效交互，减轻FSS中的背景干扰，利用查询特征的语义和空间感知，增强模型的鲁棒性，以解决现有FSS模型存在的问题。</p><h2 id="研究现状："><a href="#研究现状：" class="headerlink" title="研究现状："></a><strong>研究现状：</strong></h2><ul><li><strong>Few - Shot Semantic Segmentation（FSS）</strong>：FSS旨在用<strong>少量标注样本</strong>为新类别生成分割模型，基于度量学习的FSS主要分为基于原型和基于像素匹配两类方法。<strong>基于原型的方法</strong>用原型代表目标类信息进行匹配预测；<strong>基于像素匹配</strong>的方法建立支持像素和查询像素的密集关联。</li><li><strong>Transformer应用</strong>：Transformer因能捕捉长距离相关性，在FSS中得到应用，如动态调整分类器权重、过滤无关像素、聚合多级别支持掩码等。</li></ul><h2 id="提出的模型："><a href="#提出的模型：" class="headerlink" title="提出的模型："></a><strong>提出的模型：</strong></h2><p>本文提出了一种基于Transformer的自适应原型匹配网络（Transformer - Based Adaptive Prototype Matching Network），用于<strong>少样本语义分割（Few - Shot Semantic Segmentation，FSS）<strong>任务，以解决现有FSS模型存在的</strong>固有偏差、注意力偏差和空间感知偏差</strong>导致的对背景噪声敏感的问题。该模型主要包含以下三个模块： </p><ol><li><blockquote><p><strong>目标增强模块（Target Enhancement Module，TEM）</strong>    <strong>设计目的</strong>：缓解骨干网络的<strong>固有偏差</strong>，增强前景特征。在特征提取阶段，以往工作依赖预训练骨干网络直接提取的特征，存在固有偏差，倾向于提取与当前任务无关的特征。    <strong>具体方法</strong>：引入基于卷积Transformer架构的多尺度局部感知调制Transformer进行多尺度特征提取，采用多尺度自适应局部注意力增强前景信息、减轻背景干扰；用可逆神经网络（INN）替代标准多层感知器（MLP），在前馈过程中保留更细粒度的特征。 </p></blockquote></li><li><blockquote><p><strong>双约束聚合模块（Dual Constraint Aggregation Module，DCAM）</strong>    <strong>设计目的</strong>：解决特征匹配阶段的<strong>注意力偏差</strong>问题。现有方法利用单层注意力机制建立支持集和查询集的关系，在目标类别存在显著类内差异时，这种关系不足以准确匹配，导致注意力偏差。    <strong>具体方法</strong>：由类内差异表示和双语义感知注意力机制两个关键部分组成。类内差异表示利用一组可学习向量建模支持集和查询集之间的差异；双语义感知注意力机制通过两层约束，先以支持原型为参考在查询特征中选择匹配置信度高的点，再以此为指导在整个查询特征图中寻找特征相似度高的点，生成鲁棒的支持类别原型。 </p></blockquote></li><li><blockquote><p><strong>双分类模块（Dual Classification Module，DCM）</strong>    <strong>设计目的</strong>：解决特征分类阶段的<strong>空间感知偏差</strong>问题。现有方法主要基于语义一致性进行预测，忽略了目标对象的空间一致性，导致难以准确定位目标类别。    <strong>具体方法</strong>：将分割任务解耦为语义对齐和空间对齐两个子任务。通过优化查询特征和类别原型生成基于语义相似度的掩码来识别目标类别；利用查询特征的内在引导，挖掘目标对象自身的空间一致性，得到基于空间分布概率的掩码用于精确的定位，最后将两个掩码相加得到最终的查询前景分割图。 实验结果表明，该模型在PASCAL - 5i和COCO - 20i两个基准数据集上取得了优于现有方法的性能，且参数数量较少。</p></blockquote></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-36-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-36-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-36-07"></p><h2 id="实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）"><a href="#实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）" class="headerlink" title="实验（compared with the state-of-the-art models and ablation experiments）"></a><strong>实验（compared with the state-of-the-art models and ablation experiments）</strong></h2><ul><li><h3 id="Comparison-with-the-State-of-the-Arts"><a href="#Comparison-with-the-State-of-the-Arts" class="headerlink" title="Comparison with the State-of-the-Arts"></a><strong>Comparison with the State-of-the-Arts</strong></h3></li></ul><p>数据集：PASCAL-5${^i}$，COCO-20${^i}$</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-39-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-39-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-39-17"></p><ul><li><h3 id="ablation-experiments"><a href="#ablation-experiments" class="headerlink" title="ablation experiments"></a><strong>ablation experiments</strong></h3></li></ul><ol><li><strong>组件分析</strong>：该方法包含目标增强模块（TEM）、双约束聚合模块（DCAM）和双分类模块（DCM）三个主要模块。与基线相比，单独使用TEM增强查询前景特征可使性能提升0.9%，单独使用DCAM增强类别原型的判别能力可提升2.2%，TEM和DCAM协同作用可提升2.6%，使用DCM实现语义对齐和空间对齐可额外提升1.3%。模型整体比基线提升了3.9%，表明引入的模块有效解决了固有偏差、注意力偏差和空间感知偏差问题，减少了背景干扰，实现了精确分割。 </li><li><strong>目标增强模块（TEM）</strong>：TEM旨在减轻骨干网络的固有偏差并增强查询前景区域。通过与其他方法在计算量和准确性方面进行对比实验，包括采用自对齐模块（SA）、卷积变压器架构（SAM）、多尺度自适应局部注意力（MSLA + MLP）以及用可逆神经网络（INN）代替多层感知器（MLP）作为前馈网络（MSLA + INN）。结果表明，该方法在降低计算复杂度的同时保持了较高的准确性，且前馈网络在略微增加计算成本的情况下保留了更多特征细节。 </li><li><strong>双约束聚合模块（DCAM）</strong>：对DCAM中的关键组件进行了全面分析，通过修改模型采用不同的注意力机制，如原始的普通注意力（VA）、掩码注意力（MA）、双语义感知注意力（DSAA）和类内差异表示（IDR）。结果显示，使用掩码注意力减轻背景噪声干扰对性能提升影响不大，因为支持集和查询集之间的相似度掩码在类内差异较大时准确性存在挑战。而双语义感知注意力机制通过可学习的方式减轻背景干扰，能应对类内差异的敏感性，类内差异表示在三种不同的注意力机制中都有益。</li><li><strong>双分类模块（DCM）</strong>：通过消融实验评估不同的DCM组件。仅使用基于语义相似度的掩码可使模型性能提升1.5%，证明了优化类别原型和查询特征的必要性；仅使用基于空间分布概率的分割图时，性能下降2.3%，这是因为仅依赖查询图像本身的前景分布会使模型偏向已知类别的区域，导致对未知类别的分割失败。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-19.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-19.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-41-19"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-41-25"></p><h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a><strong>结论：</strong></h2><blockquote><p>作者提出了一种<strong>基于Transformer的自适应原型匹配网络</strong>，以应对少样本语义分割（FSS）中<strong>固有偏差、注意力偏差和空间感知偏差</strong>导致的背景干扰问题。该网络包含目标增强模块（TEM）、双约束聚合模块（DCAM）和双分类模块（DCM）。TEM通过多尺度局部上下文相关性增强前景特征，解决固有偏差；DCAM利用双语义感知注意力机制加强约束，处理注意力偏差；DCM将分割任务解耦为语义对齐和空间对齐，缓解空间感知偏差。实验表明，该方法在PASCAL - 5i和COCO - 20i数据集上以最少的参数达到了最先进的性能，有效减少了背景干扰，实现了精确分割。 </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation</title>
      <link href="/post/dsmf-net-dual-semantic-metric-learning-fusion-network-for-few-shot-aerial-image-semantic-segmentation/"/>
      <url>/post/dsmf-net-dual-semantic-metric-learning-fusion-network-for-few-shot-aerial-image-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Chinese Academy of Sciences、University of Chinese Academy of Sciences</strong></p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Semantic segmentation</strong> of aerial images is crucial yet resource-intensive. Inspired by human ability to learn rapidly, few-shot semantic segmentation offers a promising solution by utilizing limited labeled data for efficient model training and generalization. However, the intrinsic complexities of aerial images, compounded by scarce samples, often result in inadequate feature representation and semantic ambiguity, detracting from themodel’s performance. In this article, we propose to tackle these challenging problems via dual semantic metric learning and multisemantic features fusion<br>and introduce a novel few-shot segmentation Network (DSMF-Net). On the one hand, we consider the inherent semantic gap between the feature of graph and grid structures and metric learning of few-shot segmentation. To exploit multiscale global semantic context, we construct scale-aware graph prototypes from different stages of the feature layers based on graph convolutional networks (GCNs), while also incorporating prior-guided metric learning to further enhance context at the high-level convolution features. On the other hand, we design a pyramid-based fusion and condensa-<br>tion mechanism to adaptively merge and couple the multisemantic information from support and query images. The indication and fusion of different semantic features can effectively emphasize the representation and coupling abilities of the network. We have conducted extensive experiments over the challenging iSAID-5i andDLRSD benchmarks. The experiments have demonstrated our network’s effectiveness and efficiency, yielding on-par performance with the state-of-the-art methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>航空图像的语义分割</strong>是一个非常重要的问题。受人类快速学习能力的启发，少射语义分割通过利用有限的标记数据进行有效的模型训练和泛化，提供了一种很有前途的解决方案。然而，航空图像固有的复杂性，加上稀缺的样本，往往导致特征表示不足和语义模糊，从而降低了模型的性能。在本文中，我们提出通过双语义度量学习和多语义特征融合来解决这些具有挑战性的问题，并引入了一种新的少量样本学习分割网络(DSMFNet)。一方面，我们考虑了图和网格结构特征之间固有的语义差距和少量样本学习分割的度量学习。为了利用多尺度全局语义上下文，我们基于图卷积网络(GCNs)从特征层的不同阶段构建了尺度感知的图原型，同时还结合了先验引导的度量学习来进一步增强高级卷积特征的上下文。另一方面，我们设计了一种基于金字塔的融合与凝聚机制来自适应地融合和耦合来自支持和查询图像的多语义信息。不同语义特征的表示和融合可以有效地强调网络的表示和耦合能力。我们对具有挑战性的iSAID-5i和dlrsd基准进行了广泛的实验。实验证明了我们的网络的有效性和效率，产生了与最先进的方法相当的性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于航空影像少样本语义分割问题，研究背景如下：</p><ul><li><strong>语义分割需求与挑战</strong>：语义分割是计算机视觉基础技术，在城市管理、环境监测等领域应用广泛。但传统航空影像语义分割模型训练需大量标注数据，获取耗时耗力。</li><li><strong>少样本学习的潜力</strong>：少样本学习受人类学习能力启发，利用少量标注数据进行模型训练和泛化，为解决数据获取难题提供了思路。少样本语义分割作为其延伸，通过利用相关任务或领域的先验知识进行分割任务。</li><li><strong>航空影像少样本分割的困难</strong>：航空影像由机载或卫星传感器捕获，具有空间分辨率变化大、覆盖范围广的特点。不同语义对象外观差异大，同一类别对象在尺度和结构上也存在显著差异，导致特征表示不足和语义模糊，增加了少样本语义分割的难度。 </li><li><strong>现有方法的局限性</strong>：现有方法虽有一定进展，但传统卷积神经网络在捕捉航空影像的全局和可变结构关系方面效率较低，特征提取存在特征耦合和细节保留不足的问题，导致特征歧义。 基于以上背景，作者提出DSMF - Net网络，以解决航空影像少样本语义分割中的特征建模不佳和语义模糊问题。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>语义分割</strong>：FCN、U - Net等方法推动了语义分割发展，后续如DeepLabv3 +、PSPNet等通过引入新机制提升性能，但监督分割方法标注要求高。</li><li><strong>少样本语义分割</strong>：出现半监督、弱监督、无监督学习等方法，近期少样本学习受关注，如OSLSM、PL、PANet等方法不断涌现，部分还探索了知识迁移问题。</li><li><strong>航空影像少样本语义分割</strong>：不同方法被提出，如Wang等人的原型队列学习法、Yao等人的多原型框架、DMML - Net的深度特征金字塔比较网络等。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>文章提出了一种名为DSMF - Net（Dual Semantic Metric Learning Fusion Network）的新型少样本分割网络，用于解决航空影像少样本语义分割中特征建模不佳和语义模糊的问题。</p><ol><li><strong>图原型度量学习（Graph Prototypes Metric Learning）</strong><ul><li><strong>图卷积（Graph Convolution）</strong>：图卷积可有效表示图像中不同像素或区域之间的关系，捕捉长距离依赖和全局上下文信息，有助于减少特征耦合，使模型学习和区分图像内不同的结构和语义关系。</li><li><strong>尺度感知图原型（Scale - Aware Graph Prototypes）</strong>：静态网格卷积特征难以捕捉航空影像中的复杂关系，因此引入尺度感知图原型，以图投影的方式利用多尺度全局语义上下文。通过在预训练的ResNet - 50的三个中间阶段输出特征，利用GloRe单元进行全局推理，结合支持掩码加权和全局平均池化生成原型。</li><li><strong>原型度量学习（Prototype Metric Learning）</strong>：利用查询特征与原型之间的余弦距离进行度量学习，对查询特征应用相同的图投影操作，通过最小 - 最大归一化得到图原型概率图。</li></ul></li><li><strong>先验引导度量学习（Prior Guided Metric Learning）</strong>：图卷积特征金字塔为图结构数据的特征嵌入和全局信息捕捉提供了基础，但高级卷积特征中的语义信息也不能忽视。因此引入先验引导度量学习，生成高级查询和支持卷积特征之间的相似性度量，通过添加二进制支持掩码减轻背景影响，计算余弦距离并进行最小 - 最大归一化得到先验引导概率图。</li><li><strong>语义特征融合模块（SFF）</strong>：构建SFF模块解决特征耦合问题，增强模型对变化的鲁棒性。采用金字塔结构对不同尺度的特征进行上采样和下采样，通过1×1卷积合并特征生成中间尺度特征，最后插值和拼接生成新的融合特征，促进不同尺度特征的有效交互和集成。</li><li><strong>损失函数（Loss Function）</strong>：采用交叉熵损失作为主要损失函数$L_{main}$，并引入中间监督$L_{aux}$，总损失L是$L_{main}$和$L_{aux}$的加权和，其中λ设为1.0。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：iSAID-5i、DLRSD</p><ul><li><strong>SAID - 5i数据集</strong>：将提出的模型与其他流行方法进行比较，结果表明该模型明显优于当代少样本分割模型，随着骨干网络的增强，性能提升。在不同设置下，模型在各折数据上均有显著的mIoU提升，且通过配对t检验验证了模型性能提升的显著性。</li><li><strong>DLRSD数据集</strong>：在更具挑战性的DLRSD数据集上进行实验，结果显示该模型的mIoU得分高于其他方法，尤其在处理复杂场景和具有细微视觉差异的对象时表现出色。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-35-15"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-38.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-38.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-35-38"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li>消融实验：使用ResNet - 50骨干网络在1 - shot设置下进行广泛的消融实验，分析提出模块的有效性和不同设置的影响。<ul><li><strong>GPML模块</strong>：通过实验表明，基于图结构的GPML模块相比基于卷积结构的原型学习，能进一步提高模型性能，更好地捕捉航空图像中对象之间的复杂关系。</li><li><strong>SFF模块</strong>：实验验证了SFF模块在不同尺度下的特征融合策略的有效性，特别是引入图结构和合并操作后，模型的平均性能显著提升。</li><li><strong>效率评估</strong>：通过比较不同网络的参数数量和每秒帧数（FPS），证明了提出的模型在保持较低参数数量的同时，实现了较高的FPS，在少样本航空图像分割中具有高效性。</li></ul></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-36-29"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-39.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-39.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-36-39"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者聚焦于<strong>航拍影像少样本语义分割</strong>中特征建模不佳和语义模糊的问题，提出了名为<strong>DSMF-Net</strong>的少样本分割网络。该网络采用<strong>双度量学习和多语义信息融合</strong>，增强了模型的解析和表达能力。具体而言，通过尺度感知图原型以图投影方式挖掘多尺度全局语义上下文，集成先验引导度量学习增强高层语义上下文，设计基于金字塔的融合模块更好地提取和浓缩语义特征。在<strong>iSAID - 5i和DLRSD</strong>两个具有挑战性的基准数据集上的实验表明，该方法性能优越，能有效处理有限样本下的密集预测任务。不过，作者也指出，未来需在更大、更多样化的数据集上进一步评估，以全面了解其能力和局限性，后续研究将关注模型对不同航空影像类型的适应性、在大规模数据集上的泛化能力和计算复杂度。 </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Kill Two Birds with One Stone Domain Generalization for Semantic Segmentation via Network Pruning</title>
      <link href="/post/kill-two-birds-with-one-stone-domain-generalization-for-semantic-segmentation-via-network-pruning/"/>
      <url>/post/kill-two-birds-with-one-stone-domain-generalization-for-semantic-segmentation-via-network-pruning/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>浙江大学、内华达大学</strong></p></blockquote><p>::: tip</p><p>启发</p><p>:::</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Deep model</strong>s are notoriously known to perform poorly when encountering new domains with different statistics. To alleviate this issue, we present a new domain generalization method based on network pruning, dubbed NPDG. Our core idea is to prune the filters or attention heads that are more sensitive to domain shift while preserving those domain-invariant ones. To this end, we propose a new pruning policy tailored to improve generalization ability, which identifies the filter and head sensibility of domain shift by judging its activation variance among different domains (unary manner) and its correlation to other filter (binary manner). To better reveal those potentially sensitive filters and heads, we present a differentiable style perturbation scheme to imitate the domain variance dynamically. NPDG is trained on a single source domain and can be applied to both CNN- and Transformer-based backbones. To our knowledge, we are among the pioneers in tackling domain generalization in segmentation via network pruning. NPDG not only improves the generalization ability of a segmentation model but also decreases its computation cost. Extensive experiments demonstrate the state-of-the-art generalization performance of NPDG with a lighter-weight structure.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>众所周知，深度模型在遇到具有不同统计数据的新领域时表现不佳。为了解决这个问题，我们提出了一种新的基于<strong>网络剪枝</strong>的域泛化方法，称为NPDG。我们的核心思想是剪枝过滤器或注意头，更敏感的领域转移，同时保留那些领域不变的。为此，我们提出了一种新的剪枝策略来提高泛化能力，该策略通过判断不同域之间的激活方差(一元方式)和与其他滤波器的相关性(二值方式)来识别滤波器和域漂移的头部敏感性。为了更好地揭示那些潜在的敏感滤波器和头部，我们提出了一种可微风格的摄动方案来动态地模拟域方差。NPDG在单一源域上训练，可以应用于基于CNN和transformer的主干。据我们所知，我们是通过网络剪枝处理分割领域泛化的先驱之一。NPDG不仅提高了分割模型的泛化能力，而且降低了分割模型的计算量。大量的实验证明了具有较轻重量结构的NPDG具有最先进的泛化性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>‍深度学习方法在训练和测试数据独立同分布时，能在各种视觉任务中取得显著成功。然而在实际应用中，深度模型部署到统计特性不同的新环境时，性能会大幅下降。为缓解这一问题，<strong>领域泛化</strong>（<strong>DG</strong>）被提出，旨在增强深度神经网络对未见目标分布的泛化能力。 与领域自适应（DA）不同，DG在训练时无法获取目标域数据，更具挑战性。DG研究主要分为多源和单源两种设置，多源DG假设各源域存在共享因素，但多源样本获取和标注耗时费力，单源DG更具现实意义，因此成为研究热点。 现有单源DG方法多通过数据增强或风格迁移创建多个增强域来模拟未见域，但数据生成与下游任务独立，导致结果欠佳。还有方法尝试让模型学习域不变表示或解耦潜在表示，但存在网络结构或损失函数设计复杂，以及域无关特征占用存储空间和推理时间的问题。 基于此，本文提出一种基于网络剪枝的单源领域泛化方法NPDG，旨在解决上述问题，提高模型泛化能力并降低计算成本。 </p><p>‍</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><p>‍</p><ul><li><strong>领域适应与泛化</strong>：领域适应（DA）和领域泛化（DG）旨在让模型在未标记目标域表现良好。DA可获取目标数据，有分布对齐、合成样本等策略；DG无法获取目标数据，更具挑战性，分为多源和单源方法，单源DG因数据收集和标注成本高而受关注。</li><li><strong>网络剪枝</strong>：旨在减少网络复杂度，分为非结构化和结构化剪枝。多数现有方法用于图像分类，未考虑领域差距，部分跨领域剪枝方法聚焦粗粒度视觉任务。</li></ul><h2 id="提出的模型（NPDG）"><a href="#提出的模型（NPDG）" class="headerlink" title="提出的模型（NPDG）"></a>提出的模型（NPDG）</h2><p>‍</p><ol><li>可微风格扰动（Differentiable Style Perturbation, DSP）模块：<ul><li>受AdaIN启发，通过额外的领域变分自编码器（D - VAE）将风格统计信息编码为标准分布，动态生成具有高多样性的域外数据，以匹配模型的剪枝状态。</li><li>训练目标是最小化包含AdaIN损失、KL散度损失和重建损失的总损失。</li><li>在部署阶段，通过干扰采样向量ε生成任意新领域，且梯度可直接反向传播到ε，使整个生成过程可微。</li></ul></li><li>网络剪枝策略：<ul><li>为每个滤波器引入可学习的缩放因子γ，通过联合训练后缩放因子接近零的滤波器被认为是要被修剪的。</li><li>对于基于CNN的骨干网络，将γ重新用于批量归一化（BN）层；对于基于Transformer的模型，为每个注意力激活分配缩放因子。</li><li>训练目标包括任务损失和稀疏正则化项，通过修改稀疏正则化函数F(γ)来重新加权香草L1正则化，以抑制对域敏感的滤波器或注意力头。</li></ul></li><li>滤波器&#x2F;头敏感性度量：<ul><li><strong>一元滤波器&#x2F;头敏感性（Unary Filter&#x2F;Head Sensitivity）</strong>：测量第i个滤波器&#x2F;头在域转移下的激活方差，通过对共享相同内容但不同风格的小批量图像进行前向传播，计算激活图的方差并归一化得到wU i。</li><li><strong>二元滤波器&#x2F;头敏感性（Binary Filter&#x2F;Head Sensitivity）</strong>：考虑域转移下滤波器之间的二元关系，通过计算协方差矩阵并对其行求和得到wB i，以识别与同一层中其他滤波器高度相关的滤波器。</li><li>最终的滤波器敏感性w由一元和二元滤波器敏感性加权求和得到，即w &#x3D; λwU + (1 - λ)wB，其中λ是控制两者相对重要性的超参数。</li></ul></li></ol><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-31-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-31-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-31-14"></p><p>‍</p><p>‍</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p><strong>数据集：GTA5、SYNTHIA、Cityscapes、BDD100K、Mapillary</strong></p></blockquote><p>‍</p><ul><li><strong>与基于DG的方法比较</strong>：在合成到真实、真实到合成和跨真实的DG任务中，NPDG与多个先进的DG方法对比。仅使用DSP就能大幅提升基线方法的mIoU，在此基础上进行域敏感滤波器剪枝可进一步提高泛化能力。与SHADE方法相比，NPDG在多数情况下表现更优，且模型结构更轻、计算成本更低。在使用Transformer作为分割骨干时，NPDG在所有目标数据集上至少比基线模型提高4%，在7个DG任务中的3个达到了最优结果。</li><li><strong>鲁棒性</strong>：通过给出实验的标准差评估NPDG的鲁棒性。各剪枝迭代可能导致模型结构略有不同，使指标有轻微波动，但方差不大。较高的剪枝率会导致更大的偏差，与仅使用DSP的模型相比，域敏感滤波器剪枝带来了显著提升。</li><li><strong>效率</strong>：与现有DG方法相比，NPDG能用更轻量级的模型达到最优的分割精度，节省超过17 GFLOPs和35M参数。但进一步提高剪枝率（超过30%）会损害泛化性能，因为分割是细粒度任务，过多剪枝会降低语义边界的分割精度。</li></ul><p>‍</p><p>‍</p><p>‍<strong>与网络剪枝（NP）方法比较</strong>：选择在普通分类和分割任务中有效的NP方法，在ResNet - 101和VGG - 16上评估剪枝性能。由于这些方法在滤波器或权重剪枝时未考虑域偏移，在新域中的性能比未剪枝的基线模型下降。其中，SFP方法的GFLOPs和内存成本最低，而基于Network Slimming的方法（包括NPDG）在隐式训练过程中剪枝滤波器，大量剪枝滤波器位于浅层。</p><p>‍</p><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-35-11"></p><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-35-18"></p><p>‍</p><h2 id="实验（Ablation-Experiments）🥇"><a href="#实验（Ablation-Experiments）🥇" class="headerlink" title="实验（Ablation Experiments）🥇"></a>实验（Ablation Experiments）🥇</h2><p>‍对NPDG的核心组件（DSP模块、一元和二元滤波器敏感性）进行消融研究。所有组件都有助于提高基线模型的泛化性能，DSP生成的新变体域比随机采样策略效果更好。结合一元和二元滤波器敏感性可使mIoU达到最高，表明两者具有互补作用。</p><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-37-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-37-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-37-26"></p><h1 id="‍超参数研究"><a href="#‍超参数研究" class="headerlink" title="‍超参数研究"></a>‍超参数研究</h1><ul><li><strong>一元和二元权重的比例λ</strong>：通过网格搜索确定λ值，ResNet101和MiT - B5模型在λ &#x3D; 0.4时性能最优，表明一元和二元敏感性剪枝都有贡献，二元剪枝效果略更明显。</li><li><strong>剪枝率r</strong>：剪枝率是灵活参数，在分割任务中，过高的剪枝率会导致边缘模糊，影响分割性能。实验表明，最优剪枝率在20% - 40%之间，约30%时泛化效果最佳。可使用验证集找到剪枝率和mIoU的精确权衡，实际应用中，若没有验证集，使用约30%的剪枝率通常可行。</li><li><strong>剪枝阈值t</strong>：剪枝阈值不是非常敏感的超参数，在一定范围内取值均可。只要稀疏训练迭代次数足够，就能识别出满足要求的滤波器。一般设置t &#x3D; 0.1。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种基于<strong>网络剪枝</strong>的<strong>领域泛化</strong>方法NPDG，得出以下结论：</p><ol><li><strong>创新性</strong>：NPDG通过定制的剪枝策略，从一元和二元分析辨别滤波器对领域偏移的敏感性，同时引入可微风格扰动方案动态模拟领域变化，助力识别敏感滤波器，是利用网络剪枝解决领域泛化问题的先驱。 </li><li><strong>有效性</strong>：在CNN和Transformer架构上的大量实验表明，NPDG能以更轻量级的模型实现语义分割泛化的最优性能。 </li><li><strong>局限性与展望</strong>：当前NPDG主要考虑风格差异导致的领域偏移，难以识别所有因素，未来将全面解决领域偏移问题。此外，目前依赖经验值选择超参数，未来需开发测试时训练的方法确定超参数。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Stronger, Fewer, &amp; Superior Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation（DGSS）</title>
      <link href="/post/stronger-fewer-superior-harnessing-vision-foundation-models/"/>
      <url>/post/stronger-fewer-superior-harnessing-vision-foundation-models/</url>
      
        <content type="html"><![CDATA[<h2 id="中国科学技术大学，上海人工智能实验室"><a href="#中国科学技术大学，上海人工智能实验室" class="headerlink" title="中国科学技术大学，上海人工智能实验室"></a><strong>中国科学技术大学，上海人工智能实验室</strong></h2><p><a href="https://github.com/w1oves/Rein.git">https://github.com/w1oves/Rein.git</a></p><blockquote><p>摘要：In this paper, we first assess and harness various Vision Foundation Models (VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS). Driven by the motivation that Leveraging Stronger pre-trained models and Fewer trainable parameters for Superior generaliz- ability, we introduce a robust fine-tuning approach, namely “Rein”, to parameter-efficiently harness VFMs for DGSS. Built upon a set of trainable tokens, each linked to distinct instances, Rein precisely refines and forwards the feature maps from each layer to the next layer within the backbone. This process produces diverse refinements for different categories within a single image. With fewer trainable parameters, Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full parameter fine-tuning. Extensive experiments across various settings demonstrate that Rein significantly outperforms state-of-the-art methods. Remarkably, with just an extra 1% of trainable parameters within the frozen backbone, Rein achieves a mIoU of78.4% on the Cityscapes, without accessing any real urban-scene datasets. Code is available at <a href="https://github.com/w1oves/Rein.git">https://github.com/w1oves/Rein.git</a>.</p></blockquote><blockquote><p>翻译：在本文中，我们首先在领域泛化语义分割（DGSS）任务中，评估并应用了多种视觉基础模型（VFM）。我们提出的动机是：“通过利用更强大的预训练模型和更少的可训练参数，获得更好的泛化能力”。基于此，我们提出了一种高效的微调方法——“Rein”，该方法能够以参数高效的方式利用VFM来解决DGSS任务。Rein方法依赖于一组可训练的标记，每个标记与特定实例对应，能够精确地细化并将特征图从每一层传递到骨干网络的下一层。这样，Rein能够在单张图像中为不同的类别生成多样化的细化结果。通过减少可训练的参数，Rein在微调VFM时，效果出乎意料地优于完全参数微调。通过广泛的实验验证，Rein显著超越了现有的最先进方法。值得一提的是，仅在冻结的骨干网络中增加1%的可训练参数，Rein便在Cityscapes数据集上达到了78.4%的mIoU，而且无需使用任何真实的城市场景数据集。代码已发布，您可以通过<a href="https://github.com/w1oves/Rein.git%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/w1oves/Rein.git访问。</a></p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-26_21-13-37"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-26_21-13-59"></p><p>​                                                                                <strong>模型结构图</strong></p><p><strong>本文的研究背景：</strong> </p><ul><li><strong>传统DGSS方法的局限</strong>：以往DGSS方法着重提升模型在多未见领域的预测准确性，但多采用VGGNet、MobileNetV2和ResNet等经典骨干网络，且依赖复杂数据增强和领域不变特征提取策略，对更强的VFMs在DGSS中的效能探索不足。 </li><li><strong>VFMs的潜力与挑战</strong>：近年来，CLIP、MAE、SAM等大规模VFMs显著提升了计算机视觉任务的性能，其在不同未知场景下展现出强大泛化能力。然而，将VFMs用于DGSS任务存在挑战，常用数据集规模远小于ImageNet，对VFMs大量可训练参数进行微调会导致泛化能力受限，且现有的参数高效微调策略大多不适用于DGSS。-</li><li><strong>研究动机</strong>：基于利用更强预训练模型和更少可训练参数实现更优泛化能力的动机，作者评估并利用VFMs进行DGSS研究，提出“Rein”微调方法，以高效利用VFMs解决DGSS问题。</li></ul><p><strong>研究现状：</strong></p><ul><li><strong>领域广义语义分割（DGSS）</strong>：传统方法聚焦提升模型跨多未见领域的预测准确性，采用复杂数据增强和领域不变特征提取策略，多使用VGGNet、MobileNetV2等旧骨干网络。 </li><li><strong>视觉基础模型（VFMs）</strong>：如CLIP、MAE、SAM等在计算机视觉挑战中表现出色，具有显著的跨场景泛化能力，但在DGSS任务中的表现缺乏专门研究。 </li><li><strong>参数高效微调（PEFT）</strong>：在自然语言处理领域取得成功，部分方法开始应用于计算机视觉，但大多不是为DGSS设计，难以对单张图像中不同实例的特征进行细化。</li></ul><p><strong>研究思路：</strong></p><p>本文聚焦于在领域泛化语义分割（DGSS）中利用视觉基础模型（VFMs），研究思路清晰，具体如下：</p><ol><li><strong>提出问题</strong>：先前DGSS方法多采用传统骨干网络，而大规模VFMs虽在计算机视觉挑战中表现出色，但在DGSS中的性能及利用方式尚不明确。因此，作者提出评估VFMs在DGSS中的性能以及如何有效利用VFMs的问题。</li><li><strong>构建框架</strong>：以利用更强预训练模型和更少可训练参数实现更优泛化能力为动机，作者引入<strong>Rein</strong>微调方法，在骨干网络层间嵌入该机制，以有效利用VFMs的强大能力。</li><li><strong>选择方法</strong>：选择CLIP、MAE、SAM、EVA02和DINOv2等五种不同训练策略和数据集的VFMs进行评估。设置“Full”和“Freeze”两个基本基线，并提出“Rein”方法。采用AdamW优化器，设置特定学习率、迭代次数、批量大小等进行训练。</li><li><strong>分析数据</strong>：在多个数据集和三种泛化设置下进行实验，对比Rein与现有DGSS和参数高效微调（PEFT）方法的性能。通过消融实验分析Rein各组件的有效性、令牌长度和秩对模型性能的影响，以及训练速度、GPU内存使用和模型存储要求。</li><li><strong>得出结论</strong>：实验表明，冻结的VFMs性能优于先前DGSS方法，Rein以更少可训练参数显著增强VFMs的泛化能力，大幅超越现有方法。证明了VFMs在DGSS领域的巨大潜力以及Rein方法的有效性。</li></ol><p><strong>本文的创新点：</strong> </p><ol><li><strong>评估并利用视觉基础模型（VFMs）</strong>：首次在**领域泛化语义分割（DGSS）**中评估多种VFMs，证实其强大泛化能力，为该领域建立重要基准。 </li><li><strong>提出“Rein”微调方法</strong>：通过可学习令牌对特征图进行实例级细化，以较少可训练参数有效利用VFMs，显著提升泛化性，超越现有方法。</li><li><strong>设计优化策略</strong>：采用层共享MLP权重和低秩token序列，减少参数冗余，提高训练效率。</li></ol><blockquote><p>写作启发：<strong>领域泛化语义分割（DGSS）</strong>、<strong>视觉基础模型（VFMs）</strong>、<strong>参数高效微调（PEFT</strong>）</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LGAD Local and Global Attention Distillation for Efficient Semantic Segmentation</title>
      <link href="/post/lgad-local-and-global-attention-distillation-for-efficient-semantic-segmentation/"/>
      <url>/post/lgad-local-and-global-attention-distillation-for-efficient-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Shaoxing University、Central South University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Efficient <strong>semantic segmentation</strong> is essential for a wide array of computer vision applications, and knowledge distillation has emerged as a promising methodology for model compression and efficiency. However, we observed that an excess of positive pixels can dilute attention weights, hindering the student model’s learning process. To tackle this significant challenge, we introduce the Local and Global Attention Distillation (LGAD) framework, a pioneering block-based technique that distills both local and global attention. The LGAD framework segments feature maps and output probabilities into well-defined local and global blocks, effectively mitigating the dilution of attention weights. By doing so, it enhances the distinction between positive and negative pixels, particularly amplifying the focus on salient regions within each local and global block. We have conducted comprehensive experiments on three benchmark datasets, Cityscapes, CamVid, and Pascal VOC 2012. The experiment results demonstrate the effectiveness of our proposed LGAD and confirm its superiority over several state-of-the-art distillation methods for semantic segmentation.</p><p>::: tip</p><p><strong>正像素：通常表示目标、高亮度、激活区域或有效数据。</strong></p><p><strong>负像素：通常表示背景、低亮度、抑制区域或噪声&#x2F;无效数据</strong></p><p>:::</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>高效的语义分割对于广泛的计算机视觉应用至关重要，知识蒸馏已经成为一种很有前途的模型压缩和效率方法。然而，我们观察到过多的正像素会稀释注意力权重，阻碍学生模型的学习过程。为了应对这一重大挑战，我们引入了局部和全局注意力蒸馏(LGAD)框架，这是一种开创性的基于块的技术，可以提取局部和全局注意力。LGAD框架将特征图和输出概率分割为定义良好的局部和全局块，有效地减轻了注意力权重的稀释。通过这样做，它增强了正像素和负像素之间的区别，特别是放大了对每个局部和全局块内显著区域的关注。我们在cityscape、CamVid和Pascal VOC 2012三个基准数据集上进行了全面的实验。实验结果证明了我们所提出的语义分割方法的有效性，并证实了它比几种最先进的语义分割方法的优越性。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割在<strong>自动驾驶、机器人导航</strong>等众多实际应用中至关重要，深度卷积神经网络（DCNNs）成为主流方法，如PSPNet、DeepLab系列等。然而，这些模型存在存储和计算开销大的问题，限制了其在现实场景中的部署，因此开发紧凑且高效的分割模型成为研究热点。 知识蒸馏是一种有前景的模型压缩技术，可通过将大而复杂模型（教师模型）的知识转移到小模型（学生模型）来提升学生模型性能。已有研究者将知识蒸馏引入高效语义分割并提出多种框架，但现有知识蒸馏方法主要集中于全局交互的蒸馏。 研究发现过多正像素会稀释注意力权重，导致正、负像素注意力值差距小，阻碍学生模型识别和学习特征。如图1所示，正像素注意力值约为0.0004，背景像素近于零。为解决这一挑战，作者提出了**Local and Global Attention Distillation（LGAD）**框架，旨在通过将特征图和输出概率划分为局部和全局块，增强正、负像素注意力值差距，提升学生模型的语义分割性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>语义分割模型</strong>：<strong>深度卷积神经网络（DCNNs）<strong>成为主流方法，如PSPNet、DeepLab系列等取得了不错的性能，但存在存储和计算开销大的问题。为解决此问题，出现了一些</strong>轻量级框架</strong>，如ENet、SegNet等，还有基于<strong>Vision Transformer</strong>的语义分割框架。</li><li><strong>知识蒸馏</strong>：作为模型压缩的有效技术，被广泛应用于语义分割。现有方法主要集中在对齐中间特征图和输出概率，如SKDS、IFVD、IDD等，但大多聚焦于全局交互的蒸馏。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-44-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-44-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-44-32"></p><p>本文提出了用于高效**语义分割的局部和全局注意力蒸馏（Local and Global Attention Distillation，LGAD）**框架。</p><p><strong>模型组件</strong></p><ul><li><strong>局部注意力特征蒸馏（Local Attention Feature Distillation）</strong>：聚焦于蒸馏教师模型中间特征图中的有价值知识，让学生模型关注教师模型表示中的关键区域。</li><li><strong>局部注意力输出蒸馏（Local Attention Output Distillation）</strong>：旨在蒸馏分割输出中的有价值知识，使学生模型获得教师模型关注特定感兴趣区域并进行精确预测的能力。</li><li><strong>全局注意力特征蒸馏（Global Attention Feature Distillation）</strong>：与局部注意力特征蒸馏类似，但侧重于全局层面的特征蒸馏。</li><li><strong>全局注意力输出蒸馏（Global Attention Output Distillation）</strong>：与局部注意力输出蒸馏类似，但侧重于全局层面的输出蒸馏</li></ul><p><strong>损失函数</strong>：总损失函数为$L &#x3D; L_{seg} + \lambda_1 \cdot (L_{lafd} + L_{gafd}) + \lambda_2 \cdot (L_{laod} + L_{gaod})$，其中$L_{seg}$是语义分割的交叉熵损失，$\lambda_1 &#x3D; 30$和$\lambda_2 &#x3D; 3$是两个超参数，用于平衡LGAD框架中不同组件的影响。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：Cityscapes、Pascal VOC 2012、CamVid</p><p>评估指标：mIoU、模型参数、浮点运算次数（FLOPs）</p><ul><li><strong>Cityscapes</strong>：在验证集上，对于PSPNet - R18学生模型，LGAD将mIoU提升了7.12%，优于SKDS、IFVD、CWD和APD等方法；对于PSPNet - B0学生模型，LGAD使mIoU提升了7.39%，也超过了对比方法。</li><li><strong>CamVid</strong>：在测试集上，LGAD能提升两个学生模型的性能。对于PSPNet - R18，提升了2.9%，超过SKDS、IFVD和CWD；对于PSPNet - B0，提升了3.5%，同样优于对比方法。</li><li><strong>Pascal VOC 2012</strong>：在验证集上，LGAD大幅提升了两个学生模型的性能。对于PSPNet - R18，mIoU提升了5.39%；对于PSPNet - B0，提升了2.98%，均超过了对比方法。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-49.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-49.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-48-49"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-48-58"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-03.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-03.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-49-03"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>不同损失项的有效性</strong>：在Cityscapes验证集上，以PSPNet - R18为学生模型进行实验。结果表明，局部和全局注意力输出蒸馏损失（Llaod和Lgaod）能独立提升学生模型性能，且二者结合效果更佳；局部和全局注意力特征蒸馏损失（Llafd和Lgafd）同时应用时也能进一步提升性能；当四个蒸馏损失项都应用时，提升达到7.12%，验证了LGAD的有效性和整合局部与全局注意力蒸馏的重要性。</li><li><strong>窗口大小的影响</strong>：研究了不同局部块数量（P×P）和全局块数量（G×G）对蒸馏的影响，设置P &#x3D; G &#x3D; {1, 2, 4, 8}。结果显示，窗口数量为2×2时带来最高的mIoU值76.22%，随着窗口数量增加，学生模型的mIoU得分略有下降，这表明窗口数量过大时，学生模型可能难以有效学习。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-49-40"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者针对<strong>知识蒸馏</strong>在语义分割中存在的正像素过多稀释注意力权重、阻碍学生模型学习的问题，提出了<strong>Local and Global Attention Distillation（LGAD）框架</strong>。该框架将特征图和输出概率划分为块，设计局部和全局注意力蒸馏方法，增强了学生模型识别和学习判别特征的能力。 通过在Cityscapes、CamVid和Pascal VOC 2012三个基准数据集上的大量实验，验证了LGAD框架的有效性，且其性能优于多个现有最先进的知识蒸馏方法。作者认为该方法在语义分割领域引入了有意义的进展，为开发轻量级且准确的密集预测模型提供了有价值的见解。 </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Class Tokens Infusion for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/class-tokens-infusion-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/class-tokens-infusion-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Weakly Supervised Semantic Segmentation (WSSS)</strong> relies on Class Activation Maps (CAMs) to extract spatial information from image-level labels. With the success of Vision Transformer (ViT), the migration of ViT is actively conducted in WSSS. This work proposes a novel WSSS framework with Class Token Infusion (CTI). By infusing the class tokens from images, we guide class tokens to possess class-specific distinct characteristics and global-local consistency. For this, we devise two kinds of token infusion: 1) Intra-image Class Token Infusion (I-CTI) and 2)Cross-image Class Token Infusion (C-CTI). In I-CTI, we infuse the class tokens from the same but differently augmented images and thus make CAMs consistent among var-<br>ious deformations (i.e. view, color). In C-CTI, by infusing the class tokens from the other images and imposing the resulting CAMs to be similar, it learns class-specific distinct characteristics. Besides the CTI, we bring the background (BG) concept into ViT with the BG token to reduce the false positive activation ofCAMs. We demonstrate the effectiveness ofour method on PASCAL VOC 2012 and MS COCO 2014 datasets, achieving state-of-the-art results in weakly supervised semantic segmentation. The code is available at <a href="https://github.com/yoon307/CTI">https://github.com/yoon307/CTI</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>弱监督语义分割(WSSS)依靠类激活图(CAMs)从图像级标签中提取空间信息。随着视觉transformer(ViT)的成功，视觉transformer的迁移在WSSS中得到了积极的开展。本文提出了一种基于类标记注入(CTI)的WSSS框架。通过注入来自图像的类标记，我们引导类标记具有特定于类的独特特征和全局-局部一致性。为此，我们设计了两种标记注入:1)图像内类标记注入(I-CTI)和2)跨图像类标记注入(C-CTI)。在I-CTI中，我们从相同但不同的增强图像中注入类标记，从而使cam在各种变形(即视图，颜色)之间保持一致。在C-CTI中，通过注入来自其他图像的类标记并强制生成的cam相似，它学习特定于类的独特特征。除了CTI之外，我们还通过BG标记将背景(BG)概念引入ViT，以减少cam的误报激活。我们在PASCAL VOC 2012和MS COCO 2014数据集上证明了我们的方法的有效性，在弱监督语义分割中取得了最先进的结果。代码可在<a href="https://github.com/yoon307/CTI%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/yoon307/CTI上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法存在的问题，具体研究背景如下： </p><ul><li><strong>WSSS的兴起</strong>：全监督语义分割虽在多领域表现出色，但标注成本高、耗时长。为减轻标注负担，WSSS应运而生，其利用图像级标签、涂鸦和边界框等弱监督信息进行研究，其中仅利用图像级分类标签的设置最具实用性和挑战性。</li><li><strong>现有方法的局限性</strong>：传统WSSS研究多依赖卷积神经网络（CNNs）生成类激活图（CAMs），但CNN的感受野有限，导致CAMs存在稀疏性问题，仅关注物体的判别区域。Vision Transformer（ViT）虽能缓解该问题，但原始ViT使用单类令牌进行分类，定位图缺乏类别特异性，且基于多类令牌的ViT仍存在类令牌特征表示相关性高、CAMs过度扩展导致假阳性区域增加等问题。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：利用图像级标签、涂鸦、边界框等弱监督信息进行研究，其中基于图像级分类标签的研究最具挑战性。主要通过类激活图（CAMs）定位目标，为提高CAMs精度，出现了对抗擦除、局部 - 全局一致性等方法，也有不少工作对CAMs进行后处理以获取可靠标签。</li><li><strong>基于视觉Transformer（ViT）的WSSS</strong>：ViT凭借自注意力机制能捕捉长距离依赖，缓解了CAMs稀疏性问题。一些工作采用多类令牌或直接用补丁令牌训练分类器来提取特定类别的激活图。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>本文提出了一种基于视觉Transformer（ViT）的弱监督语义分割（WSSS）框架，该框架引入了类令牌注入（Class Token Infusion，CTI）和背景令牌（Background Token，BGT）两种方法，以解决传统多类令牌WSSS方法的局限性，生成更精确的类激活图（Class Activation Maps，CAMs）。</p><h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p>使用三个配对图像进行训练，将每个图像分割成 $N \times N$ 个补丁并嵌入为补丁令牌 $T_{patch}$，同时使用 $C$ 个前景类令牌 $T_{cls - fg}$ 和 1 个背景类令牌 $T_{cls - bg}$ 组成输入类令牌 $T_{cls}$。将类令牌和补丁令牌连接形成输入令牌 $T_{input}$，添加位置嵌入后输入到 $L$ 个Transformer块中。从最后一个Transformer块的补丁令牌输出 $T_{L_{patch}}$ 中获取CAMs $M$，并通过池化类令牌输出 $T_{L_{cls}}$ 得到类预测 $y_{pred}$，使用多标签软边缘损失计算分类损失 $L_{cls}$ 和补丁级分类损失 $L_{cls - patch}$。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-29_21-01-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-29_21-01-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-29_21-01-48"></p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><ol><li><strong>数据集</strong>：选用PASCAL VOC 2012和MS - COCO 2014这两个广泛使用的基准数据集。前者包含20个前景对象类和1个背景类，有10582、1449和1456张图像分别用于训练集、验证集和测试集；后者更具挑战性，有82k训练集和40k验证集，包含80个前景对象类和1个背景类。</li><li><strong>评估指标</strong>：采用平均交并比（mIoU）评估语义分割性能，在验证集上评估语义分割模型，在训练集上评估类激活图（CAMs）性能，PASCAL VOC 2012测试集结果通过在线官方服务器评估。</li></ol></blockquote><ol><li><strong>PASCAL VOC数据集</strong>：在训练集上，所提方法在CAMs（种子）和伪像素级真值（掩码）方面性能均优于其他方法，相比第二好的结果，种子性能提升1.8%p，掩码性能提升0.9%p。在语义分割性能上，基于高质量标签训练的模型在验证集和测试集上均大幅超越现有技术，且语义分割模型性能优于伪标签，比第二好的模型在验证集上有超过1.7%p的提升。</li><li><strong>MS COCO数据集</strong>：在该数据集上训练和评估模型，虽数据集类别多、场景复杂，但所提方法取得45.4%的mIoU，显示出良好的泛化能力，有效减少了ViT - based WSSS方法在该数据集上因错误激活和类间激活重叠导致的性能差距。</li></ol><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ol><li>组件分析<ul><li>引入背景类令牌（BGT）和背景CAMs概念到ViT，相比基线有2.9%p的提升。</li><li>提出的图像内类令牌注入（I - CTI）额外带来1.1%p的性能提升。</li><li>结合图像间类令牌注入（C - CTI），性能提升至69.5%。</li></ul></li><li><strong>背景类令牌的重要性</strong>：训练无BGT但有BG CAM的基线模型，结果比基线下降4.1%p，凸显BGT在训练BG CAM中的重要性。</li><li><strong>类令牌注入的作用</strong>：通过t - SNE可视化不同层的类令牌，表明所提方法的类令牌在各层特征空间区分度好，生成的CAMs更具独特性，不侵犯其他类区域，而基线的类令牌区分度差，CAMs存在错误激活区域。</li><li><strong>注入索引的影响</strong>：改变注入索引L1从2到10（总层数L为12），mIoU性能在L1设为3时最高，虽不同索引有轻微性能差异，但均高于无CTI的情况，因此将注入索引设为3。</li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者旨在增强ViT中类别令牌的特定类别表示能力，以实现图像中物体的清晰定位，得出以下结论：</p><ol><li><strong>提出CTI方法</strong>：提出两种类别令牌注入（CTI）方法，即图像内类别令牌注入（I - CTI）和跨图像类别令牌注入（C - CTI）。I - CTI使激活图具有全局 - 局部一致性，C - CTI让类别令牌和激活图具备跨图像的一致特定类别知识。 </li><li><strong>引入背景令牌</strong>：将背景令牌（BGT）引入ViT，有效解决了激活图过度扩展问题，减少了错误激活。</li><li><strong>实验验证有效性</strong>：在PASCAL VOC 2012和MS COCO 2014数据集上的大量实验结果，支持了所提方法的有效性和泛化性，该方法在两个数据集上均达到了最先进水平。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation</title>
      <link href="/post/use-universal-segment-embeddings-for-open-vocabulary-image-segmentation/"/>
      <url>/post/use-universal-segment-embeddings-for-open-vocabulary-image-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Bosch Research North America、Bosch Center for Artificial Intelligence (BCAI)</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>The <strong>open-vocabulary image segmentation</strong> task involves partitioning images into semantically meaningful segments and classifying them with flexible text-defined categories. The recent vision-based foundation models such as the Segment Anything Model (SAM) have shown superior performance in generating class-agnostic image segments. The main challenge in open-vocabulary image segmentation now lies in accurately classifying these segments into text-defined categories. In this paper, we introduce the Universal Segment Embedding (USE) framework to address this challenge. This framework is comprised of two key components: 1) a <strong>data pipeline</strong> designed to efficiently curate a large amount of segment-text pairs at various granularities, and 2) a <strong>universal segment embedding model</strong> that enables precise segment classification into a vast range oftext defined categories. The USE model can not only help open-vocabulary image segmentation but also facilitate otherdownstream tasks (e.g., querying and ranking). Through comprehensive experimental studies on semantic segmen-<br>tation and part segmentation benchmarks, we demonstrate that the USE framework outperforms state-of-the-art open-<br>vocabulary segmentation methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>开放词汇图像分割任务包括将图像划分为语义上有意义的片段，并使用灵活的文本定义类别对其进行分类。近年来，基于视觉的基础模型(如SAM)在生成与类别无关的图像片段方面表现出了优异的性能。目前，开放词汇表图像分割的主要挑战在于将这些片段准确地分类到文本定义的类别中。在本文中，我们引入了通用段嵌入(USE)框架来解决这一挑战。该框架由两个关键组件组成:1)一个数据解决方案，旨在有效地管理各种粒度的大量片段-文本对;2)一个通用的片段嵌入模型，能够将精确的片段分类到大量文本定义的类别中。USE模型不仅可以帮助开放词汇表图像分割，还可以促进其他下游任务(例如查询和排序)。通过对语义切分和零件切分基准的综合实验研究，我们证明了USE框架优于最先进的开放词汇切分方法。</p><p>::: tip</p><p>启发</p><p>:::</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>开放词汇图像分割的目的是将图像分割成语义上有意义的片段，并用文本定义的任意类对其进行分类。最近的基础模型在将图像的像素分为有意义的片段上效果显著，例如SAM，然而，现有的开放词汇图像分割方法面临着挑战：<strong>端到端的方法</strong>不能将基础模型生成的图像段作为输入或提示来分配类标签；<strong>两阶段的方法</strong>由于人类标签的限制，它们在分类不同粒度的片段方面仍然受到限制。基于上述的存在的问题，本文的作者提出了<strong>通用片段嵌入框架（Universal Segment Embedding，USE）</strong>，该框架由两个关键的组件：数据方案和通用片段嵌入模型。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li>多模态表示学习：</li></ul><p>从大规模图像-文本数据中学习(例如CLIP)在将视觉概念与文本描述联系起来方面显示出了很好的结果，然而，对于文本数据的多模态表示学习，目前的研究还很少。</p><ul><li>开放词汇图像分割：</li></ul><p>在<strong>自动驾驶</strong>等现实世界视觉任务需求日益增长的驱动下，开放词汇图像分割的重要性正在迅速增长，现有的方法可以分为两类:端到端方法和两阶段方法，但是上述两种方法都存在不足，不能满足本文的任务需求。</p><ul><li>改进图像-文本数据集：</li></ul><p>现有的工作可以分为两类:数据过滤和数据改进。数据过滤旨在通过过滤噪声的图像-文本对来提高模型训练的效率和鲁棒性，而数据改进则侧重于改善图像和文本数据的对齐。</p><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><strong>USE Data Pipeline：</strong></p><ol><li><strong>多粒度图像字幕生成</strong>：利用多模态大语言模型（MLLM），通过特定提示生成图像中对象及其属性的详细描述，以获取更丰富的语义信息。</li><li><strong>从字幕中进行指代表达定位</strong>：从字幕中提取名词短语并扩展为指代表达，使用开放词汇定位模型（如Grounding DINO）获取与这些表达对应的边界框，生成框 - 文本对。</li><li><strong>使用框提示生成掩码</strong>：将边界框转换为掩码，使用图像分割模型SAM生成掩码，并进行后处理，最后通过基于掩码的非极大值抑制（NMS）合并段 - 文本对。</li></ol><p><strong>USE Model：</strong></p><ol><li><strong>图像编码器</strong>：利用预训练的视觉变换器（ViTs）提取图像块嵌入，通过多级别特征合并，结合CLIP和DINOv2的信息，同时获取全局图像特征。在训练过程中，CLIP和DINOv2保持冻结，仅线性层归一化（LLN）模块和块尺度参数可训练。</li><li><strong>段嵌入头</strong>：根据输入段从图像块嵌入中提取段嵌入，并将其映射到视觉 - 语言联合空间。通过计算段在每个图像块内的面积并归一化，得到段在每个块内的权重，进而计算加权平均嵌入，最后通过线性层映射为段嵌入。</li><li><strong>训练与损失</strong>：使用段 - 文本对比损失来训练模型，在训练时，每个段随机采样一个文本描述来计算文本嵌入。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>训练集：COCO、Visual Genome (VG)       测试集：ADE20K、Pascal Context</p><blockquote><p>评估方法：使用类无关掩码，通过提示SAM生成掩码，经过滤和合并后，用模型获取掩码嵌入，计算与目标类文本嵌入的相似度，转换为概率，聚合像素上所有片段的概率进行类别预测。</p><p>对比结果：与最先进的开放词汇语义分割方法在<strong>ADE20K和Pascal Context数据集</strong>上对比，以平均交并比（mIoU）评估性能。结果表明，USE方法在所有数据集上大幅优于最先进的两阶段方法，与端到端方法相比平均性能最佳，在COCO和VG图像上训练时性能进一步提升。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-31-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-31-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-26_14-31-25"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>预训练骨干网络选择</strong>：在ADE20K数据集和开放词汇语义分割任务上研究，结果表明结合CLIP和DINOv2可获得性能提升。</li><li><strong>图像编码器架构设计</strong>：研究cls令牌对性能的影响，结果显示包含cls令牌可提高mIoU。</li><li><strong>定性比较</strong>：对比从真实标注和MLLM增强标注中提取的对象，MLLM增强标注能捕获更细粒度的对象和部件。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-33-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-33-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-26_14-33-30"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出用于<strong>开放词汇图像分割的USE框架</strong>，通过实验研究得出以下结论：</p><blockquote><ol><li><strong>方法有效性</strong>：该框架结合精心设计的数据管道和轻量级嵌入模型，能在无人工标注下以零样本方式有效对图像片段进行分类。 </li><li><strong>性能优越性</strong>：在语义分割和部件分割任务的实验中，USE框架在<strong>ADE20K、Pascal Context</strong>和<strong>PartImageNet</strong>等数据集上，大幅超越现有最先进的两阶段方法，平均性能也优于端到端方法。 </li><li><strong>研究意义</strong>：此工作为构建开放词汇图像分割的基础模型和基于片段的表征学习提供了参考，有望推动相关领域的研究发展。</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation</title>
      <link href="/post/llmformer-large-languagemodel-for-open-vocabulary-semantic/"/>
      <url>/post/llmformer-large-languagemodel-for-open-vocabulary-semantic/</url>
      
        <content type="html"><![CDATA[<p>Hunan University、Monash University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Open-vocabulary (OV) semantic segmentation</strong> has attracted increasing attention in recent years, which aims to recognize<br>objects in an open class set for real-world applications. While prior OV semantic segmentation approaches have relied on<br>additional semantic knowledge derived from vision-language (VL) pre-training, such as the popular CLIP model, this paper<br>introduces a novel paradigm by harnessing the unprecedented capabilities of large language models (LLMs). Inspired by<br>recent breakthroughs in LLMs that provide a richer knowledge base compared to traditional vision-language pre-training, our proposed methodology capitalizes on the vast knowledge embedded within LLMs for OV semantic segmentation. Particularly, we partition LLM knowledge into object, attribute, and relation priors, and propose three novel attention modules-semantic, scaled visual, and relation attentions, to utilize the LLM priors. Extensive experiments are conducted on common benchmarks including ADE20K (847 classes) and Pascal Context (459 classes). The results show that our model outperforms previous state-of-the-art (SoTA) methods by up to 7.2% absolute. Moreover, unlike previous VL-pre-training-based works, our method can even predict OV segmentation results without target candidate classes.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>开放词汇语义分割近年来受到越来越多的关注，其目的是为了在现实应用中识别开放类集中的对象。虽然之前的开放词汇语义分割方法依赖于来自视觉语言(VL)预训练的额外语义知识，例如流行的CLIP模型，但本文通过利用大型语言模型(大语言模型)前所未有的能力引入了一种新的范式。与传统的视觉语言预训练相比，大语言模型提供了更丰富的知识库，受其最新突破的启发，我们提出的方法利用大语言模型中嵌入的大量知识进行开放词汇语义分割。特别地，我们将大语言模型知识划分为对象先验、属性先验和关系先验，并提出了语义关注、尺度视觉关注和关系关注三个新的关注模块来利用大语言模型先验。在包括ADE20K(847个类)和Pascal Context(459个类)在内的常见基准测试上进行了广泛的实验。结果表明，我们的模型比以前的最先进的(SoTA)方法高出7.2%。此外，与之前基于vl预训练的工作不同，我们的方法甚至可以在没有目标候选类的情况下预测开放词汇分割结果。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>基于先验的语义分割方法可以识别固定集的目标种类，但不能够很好的处理真实世界中各种新的目标，由此引出开放词汇语义分割。开放词汇语义分割分为两种：一阶段和两阶段，尽管取得了不错的效果，但是大多数还是利用预训练的视觉语言模型提取embeding，这种方式仅提供有限的语义信息。随着大语言模型的发展，由于其提供了对场景的综合理解能力，本文作者尝试采用LLM的知识解决开放词汇语义分割中的挑战，即利用大语言模型描述中的目标名字、目标属性和目标关系。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_15-58-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_15-58-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_15-58-54"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>固定集语义分割</strong>：早期采用CNN架构，后引入多尺度组合、全局上下文建模和transformer等方法，但难以利用大预训练模型知识，且只能识别固定集对象。</li><li><strong>开放词汇语义分割</strong>：分为单阶段和两阶段方法。两阶段方法依赖训练良好的掩码生成器，计算成本高；单阶段方法虽有改进，但大多仅从视觉 - 语言预训练模型提取知识，语义信息有限。</li><li><strong>大语言模型</strong>：在许多领域取得成功，具备综合复杂推理能力，部分模型可理解视觉内容，但主要用于通用表示和预测</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-00-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-00-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-00-15"></p><ol><li><p>整体架构</p><p>LLMFormer由三个主要部分组成：</p><ul><li><strong>图像特征提取（Image Feature Extraction）</strong>：使用多模态大语言模型（MLLM）中的视觉编码器、适配器（ViT Adapter）和多尺度可变形注意力（MSDA）模块，以捕获多尺度的图像特征。</li><li><strong>LLM先验提取（LLM Prior Extraction）</strong>：通过向LLM输入问题（如“描述图像”），获取图像的全面描述，并利用语言解析工具从中提取对象、属性和关系先验知识。</li><li><strong>LLM先验引导的分割（LLM-Prior-Guided Segmentation）</strong>：引入了三种新型注意力模块，分别是语义注意力（Semantic Attention）、缩放视觉注意力（Scaled Visual Attention）和关系注意力（Relation Attention），以利用LLM的先验知识进行开放词汇语义分割。</li></ul></li><li><p>关键组件</p></li></ol><ul><li><strong>语义注意力（Semantic Attention）</strong>：将对象和属性先验知识嵌入到掩码嵌入中，以增强开放词汇对象的发现、掩码预测和分类能力。该模块通过多头交叉注意力机制，捕捉对象先验与掩码之间的对应关系。</li><li><strong>缩放视觉注意力（Scaled Visual Attention）</strong>：基于属性先验知识，为每个掩码选择合适的视觉特征图，以更好地分割不同大小的对象。具体来说，该模块根据属性先验生成掩码的属性嵌入，并通过多层感知机（MLP）预测尺度选择分数，从而选择合适尺度的特征图。</li><li><strong>关系注意力（Relation Attention）</strong>：利用LLM的关系先验知识，学习掩码之间的关系。该模块通过生成对象关系图，并将其映射到掩码级别，然后通过多头自注意力机制将关系先验知识编码到掩码嵌入中。</li></ul><p>::: tip</p><p>重要</p><p>:::</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：训练：COCO-Stuff，测试：ADE20K、Pascal Context、Pascal VOC</p></blockquote><ul><li><strong>Pascal Context数据集</strong>：与FC - CLIP相比，在PC - 459和PC - 59上分别提高了7.2%和5.8%；与SAN相比，分别提高了8.3%和4.0%。</li><li><strong>ADE20K数据集</strong>：在A - 847和A - 150上均达到了最先进的性能，超过FC - CLIP分别为1.7%和4.4%，超过使用VIT - L骨干的SAN分别为2.8%和5.2%。</li><li><strong>Pascal VOC 2012数据集</strong>：取得了最佳的mIoU，显著超过之前的SOTA方法FC - CLIP 1.4%。在Open IoU指标下也有显著提升，证明了模型的开放词汇能力。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-04-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-04-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-04-53"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-00.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-00.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-05-00"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-05-17"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>主要组件的影响</strong>：语义注意力模块能显著提升性能，基于大语言模型（LLM）属性的尺度选择和关系注意力可进一步提高性能，完整模型在A - 847和A - 150上取得最佳性能。</li><li><strong>语义注意力</strong>：使用对象先验能持续提高开放词汇分割性能，属性先验可进一步增强结果，交叉注意力在整合先验的方法中表现最优。</li><li><strong>缩放视觉注意力</strong>：多尺度方法优于单尺度方法，基于属性先验的尺度选择方法优于其他非选择方法。</li><li><strong>关系注意力</strong>：完整模型通过整合LLM关系先验到自注意力图中，在A - 847和A - 150上分别比仅使用原始注意力图的模型提高了1.7%和2.3%。</li><li><strong>不同注意力方法</strong>：模型的注意力方法显著优于掩码注意力和TSG注意力，因为它们能利用LLM先验来改进开放词汇语义分割。</li><li><strong>成本比较</strong>：与Zegformer和OV - Seg相比，模型的可训练参数更少，同时精度更高。使用LLAVA - Phi2 - 2.7B可进一步降低计算开销，性能仅有轻微下降。</li><li><strong>不同提示</strong>：详细提示（描述所有对象、属性和关系）能进一步提高分割性能，但实验中主要使用简单提示。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><blockquote><p>作者提出了LLMFormer这一利用大语言模型（LLM）知识进行开放词汇语义分割（OV）的新方法，并得出以下结论： </p></blockquote><ol><li><strong>方法有效性</strong>：提出语义、缩放视觉和关系三种注意力模块，利用LLM的对象、属性和关系先验知识进行分割，大量实验证明LLMFormer及各注意力模块有效。 </li><li><strong>性能优势</strong>：在ADE20K、Pascal Context和Pascal VOC等数据集上显著优于现有方法，绝对提升最高达7.2%，还能在无预定义候选类别的情况下预测OV分割结果，更适用于实际应用。</li><li><strong>未来方向</strong>：当前工作虽提升了OV识别能力，但LLM参数多致速度慢，且存在细粒度分类和边界分割问题，未来将研究效率、细粒度分类和分割问题。</li></ol><blockquote><p>不足及展望：使用LLM带来了巨大的参数量，导致速度减小；本文当前的工作关注于提升开放词汇的分类能力，其他的细粒度分类和边界框分割问题没有涉及，这也是未来工作的研究方法。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation</title>
      <link href="/post/corrmatch-label-propagation-via-correlation-matching-for-semi-supervised-semantic-segmentation/"/>
      <url>/post/corrmatch-label-propagation-via-correlation-matching-for-semi-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Nankai University、NKIARI, Shenzhen Futian、SICE, UESTC</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>This paper presents a simple but performant semi-supervised semantic segmentation approach, called CorrMatch. Previous approaches mostly employ complicated training strategies to leverage unlabeled data but overlook the role ofcorrelation maps in modeling the relationships between pairs oflocations. We observe that the correlation maps not only enable clustering pixels ofthe same category easily but also contain good shape information, which previous works have omitted. Motivated by these, we aim to improve the use efficiency of unlabeled data by designing two novel label propagation strategies. First, we propose to conduct pixel propagation by modeling the pairwise similarities of pixels to spread the high-confidence pixels and dig out more. Then, we perform region propagation to enhance the pseudo labels with accurate class-agnostic masks extracted from the correlation maps. CorrMatch achieves great performance on popular segmentation benchmarks. Taking the DeepLabV3+ with ResNet-101 backbone as our segmentation model, we receive a 76%+ mIoU score on the Pascal VOC 2012 dataset with only 92 annotated images. Code is available at <a href="https://github.com/BBBBchan/CorrMatch">https://github.com/BBBBchan/CorrMatch</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>本文提出了一种简单但高性能的半监督语义分割方法 CorrMatch。现有方法大多采用复杂的训练策略来利用未标注数据，但忽视了关联图在建模像素位置关系中的重要作用。我们发现，关联图不仅能够轻松实现同类像素的聚类，还包含了被以往研究忽略的优质形状信息。基于这些观察，我们设计了两种创新的标签传播策略来提升未标注数据的使用效率。首先，我们提出<strong>像素传播策略</strong>，通过建模像素对的相似性关系来扩展高置信度像素区域，并挖掘更多潜在的高置信度像素。其次，我们开发了<strong>区域传播策略</strong>，通过从关联图中提取精确的类别无关掩码来增强伪标签质量。CorrMatch 在主流分割基准测试中表现优异：当使用 ResNet-101 为主干的 DeepLabV3+ 模型时，在仅含 92 张标注图像的 Pascal VOC 2012 数据集上实现了 76%+ 的 mIoU。代码已开源：<a href="https://github.com/BBBBchan/CorrMatch%E3%80%82">https://github.com/BBBBchan/CorrMatch。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦半监督语义分割领域，旨在解决深度学习方法对大规模像素级标注数据集的依赖问题，具体研究背景如下： </p><ul><li><strong>标注成本高</strong>：基于深度学习的语义分割方法通常需要大量像素级标注图像，但准确标注分割数据集成本高、耗时长，限制了其应用。 </li><li><strong>半监督学习受关注</strong>：为减少对大规模准确标注数据的需求，研究者提出弱监督、半监督和无监督分割方法。其中，半监督语义分割仅需少量标注数据和大量未标注数据进行训练，更接近现实场景，受到广泛关注。 </li><li><strong>现有方法存在不足</strong>：现有半监督语义分割方法多采用复杂训练策略，如Mean Teacher架构或自训练策略，需要额外网络或训练阶段，增加了训练复杂度。此外，常用的固定阈值筛选伪标签方法难以有效利用未标注数据。</li><li><strong>Correlation map的潜力</strong>：像素间的相关性可反映成对相似性，相关性地图不仅能轻松聚类同一类别的像素，还包含良好的形状信息，但以往研究忽略了其在建模位置对关系中的作用。 基于以上背景，作者提出了CorrMatch方法，通过设计两种新颖的标签传播策略，提高未标注数据的使用效率。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>方法多样</strong>：在半监督语义分割领域，已有多种方法被提出，如采用Mean Teacher架构的方法（U2PL、PS - MT等）、基于自训练策略的方法（ST++、SimpleBase等），以及近期的UniMatch等单阶段框架。</li><li><strong>成果显著</strong>：这些方法在一些公开数据集（如Pascal VOC 2012、Cityscapes）上取得了一定成果，推动了半监督语义分割技术的发展。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-03-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-03-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-03-57"></p><ol><li><strong>模型框架</strong>：CorrMatch是一个单阶段框架，基于具有弱到强一致性正则化的简单框架构建。对于有标签图像，使用标准的交叉熵损失；对于无标签图像，主要通过强制预测一致性来利用，同时考虑弱增强和强增强图像在高置信区域的对数似然一致性。</li><li>标签传播策略<ul><li><strong>像素传播</strong>：通过计算相关图并将其传播到预测中，增强模型对像素对之间相似性的整体感知，从而提高无标签数据的利用率。具体步骤为：首先通过网络编码器后的线性层提取特征，计算特征向量对之间的相关性得到相关图；然后将相关图传播到模型的对数似然输出中，得到另一种预测表示；最后计算该预测表示与高置信伪标签之间的相关损失作为监督。</li><li><strong>区域传播</strong>：利用相关图中隐含的形状信息来增强伪标签。具体做法是将相关图的每一行进行归一化并转换为二值图，当二值图与高置信区域有较大重叠时，计算高置信形状内每个唯一类别的数量，找到最显著的类别，并将该类别传播到增强的伪标签和扩展的高置信区域中。为了提高效率，采用随机采样的方法。</li></ul></li><li>其他策略<ul><li><strong>动态阈值</strong>：使用与训练过程相关的动态阈值策略，避免固定阈值过严或过松对模型收敛的不利影响。通过指数移动平均（EMA）根据对数似然输出迭代更新阈值。</li><li><strong>损失函数</strong>：整体目标函数是监督损失和无监督损失的组合。监督损失是基本监督损失和监督相关损失的组合；无监督损失包括无监督硬损失、软损失和相关损失。</li></ul></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：Pascal VOC 2012 、Cityscapes</p><ul><li><strong>经典Pascal VOC 2012</strong>：在不同分割比例下与其他最先进方法比较，CorrMatch在全分割比例下mIoU达到81.8%，且在各分割比例上均优于现有方法，如比UniMatch在各分割比例上分别高出1.2%、1.3%、0.6%、0.7%和0.6%。</li><li><strong>aug Pascal VOC 2012</strong>：在不同训练尺寸和分割比例下进行实验，结果显示CorrMatch始终优于现有最佳方法。例如，在321×321训练尺寸下，比监督基线在1&#x2F;16、1&#x2F;8和1&#x2F;4分割比例上分别提高12.0%、7.4%和5.5%，比UniMatch在各分割比例上分别高出1.1%、0.8%和1.1%。</li><li><strong>Cityscapes</strong>：采用滑动窗口评估和在线难例挖掘（OHEM）损失技术，CorrMatch在所有分割比例下均优于其他方法，比UniMatch在1&#x2F;16、1&#x2F;8、1&#x2F;4和1&#x2F;2分割比例上分别高出0.7%、0.6%、0.2%和0.9%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-08-09"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-08-15"></p><p>: : : important</p><p>重要！！！</p><p>: : :</p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>组件有效性</strong>：验证了CorrMatch不同组件的有效性，包括硬无监督损失、软损失、标签传播等。完整的CorrMatch在92和1464分割比例下mIoU分别达到76.4%和81.8%，比基线分别提高2.8%和1.8%。动态阈值策略与标签传播策略配合良好。</li><li><strong>标签传播策略的影响</strong>：像素传播策略带来了一定的性能提升，区域传播策略进一步提高了性能。例如，像素传播策略在92、366和1464分割比例上分别提高1.4%、0.4%和0.8%，区域传播策略在此基础上分别再提高0.6%、0.5%和0.5%。</li><li><strong>特征提取位置</strong>：默认从骨干网络提取特征，实验表明使用骨干网络特征的性能始终优于其他位置。</li><li><strong>不同采样策略</strong>：比较了随机采样和均匀采样方法，随机采样效果更好，其中随机采样128个样本时性能最佳。</li><li><strong>不同初始值</strong>：基于EMA的阈值更新策略对不同初始值不敏感，在训练早期所有阈值都会快速趋近相似值。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-09-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-09-06"></p><h1 id="其他实验"><a href="#其他实验" class="headerlink" title="其他实验"></a>其他实验</h1><ul><li><strong>统计分析</strong>：统计挖掘比例和有效伪标签比例，结果表明使用标签传播策略后，这两个比例显著高于未使用时，说明未标记数据的利用率得到有效提高。</li><li><strong>定性分析</strong>：可视化结果显示，使用标签传播策略后，高置信区域的像素数量和完整性明显优于未使用时，能够有效扩展高置信区域并填充正确类别。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-10-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-10-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-10-04"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种名为<strong>CorrMatch的半监督语义分割方法</strong>，通过实验分析得出以下结论： </p><ul><li><strong>策略有效</strong>：重新考虑了相关图的使用，设计了<strong>像素传播和区域传播</strong>两种标签传播策略，能利用相关图中的相似性和形状信息，显著扩大高置信度区域，有效提升伪标签的整体质量。 </li><li><strong>效率提升</strong>：这些策略使模型能更高效地利用未标记数据，解决了传统方法中阈值选择困难、未标记数据利用不充分等问题。 </li><li><strong>性能优越</strong>：在Pascal VOC 2012和Cityscapes等数据集上，CorrMatch始终优于其他现有方法，取得了新的最先进性能，且推理过程无额外计算负担。</li></ul><blockquote><p>我们提出了CorrMatch，它可以利用标签传播和相关匹配来发现更准确的高置信度区域，用于半监督语义分割。CorrMatch的主要贡献是重新考虑相关映射的使用，并设计了两种标签传播策略来丰富伪标签。利用这些策略，CorrMatch显著扩展了高置信度区域，从而可以更有效地利用未标记的数据。实验证明了我们的cormatch算法优于其他方法。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation</title>
      <link href="/post/cc4s-encouraging-certainty-and-consistency-in-scribble-supervised-semantic-segmentation/"/>
      <url>/post/cc4s-encouraging-certainty-and-consistency-in-scribble-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="Peking-University、Shandong-Universit"><a href="#Peking-University、Shandong-Universit" class="headerlink" title="Peking University、Shandong Universit"></a>Peking University、Shandong Universit</h2><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Deep learning-based</strong> solutions have achieved impressive performance in semantic segmentation but often require large<br>amounts of training data with fine-grained annotations. To alleviate such requisition, a variety of weakly supervised annotation<br>strategies have been proposed, among which scribble supervision is emerging as a popular one due to its user-friendly annotation way. However, the sparsity and diversity of scribble annotations make it nontrivial to train a network to produce deterministic and consistent predictions directly. To address these issues, in this paper we propose holistic solutions involving the design of network structure, loss and training procedure, named <strong>CC4S</strong> to improve Certainty and Consistency for Scribble-Supervised Semantic Segmentation. Specifically, to reduce uncertainty, CC4S embeds a random walkmodule into the network structure to make neural representations uniformly distributed within similar semantic regions, which works together with a soft entropy loss function to force the network to produce deterministic predictions. To encourage consistency, CC4S adopts self-supervision training and imposes the consistency loss on the eigenspace of the probability transition matrix in the random walk module (we named neural eigenspace). Such self-supervision inherits the category-level discriminability from the neural eigenspace and meanwhile helps the network focus on producing consistent predictions for the salient parts and neglect semantically heterogeneous backgrounds. Finally, to further improve the performance, CC4S uses the network predictions<br>as pseudo-labels and retrains the network with an extra color constraint regularizer. From comprehensive experiments, CC4S<br>achieves comparable performance to those from fully supervised methods and shows promising robustness under extreme supervision cases.</p><p>代码： <a href="https://github.com/panzhiyi/CC4S">https://github.com/panzhiyi/CC4S</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>基于深度学习的方法在语义分割中取得了令人印象深刻的性能，但通常需要大量带有细粒度标注的训练数据。为了减少这种需求，研究者提出了多种弱监督标注策略，其中<strong>涂鸦监督</strong>因其用户友好的标注方式而逐渐流行。然而，涂鸦标注的稀疏性和多样性使得直接训练网络生成确定且一致的预测具有挑战性。为解决这些问题，本文提出了包含网络结构设计、损失函数和训练流程的完整解决方案——CC4S（提升涂鸦监督语义分割确定性与一致性的方法）。具体而言，为降低不确定性，CC4S在网络架构中嵌入<strong>随机游走模块</strong>，使神经表征在相似语义区域内均匀分布。该模块与软熵损失函数共同作用，迫使网络生成确定性预测结果。为增强一致性，CC4S采用自监督训练策略，在随机游走模块的概率转移矩阵特征空间（称为神经特征空间）中施加一致性损失。这种自监督机制既继承了神经特征空间的类别级判别能力，又能促使网络专注于对显著区域生成一致预测，同时忽略语义异构的背景区域。为进一步提升性能，CC4S将网络预测结果作为伪标签，通过引入额外的色彩约束正则化项对网络进行重训练。综合实验表明，CC4S取得了与全监督方法相媲美的性能，在极端监督条件下也展现出良好的鲁棒性。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于涂鸦监督语义分割领域，旨在解决该领域存在的问题，其研究背景主要如下：</p><ul><li><strong>数据标注难题</strong>：基于深度学习的语义分割方法虽表现出色，但需大量细粒度标注的训练数据。以Cityscapes为例，手动生成像素级语义分割标注平均耗时3 - 5分钟，收集大规模标注数据集并非易事。</li><li><strong>弱监督方法兴起</strong>：为缓解数据标注压力，多种弱监督标注策略应运而生，如<strong>图像级监督、边界框监督、点监督和涂鸦监督</strong>等。其中，涂鸦监督因<strong>标注方式友好</strong>且能提供有效监督信息，受到越来越多关注。</li><li><strong>涂鸦监督现存问题</strong>：尽管涂鸦监督语义分割取得了一定进展，但仍存在预测结果不确定和不一致的问题。标注稀疏会导致预测结果不确定，而标注的多样性会使网络难以学习到稳定一致的分割模式，从而产生不一致的预测结果。 基于以上背景，作者提出了CC4S方法，以提高涂鸦监督语义分割的确定性和一致性，减少标注稀疏和多样性带来的影响。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-12-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-12-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-12-13"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><blockquote><p>弱监督语义分割：分为四种，图像级监督、边界框监督、点监督和涂鸦监督。图像级监督仅为整个图像提供类别标签，缺乏定位信息；边界框监督仍然缺乏可靠和有效的措施来产生高质量的物体掩膜；点监督通过在每个图像对象内标记带有类别信息的点来完成注释；涂鸦监督是一种用户友好的弱监督形式。</p></blockquote><blockquote><p>涂鸦监督语义分割：现有方法包括利用辅助任务信息、图割算法传播标注、在损失函数引入分割正则化等，但仍存在预测不确定和不一致问题。</p></blockquote><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>本文提出了一种名为<strong>CC4S（Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation）<strong>的模型，旨在解决涂鸦监督语义分割任务中预测结果的</strong>不确定性和不一致性</strong>问题</p><p>核心网络包含两个模块：</p><ul><li><strong>ResNet骨干网络</strong>：用于提取图像的特征。</li><li><strong>相似度测量模块（SMM）</strong>：计算每两个神经元之间的转移概率，形成转移矩阵。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-24-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-24-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-24-44"></p><h4 id="减少神经表示的不确定性"><a href="#减少神经表示的不确定性" class="headerlink" title="减少神经表示的不确定性"></a>减少神经表示的不确定性</h4><h4 id="神经特征空间的自监督学习"><a href="#神经特征空间的自监督学习" class="headerlink" title="神经特征空间的自监督学习"></a>神经特征空间的自监督学习</h4><h4 id="带有颜色约束的伪标签再训练"><a href="#带有颜色约束的伪标签再训练" class="headerlink" title="带有颜色约束的伪标签再训练"></a>带有颜色约束的伪标签再训练</h4><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：Pascal VOC 2012 and Pascal Context</p><p>比较的方法：Scribblesup, RAWKS, NCL, GraphNet，KCL, BPG, URSS, PSI, SPML, A2GNN, DBFNet, PCE , CCL , TEL and CDL</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-30-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-30-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-30-25"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-29-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-29-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-29-58"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者通过研究得出以下结论：</p><ol><li>仅使用涂鸦注释进行语义分割会导致预测结果<strong>不确定和不一致</strong>。为此，提出了两种策略，即减少神经表示的不确定性以产生可靠结果，以及在神经特征空间进行自监督以保证输出的一致性。</li><li>结合<strong>伪标签再训练</strong>，该方法达到了最先进的性能，甚至可与全标签监督方法相媲美，且整个过程无需额外信息或注释准备要求。</li><li>大量的消融实验和中间可视化验证了所提解决方案的有效性。</li><li>该方法在涂鸦<strong>随机丢弃或按比例缩小</strong>的困难情况下也能表现良好，具有较强的鲁棒性。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings</title>
      <link href="/post/scaling-upmulti-domain-semantic-segmentation-with-sentence/"/>
      <url>/post/scaling-upmulti-domain-semantic-segmentation-with-sentence/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>The <strong>state-of-the-art semantic segmentation methods</strong> have achieved impressive performance on predefined close-set individual datasets, but their generalization to zero-shot domains and unseen categories is limited. Labeling a large-scale dataset is challenging and expensive, Training a robust semantic segmentation model on multi-domains has drawn much attention. However, inconsistent taxonomies hinder the naive merging of current publicly available annotations. <strong>To address this, we propose a simple solution to scale up the multi-domain semantic segmentation dataset with less human effort</strong>. We replace each class label with a sentence embedding, which is a vector-valued embedding of a sentence describing the class. This approach enables the merging of multiple datasets from different domains, each with varying class labels and semantics. We merged publicly available noisy and weak annotations with the most finely annotated data, over 2 million images, which enables training a model that achieves performance equal to that of state-of-the-art supervised methods on 7 benchmark datasets, despite not using any images therefrom. Instead of manually tuning a consistent label space, we utilized a vector-valued embedding of short paragraphs to describe the classes. By fine-tuning the model on standard semantic segmentation datasets, we also achieve a significant improvement over the state-of-the-art supervised segmentation on NYUD-V2 (Silberman et al., in: European conference on computer vision, Springer, pp 746–760, 2012) and PASCAL-context (Everingham et al. in Int J Comput Visi 111(1):98–136, 2015) at 60% and 65% mIoU, respectively. Our method can segment unseen labels based on the closeness of language embeddings, showing strong generalization to unseen image domains and labels. Additionally, it enables impressive performance improvements in some adaptation applications, such as depth estimation and instance segmentation. Code is available at <a href="https://github.com/YvanYin/SSIW">https://github.com/YvanYin/SSIW</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>当前最先进的语义分割方法在预设的封闭数据集上表现出色，但其在零样本（zero-shot）领域和未见过类别上的泛化能力仍然有限。由于大规模数据标注既困难又昂贵，开发跨领域通用的鲁棒语义分割模型成为研究热点。然而，现有公开数据集的不同分类标准阻碍了它们的直接融合。为此，我们提出了一种高效扩展多领域语义分割数据集的方法：用文本嵌入（text embedding）替代传统类别标签，这种向量化的语义表示可以融合不同领域、不同标签体系的数据。通过整合包含 200 万张图像的精细标注数据与公开的带噪声弱标注数据，我们训练的模型在 7 个主流测试集上达到了监督学习的顶尖水平，尽管完全没有使用这些测试集的训练图像。与人工统一标签体系不同，我们通过语言模型生成短文本描述来表征类别语义。在标准数据集微调后，模型在 NYUD-V2 (Silberman et al., 2012) 和 PASCAL-context (Everingham et al., 2015) 上分别取得 60% 和 65% 的平均交并比（mIoU），显著超越现有监督方法。该方法通过计算语义相似度实现未见过标签的分割，展现出优异的跨领域泛化能力，**同时在深度估计、实例分割等下游任务中带来显著性能提升。**代码已开源：<a href="https://github.com/YvanYin/SSIW">https://github.com/YvanYin/SSIW</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割是计算机视觉的基础任务，在自动驾驶、农业机器人和医学等领域应用广泛。当前语义分割方法虽在预定义封闭集数据集上表现出色，但存在明显局限：</p><ol><li><strong>泛化能力不足</strong>：这些方法假设测试集中的所有类别都在训练时出现，然而现实场景并非如此，且模型受限于训练数据集的图像领域，难以泛化到新领域和标签。</li><li><strong>数据集合并难题</strong>：训练多领域语义分割模型是提升模型鲁棒性和泛化能力的自然途径，但直接合并不同领域的数据集会导致标签分类体系冲突，手动统一标签集和重新标注的方法不仅费力，在开放集场景下也存在局限性。</li><li><strong>现有零样本方法的缺陷</strong>：现有解决开放集问题的方法多在小数据集上实验，限制了其在现实场景中的应用潜力。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>语义分割</strong>：深度学习方法在特定高质量数据集上取得显著成果，但泛化能力受限。如FCN开启全卷积方法，后续ResNet、Transformer等推动性能提升。</li><li><strong>零样本语义分割</strong>：分为判别式和生成式方法，前者如Xian等将像素特征转换到语义词嵌入空间，后者如ZS3Net用生成模型生成像素特征。</li><li><strong>跨领域密集预测</strong>：有方法合并分割数据集提升性能和泛化能力，如Ros合并六个驾驶数据集，Lambert提出统一分类法合并多领域数据集。</li><li><strong>零样本学习标签编码</strong>：许多方法为类别标签生成语义嵌入，如Bucher用Word2Vec编码标签，Lseg用语言嵌入监督类别标签。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><ol><li><ul><li><p><strong>创建语言嵌入</strong>：从Wikipedia收集每个类别的简短描述，使用CLIP - ViT语言模型将这些描述编码为向量值的句子嵌入。这种方式能保留标签之间的语义关系，相比单字标签嵌入，更能反映类别间的语义相似性，有助于零样本标签的分割。</p></li><li><p><strong>混合数据的异构约束</strong>：为解决合并数据集中标注质量不平衡的问题，提出了异构损失函数。</p></li></ul></li></ol><ul><li><strong>高质量标注数据集</strong>：对所有样本施加像素级损失。<ul><li><strong>粗标注数据集（如OpenImages）</strong>：通过自适应阈值，对高置信度样本施加损失，忽略噪声较大的部分。</li></ul></li><li><strong>弱标注数据集（如Objects365）</strong>：采用蒸馏方法，利用CLIP分类模型的知识，对分割模型进行监督。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_09-38-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_09-38-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_09-38-37"></p><h2 id="实验（Compared-with-SOTA-and-ablation-experiment）"><a href="#实验（Compared-with-SOTA-and-ablation-experiment）" class="headerlink" title="实验（Compared with SOTA and ablation experiment）"></a>实验（Compared with SOTA and ablation experiment）</h2><h3 id="1-数据集与实现细节"><a href="#1-数据集与实现细节" class="headerlink" title="1. 数据集与实现细节"></a>1. 数据集与实现细节</h3><ul><li><strong>训练数据</strong>：合并了7个高质量语义分割数据集（<strong>ADE20K、Mapillary、COCO Panoptic、IDD、BDD100K、Cityscapes、SUNRGBD</strong>），并从OpenImagesV6和Objects365中采样部分数据用于训练。这些数据集涵盖了不同的标注风格和图像领域，总训练图像约<strong>200万张</strong>。</li><li>测试数据<ul><li><strong>语义分割测试</strong>：在8个零样本数据集（CamVid、KITTI、Pascal VOC、Pascal Context、ScanNet、WildDash1、WildDash2、YoutubeVIS）上进行测试，并在NYUv2和Pascal Context上微调模型以评估性能。</li><li><strong>下游应用测试</strong>：在零样本数据集上创建伪语义标签，用于提升实例分割和单目深度估计的性能。实例分割在COCO数据集上进行测试，深度估计在NYUDv2、KITTI、DIODE、ScanNet和Sintel等数据集上进行评估。</li></ul></li><li>评估指标<ul><li><strong>语义分割</strong>：使用平均交并比（mIoU）进行评估。</li><li><strong>实例分割</strong>：使用平均精度（AP）进行评估。</li><li><strong>深度估计</strong>：采用绝对相对误差（AbsRel）和满足特定条件的像素百分比（δτ）进行评估。</li></ul></li><li><strong>多尺度评估</strong>：在评估语义分割性能时，将测试图像调整为多个尺度（0.5 - 1.75，步长为0.25）输入模型，然后平均得分作为最终预测结果。</li><li><strong>实现细节</strong>：使用HRNet - W48和Segformer两种网络架构进行实验。训练时，采用不同的优化器和学习率衰减策略，并对图像进行数据增强处理。推理时，将图像短边调整为三种分辨率（480&#x2F;720&#x2F;1080），并根据需要采用多尺度或单尺度测试。</li></ul><h3 id="2-实验内容"><a href="#2-实验内容" class="headerlink" title="2. 实验内容"></a>2. 实验内容</h3><ul><li><p><strong>语义分割评估</strong></p><p>   在15个数据集上进行评估，以验证模型的鲁棒性和有效性。</p><ul><li><strong>鲁棒性评估</strong>：与现有最先进的方法在6个零样本数据集上进行比较，结果表明该方法在CamViD、ScanNet和WildDash1上达到了最先进的性能，并且在混合数据集上训练的模型比在单个数据集上训练的HRNet更具鲁棒性。</li><li><strong>Wilddash2评估</strong>：在Wilddash2基准测试中，该方法取得了最先进的性能。</li><li><strong>异质损失聚合效果评估</strong>：提出异质损失来监督合并的数据集，实验结果表明，在聚合OpenImages和Objects365时，使用异质损失可以持续提高所有零样本数据集的性能。</li><li><strong>未见标签泛化能力评估</strong>：通过在YoutubeVIS数据集上采样具有5个零样本标签的约2100张图像进行实验，结果表明该方法比Mseg和JoEm更具鲁棒性，并且使用句子编码可以获得更好的性能。</li><li><strong>小数据集微调评估</strong>：在NYUv2和Pascal Context上微调模型，与其他预训练权重相比，该方法的预训练权重可以显著提升性能，超过现有最先进的方法。</li><li><strong>蒸馏效果评估</strong>：使用CLIP对模型进行知识蒸馏，实验结果表明，即使只在裁剪的边界框区域进行粗略的知识蒸馏，性能仍然可以得到显著提升。</li><li><strong>语言嵌入合并训练数据效果评估</strong>：比较了两种合并训练数据标签的方法，结果表明，使用句子嵌入来表示标签可以更好地解决标签冲突问题，从而获得更好的性能。</li><li><strong>与Lseg比较</strong>：与Lseg方法在不同粒度的标签集上进行比较，结果表明，在细化标签集上，该方法的分割结果更优。</li></ul></li><li><p><strong>下游应用提升评估</strong></p><ul><li><strong>单目深度估计</strong>：在多个零样本深度数据集上创建伪语义标签，将其转换为像素级语言嵌入并输入到深度预测网络中。实验结果表明，与基线方法LeReS相比，添加创建的嵌入可以持续提高所有数据集的性能。</li><li><strong>实例分割</strong>：在采样的Objects365上创建伪实例掩码，用于训练CondInst模型。实验结果表明，使用仅25%的COCO数据，该方法可以达到与基线方法相当的性能，并且在使用完整的COCO数据进行微调时，性能比基线方法高约4% AP。</li></ul></li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出一种语义分割方法，能在多个零样本跨域数据集上取得良好性能。具体结论如下：</p><ol><li><strong>数据融合</strong>：从维基百科收集标签简短描述，编码为向量嵌入替代标签，可轻松合并多数据集，得到强大鲁棒的分割模型。</li><li><strong>损失函数</strong>：提出异质损失，利用噪声和弱标注数据集。 </li><li><strong>性能表现</strong>：在7个跨域数据集上，性能优于或与当前最先进方法相当，模型能分割零样本标签。</li><li><strong>下游应用</strong>：该模型显著提升单目深度估计和实例分割等下游应用的性能。不过，模型性能可能受语言模型表示限制，对训练语言空间外的类别泛化能力不足，但增加数据类别和改进语言模型有望解决。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Scribble Hides Class Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label</title>
      <link href="/post/scribbl-hides-class-promoting-scribble-based-weakly-supervised-semantic-segmentation-with-its-class-label/"/>
      <url>/post/scribbl-hides-class-promoting-scribble-based-weakly-supervised-semantic-segmentation-with-its-class-label/</url>
      
        <content type="html"><![CDATA[<p>Peking University, Beijing, China</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Scribble-based weakly-supervised semantic segmentation</strong> using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we pro-pose a class-driven scribble promotion network, which uti-<br>lizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction,which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label’s boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness<br>of our method. The code is available at <a href="https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network">https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p> **基于涂鸦的弱监督语义分割（Weakly-supervised Semantic Segmentation, WSSS）**通过使用稀疏涂鸦监督正逐渐受到关注，相较于需要完整标注的替代方案，这种方法能显著降低标注成本。现有方法主要通过基于局部特征线索的像素扩散机制，将已标注像素的特征传播至未标注区域来生成伪标签。然而，这种扩散过程未能有效利用全局语义信息和类别特异性特征线索，而这些要素对实现高质量的语义分割至关重要。本研究提出了一种类驱动的涂鸦增强网络，该网络通过协同利用涂鸦标注和基于图像级类别及全局语义引导的伪标签实现监督。考虑到直接使用伪标签可能误导分割模型，我们特别设计了定位校正模块，用于在特征空间中修正前景目标表示。为了深度融合两种监督方式的优势，我们还开发了距离熵损失函数，通过涂鸦标注与伪标签边界的可靠区域确定机制，动态调整各像素点的置信权重以降低不确定性。在ScribbleSup数据集上采用不同质量涂鸦标注的实验表明，我们的方法在性能表现和鲁棒性方面均优于现有所有方法。相关代码已开源：<a href="https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network%E3%80%82">https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割领域已经取得很大的进步，但是也面临着一些挑战：手动的处理大量的数据集费时费力，且不能在现实世界中进行语义分割。基于涂鸦的WSSS的内在挑战在于稀疏标签提供的部分监督，现存的方法有三种：正则化损失、一致性学习和标签扩散。基于正则化损失的方法设计了特定的损失函数来提高模型的稳定性，基于一致性学习的方法旨在捕获不变特征，从而通过一致性损失来提高细粒度分割性能。基于标签扩散的方法通过将标记的像素扩散到未标记的像素来生成像素级伪标签，但以上的方法都存在不足，利用涂鸦标签的方法也存在不足。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>图像级弱监督语义分割</strong>：早期深度学习图像分类成果推动特征可视化工作，如引入类激活图（CAM）技术，后续有多种方法基于此生成语义伪标签以训练分割网络，还出现利用像素相关性、自注意力机制等的方法。</li><li><strong>涂鸦级弱监督语义分割</strong>：早期采用传统交互式分割方法，近年分为正则化损失、一致性学习和标签扩散三类方法。部分新方法尝试自适应生成伪标签。</li><li><strong>其他弱监督语义分割</strong>：点级和边界框级标注也是常见方式，但在训练监督和人工成本间难以平衡。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_14-48-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_14-48-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_14-48-25"></p><p>利用从<strong>稀疏涂鸦</strong>中提取的图像级类别标签为图像监督分割提供全局线索，生成全局考虑的伪标签，同时引入定位校正模块（Localization Rectification Module，LoRM）和距离熵损失（Distance Entropy Loss，DEL）来结合两种监督的优势。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：PASCAL VOC2012 and SBD</p></blockquote><h3 id="1-ScribbleSup数据集上的比较"><a href="#1-ScribbleSup数据集上的比较" class="headerlink" title="1. ScribbleSup数据集上的比较"></a>1. ScribbleSup数据集上的比较</h3><ul><li><strong>模型配置</strong>：部署resnet101作为骨干网络，deeplabV3+作为分割器，超参数设置为(λs &#x3D; e2, λc &#x3D; e7)以生成最佳结果。</li><li><strong>公平性处理</strong>：对于先前工作RAWKS和NCL采用的CRF后处理，因其耗时较长，在比较中进行了考虑。对于近期工作TEL和AGMM，为确保公平性，使用标准涂鸦重新实现它们。</li><li><strong>实验结果</strong>：该方法优于所有先前方法，比TEL高0.6%，比AGMM高1.6%。测试结果从PASCAL VOC2012网站获取。可视化比较显示，近期方法未能捕捉到正确的全局语义。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-01-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-01-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-01-53"></p><h3 id="2-涂鸦收缩和丢弃实验"><a href="#2-涂鸦收缩和丢弃实验" class="headerlink" title="2. 涂鸦收缩和丢弃实验"></a>2. 涂鸦收缩和丢弃实验</h3><p>由于基于涂鸦的注释具有灵活性，用户注释的涂鸦长度可能不同，有时会丢弃一些对象。因此，评估模型在不同收缩或丢弃比率下的鲁棒性很重要。实验结果表明，随着丢弃或收缩比率的增加，模型性能下降。当涂鸦收缩到点（收缩比率 &#x3D; 1）时，AGMM和TEL的性能下降约10%，而该方法的性能仅下降不到1%，显示出其鲁棒性。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-02-32"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><h3 id="3-组件消融实验"><a href="#3-组件消融实验" class="headerlink" title="3. 组件消融实验"></a>3. 组件消融实验</h3><ul><li><strong>模型配置</strong>：采用resnet50作为骨干网络，deeplabV2作为分割器，使用ScribbleSup数据集进行训练和验证。</li><li><strong>超参数调整</strong>：通过网格搜索找到距离熵损失所有组件的最佳超参数组合，即λs &#x3D; 1, λc &#x3D; 6。</li><li><strong>实验结果</strong>：单独使用涂鸦或伪标签作为基本监督产生的结果不理想（约67%），而同时使用两者产生了更好的结果（72.13%），表明涂鸦和伪标签提供了互补的监督。仅添加Ldc会使模型性能下降到与仅使用Lsegc几乎相同的水平，这是由于模型对伪标签中的噪声标签过拟合，而LoRM可以解决这个问题，将模型性能从67.33%提高到73.64%。与基线相比，所有组件都能提高性能，同时使用所有组件可获得最佳性能。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-02-52"></p><h3 id="4-伪标签消融实验"><a href="#4-伪标签消融实验" class="headerlink" title="4. 伪标签消融实验"></a>4. 伪标签消融实验</h3><p>使用deeplabV3+作为分割器，对不同伪标签进行实验，以评估其影响。结果表明，随着伪标签基础准确率的提高，该方法的性能也随之提高，这表明该方法直接受益于图像级弱监督语义分割方法，是一个有前途的发展方向。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-03-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-03-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-03-15"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于**基于涂鸦的弱监督语义分割（scribble-based WSSS）**问题的类驱动涂鸦提升网络（<strong>CDSP</strong>），并得出以下结论： </p><blockquote><ol><li><p>引入定位校正模块（LoRM）解决模型对噪声标签过拟合问题，通过参考其他前景位置的特征表示，校正被误导的前景特征。 </p></li><li><p>采用距离熵损失（DEL）增强网络鲁棒性，根据涂鸦和伪标签边界确定可靠区域，为预测分配不同置信度。 </p></li><li><p>实验结果表明，该方法优于现有方法，在不同质量涂鸦的实验中表现出卓越的鲁棒性，达到了当前最优性能，证明了利用图像级类别信息生成全局伪标签用于基于涂鸦的弱监督语义分割的有效性。</p></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Learning Generalized Medical Image Segmentation from Decoupled Feature Queries</title>
      <link href="/post/learning-generalized-medical-image-segmentation-from-decoupled-feature-queries/"/>
      <url>/post/learning-generalized-medical-image-segmentation-from-decoupled-feature-queries/</url>
      
        <content type="html"><![CDATA[<p>Jarvis Research Center、Wuhan University、Guangxi Medical University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Domain generalized medical image segmentation requires models to learn from multiple source domains and generalize well to arbitrary unseen target domain. Such a task is both technically challenging and clinically practical, due to the domain shift problem (i.e., images are collected from different hospitals and scanners). Existing methods focused on either learning shape-invariant representation or reaching consensus among the source domains. An ideal generalized representation is supposed to show similar pattern responses within the same channel for cross-domain images. However, to deal with the significant distribution discrepancy, the network tends to capture similar patterns by mul-tiple channels, while different cross-domain patterns are also allowed to rest in the same channel. To address this issue, we propose to leverage channel-wise decoupled deep features as queries. With the aid of cross-attention mechanism, the long-range dependency between deep and shallow features can be fully mined via self-attention and then guides the learning of<br>generalized representation. Besides, a relaxed deep whitening transformation is proposed to learn channel-wise decoupled features in a feasible way. The proposed decoupled feature query (DFQ) scheme can be seamlessly integrate into the Transformer segmentation model in an end-to-end manner. Extensive experiments show its state-of-the-art performance, notably outperforming the runner-up by 1.31% and 1.98% with DSC metric on generalized fundus and prostate benchmarks, respectively. Source code is available at <a href="https://github.com/BiQiWHU/DFQ">https://github.com/BiQiWHU/DFQ</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>域泛化医学图像分割</strong>任务要求模型能够从多个源域学习，并有效泛化到任意未知目标域。这种任务不仅在技术上具有挑战性，同时也具有重要的临床应用价值，其核心难点在于域偏移问题 (即医学图像采集自不同医院和扫描设备) 。现有方法主要聚焦于学习形状不变性表征或实现源域间的特征共识。理想的泛化表征应确保跨域图像在相同特征通道上呈现相似的模式响应。然而，面对显著的分布差异，网络往往通过多个通道捕捉相似模式，而不同跨域模式又可能共存于同一通道。针对这一矛盾，我们提出基于通道解耦深度特征的查询机制。通过交叉注意力机制，深度特征与浅层特征间的长程依赖关系可经由自注意力充分挖掘，从而指导泛化表征的学习。此外，我们开发了一种自适应深度白化变换，以更灵活的方式实现通道解耦特征学习。所提出的解耦特征查询 (DFQ) 框架能够以端到端方式无缝集成于Transformer分割模型。大量实验验证了该方案的先进性，在眼底和前列腺的泛化性基准测试中，其Dice相似系数 (DSC) 指标分别以1.31%和1.98%的优势显著超越次优方法。项目代码已开源：<a href="https://github.com/BiQiWHU/DFQ%E3%80%82">https://github.com/BiQiWHU/DFQ。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><ol><li><strong>数据分布差异</strong>：多数现有医学图像分割方法假定训练和测试样本遵循相同统计分布，但实际中，医学图像来自不同医院、由不同水平的标注者标注，存在显著的领域偏移问题，导致模型泛化能力要求高。 </li><li><strong>现有方法不足</strong>：过去医学图像分割的领域适应研究需目标域样本参与训练，只能泛化到训练中见过的目标域。现有领域泛化医学图像分割方法主要分为学习形状不变特征和明确学习多源域间的域间偏移两类，但难以应对不同成像条件下任意未见领域的特征分布变化。 </li><li><strong>特征问题</strong>：领域偏移使深度学习模型同一通道中不同领域的医学图像激活模式差异大，浅层特征的特征不对齐问题明显，网络为捕捉各领域模式会在多通道学习相似模式，导致特征冗余，影响模型对未见领域的泛化能力。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>医学图像分割技术发展</strong>：深度学习技术推动医学图像分割发展，早期U - Net及其变体占主导，后DeepLab及改进模型成为趋势，近期Vision Transformer因强大特征表示能力受关注，且弱监督、半监督和多标注场景下的分割研究也有开展。</li><li><strong>领域泛化研究</strong>：计算机视觉和机器学习领域对领域泛化广泛研究，计算机视觉中领域泛化分割多聚焦驾驶场景，医学图像分割领域泛化旨在从源域学习泛化到任意未见目标域的语义表示，现有方法分学习形状不变特征和学习域间偏移两类</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-06-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-06-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-06-07"></p><ol><li>减少通道冗余<ul><li><strong>问题</strong>：为应对不同领域间的分布差异，深度神经网络倾向于在多个通道中提取相似模式，导致特征冗余。</li><li><strong>解决方案</strong>：提出松弛深度白化变换（RDWT）。传统的深度白化变换（DWT）在学习解相关表示时存在问题，可能无法有效消除通道相关性。而RDWT通过在计算协方差矩阵之前对特征进行归一化，只关注通道之间的相关性，更有效地减少了特征冗余。</li></ul></li><li>从解耦特征查询中学习<ul><li><strong>问题</strong>：通道解耦特征虽增强了深度神经网络在跨领域场景中的表示能力，但松弛白化变换损失无法保证不同领域的医学图像在同一通道上显示相似的特征响应。</li><li><strong>解决方案</strong>：利用自注意力机制中固有的长距离依赖关系。在解码高层特征时，查询由深层特征生成，键和值基于浅层特征。深层特征查询对不同领域浅层表示的一致性施加了隐式约束。</li></ul></li><li>解码泛化表示<ul><li><strong>方法</strong>：通过一个由权重$W_1$和偏置$b_1$参数化的线性层对学习到的泛化表示进行特征融合，然后将结果输入到语义分割头进行最终预测。</li><li><strong>损失函数</strong>：总损失函数$L$是标准的二元交叉熵损失和Dice损失（记为$L_{seg}$）与每个特征的$L_{i_{RDWT}}$的组合，即$L &#x3D; L_{seg} + λ · \sum_{i&#x3D;1}^{4}L_{i_{RDWT}}$，其中$λ$设置为$1 × 10^{-4}$。</li></ul></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-04-46.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-04-46.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-04-46"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-05-05"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-05-13"></p><ol><li>与现有最优方法对比<ul><li><strong>前列腺分割基准</strong>：论文提出的方法显著优于现有最优方法。与次优方法相比，在第一、二、四、五和六个领域的ASD指标分别提高了0.14%、0.18%、0.49%、0.38%和0.26%；DSC指标在六个领域中的五个领域超过了所有现有方法，最高提升了2.08%。</li><li><strong>眼底图像分割基准</strong>：该方法同样显著优于现有最优方法。与次优的RAM - DSIR方法相比，平均DSC提高了1.63%，ASD改善了0.80%。</li></ul></li><li>对DFQ的理解<ul><li><strong>减少通道冗余</strong>：通过计算并可视化特征查询的协方差矩阵，发现所提出的DFQ方案在消除非对角元素方面表现最佳。</li><li><strong>跨领域特征对齐</strong>：通过t - SNE可视化特征空间，表明DFQ使来自不同领域的样本更均匀地混合，有助于最小化领域差距。</li></ul></li><li><strong>可视化分割结果</strong>：与现有方法相比，所提出的方法显示出更精确和合理的预测。</li></ol><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>各组件实验</strong>：DFQ框架由分割骨干网络、特征查询和松弛深度白化变换（RDWT）三个关键组件组成。实验表明，使用特征查询使DSC提高了0.94%，ASD提高了0.88%；RDWT进一步使DSC提高了1.16%，ASD提高了0.68%。</li><li><strong>各尺度实验</strong>：研究了解耦查询和风格不变的键与值的影响。结果显示，使用风格解耦的键和值（F1）以及风格解耦的查询（F2、F3、F4）都对分割结果有积极贡献，其中风格解耦查询的贡献更大。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li><p><strong>方法创新</strong>：为解决跨领域医学图像的特征不对齐问题，提出了放松的深度白化变换（RDWT），增强了通道表示能力并减少通道冗余；创新性地使用解耦的深度特征作为查询，引导整个框架学习不同领域相似的通道特征模式。</p></li><li><p><strong>性能表现</strong>：大量实验表明，该方法在前列腺和眼底图像分割基准测试中显著优于现有最先进的方法，在多个指标上取得了最佳性能，如在眼底和前列腺基准测试中，DSC指标分别至少高出1.31%和1.98%，展现出了卓越的领域泛化能力。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/progressive-feature-self-reinforcement-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/progressive-feature-self-reinforcement-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p><strong>Zhejiang Lab、Xidian University、Zhejiang University、University of Manchester</strong></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Compared to conventional semantic segmentation with pixel-level supervision, <strong>weakly supervised semantic segmentation(WSSS)</strong> with image-level labels poses the challenge that it commonly focuses on the most discriminative regions, resulting in a disparity between weakly and fully supervision scenarios. A typical manifestation is the diminished precision on object boundaries, leading to deteriorated accuracy of WSSS. To alleviate this issue, we propose to adaptively partition the image content into certain regions (e.g., confident<br>foreground and background) and uncertain regions (e.g., object boundaries and misclassified categories) for separate processing. For uncertain cues, we propose an adaptive masking strategy and seek to recover the local information with self-distilled knowledge.We further assume that confident regions should be robust enough to preserve the global semantics, and introduce a complementary self-distillation method that constrains semantic consistency between confident regions and an augmented view with the same class labels. Extensive experiments conducted on PASCAL VOC 2012 and MS COCO 2014 demonstrate that our proposed single-stage approach for WSSS not only outperforms state-of-the-art counterparts but also surpasses multi-stage methods that trade complexity foraccuracy.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>相较于需要像素级标注的传统语义分割方法，仅使用图像级标签的<strong>弱监督语义分割 (Weakly Supervised Semantic Segmentation, WSSS)</strong> 面临一个关键挑战：这类方法通常会过度关注最具区分度的区域，导致弱监督与全监督方法间存在显著性能差异。这种现象的典型表现是物体边界区域的识别精度下降，从而影响 WSSS 的整体准确性。为解决这一问题，我们提出将图像内容自适应划分为确定区域（如高置信度的前景和背景）与不确定区域（如物体边界和易混淆类别）进行差异化处理。针对不确定区域，我们设计了一种自适应掩码策略，通过<strong>自蒸馏知识 (self-distilled knowledge)</strong> 来恢复局部特征信息。同时我们提出，确定区域应当具备足够的鲁棒性以保持全局语义特征，为此开发了互补式自蒸馏方法，通过约束高置信区域与经过数据增强的同类别图像视图之间的语义一致性来强化模型。在 PASCAL VOC 2012 和 MS COCO 2014 数据集上的大量实验表明，我们提出的单阶段 WSSS 方法不仅超越了当前最先进的同类方案，其性能表现甚至优于那些通过增加模型复杂度来提升准确率的多阶段方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法存在的问题，提升分割性能。研究背景如下：</p><ul><li><strong>WSSS的优势与挑战</strong>：与传统的像素级监督语义分割相比，WSSS使用如边界框、涂鸦、点和图像级标签等“弱”标签，可降低标注成本。其中，图像级标签最为经济，但难以利用。 -</li><li><strong>现有方法的局限性</strong>：基于图像级标签的WSSS常用方法是先训练图像分类网络，生成类激活图（CAMs）作为种子区域，再将其细化为伪分割标签来监督分割网络。然而，CAMs本质上存在缺陷，它主要关注对象最具判别性的区域，导致前景对象与背景的边界区域以及多语义不同对象内的误分类区域存在高度不确定性，影响分割精度。 </li><li><strong>本文的研究目标</strong>：为解决上述问题，本文提出一种渐进式特征自我强化方法，通过自适应划分图像内容为确定区域和不确定区域并分别处理，以明确不确定区域的视觉语义，提高WSSS性能。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割方法</strong>：多阶段方法先通过分类模型生成类激活图（CAMs）作为伪标签，再训练分割模型评估性能，部分采用视觉变换器提升长程建模能力；单阶段方法将分类、伪标签细化和分割联合训练，但性能常不如多阶段方法。</li><li><strong>自蒸馏方法</strong>：将自监督学习与知识蒸馏结合，动态构建教师网络，简化训练过程并取得不错效果。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-23-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-23-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_10-23-43"></p><p>采用<strong>编码器 - 解码器</strong>架构实现图像级监督的语义分割。</p><ul><li><strong>编码器</strong>：使用在 ImageNet 上预训练的 ViT - B 视觉变压器，由图像级类标签监督。采用补丁令牌对比（PTC）进行亲和性学习，以约束最后一层补丁令牌之间的亲和性，防止过度平滑。</li><li><strong>解码器</strong>：借鉴 DeepLab 中的轻量级卷积解码器，由类激活映射（CAMs）生成的伪分割标签监督。</li><li><strong>聚合模块</strong>：将patch token汇总为一个类token。</li><li><strong>投影器</strong>：由 3 层感知器和权重归一化的全连接层组成，将所有令牌转换到合适的特征空间进行特征学习。</li><li><strong>自蒸馏机制</strong>：通过学生和教师管道实现自蒸馏，以改进模型训练。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：PASCAL VOC 2012、MS COCO 2014</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_10-26-30"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_10-26-43"></p><ul><li><strong>PASCAL VOC 2012</strong>：提出的特征自强化（FSR）方法在验证集和测试集上的mIoU分别达到75.7%和75.0%，显著优于其他单阶段方法，甚至超过了一些复杂的多阶段方法，如比BECO分别高2.0%和1.5%。与使用图像级标签和现成显著性图的多阶段方法相比，也取得了更优的性能。</li><li><strong>MS COCO 2014</strong>：在验证集上mIoU达到45.5%，优于之前的单阶段解决方案，略高于多阶段的MCTformer + 0.2%，进一步证明了该方法的优越性。</li></ul><h2 id="实验（Alabtion-Experiments）"><a href="#实验（Alabtion-Experiments）" class="headerlink" title="实验（Alabtion Experiments）"></a>实验（Alabtion Experiments）</h2><ul><li><p><strong>不确定特征选择分析</strong>：比较了基于边缘和基于CAM的两种严格选择不确定特征的方法，基于CAM的选择略优于基于边缘的选择，且当基于CAM的选择不那么严格时，性能进一步提升，经验上掩码比率r &#x3D; 0.4效果最佳。不确定特征掩码在大多数情况下比随机特征掩码性能更高，表明强化不确定特征对语义澄清很重要。</p></li><li><p><strong>特征自强化分析</strong></p><p>展示了FSR在不确定区域（unc.FSR）和确定区域（cer.FSR）的消融结果。unc.FSR在伪标签和预测标签上都有显著提升，证明了强化不确定特征的有效性。结合unc.FSR和cer.FSR可以进一步提高伪标签和预测标签的质量，表明强化确定特征与unc.FSR互补，增强了全局理解。</p><ul><li><strong>unc.FSR分析</strong>：通过分析注意力机制，计算每个注意力头在Transformer层上的平均注意力熵。应用unc.FSR时，深层（如第7 - 11层）的熵更高且更集中，表明unc.FSR通过提高深层的上下文程度有利于语义分割。</li><li><strong>cer.FSR分析</strong>：将确定特征的注意力聚合（MCA）与全局平均池化（GAP）和全局最大池化（GMP）两种传统方法进行比较。GAP性能优于GMP，MCA大幅优于GAP，表明注意力加权机制优于平均加权。可视化的类到补丁注意力图显示类令牌可以自适应地学习关注目标区域。</li></ul></li><li><p><strong>数据增强</strong>：与其他数据增强方法的比较结果表明，数据增强对性能的影响有限。例如，加入高斯模糊或日光化处理时，性能在预期范围内波动；使用强大的AutoAugment时，结果略有下降，因为强增强可能会干扰分割目标。</p></li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种<strong>基于语义不确定性引导的弱监督语义分割方法</strong>，并将其集成到单阶段框架中，在PASCAL VOC 2012和MS COCO 2014基准上验证了有效性。具体结论如下： 1. <strong>方法设计有效</strong>：设计了基于激活的掩码策略，利用自蒸馏知识恢复局部信息，并引入自蒸馏方法增强语义一致性，能有效估计边界。 2. <strong>性能表现优异</strong>：在两个基准测试中，所提方法显著优于其他单阶段方法，甚至超过了一些复杂的多阶段方法，证明了基于Transformer的单阶段训练的有效性。 3. <strong>消融实验验证</strong>：消融实验表明，增强不确定特征和确定特征对语义分割都很重要，两者结合可进一步提高伪标签和预测标签的质量。 </p><blockquote><p>启发：弱监督语义分割、自蒸馏</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch教程</title>
      <link href="/post/20250312-pytorch-jiao-cheng/"/>
      <url>/post/20250312-pytorch-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-02-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-02-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_17-02-44"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-10-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-10-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_17-10-25"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-12_14-39-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-12_14-39-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-12_14-39-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_15-05-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_15-05-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_15-05-58"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation</title>
      <link href="/post/relevant-intrinsic-feature-enhancement-network-for-few-shot-semantic-segmentation/"/>
      <url>/post/relevant-intrinsic-feature-enhancement-network-for-few-shot-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>University of Chinese Academy of Sciences、Chinese Academy of Sciences、Alibaba group</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><p>For few-shot semantic segmentation, the primary task is to extract class-specific intrinsic information from limited labeled data. However, the semantic ambiguity and inter-class similarity of previous methods limit the accuracy of pixel-level foreground-background classification. To alleviate these issues, we propose the Relevant Intrinsic Feature Enhancement Network (RiFeNet). To improve the semantic consistency of foreground instances, we propose an unlabeled branch as an efficient data utilization method, which teaches the model how to extract intrinsic features robust to intra-class differences. Notably, during testing, the proposed unlabeled branch is excluded without extra unlabeled data and computation. Furthermore, we extend the inter-class variability between foreground and background by proposing a novel multi-level prototype generation and interaction module. The different-grained complementarity between global and local prototypes allows for better distinction between similar categories. The qualitative and quantitative performance of RiFeNet surpasses the state-of-the-art methods on PASCAL−5i and COCO benchmarks.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a><strong>翻译</strong></h2><p>对于<strong>少样本语义分割任务</strong>，核心挑战在于如何从有限标注数据中提取类别本质特征。传统方法常受限于语义模糊性和类间相似性，导致前景-背景的像素级分类精度不足。为此，我们提出相关本质特征增强网络 (Relevant Intrinsic Feature Enhancement Network, RiFeNet)。该网络创新性地引入无标注分支训练策略，通过指导模型提取对类内差异具有鲁棒性的本质特征，显著提升了前景实例的语义一致性。值得一提的是，该无标注分支在测试阶段可完全移除，无需额外无标注数据支持且不增加计算负担。</p><p>为增强类间区分度，我们设计了多层次原型生成与交互模块。该模块通过建立全局原型（表征整体类别特征）与局部原型（捕捉细节特征）之间的多粒度互补关系，有效提升相似类别的可区分性。实验表明，RiFeNet 在 PASCAL-5i 和 COCO 基准测试中，无论是定性可视化结果还是定量评估指标，均超越了当前最先进的语义分割方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><p><strong>语义分割</strong>是计算机视觉领域的基础且关键任务，在医疗图像理解、工业缺陷检测等众多视觉任务中应用广泛。随着卷积神经网络和基于Transformer方法的发展，全监督语义分割取得显著成功，但获取像素级标注需大量人力和成本。因此，少样本语义分割范式受到关注，该范式让模型利用少量标注数据学习分割，再迁移到查询输入进行测试。</p><p> 然而，以往少样本语义分割方法存在语义模糊和类间相似性问题，影响分割效果。对于前景对象，同一类不同实例存在语义模糊，类内差异会导致查询图像出现语义错误；在区分前景和背景方面，类间相似性使像素级二分类困难，不同类但纹理相似的对象同时出现时，前景和背景的局部特征易混淆。 为解决这些问题，本文提出相关内在特征增强网络（RiFeNet），旨在提高少样本任务中前景分割性能，增强前景语义一致性，扩大前景和背景的类间差异。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><strong>语义分割</strong>：自全卷积网络（FCN）将语义分割转化为像素级分类后，编码器 - 解码器架构被广泛应用，近期研究聚焦于多尺度特征融合、注意力模块插入和上下文先验等。受视觉变压器启发，相关方法在语义分割任务中表现良好，但难以应对稀疏训练数据。</li><li><strong>少样本分割</strong>：主流方法分为原型提取和空间相关两类。空间相关方法虽保留空间结构，但计算复杂度高、参数多；原型学习方法以较低计算成本取得不错效果，如SG - ONE、PFENet等。</li><li><strong>少样本分割中无标签数据利用</strong>：少数研究探索了无标签数据的利用，如PPNet和Soopil的方法，但都需额外无标签数据，与原始少样本任务设置不符</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><p>本文提出了<strong>相关内在特征增强网络（Relevant Intrinsic Feature Enhancement Network，RiFeNet）</strong>，用于解决少样本语义分割任务中存在的语义模糊和类间相似性问题，以下是该模型的详细介绍： </p><ol><li><strong>整体架构</strong>：RiFeNet由三个共享主干网络的分支组成，在传统的支持 - 查询框架基础上增加了一个无标签分支，帮助模型学习保证语义一致性。其前向传播过程包含三个主要模块：   <ul><li><strong>多级原型生成模块</strong>：从支持特征中提取全局原型，从查询分支中提取局部原型，为更好的类间区分提供多粒度证据。    </li><li><strong>多级原型交互模块</strong>：构建不同粒度原型之间的交互，增强特征挖掘能力，以进行准确的识别。   </li><li><ul><li><strong>特征激活模块</strong>：使用n层Transformer编码器进行特征激活，激活包含目标类对象的像素并停用其他像素，提供最终的分割结果。</li></ul></li></ul></li><li><strong>无标签数据特征增强</strong>：引入辅助无标签分支作为有效的数据利用方法，通过对训练样本的子集进行重采样作为无标签数据，并应用相同的分割损失，教导模型避免学习有标签输入的特定样本偏差。无标签分支与查询分支共享参数，使用相互生成的伪标签进行训练。 </li><li><strong>多级原型处理</strong></li></ol><ul><li><strong>全局支持原型生成</strong>：通过对全局特征进行掩码平均池化，从支持特征中提取全局原型，以捕获高维类别级别的类别信息。    </li><li><strong>局部查询原型生成</strong>：从查询分支中额外提取局部原型，为二进制分类提供细粒度信息。使用先验掩码和局部平均池化，并通过1×1卷积和通道注意力机制进行细化。</li><li>-<strong>多级原型交互</strong>：将生成的全局和局部原型扩展到特征图的大小，然后与查询特征和先验掩码连接，经过1×1卷积和激活操作，得到增强的查询特征。</li></ul><ol start="4"><li><strong>特征激活</strong>：使用Transformer编码器对增强的查询特征进行自注意力和交叉注意力处理，输出经过调整大小后传递给分类头，得到最终的逐像素分割结果。</li><li><strong>损失函数</strong>：RiFeNet的损失函数是主损失和自监督损失的加权和。主损失使用DICE损失计算查询输入的预测结果与真实标签之间的差异，自监督损失用于无标签分支的训练，权重β经验性地设置为0.5。 实验结果表明，RiFeNet在PASCAL - 5i和COCO数据集上的表现优于现有方法，证明了其在少样本语义分割任务中的有效性。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a><strong>实验（Compared with SOTA）</strong></h2><p>数据集：PASCAL-5${^i}$ 、COCO</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_09-38-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_09-38-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_09-38-26"></p><ul><li><p><strong>PASCAL - 5i数据集</strong>：RiFeNet在大多数实验场景下优于最佳方法。在单样本设置下比CyCTR高约3.5%，五样本设置下高约2%。与现有最先进的DCAMA相比，使用ResNet50骨干时，单样本设置下高出2.5%；使用ResNet101时，高出2.7%。单样本设置下增益更大，原因是随着有标签图像增加，无标签数据与有标签图像的比例从2降至0.4，无标签分支的积极影响减小。</p></li><li><p><strong>COCO数据集</strong>：在该数据集的复杂场景下，RiFeNet在单样本设置的几乎所有分割中仍比当前最佳的DCAMA高出0.8%。定性结果也证明了RiFeNet的有效性，在支持集和查询集中前景对象姿态、外观和拍摄角度差异较大的情况下，RiFeNet在保持前景语义一致性方面有显著改进，在处理前景与背景相似性问题上表现更好。</p></li></ul><h2 id="实验（Ablation-Experiments）"><a href="#实验（Ablation-Experiments）" class="headerlink" title="实验（Ablation Experiments）"></a><strong>实验（Ablation Experiments）</strong></h2><ul><li><strong>关键组件有效性</strong>：在单样本设置下，使用无标签分支或多级别原型交互均可使性能提升约2%，两者结合时，RiFeNet在基线基础上提高3.1%。</li><li><strong>多级别原型设计选择</strong>：实验证明了为无标签分支添加引导等操作的合理性和可靠性。</li><li><strong>无标签分支设计选择</strong>：无引导查询原型的无标签分支性能比基线更差，增加基线的训练迭代次数对性能影响不大，证明该方法的有效性源于学习到的判别性和语义特征，而非数据的多次采样。</li><li><strong>不同超参数</strong>：在单样本元训练过程中，无标签图像数量设置为2时效果最佳。初始时，随着无标签图像数量增加，模型分割效果提升；数量继续增加，准确率反而下降，原因是无标签增强效果过强会使特征挖掘注意力转向无标签分支，干扰查询预测，导致特征模糊。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h2><p>在少样本分割任务中，传统方法存在<strong>语义模糊和类间相似性问题</strong>。为此，作者提出了<strong>相关内在特征增强网络（RiFeNet）</strong>。该网络引入无标签分支，在不增加额外数据的情况下，约束前景语义一致性，提高了前景的类内泛化能力。同时，提出多级原型生成与交互模块，进一步增强了背景和前景的区分度。 实验表明，RiFeNet在PASCAL - 5i和COCO基准测试中超越了现有技术水平，定性结果也证明了其有效性。消融实验显示，无标签增强和多级原型策略共同作用时，RiFeNet性能提升显著。综上，RiFeNet是一种有效的少样本语义分割模型。 </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation</title>
      <link href="/post/scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation/"/>
      <url>/post/scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation/</url>
      
        <content type="html"><![CDATA[<p><strong>Hohai University, Nanjing, China</strong></p><p><strong>RMIT University, Melbourne, Australia</strong></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><p><strong>Scribble-supervised semantic segmentation</strong> presents a cost-effective training method that utilizes annotations generated through scribbling. It is valued in attaining high performance while minimizing annotation costs, which has made it highly regarded among researchers. Scribble supervision propagates information from labeled pixels to the surrounding unlabeled pixels, enabling semantic segmentation for the entire image. However, existing methods often ignore the features of classified pixels during feature<br>propagation. To address these limitations, this paper proposes a prototype-based feature augmentation method that leverages feature prototypes to augment scribble supervision. Experimental results demonstrate that our approach achieves state-of-the-art performance on the PASCAL VOC 2012 dataset in scribble-supervised semantic segmentation tasks. The code is available at<br><a href="https://github.com/TranquilChan/PFA">https://github.com/TranquilChan/PFA</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a><strong>翻译</strong></h2><p>**涂鸦监督语义分割（Scribble-supervised semantic segmentation）**提出了一种经济高效的训练方法，通过使用涂鸦生成的标注进行模型训练。该方法因能在显著降低标注成本的同时实现高性能表现，因此备受研究人员推崇。其核心原理是通过将已标注像素的信息传递至相邻未标注区域，从而完成整幅图像的语义分割。然而，我们发现现有方法在特征传递过程中普遍忽视已分类像素的特征特性。针对这一局限性，本文提出基于原型（prototype）的特征增强方法，通过挖掘特征原型（feature prototypes）的统计特性来强化涂鸦监督效果。实验表明，我们的方法在 PASCAL VOC 2012 数据集的涂鸦监督语义分割任务中达到了当前最佳水平，相关代码已开源：<a href="https://github.com/TranbilChan/PFA%E3%80%82">https://github.com/TranbilChan/PFA。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><p><strong>标注成本问题</strong>：深度学习技术推动了深度神经网络在图像分割的发展，但是，对于标注像素级别的样本需要大量的人力和财力，并且其标注过程也非常繁琐。因此，研究者越来越关注利用涂鸦标签进行监督学习的方法。<strong>涂鸦标签属于弱监督学习</strong>，相比像素级标注，能显著减少标注工作量、提高效率，且比点、边界框和图像级标签提供更多关键语义信息。</p><p><strong>现有方法的局限性</strong>：现有涂鸦监督语义分割方法主要依赖<strong>正则化损失、一致性损失、伪建议、辅助任务和标签扩散</strong>等，但这些方法存在一定缺陷。例如，正则化方法常忽略利用高层语义信息，一致性损失未在类别层面提供直接监督，伪标签方法耗时，辅助任务会引入额外数据和预测误差，标签扩散主要依赖局部信息，且许多方法忽略了正确分类像素特征在指导边界区域像素分类中的作用。 基于以上背景，作者提出基于原型的特征增强方法，以解决现有方法的不足，提高涂鸦监督语义分割的性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><p><strong>标注方式</strong>：图像语义分割任务训练通常需大量高质量标注样本，像素级标注耗时耗力，因此弱监督学习方法受关注，如使用涂鸦、点、边界框和图像级标签等。其中，涂鸦监督能提供更多关键语义信息，表现更优。</p></li><li><p><strong>现有方法</strong>：现有涂鸦监督语义分割方法主要依赖正则化损失、一致性损失、伪建议、辅助任务和标签扩散等，但这些方法存在一定局限性。</p></li><li><p><strong>原型方法</strong>：特征原型在计算机视觉任务中用于增强模型识别能力，部分方法在弱监督语义分割中探索了原型的使用，但未充分发挥其特征增强和引导作用。</p></li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-29-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-29-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_19-29-04"></p><ul><li><strong>特征提取</strong>：使用基于Mix Transformer的编码器（Segformer中的MiT-B1）提取初始特征图。</li><li><strong>初始预测</strong>：将特征图输入解码器生成语义分割预测图，通过部分交叉熵损失（partial cross-entropy loss），利用涂鸦标签进行监督，细化预测结果。</li><li><strong>原型提取与更新</strong>：从初始预测图的高置信区域中提取对应特征向量，通过加权平均形成局部原型。在训练迭代中，局部原型动态更新全局原型。</li><li><strong>特征增强</strong>：使用局部和全局原型通过原型特征增强器对初始特征进行增强。</li><li><strong>一致性监督</strong>：将增强后的特征图再次通过解码器生成增强预测图，使用一致性损失（consistency loss）对初始预测图和增强预测图进行约束。</li></ul><h2 id="实验过程（Compared-with-SOTA）"><a href="#实验过程（Compared-with-SOTA）" class="headerlink" title="实验过程（Compared with SOTA）"></a><strong>实验过程（Compared with SOTA）</strong></h2><p>数据集：<strong>PASCAL-Scribble</strong></p><ul><li>选择<strong>MiT-B1</strong>作为骨干网络，与现有方法在<strong>PASCAL VOC 2012</strong>验证集上进行比较。</li><li>与当前最先进的方法TEL相比，尽管MiT-B1骨干网络在全监督数据集上的性能稍弱，但该方法的mIoU仍提高了0.6%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-33-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-33-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_19-33-24"></p><h2 id="实验过程（Ablation-Experiments）"><a href="#实验过程（Ablation-Experiments）" class="headerlink" title="实验过程（Ablation Experiments）"></a><strong>实验过程（Ablation Experiments）</strong></h2><ul><li><strong>各组件有效性</strong>：以仅使用部分交叉熵损失作为基线，对<strong>局部原型增强</strong>和<strong>全局原型增强</strong>方法进行消融实验。结果表明，同时使用两种原型增强时性能最佳，mIoU比基线提高了10.4%。</li><li><strong>原型设置</strong>：实验发现，当每个类别的全局原型数量增加到约5时，mIoU的增加趋于饱和；在原型提取时，k百分比为8%时方法性能较好。</li><li><strong>骨干网络影响</strong>：研究了不同骨干网络对方法的影响，发现基于Transformer的骨干网络在效率和性能上限方面表现更优。使用MiT - B5时，mIoU达到81.5%，显著超过现有方法。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h2><p>作者提出了一种基于原型的特征增强方法用于涂鸦监督语义分割，得出以下结论：</p><ul><li><p>从<strong>涂鸦监督</strong>初始结果的置信部分提取原型，利用这些原型增强初始特征，并根据涂鸦监督的具体情况采用不同原型策略，能以正确分类像素的原型引导错误分类像素的分类，提升预测性能。</p></li><li><p>实验结果表明，该方法在PASCAL VOC 2012数据集上达到了最先进的性能，相比当前最优方法TEL，使用稍弱的骨干网络MiT-B1仍使mIoU提高了0.6%。</p></li><li><p><strong>未来计划将此方法应用于其他任务，以挖掘其巨大潜力和应用价值 （下一个创新点）</strong></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation</title>
      <link href="/post/cross-domain-few-shot-semantic-segmentation-via-doubly-matching-transformation/"/>
      <url>/post/cross-domain-few-shot-semantic-segmentation-via-doubly-matching-transformation/</url>
      
        <content type="html"><![CDATA[<p>Nanjing University of Aeronautics and Astronautics 、State Key Laboratory of Integrated Services Networks, Xidian University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><blockquote><p>Cross-Domain Few-shot Semantic Segmentation (CD-FSS) aims to train generalized models that can segment classes from different domains with a few labeled images. Previous works have proven the effectiveness of feature transformation in ad-<br>dressing CD-FSS. However, they completely rely on support images for feature transformation, and repeatedly utilizing a few support images for each class may easily lead to overfitting and overlooking intra-class appearance differences. In this paper,<br>we propose a Doubly Matching Transformation-based Network (DMTNet) to solve the above issue. Instead of completely relying on support images, we propose Self-Matching Transformation (SMT) to construct query-specific transformation matri-<br>ces based on query images themselves to transform domain-specific query features into domain-agnostic ones. Calculating query-specific transformation matrices can prevent overfitting, especially for the meta-testing stage where only one or several images are used as support images to segment hundreds or thousands of images. After obtaining domain-agnostic features, we exploit a Dual Hypercorrelation Construction (DHC) module to explore the hypercorrelations between the query im-<br>age with the foreground and background of the support image, based on which foreground and background prediction maps are generated and supervised, respectively, to enhance the segmentation result. In addition, we propose a Test-time Self-Finetuning (TSF) strategy to more accurately self-tune the query prediction in unseen domains. Extensive experiments on four popular datasets show that DMTNet achieves superior performance over state-of-the-art approaches. Code is available at<br><a href="https://github.com/ChenJiayi68/DMTNet">https://github.com/ChenJiayi68/DMTNet</a>.</p></blockquote><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a><strong>翻译</strong></h2><blockquote><p>**跨域少样本语义分割（CD-FSS）**旨在训练能够以少量标注图像对不同领域进行分割的通用模型。以往的研究已经证明了特征转换在解决CD-FSS问题中的有效性。然而，这些方法完全依赖于支持图像进行特征转换，而重复利用少量支持图像来处理每个类别，容易导致过拟合，并忽视类内外观的差异。为了解决上述问题，本文提出了一种基于双重匹配转换的网络（DMTNet）。我们并不完全依赖支持图像，而是提出了自匹配转换（SMT），通过查询图像自身构建特定的转换矩阵，将领域特定的查询特征转换为领域无关的特征。计算查询特定的转换矩阵有助于防止过拟合，特别是在元测试阶段，此时仅使用一张或几张图像作为支持图像来对数百或数千张图像进行分割。在获得领域无关特征后，我们利用双重超相关构建（DHC）模块，探索查询图像与支持图像前景和背景之间的超相关性，基于此生成前景和背景的预测图并进行监督，从而增强分割结果。此外，我们还提出了一种测试时自我微调（TSF）策略，以更准确地在未知领域自我调整查询预测。在四个流行数据集上的大量实验表明，DMTNet在性能上优于现有的最先进方法。相关代码可在 <a href="https://github.com/ChenJiayi68/DMTNet">https://github.com/ChenJiayi68/DMTNet</a> 获取。</p></blockquote><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><blockquote><p><strong>语义分割</strong>近年来依赖大规模标注数据集取得快速发展，但实际场景中收集大量训练数据耗时且成本高。少样本语义分割（FSS）应运而生，旨在用少量标注支持图像实现查询图像的准确分割，常采用元学习，但在实际应用中，源数据集和目标数据集存在较大领域差距，导致FSS模型对未见领域的泛化能力较差。 为解决FSS模型在跨领域场景下性能显著下降的问题，跨领域少样本语义分割（CD - FSS）被提出。现有主要的CD - FSS方法PATNet通过将特定领域特征转换为领域无关特征来消除领域差距，但在元测试阶段仅基于少量支持图像的转换矩阵为大量查询图像生成领域无关特征，易导致过拟合。此外，多数现有CD - FSS方法在分割过程中只关注前景目标区域，忽略背景区域。 基于上述问题，本文提出一种基于<strong>双重匹配变换的网络（DMTNet）</strong>，以解决特征变换过度依赖支持图像、类内外观差异以及信息利用不充分等关键问题。 </p></blockquote><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><strong>少样本语义分割（FSS）</strong>：现有方法分为基于度量和基于关系两类。前者将支持图像表示为类原型，通过非参数测量工具分割查询图像；后者构建支持 - 查询对的密集对应关系。但在源域和目标域差距大时，性能会下降。</li><li><strong>跨域语义分割</strong>：分为域<strong>自适应语义分割（DASS）<strong>和</strong>域泛化语义分割（DGSS）</strong>。DASS通过联合使用源域和目标域数据训练模型；DGSS通过归一化和白化（NW）、域随机化（DR）等方法缩小域差距。</li><li><strong>跨域少样本语义分割（CD - FSS）</strong>：近期提出了一些方法，如PixDA、RTD、PATNet等，旨在解决少样本和域差距问题。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-05-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-05-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-05-44"></p><p><strong>本文提出了一种基于双重匹配变换的网络（Doubly Matching Transformation-based Network，DMTNet）用于跨域少样本语义分割（Cross-Domain Few-shot Semantic Segmentation，CD-FSS）</strong></p><ol><li><p><strong>自匹配变换模块（Self-Matching Transformation，SMT）</strong></p><ul><li><p><strong>相似性自匹配</strong>：通过查询特征与支持图像的前景和背景原型之间的基于相似性的自匹配，为查询图像生成粗略的分割掩码。将支持特征划分为多个局部特征，通过测量支持局部原型与查询全局特征之间的相似性，生成更细粒度的预测查询掩码，并使用二元交叉熵（BCE）损失函数进行监督。</p></li><li><p><strong>自适应特征变换</strong>：为支持和查询特征分别构建专门的变换矩阵，确保在自适应变换过程中前景对象的不变性。通过求解线性方程得到变换矩阵，同时提出整合支持图像的广义逆来优化查询图像的广义逆。</p></li></ul></li><li><p><strong>双超相关构建模块（Dual Hypercorrelation Construction，DHC）</strong>：探索查询特征与支持图像的前景和背景特征在无域特征空间中的密集相关性。分别基于支持前景特征和背景特征与查询特征构建4D相关张量，将得到的密集相关图输入到4D卷积金字塔编码器和2D卷积金字塔解码器中，生成预测的查询前景掩码和背景掩码，并使用BCE损失函数进行训练监督。</p></li><li><p><strong>测试时自微调策略（Test-time Self-Finetuning，TSF）</strong>：在元测试阶段，通过尝试预测支持图像的真实掩码来微调网络，使模型学习目标域的风格信息，从而为查询图像生成更准确的掩码。只微调编码器的少数参数，避免对支持图像过拟合。</p></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a><strong>实验（Compared with SOTA）</strong></h2><blockquote><p>数据集：PASCAL VOC 2012、ISIC2018、Chest X-ray、 Deepglobe、 and FSS-1000</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-10-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-10-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-10-33"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-14-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-14-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-14-22"></p><p>与迁移学习、少样本语义分割和跨域少样本语义分割的几种先进方法进行比较，<strong>DMTNet在四个数据集的平均结果上表现优异</strong>，在1 - shot设置下达到**59.74%<strong>的平均IoU，在5 - shot设置下达到</strong>66.01%**的平均IoU。与最先进的PATNet相比，在1 - shot和5 - shot设置下分别提高了3.68%和4.02%。</p><h2 id="实验（Ablation-Study）"><a href="#实验（Ablation-Study）" class="headerlink" title="实验（Ablation Study）"></a><strong>实验（Ablation Study）</strong></h2><p>验证了SMT、DHC和TSF三个关键模块的有效性，使用所有三个模块时模型性能最佳，移除任何一个模块都会导致平均性能下降。同时，通过实验确定了TSF策略中微调编码器层能取得最佳性能。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-12-18"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-35.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-35.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-12-35"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h2><p><strong>作者提出用于跨域少样本语义分割的DMTNet，并得出以下结论：</strong></p><ol><li><strong>DMTNet利用SMT模块基于自身原型为支持和查询图像计算变换矩阵，将特定领域特征自适应转换为通用特征，避免过度依赖支持图像导致过拟合。</strong> </li><li><strong>DHC模块在通用特征空间中探索查询图像与支持图像前景和背景的双重超相关性，生成并监督前景和背景预测掩码，提升分割效果。</strong> </li><li><strong>在元测试阶段，TSF策略微调少量参数，使模型学习目标域风格信息，进一步提高分割性能。</strong></li><li><strong>大量实验表明，DMTNet在四个具有不同领域差距的数据集上有效，达到了当前最优性能。</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Prompt-and-Transfer Dynamic Class-Aware Enhancement for Few-Shot Segmentation</title>
      <link href="/post/prompt-and-transfer-dynamic-class-aware-enhancement-for-few-shot-segmentation/"/>
      <url>/post/prompt-and-transfer-dynamic-class-aware-enhancement-for-few-shot-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="中科院"><a href="#中科院" class="headerlink" title="中科院"></a>中科院</h2><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a><strong>摘要：</strong></h2><blockquote><p>For more efficient generalization to unseen domains(classes), most Few-shot Segmentation (FSS) would directly exploit pretrained encoders and only fine-tune the decoder, especially in the current era of large models. However, such fixed feature<br>encoders tend to be class-agnostic, inevitably activating objects that are irrelevant to the target class. In contrast, humans can<br>effortlessly focus on specific objects in the line of sight. This paper mimics the visual perception pattern ofhumanbeings and proposes a novel and powerful prompt-driven scheme, called “Prompt and Transfer” (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task. Three key points are elaborated to enhance the prompting: 1) Cross-modal linguistic information is introduced to initialize prompts for each task. 2) Semantic Prompt Transfer (SPT) that precisely transfers the class-specific semantics within the images to prompts. 3) Part Mask Generator(PMG)thatworks in conjunction withSPT to adaptively generate different but complementary part prompts for different individuals. Surprisingly, PAT achieves competitive performance on 4 different tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new state-of-the-arts on 11 benchmarks.</p></blockquote><h2 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a><strong>翻译：</strong></h2><blockquote><p>为了更高效地在未知领域（类别）中进行泛化，大多数少样本分割（FSS）方法通常会直接使用预训练的编码器，并仅微调解码器，尤其是在当前大模型时代。然而，这种固定的编码器通常是类别无关的，往往会错误地激活与目标类别无关的物体。与此不同，人类可以轻松聚焦于视线中的特定物体。本文模仿人类的视觉感知方式，提出了一种新颖且高效的基于提示的方案——“提示与迁移”（PAT）。该方案构建了一种动态的类别感知提示机制，能够调整编码器专注于当前任务中的目标类别（感兴趣的物体）。为了增强提示效果，本文重点介绍了三项关键技术：1）引入跨模态的语言信息来初始化每个任务的提示。2）语义提示迁移（SPT），通过精确地将图像中的类别特定语义迁移到提示中，提升模型的识别能力。3）部分掩码生成器（PMG），与SPT协同工作，为不同个体生成不同但互补的部分提示。令人惊讶的是，PAT在四个任务中表现优异，包括标准FSS、跨域FSS（如计算机视觉、医学、遥感领域）、弱标签FSS和零-shot分割，并在11个基准测试中设立了新的技术标准。</p></blockquote><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><blockquote><p>本文聚焦于**少样本分割（Few-shot Segmentation，FSS）**领域，其研究背景主要源于当前FSS方法存在的局限性以及人类视觉感知模式带来的启示： </p><ol><li><strong>数据驱动方法的局限</strong>：深度学习在计算机视觉任务中取得显著进展，但这些数据驱动的技术在标注数据不足时表现不佳，半监督学习也难以很好地泛化到未见类别。因此，少样本学习（FSL）应运而生，旨在利用少量标注样本快速泛化到未见领域。 </li><li><strong>现有FSS方法的问题</strong>：为了更有效地泛化到未见类别，大多数FSS方法直接使用预训练编码器，仅微调解码器。然而，这种固定参数的特征编码器往往对类别不敏感，会激活与目标类别无关的对象，增加后续解码器分割新类别的负担，且这一问题未得到实质性解决。 </li><li><strong>人类视觉感知的启示</strong>：人类能够以独特的视觉感知模式选择性地关注视线中的关键对象。受此启发，作者认为理想的FSS特征编码器应具有类别感知能力，能够针对不同任务激活相应的类别对象。因此，本文提出了一种基于提示学习的“Prompt and Transfer”（PAT）方法，以动态驱动编码器关注特定对象，实现类别感知增强。</li></ol></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-30-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-30-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-30-17"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><strong>少样本学习（FSL）</strong>：多数方法遵循元学习范式，可分为优化和度量两类，部分方法引入文本信息用于分类。</li><li><strong>少样本分割（FSS）</strong>：主要有原型匹配、特征融合和像素匹配三种方法，多数采用预训练编码器并微调解码器。部分研究开始探索适用于FSS的特征编码器。</li><li><strong>提示学习</strong>：源于自然语言处理，计算机视觉领域尝试引入可学习参数激活语义知识，部分工作探索了与少样本学习的结合</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><ol><li>提出“Prompt and Transfer”（PAT）动态类别感知提示范式，模仿人类视觉感知模式，动态驱动编码器关注特定对象，解决固定编码器类别无关问题。 </li><li>构建三个关键增强点：引入跨模态语言信息初始化提示；设计语义提示转移（SPT）精确转移语义；构建部分掩码生成器（PMG）挖掘细粒度语义提示。 </li><li>在多个任务和基准上取得新的最优性能，且可扩展到跨领域、弱标签和零样本分割等场景。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-34-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-34-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-34-33"></p><h2 id="实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）"><a href="#实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）" class="headerlink" title="实验（compared with the state-of-the-art models and ablation experiments）"></a><strong>实验（compared with the state-of-the-art models and ablation experiments）</strong></h2><ol><li><strong>Comparison with the State-of-the-Arts</strong></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-36-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-36-33"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-50.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-50.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-36-50"></p><ol start="2"><li><h2 id="ablation-experiments"><a href="#ablation-experiments" class="headerlink" title="ablation experiments"></a><strong>ablation experiments</strong></h2></li></ol><blockquote><ol><li><strong>组件分析</strong>：</li></ol><p>   ​         <strong>引入前景提示（FG）</strong>：与基线相比，在PASCAL - 5i和COCO - 20i数据集上分别实现了0.96%和1.61%的平均交并比（mIoU）提升，证明了动态类别感知提示范式对少样本分割（FSS）的有效性。    </p><p>   ​            <strong>结合语义提示转移（SPT）</strong>：在PASCAL - 5i和COCO - 20i数据集上分别实现了2.27%和3.44%的mIoU提升，表明从支持和查询图像中提取特定类别的线索能显著增强类别感知能力，使编码器更精准地聚焦于目标对象。    </p><p>   ​          <strong>结合部分掩码生成器（PMG）</strong>：在PASCAL - 5i和COCO - 20i数据集上分别实现了3.38%和4.78%的mIoU提升，说明挖掘细粒度的部分语义能进一步发挥提示的作用，实现更精确的分割。    </p><p>   ​        <strong>引入背景提示（BG）并结合SPT</strong>：在PASCAL - 5i和COCO - 20i数据集上分别实现了4.36%和5.91%的mIoU提升，体现了背景语义对分割的重要性。 </p><ol start="2"><li><p><strong>提示初始化消融实验</strong>：    - <strong>随机初始化提示</strong>：在没有SPT和PMG持续增强其类别感知能力的情况下，使用随机初始化的提示来调整编码器不会带来性能提升。    - <strong>引入额外类别语义</strong>：将额外的类别语义作为初始提示可以调整编码器，使其初步定位目标，提高分割精度。    - <strong>语言信息的优势</strong>：语言信息比支持平均令牌更具类别代表性，使用CLIP提取的语言信息作为初始提示具有最优的分割结果。 </p></li><li><p><strong>提示增强消融实验</strong>：    - <strong>部分掩码生成器（PMG）</strong>：        - <strong>定性评估</strong>：可视化结果显示，PMG可以将目标对象清晰地划分为不同的互补部分区域，证明其能够自适应地生成不同的部分掩码。        - <strong>部分掩码数量影响</strong>：当部分掩码数量（Np）从1增加到8时，mIoU精度随之增加；继续增加数量，mIoU精度反而下降，过多的部分掩码可能导致目标对象无法清晰划分，产生冗余和噪声。    - <strong>语义提示转移（SPT）</strong>：        - <strong>支持和查询语义的重要性</strong>：仅将支持图像或查询图像的目标语义转移到提示中会导致不同程度的性能下降，说明两者对于查询图像的分割都至关重要。        - <strong>高斯抑制的作用</strong>：SPT中的高斯抑制通过调整注意力分布，使特定区域的全局语义更好地聚合到提示中，从而提高分割精度。    - <strong>提示增强次数</strong>：在变压器编码器的最后L个块中进行提示增强，更多的语义迁移次数通常能带来更高的分割精度，但综合效率和性能考虑，选择3次（L &#x3D; 3）较为合适。</p></li><li><p><strong>骨干网络设置消融实验</strong>：    - <strong>不同骨干网络的性能</strong>：使用DeiT - B&#x2F;16骨干网络在所有设置下具有最佳的分割精度，DeiT - S&#x2F;16次之，较小的ViT - S&#x2F;16或DeiT - T&#x2F;16虽然分割性能较差，但推理速度具有竞争力。    - <strong>块数量的影响</strong>：不同骨干网络中，块的数量并非越多越好，例如两种ViT变体在10个块时效果最佳，三种DeiT变体在11个块时效果最佳。 </p></li><li><p><strong>使用一致编码器的比较</strong>：在使用一致的特征编码器设置下，PAT取得了最佳的分割性能；盲目使用变压器提取特征可能无法带来预期的性能提升；引入额外的语言信息有助于生成更强大的提示，以生成类别感知特征。</p></li><li><p><strong>PAT与其他FSS方法的结合</strong>：使用更复杂的解码器不一定能优于简单的相似度计算；配备PAT提出的动态类别感知编码器后，其他FSS方法的性能有显著提升，表明该编码器能灵活地为不同的新类别生成类别感知特征，并且与基于解码器的方法具有良好的兼容性。</p></li></ol></blockquote><h2 id="研究结论"><a href="#研究结论" class="headerlink" title="研究结论"></a><strong>研究结论</strong></h2><blockquote><p>作者在摒弃以往冻结编码器以泛化到未见类别的小样本分割（FSS）做法后，提出了一种新颖的动态类别感知提示范式（PAT）。该范式模仿人类视觉感知模式，用于调整编码器以聚焦不同FSS任务中的特定对象。实验结果表明，PAT在三个流行的FSS基准测试中创造了新的最优性能。令人惊喜的是，当将其扩展到<strong>跨领域、弱标签甚至零样本</strong>等更现实的场景时，也取得了令人满意的结果。作者希望这项工作能为小样本场景下的编码器设计提供新视角，并激发未来相关研究聚焦于此。 </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Prompting Multi-Modal Image Segmentation with Semantic Grouping</title>
      <link href="/post/prompting-multi-modal-image-segmentation-with-semantic-grouping/"/>
      <url>/post/prompting-multi-modal-image-segmentation-with-semantic-grouping/</url>
      
        <content type="html"><![CDATA[<h2 id="University-of-Chinese-Academy-of-Sciences"><a href="#University-of-Chinese-Academy-of-Sciences" class="headerlink" title="University of Chinese Academy of Sciences"></a>University of Chinese Academy of Sciences</h2><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a><strong>摘要：</strong></h2><p>Multi-modal image segmentation is one of the core issues in computer vision. The main challenge lies in integrating common information between modalities while retaining specific patterns for each modality. Existing methods typically perform full fine-tuning on RGB-based pre-trained parameters to inherit the powerful representation of the foundation model. Although effective, such paradigm is not optimal due to weak transferability and scarce downstream data. Inspired by the recent success of prompt learning in language models, we propose the Grouping Prompt Tuning Framework(GoPT), which introduces explicit semantic grouping to learn modal-related prompts, adapting the frozen pre-trained foundation model to various downstream multi-modal segmentation tasks. Specifically, a class-aware uni-modal prompter is designed to balance intra- and inter-modal semantic propaga-<br>tion by grouping modality-specific class tokens, thereby improving the adaptability of spatial information. Furthermore,<br>an alignment-induced cross-modal prompter is introduced to aggregate class-aware representations and share prompt parameters among different modalities to assist in modeling common statistics. Extensive experiments show the superiority of our GoPT, which achieves SOTA performance on various downstream multi-modal image segmentation tasks by training only &lt; 1% model parameters.</p><h2 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a><strong>翻译：</strong></h2><p>多模态图像分割技术是计算机视觉领域的关键挑战。这项技术的核心难点在于如何有效融合不同模态（如图像、红外等）的共性特征，同时保留各模态独有的特征模式。当前主流方法主要通过对基于可见光（RGB）预训练模型进行全局微调，以继承基础模型的强大特征提取能力。但这种方法存在明显局限：一方面模型可迁移性较弱，另一方面下游任务标注数据往往匮乏。受大语言模型中提示学习方法取得突破的启发，我们研发了分组提示调优框架（GoPT），通过语义分组机制学习模态专属提示，使冻结的预训练模型能灵活适配多种多模态分割任务。该框架包含两大创新模块：首先是<strong>类感知单模态提示器</strong>，通过聚类同类模态特征，在保留模态内独特空间信息的同时，促进跨模态语义对齐；其次是<strong>对齐引导的跨模态提示器</strong>，通过共享提示参数聚合不同模态的类特征表示，有效捕捉多模态数据的共有统计规律。实验数据显示，GoPT 仅需微调模型不足 1% 的参数，就在多个多模态图像分割基准任务中刷新了最高性能记录，展现出显著优势。</p><h2 id="研究背景："><a href="#研究背景：" class="headerlink" title="研究背景："></a><strong>研究背景：</strong></h2><ol><li><strong>多模态融合的重要性</strong>：语义分割旨在为场景中每个像素分配语义类别，随着传感器技术发展，多模态融合用于分割成为图像解释核心问题。深度学习推动下，深度多模态融合展现出比单模态分割更显著的优势。 </li><li><strong>现存方法的挑战</strong>：现有多模态分割方法主要分为基于对齐和基于聚合的融合，但面临诸多挑战。一方面，不同成像机制的模态存在异质差距，基于对齐的融合因信息交换弱，常提供无效的跨模态融合；另一方面，不同模态有效信息不同，基于聚合的融合易忽略模态内传播，导致模态间知识共享和模态内信息处理失衡。 </li><li><strong>全微调方法的局限</strong>：多模态方法常采用基于RGB的预训练分割器，全微调虽有效，但效率低、参数存储负担大，且因样本标注有限，无法充分利用预训练模型知识获得通用表示。</li></ol><h2 id="研究现状："><a href="#研究现状：" class="headerlink" title="研究现状："></a><strong>研究现状：</strong></h2><ul><li><strong>多模态图像分割主流方法</strong>：以深度多模态融合为主，旨在利用多数据源增强细粒度细节和像素级语义，主要分为基于对齐和基于聚合的融合方法。前者通过条件损失对齐子网络嵌入，后者运用特定算子组合多模态子网络。</li><li><strong>模型训练方式</strong>：因缺乏大规模多模态训练集，现有方法通常先加载基于RGB的预训练模型参数，再在特定下游任务数据集上微调。</li><li><strong>视觉提示学习应用</strong>：提示调优作为新范式，在自然语言处理中表现出色，近期也开始应用于视觉任务。</li></ul><h2 id="提出的模型："><a href="#提出的模型：" class="headerlink" title="提出的模型："></a><strong>提出的模型：</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_14-56-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_14-56-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-06_14-56-04"></p><p>本文提出了<strong>分组提示调优框架（Grouping Prompt Tuning Framework，GoPT）</strong>，用于<strong>多模态图像分割</strong>任务，以下是该模型的详细介绍：</p><ol><li><strong>核心思想</strong>：引入显式语义分组机制到提示学习中，通过冻结预训练的基础模型，仅微调少量视觉提示参数，使模型适应各种下游多模态分割任务，以解决现有方法在整合模态间信息和保留各模态特定模式方面的挑战。 </li><li><strong>整体架构</strong>    <ul><li><strong>输入处理</strong>：将RGB图像和辅助模态图像输入到补丁嵌入层，生成对应的RGB标记和辅助模态标记。    </li><li><strong>提示生成</strong>：把标记送入分组提示器，生成特定于模态的提示。    </li><li><strong>特征融合</strong>：将学习到的提示作为残差添加到原始RGB流中，再输入到基础模型的下一层。</li></ul></li><li><strong>主要组件</strong>    <ul><li><strong>类感知单模态提示器（Class-Aware Uni-Modal Prompter，CUP）</strong>：通过引入特定于模态的类标记，对辅助模态的视觉概念进行分层渐进分组，平衡模态内和模态间的语义传播，提高空间信息的适应性。    </li><li><strong>对齐诱导跨模态提示器（Alignment-Induced Cross-Modal Prompter，ACP）</strong>：根据显式分组的语义相似性，聚合辅助模态的类感知表示，将其他数据源的关键模式整合到RGB流中，生成新的跨模态对齐诱导提示，辅助建模模态公共统计信息。</li></ul></li><li><strong>优化策略</strong>：使用基于RGB的预训练基础模型的参数初始化多模态分割模型，在提示调优过程中，仅更新分组提示器和分割头的梯度值，以少量提示参数促进模型快速收敛，并有效继承预训练基础模型的先验知识。 </li><li><strong>实验验证</strong>：在多个下游多模态图像分割任务（RGB - D、RGB - T、RGB - SAR分割）上进行了广泛实验，结果表明GoPT仅训练不到1%的模型参数，就能在各项指标上取得优于现有方法的性能，展现出高效性和优越性。</li></ol><h2 id="实验过程（与SOTA方法的对比）："><a href="#实验过程（与SOTA方法的对比）：" class="headerlink" title="实验过程（与SOTA方法的对比）："></a><strong>实验过程（与SOTA方法的对比）：</strong></h2><p>数据集：NYUDv2、SUN RGB-D、MFNet、PST900、WHU-OS</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-02-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-02-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-06_15-02-29"></p><h2 id="实验过程（消融实验）："><a href="#实验过程（消融实验）：" class="headerlink" title="实验过程（消融实验）："></a><strong>实验过程（消融实验）：</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-05-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-05-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-06_15-05-05"></p><p>通过消融实验，证明了GoPT已经达到了SOTA的标准</p><ul><li>Effectiveness of Prompter Structure  （<strong>table 4，row 7 and row 8</strong>）</li><li>Impact of Multi-Modal Information   （<strong>table 4</strong>）</li><li>Number of Grouping Prompter          <strong>（Figure 6）</strong></li><li>Hard vs. Soft Assignment                <strong>（Figure 7）</strong></li></ul><p><strong>结论：</strong></p><p>作者提出了用于<strong>多模态图像分割</strong>的参数高效视觉调优框架GoPT，通过在提示学习中引入显式语义分组，使冻结的预训练基础模型适应各种下游多模态分割任务。具体而言，设计了类感知单模态提示器（CUP），通过对特定模态的类令牌进行分组，平衡了模态内和模态间的语义传播；引入了对齐诱导的跨模态提示器（ACP），聚合类感知表示并辅助建模公共统计信息。大量下游任务实验表明，GoPT在准确性和效率上达到了最佳平衡，仅训练不到1%的模型参数，就在多个下游多模态图像分割任务中取得了SOTA性能，证明了该框架的优越性和泛化性。 </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Disentangle then Parse Night-time Semantic Segmentation with Illumination Disentanglement</title>
      <link href="/post/disentangle-then-parse-night-time-semantic-segmentation-with-illumination-disentanglement/"/>
      <url>/post/disentangle-then-parse-night-time-semantic-segmentation-with-illumination-disentanglement/</url>
      
        <content type="html"><![CDATA[<p>​University of Science and Technology of China               Shanghai AI Laboratory</p><p><strong>摘要：</strong></p><p>Most prior semantic segmentation methods have been developed for day-time scenes, while typically underperforming in night-time scenes due to insufficient and complicated lighting conditions. In this work, we tackle this challenge by proposing a novel night-time semantic segmentation paradigm, i.e., disentangle then parse (DTP). DTP explicitly disentangles night-time images into light-invariant reflectance and light-specific illumination components and then recognizes semantics based on their adaptive fusion. Concretely, the proposed DTP comprises two key components: 1) Instead of processing lighting-entangled features as in prior works, our Semantic-Oriented Disentanglement (SOD) framework enables the extraction of re-<br>flectance component without being impeded by lighting, allowing the network to consistently recognize the semantics under cover of varying and complicated lighting conditions. 2) Based on the observation that the illumination component can serve as a cue for some semantically confused regions, we further introduce an Illumination-Aware Parser (IAParser) to explicitly learn the correlation between semantics and lighting, and aggregate the illumination features to yield more precise predictions. Extensive experiments on the night-time segmentation task with various settings demonstrate that DTP significantly outperforms state-of-the-art methods. Furthermore, with negligible additional parameters, DTP can be directly used to benefit existing day-time methods for night-time segmentation. Code and dataset are available at <a href="https://github.com/w1oves/DTP.git">https://github.com/w1oves/DTP.git</a>.</p><p><strong>翻译：</strong></p><p>大多数现有的语义分割方法都是为白天场景设计的，因此在夜间场景中往往表现不佳，主要是因为夜间的光照条件复杂且不足。为了解决这个问题，本文提出了一种全新的夜间语义分割方法——“先解耦再解析”（DTP）。DTP方法首先将夜间图像分解为不受光照影响的<strong>反射成分</strong>和与光照相关的<strong>光照成分</strong>，然后通过自适应融合这两部分信息来进行语义识别。具体来说，DTP包含两个关键技术：1）与以往方法将光照信息与特征混合的做法不同，我们提出的“语义导向解耦”（SOD）框架能够在不受光照影响的情况下提取反射成分，这样可以帮助网络在复杂多变的光照条件下持续准确地识别语义。2）通过观察到光照成分可以作为一些语义模糊区域的线索，我们引入了“光照感知解析器”（IAParser），该模块能够学习语义与光照之间的关系，并聚合光照特征，从而提高预测的精度。在各种夜间分割任务的实验中，DTP显著超越了现有的先进方法。而且，DTP几乎不增加额外的参数，能够直接用于现有的白天分割方法，提升其在夜间的分割效果。相关代码和数据集可以在<a href="https://github.com/w1oves/DTP.git%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/w1oves/DTP.git下载。</a></p><p><strong>研究背景：</strong></p><blockquote><p>大多现存的语义分割方法都是基于白天场景开发的，这些场景光照充足且均匀。然而在实际应用中，视觉系统近一半时间需在光照不足且复杂的夜间环境下工作，现有白天方法在夜间性能会下降。此前采用无监督域适应技术将白天知识迁移到夜间，但因夜间缺乏对应标签，分割性能提升有限。虽有NightCity等大规模夜间数据集及相关方法提出，一定程度上改善了夜间场景表现，但它们通常基于光照纠缠的表示进行场景解析，不适合夜间复杂的光照条件。</p></blockquote><blockquote><p>夜间场景光照强度低且人工光源复杂，导致物体外观随光照变化，使光不变反射率和光特定光照之间的纠缠加剧，难以提取用于语义分割的判别特征。</p></blockquote><p><strong>研究现状：</strong></p><ol><li><strong>语义分割</strong>：基于全卷积网络（FCN）结合编码器 - 解码器架构的方法成为主流，如DeepLab系列引入空洞空间金字塔池化（ASPP），还有基于自注意力机制和Transformer的网络被应用，但大多聚焦白天场景。</li><li><strong>夜间语义分割</strong>：早期因缺乏大规模标注数据集，采用无监督域适应技术将白天知识迁移到夜间；近期有基于NightCity数据集的方法，如EGNet、NightLab等，但这些方法未明确估计光照对语义的影响，而是学习内容和光照的纠缠表示。</li><li><strong>深度表示解耦</strong>：此前探索了多种图像表示解纠缠方法，如在GAN框架中学习跨域不变表示。</li></ol><p><strong>创新点：</strong></p><ol><li>提出“先分离再解析”（DTP）的夜间语义分割范式，可提升现有日间方法在夜间的性能。</li><li>设计语义导向解纠缠框架（SOD），借助语义约束分离图像，使网络在不同光照下提取一致特征。 </li><li>引入光照感知解析器（IAParser），利用光照组件作为线索，实现更精准预测。 </li><li>细化NightCity数据集，提出NightCity-fine，为夜间分割提供更可靠基准。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-03_15-54-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-03_15-54-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-03_15-54-06"></p><p>实验：</p><ol><li><strong>数据集</strong></li></ol><ul><li><p><strong>NightCity - fine</strong>：原始的NightCity是最大的夜间语义分割数据集，但存在标注错误。作者提出了NightCity - fine，对训练集和验证集中不合理的标注进行了仔细修改，共修正了2554个标签图。    </p></li><li><p><strong>Cityscapes</strong>：这是一个自动驾驶数据集，在白天场景下从50个不同城市采集，包含2975张训练图像和500张验证图像，具有19个语义类别，图像分辨率为2048x1024。</p></li><li><p><strong>BDD100K</strong>：使用其子集BDD100K - night进行补充实验，该子集包含314张夜间训练图像和31张验证图像，其互补数据集为BDD100K - day。 </p></li><li><p><strong>与SOTA方法的对比实验</strong></p></li></ul><p>使用的数据集：<strong>NightCity, NightCity-fine,  and BDD100K-night</strong></p><ul><li><strong>消融实验</strong></li></ul><p><strong>结论：</strong></p><p>作者提出了一种新颖的夜间语义分割范式——Disentangle then Parse（DTP），并得出以下结论： </p><ol><li><strong>方法优势</strong>：提出语义导向的解纠缠（SOD）框架，使分割不受复杂光照干扰；引入光照感知解析器（IAParser），利用光照中的语义线索实现更精确预测。DTP可作为即插即用范式，以极少额外参数助力现有方法提升性能。</li><li><strong>数据集贡献</strong>：对最大的夜间分割数据集NightCity进行细化，提出NightCity - fine，用于更有效的训练和验证评估。</li><li><strong>性能验证</strong>：大量实验表明，DTP显著优于现有技术，与NightCity - fine一起为夜间分割提供了更优的基准。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SED A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation</title>
      <link href="/post/sed-a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/"/>
      <url>/post/sed-a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="天津大学，重庆大学等"><a href="#天津大学，重庆大学等" class="headerlink" title="天津大学，重庆大学等"></a><strong>天津大学，重庆大学等</strong></h2><blockquote><p>摘要：Open-vocabulary semantic segmentation strives to distinguish pixels into different semantic groups from an open<br>set of categories. Most existing methods explore utilizing pre-trained vision-language models, in which the key is to<br>adapt the image-level model for pixel-level segmentation task. In this paper, we propose a simple encoder-decoder,<br>named SED, for open-vocabulary semantic segmentation, which comprises a hierarchical encoder-based cost map generation and a gradual fusion decoder with category early rejection. The hierarchical encoder-based cost map generation employs hierarchical backbone, instead ofplain transformer, to predict pixel-level image-text cost map.<br>Compared to plain transformer, hierarchical backbone better captures local spatial information and has linear computational complexity with respect to input size. Our gradual fusion decoder employs a top-down structure to com-<br>bine cost map and the feature maps of different backbone levels for segmentation. To accelerate inference speed, we<br>introduce a category early rejection scheme in the decoder that rejects many no-existing categories at the early layer<br>ofdecoder, resulting in at most 4.7 times acceleration without accuracy degradation. Experiments are performed on<br>multiple open-vocabulary semantic segmentation datasets, which demonstrates the efficacy ofour SED method. When<br>using ConvNeXt-B, our SED method achieves mIoU score of 31.6% on ADE20K with 150 categories at 82 millisecond (ms) per image on a single A6000. Our source code is available at <a href="https://github.com/xb534/SED">https://github.com/xb534/SED</a></p></blockquote><blockquote><p>翻译：开放词汇语义分割旨在将像素划分到一个开放类别集中的不同语义组。大多数现有的方法尝试使用预训练的视觉-语言模型，关键是将图像级的模型适应为像素级的分割任务。本文提出了一种简单的编码器-解码器架构，称为SED，用于开放词汇语义分割。SED包含两部分：基于层次编码器的成本图生成和逐渐融合的解码器，并且具有类别早期拒绝的功能。基于层次编码器的成本图生成使用层次化的骨干网络，而不是传统的Transformer，来预测像素级的图像-文本成本图。与普通的Transformer相比，层次化骨干网络能够更好地捕捉局部空间信息，并且计算复杂度与输入的大小成线性关系。我们的逐渐融合解码器采用自上而下的结构，将成本图与不同层级骨干网络的特征图进行融合，完成分割任务。为了提高推理速度，我们在解码器中引入了类别早期拒绝方案，通过在解码器的早期层次中剔除不存在的类别，从而在不牺牲精度的情况下，实现最多4.7倍的加速。我们在多个开放词汇语义分割数据集上进行了实验，验证了SED方法的有效性。当使用ConvNeXt-B时，SED方法在ADE20K数据集上，针对150个类别的mIoU得分为31.6%，每张图像的推理时间为82毫秒（ms），运行于单个A6000显卡。我们的源代码可以通过<a href="https://github.com/xb534/SED%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/xb534/SED下载。</a></p></blockquote><p><strong>研究背景：</strong> <strong>传统的方法只能分割训练集的种类，不能识别出来在训练集中没有的未知场景</strong>，同时两阶段和单阶段的方法都存在不足。两阶段的框架存在不足：计算效率低，没有充分利用上下文信息；单阶段的框架存在不足：对于低分辨率的输入，主干网络对空间信息变得不敏感，即使加入额外的网络来提供空间信息，也会增加计算资源，分割种类的增加也会增加计算资源。</p><p><strong>研究现状：</strong></p><ul><li><strong>语义分割方法</strong>：传统语义分割方法主要有基于FCN和基于Transformer的方法。前者通过融合深浅层特征、利用空间金字塔网络或注意力模块等提取上下文信息；后者将Transformer用作骨干网络或分割解码器。</li><li><strong>视觉-语言模型</strong>：早期基于预训练的视觉和语言模型开发，后如CLIP从大规模图像-文本对数据中学习视觉特征，ALIGN从噪声图像-文本数据集中学习，在零样本任务上表现出色。</li><li><strong>开放词汇语义分割</strong>：早期通过学习特征映射对齐视觉和文本特征，CLIP成功后，出现基于两阶段和单阶段框架的方法。两阶段先生成掩码提案再分类，单阶段直接扩展视觉-语言模型进行分割。</li></ul><p><strong>研究方法：</strong></p><ul><li><p><strong>Hierarchical Encoder-based Cost Map（基于分层编码器的代价图）</strong></p></li><li><p><strong>Gradual Fusion Decoder（渐进式融合编码器）</strong></p></li><li><p><strong>Category Early Rejection（类别早期拒接）</strong></p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-53-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-53-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_08-53-28"></p><p><strong>实验设定（对比实验+消融实验）</strong></p><blockquote><p>训练集：COCO-Stuff 的训练集，包含大约 118k 密集标注的 171 类目标。<br>测试集：跨数据集测试。<br>ADE20K，包含 20K training 和 2K validation &#x3D;&gt; A-150 和 A-847。<br>PASCAL VOC，包含 1.5k training 和 1.5k validation &#x3D;&gt; PAS-20。<br>PASCAL-Context 来自原始的 PASCAL VOC 数据集 &#x3D;&gt; PC-59 和 PC-459。</p></blockquote><blockquote><p>模型设定：<br>基于 ConvNeXt-B&#x2F;L 视觉编码器形式的预训练 CLIP。<br>类别模板数量 P PP 同 CAT-Seg 一致，均为 80。<br>文本编码器冻结，只训练图像编码器和解码器。</p></blockquote><blockquote><p>GPU: 4xA6000<br>图像编码器学习率多乘以一个 0.01 倍的因子。<br>共 80k 次迭代。<br>训练时剪裁图像 768 × 768 大小，测试时直接放缩图像到 768 × 768大小。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-08.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-08.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_08-59-08"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_08-59-43"></p><p><strong>实验结果：本文的SED方法都表现出较好的效果，缩短了推理时间</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_09-00-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_09-00-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_09-00-24"></p><p><strong>结论与不足</strong></p><p>作者提出了用于<strong>开放词汇语义分割</strong>的SED方法，得出以下结论：</p><ol><li><strong>方法构成</strong>：SED由基于分层编码器的代价图生成和带有类别早期拒绝的渐进式融合解码器组成。先利用分层编码器生成像素级图像 - 文本代价图，再基于此和分层编码器的不同特征图，用渐进式融合解码器生成高分辨率特征图进行分割。</li><li><strong>速度提升</strong>：在解码器中引入类别早期拒绝方案，能提前拒绝不存在的类别，有效提升推理速度。</li><li><strong>效果验证</strong>：在多个数据集（ADE20K、PASCAL VOC、PASCA-Context）上的实验表明，SED在准确性和速度方面均有效。不过，模型在识别近义词类别时存在困难，未来将探索设计类别注意力策略或使用大规模细粒度数据集来解决该挑战。</li></ol><p><strong>不足：</strong></p><p><strong>类别识别局限</strong>：模型有时难以区分近义词类别，在对语义相近的类别进行分类和分割时存在困难，影响了对复杂场景的理解和处理能力</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>High Quality Segmentation for Ultra High-resolution Images</title>
      <link href="/post/high-quality-segmentation-for-ultra-high-resolution-images/"/>
      <url>/post/high-quality-segmentation-for-ultra-high-resolution-images/</url>
      
        <content type="html"><![CDATA[<h1 id="香港中文大学-Adobe-等"><a href="#香港中文大学-Adobe-等" class="headerlink" title="香港中文大学  Adobe 等"></a>香港中文大学  Adobe 等</h1><p>**摘要：**To segment 4K or 6K ultra high-resolution images needs extra computation consideration in image segmentation. Common strategies, such as down-sampling, patch crop- ping, and cascade model, cannot address well the balance issue between accuracy and computation cost. Motivated by the fact that humans distinguish among objects continu- ously from coarse to precise levels, we propose the Contin- uous Refinement Model (CRM) for the ultra high-resolution segmentation refinement task. CRM continuously aligns the feature map with the refinement target and aggregates fea- tures to reconstruct these image details. Besides, our CRM shows its significant generalization ability to fill the resolu- tion gap between low-resolution training images and ultra high-resolution testing ones. We present quantitative per- formance evaluation and visualization to show that our pro- posed method is fast and effective on image segmentation refinement. Code is available at <a href="https://github.com/dvlab-research/Entity/tree/main/CRM">https://github.com/dvlab-research/Entity/tree/main/CRM</a>.</p><p><strong>翻译:</strong> 对4K或6K超高分辨率图像进行分割时，需要额外的计算资源。常见的策略，如下采样、图像裁剪和级联模型，往往难以很好地平衡准确性和计算成本。基于人类从粗略到精细地逐步区分物体的方式，我们提出了“连续细化模型”（CRM）来进行超高分辨率图像的分割细化任务。CRM通过不断地对齐特征图和细化目标，逐步聚合特征，重建图像细节。更重要的是，CRM展现出了很强的泛化能力，能够有效弥补低分辨率训练图像与超高分辨率测试图像之间的分辨率差距。我们通过定量的性能评估和可视化结果，展示了该方法在图像分割细化上的高效性和快速性。相关代码可以在<a href="https://github.com/dvlab-research/Entity/tree/main/CRM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dvlab-research/Entity/tree/main/CRM找到。</a></p><p><strong>研究背景</strong></p><p>随着相机和显示设备的快速发展，图像分辨率越来越高，4K和6K分辨率变得常见，这在人像照片后期处理、工业缺陷检测、医学诊断等领域带来了新机遇。然而，超高分辨率图像也给经典图像分割方法带来了挑战：</p><ol><li><strong>计算成本高</strong>：大量的输入像素在计算上代价高昂，且对GPU内存需求大。 </li><li><strong>细节重建难</strong>：大多数现有方法通过插值对最终预测进行4到8倍上采样，无法在输出掩码上构建细粒度细节。 以往的分割细化方法，如针对1K - 2K分辨率图像的方法，存在图形模型依赖低级别颜色边界、基于传播的方法面临计算和内存限制、大模型易过拟合而浅细化网络细化能力有限等问题。而处理超高分辨率细化的级联解码器方法，虽能取得较好性能，但在推理时需要下采样和裁剪补丁，增加了成本、丢失了细节并破坏了全局上下文。 因此，为解决超高分辨率图像分割中精度与计算成本的平衡问题，作者提出了连续细化模型（CRM），以实现高效、精确的图像分割细化。</li></ol><p><strong>研究现状</strong></p><ol><li><strong>语义分割</strong>：FCN 引入深度卷积网络，PSPNet、DeepLab 系列等方法不断发展，输出步长多设为 4×或 8×，但直接插值预测结果存在边缘锯齿和细节缺失问题。</li><li><strong>分割细化</strong>：针对 1K 分辨率图像的细化技术能提升分割质量，但存在图形模型依赖低层次颜色边界、传播方法有计算和内存限制、模型易过拟合或细化能力有限等问题。对于 4K - 6K 超高清图像，级联解码器方法能取得较好效果，但结构复杂。</li><li><strong>隐式函数表示</strong>：在神经网络中用于表示对象或场景，如 NeRF、PixelNerf 等，其多视图一致性和光滑性有利于分割。</li></ol><p><strong>研究方法</strong></p><ol><li>提出<strong>连续细化模型（CRM）</strong>，引入<strong>隐函数</strong>，利用连续位置信息和特征对齐，有效降低计算成本，重建更多细节。 </li><li>CRM采用多<strong>分辨率推理策略</strong>，适用于<strong>低分辨率训练和超高分辨率测试</strong>，总推理时间不到CascadePSP的一半。 </li><li>实验表明，CRM在超高分辨率图像上取得最佳分割结果，还能提升现有全景分割模型性能，无需微调。</li></ol><p><strong>实验步骤</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-02_14-24-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-02_14-24-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-02_14-24-58"></p><ol><li><strong>数据集和对比方法选择</strong>    - <strong>训练数据集</strong>：遵循CascadePSP的设置，将MSRA - 10K、DUT - OMRON、EC - SSD和FSS - 1000合并为训练数据集，包含36,572张具有超过1000个语义类别的图像。    - <strong>测试数据集</strong>：使用CascadePSP提出的高分辨率图像分割数据集BIG（图像分辨率范围2K - 6K）进行超高清图像评估；还在重新标注的PASCAL VOC 2012数据集上进行评估；为证明模型的通用性，将CRM作为全景分割和实体分割的扩展进行评估。    - <strong>对比方法</strong>：选择CascadePSP作为超高清图像的主要对比方法；选择MGMatting作为掩码引导抠图方法，Segfix作为高分辨率分割细化方法；选择PanopticFCN和Entity Segmentor作为全景和实体分割的基准方法。 </li><li><strong>实现细节</strong>    - <strong>模型实现</strong>：使用PyTorch实现模型，使用去掉conv5 x的ResNet - 50作为编码器Eθ。    - <strong>训练设置</strong>：使用Adam优化器，学习率为2.25×10⁻⁴，在22,500和37,500步时将学习率降至十分之一，总步数45,000步。训练输入是从原始图像及其对应的扰动掩码中裁剪的224×224的图像块，扰动掩码是在真实掩码上随机扰动得到，随机IoU阈值在0.8 - 1.0之间。    - <strong>评估设置</strong>：在实验中从连续范围中选择4个缩放比例进行细化，CRM的总推理时间仍不到CascadePSP的一半。</li><li>定量结果评估**    - <strong>对比实验</strong>：在BIG数据集上对比CRM、CascadePSP、Segfix和MGMatting的性能，包括交并比（IoU）、平均边界准确率（mBA）、全景质量（PQ）和平均精度（AP）等指标。结果表明CRM性能更好，在高分辨率图像上运行速度更快，且FLOPs和参数更少。    - <strong>扩展实验</strong>：在全景分割和实体分割实验中，将CRM添加到PanopticFCN和EntitySeg后，它们的分割性能得到增强；在重新标注的Pascal VOC 2012数据集上，CRM比Segfix表现更好，与CascadePSP的IoU相当，但更注重细节</li><li><strong>定性结果展示</strong>    - <strong>可视化对比</strong>：展示CascadePSP、Segfix和CRM的细化结果对比，CRM的细化结果包含更多细节，仅使用语义分割标注训练就能生成类似抠图的结果，并且能更好地重建粗掩码中缺失的部分。    - <strong>应用示例</strong>：展示将CRM应用于全景分割的可视化结果，掩码细节和整体分割效果都有显著改善。</li><li><strong>消融实验</strong>    - <strong>CRM和推理分辨率</strong>：分析CRM和隐式函数对不同分辨率下性能的影响，CRM在低分辨率下能细化出较好的通用掩码，随着分辨率增加，能生成更多细节，mBA提高。    - <strong>CAM和隐式函数</strong>：验证CAM和隐式函数都是CRM不可或缺的部分，二者协同作用能提升性能。    - <strong>推理连续性的影响</strong>：性能随着采样的缩放比例数量增加而提升，更多的缩放比例意味着推理分辨率的连续性更好，有助于提高性能直至收敛。</li></ol><p><strong>结论：</strong></p><p>作者提出了用于超高清图像分割细化的连续细化模型（CRM），并得出以下结论：</p><ol><li><strong>性能优势</strong>：CRM 能连续对齐特征图与细化目标，有助于聚合特征以重建高分辨率掩码上的细节，在性能和速度方面表现出色，实验表明其在超高清图像上的分割效果最佳，还能提升现有全景分割模型的性能。</li><li><strong>泛化能力</strong>：CRM 具有显著的泛化潜力，可处理低分辨率训练和超高清测试之间的分辨率差距，即使从低分辨率细化到高分辨率，总推理时间也不到 CascadePSP 的一半。 </li><li><strong>未来展望</strong>：目前采用“低分辨率训练和超高清测试”的配置，使用超高清图像进行训练和测试仍耗资源，解决该问题将是未来工作方向。</li></ol><p><strong>不足：</strong></p><ol><li><strong>训练资源限制</strong>：目前采用“低分辨率训练和超高分辨率测试”的配置，使用超高分辨率图像进行训练和测试仍面临资源消耗大的问题，这限制了模型在实际应用中对超高分辨率数据的处理能力。</li><li><strong>未来工作待明确</strong>：如使用预训练或低分辨率训练测试。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Segment Anything</title>
      <link href="/post/segament-anything/"/>
      <url>/post/segament-anything/</url>
      
        <content type="html"><![CDATA[<h2 id="Meta-AI"><a href="#Meta-AI" class="headerlink" title="Meta AI"></a><strong>Meta AI</strong></h2><p><a href="https://segment-anything.com/">https://segment-anything.com</a></p><blockquote><p>摘要：We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion<br>masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can<br>transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that<br>its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We<br>are releasing the Segment Anything Model (SAM) and cor-responding dataset (SA-1B) of 1B masks and 11M images<br>at segment-anything.com to foster research into foundation models for computer vision</p></blockquote><blockquote><p>翻译：我们推出了Segment Anything (SA)项目：一个全新的图像分割任务、模型和数据集。通过使用我们高效的模型进行数据收集，我们成功构建了迄今为止最大的图像分割数据集，包含超过10亿个分割掩码和1100万张符合许可且尊重隐私的图像。这个模型被特别设计并训练为能够接受简单提示，因此它可以零样本迁移到不同的图像类型和任务。我们在多个任务中测试了该模型，发现它的零样本表现非常优秀——通常与之前的完全监督方法相当，甚至在某些情况下表现更好。我们将在segment-anything.com网站发布Segment Anything Model (SAM)和对应的数据集(SA-1B)，该数据集包括10亿个掩码和1100万张图像，旨在推动计算机视觉领域的基础模型研究。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-28-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-28-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-28-44"></p><p><strong>task, model, dataset, data engine, experiments, responsible AI, release</strong></p><ul><li><p>What <strong>task</strong> will enable zero-shot generalization?</p></li><li><p>What is the corresponding <strong>model</strong> architecture?</p></li><li><p>What <strong>data</strong> can power this task and model?</p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-37-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-37-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-37-11"></p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-39-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-39-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-39-37"  /><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-40-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-40-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-40-29"></p><blockquote><p>**研究背景：**本文聚焦于构建图像分割基础模型，其研究背景主要源于自然语言处理（NLP）和计算机视觉领域的发展现状与需求。</p><p>在NLP中，基于大规模网络数据集预训练的大语言模型展现出强大的零样本和少样本泛化能力，通过提示工程可适应多种任务和数据分布，且性能随模型规模、数据集大小和训练计算量的增加而提升。</p><p>计算机视觉领域虽也对基础模型有所探索，如CLIP和ALIGN利用对比学习训练文本和图像编码器实现零样本泛化，但计算机视觉问题广泛，许多问题缺乏充足的训练数据。</p><p>在图像分割方面，尚无网络规模的数据源，且现有方法难以实现强大的泛化能力。因此，本文旨在开发一个可提示的模型，并在广泛的数据集上进行预训练，以解决新数据分布下的一系列下游分割问题。具体通过定义可提示的分割任务、设计相应的模型架构（SAM）以及构建数据引擎收集大规模数据集（SA - 1B）来实现这一目标，从而推动图像分割进入基础模型时代。</p></blockquote><p><strong>研究现状：</strong></p><ul><li><strong>基础模型发展</strong>：大语言模型在自然语言处理（NLP）领域展现出强大的零样本和少样本泛化能力，通过提示工程可适应多种任务和数据分布。计算机视觉领域也在探索基础模型，如CLIP和ALIGN利用对比学习训练文本和图像编码器，实现零样本泛化。</li><li><strong>图像分割任务</strong>：图像分割领域存在多种任务，如交互式分割、边缘检测、实例分割等，但缺乏大规模、多样化的分割数据集，且现有模型在泛化能力和处理模糊提示方面存在不足。</li></ul><p><strong>创新点：</strong></p><ol><li><strong>任务创新</strong>：提出可提示分割任务，能作为预训练目标，通过提示工程实现零样本迁移到下游分割任务。<strong>（Task）</strong></li><li><strong>模型创新</strong>：设计Segment Anything Model（SAM），由图像编码器、提示编码器和掩码解码器组成，支持灵活提示、实时计算且能处理歧义。<strong>（Model）</strong></li><li><strong>数据创新</strong>：构建数据引擎收集SA - 1B数据集，含超10亿掩码，数量和质量远超现有数据集，为模型训练提供强大支撑。<strong>（Data）</strong></li></ol><p><strong>最后提出本文的不足：</strong></p><ol><li><strong>细节处理欠佳</strong>：SAM可能会遗漏图像中的精细结构，有时会生成小的、不相连的虚假组件，且生成的边界不如一些计算密集型的“放大”方法清晰。</li><li><strong>特定场景表现弱</strong>：在提供多个提示点时，专门的交互式分割方法通常会优于SAM，因为SAM更侧重于通用性和广泛适用性，而非高IoU的交互式分割。</li><li><strong>文本到掩码任务待完善</strong>：文本到掩码任务的探索还不够成熟，不够稳健，需要更多的努力来改进。</li><li><strong>特定提示设计困难</strong>：目前尚不清楚如何设计简单的提示来实现语义和全景分割。</li><li><strong>特定领域表现不佳</strong>：在特定领域，一些专门的工具可能会比SAM表现更好。</li></ol><blockquote><p>写作启发：<strong>Promptable Segment（提示词分割</strong>），<strong>Foundation models（基础模型）</strong>，<strong>数据集的创新</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FCN</title>
      <link href="/post/20250224-fcn/"/>
      <url>/post/20250224-fcn/</url>
      
        <content type="html"><![CDATA[<h1 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h1><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-51-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-51-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-51-25"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-53-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-53-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-53-54"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-55-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-55-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-55-17"></p><h2 id="FCN基本原理"><a href="#FCN基本原理" class="headerlink" title="FCN基本原理"></a>FCN基本原理</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-03-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-03-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-03-28"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-07-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-07-54"></p><h2 id="FCN细节"><a href="#FCN细节" class="headerlink" title="FCN细节"></a>FCN细节</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-13-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-13-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-13-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-14-27.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-14-27.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-14-27"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-15-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-15-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-15-24"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-19-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-19-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-19-15"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-20-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-20-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-20-21"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-21-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-21-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-21-01"></p><h2 id="FCN结果"><a href="#FCN结果" class="headerlink" title="FCN结果"></a>FCN结果</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-22-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-22-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-22-21"></p><h1 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h1><h2 id="SegNet的基本原理"><a href="#SegNet的基本原理" class="headerlink" title="SegNet的基本原理"></a>SegNet的基本原理</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-56-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-56-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_08-56-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-57-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-57-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_08-57-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-00-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-00-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-00-18"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-01-08.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-01-08.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-01-08"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-02-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-02-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-02-54"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-04-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-04-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-04-52"></p><h1 id="UNet"><a href="#UNet" class="headerlink" title="UNet"></a>UNet</h1><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-10-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-10-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-10-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-11-20.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-11-20.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-11-20"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-12-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-12-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-12-11"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-16-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-16-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-16-06"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FCN模型讲解</title>
      <link href="/post/20250224-fcn-mo-xing-jiang-jie/"/>
      <url>/post/20250224-fcn-mo-xing-jiang-jie/</url>
      
        <content type="html"><![CDATA[<figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">class FCN_VGG16(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">    FCN 的 backbone，由 VGG16 修改而来，舍弃最后的全连接层</span></span><br><span class="line"><span class="string">    以池化层为区分，一个池化层到上一个池化层之间的部分认为一个卷积块。</span></span><br><span class="line"><span class="string">    &#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(FCN_VGG16, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            # 第一个卷积块: 输入通道数：3，输出通道数：64，卷积核大小：3<span class="number">*3</span>，步长：1，填充：1</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=3, <span class="attribute">out_channels</span>=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=64, <span class="attribute">out_channels</span>=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第二个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=64, <span class="attribute">out_channels</span>=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=128, <span class="attribute">out_channels</span>=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第三个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=128, <span class="attribute">out_channels</span>=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=256, <span class="attribute">out_channels</span>=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=256, <span class="attribute">out_channels</span>=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第四个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=256, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512,out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第五个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # 每一层在 features 中的范围，&#123;0，1，2，3，4&#125; 为第一个卷积块，&#123;5，6，7，8，9&#125; 为第二个卷积块<span class="built_in">..</span>.</span><br><span class="line">        self.range = ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31))</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = &#123;&#125;</span><br><span class="line">        # 每一块的输出</span><br><span class="line">        <span class="keyword">for</span> idx, (start, end) <span class="keyword">in</span> enumerate(self.range):</span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> range(start, end):</span><br><span class="line">                input = self.features[layer](input)</span><br><span class="line">            output[<span class="string">&quot;x%d&quot;</span> % (idx + 1)] = input</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def <span class="built_in">test_vgg</span>():</span><br><span class="line">    # Backbone 的测试函数</span><br><span class="line">    input_x = torch.<span class="built_in">randn</span>((<span class="number">1</span>,<span class="number">3</span>,<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line">    vgg = <span class="built_in">FCN_VGG16</span>()</span><br><span class="line">    output_y = <span class="built_in">vgg</span>(input_x)</span><br><span class="line"></span><br><span class="line">    for key in output_y:</span><br><span class="line">        <span class="built_in">print</span>(output_y[key].<span class="built_in">size</span>())</span><br><span class="line"></span><br><span class="line"><span class="built_in">test_vgg</span>()</span><br></pre></td></tr></table></figure><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_10-19-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_10-19-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_10-19-29"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语义分割基础</title>
      <link href="/post/20250224-tu-xiang-fen-ge-ji-chu/"/>
      <url>/post/20250224-tu-xiang-fen-ge-ji-chu/</url>
      
        <content type="html"><![CDATA[<h1 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h1><h2 id="1-1-什么是图像分割"><a href="#1-1-什么是图像分割" class="headerlink" title="1.1 什么是图像分割"></a>1.1 什么是图像分割</h2><p>预测目标的轮廓</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-24-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-24-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-24-33"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-25-10.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-25-10.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-25-10"></p><h2 id="1-2-图像分割的应用场景"><a href="#1-2-图像分割的应用场景" class="headerlink" title="1.2 图像分割的应用场景"></a>1.2 图像分割的应用场景</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-26-34.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-26-34.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-26-34"></p><p>医学图像分割：医学影像，CT照片等</p><h2 id="1-3-图像分割的前景和背景"><a href="#1-3-图像分割的前景和背景" class="headerlink" title="1.3 图像分割的前景和背景"></a>1.3 图像分割的前景和背景</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-28-51.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-28-51.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-28-51"></p><blockquote><p>things和stuff</p></blockquote><h2 id="1-4-图像分割的三个层次"><a href="#1-4-图像分割的三个层次" class="headerlink" title="1.4 图像分割的三个层次"></a>1.4 图像分割的三个层次</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-31-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-31-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-31-28"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-33-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-33-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-33-52"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-35-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-35-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-35-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-37-03.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-37-03.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-37-03"></p><h1 id="2-经典数据集"><a href="#2-经典数据集" class="headerlink" title="2.经典数据集"></a>2.经典数据集</h1><h2 id="2-1-PASCAL数据集"><a href="#2-1-PASCAL数据集" class="headerlink" title="2.1 PASCAL数据集"></a>2.1 PASCAL数据集</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-43-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-43-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-43-44"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-44-22"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-44-48"></p><h2 id="2-1Cityscape-用于自动驾驶场景"><a href="#2-1Cityscape-用于自动驾驶场景" class="headerlink" title="2.1Cityscape(用于自动驾驶场景)"></a>2.1Cityscape(用于自动驾驶场景)</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-47-03.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-47-03.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-47-03"></p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-48-10.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-48-10.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-48-10" style="zoom:150%;" /><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-49-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-49-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-49-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-50-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-50-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-50-40"></p><h2 id="2-3-COCO数据集"><a href="#2-3-COCO数据集" class="headerlink" title="2.3 COCO数据集"></a>2.3 COCO数据集</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-53-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-53-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-53-15"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-54-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-54-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-54-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-55-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-55-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-55-37"></p><h1 id="3-评估指标和优化目标"><a href="#3-评估指标和优化目标" class="headerlink" title="3. 评估指标和优化目标"></a>3. 评估指标和优化目标</h1><h2 id="3-1-语义分割评估指标"><a href="#3-1-语义分割评估指标" class="headerlink" title="3.1 语义分割评估指标"></a>3.1 语义分割评估指标</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-57-42.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-57-42.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-57-42"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-59-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-59-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-59-26"></p><h2 id="3-2-语义分割常用优化目标"><a href="#3-2-语义分割常用优化目标" class="headerlink" title="3.2 语义分割常用优化目标"></a>3.2 语义分割常用优化目标</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-04-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-04-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-04-23"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-06-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-06-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-06-05"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-07-51.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-07-51.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-07-51"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-09-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-09-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-09-59"></p><h1 id="4-上采样"><a href="#4-上采样" class="headerlink" title="4. 上采样"></a>4. 上采样</h1><h2 id="4-1-图像分割网络的两个模块"><a href="#4-1-图像分割网络的两个模块" class="headerlink" title="4.1 图像分割网络的两个模块"></a>4.1 图像分割网络的两个模块</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-13-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-13-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-13-52"></p><h2 id="4-2-上采样实现方法–插值法"><a href="#4-2-上采样实现方法–插值法" class="headerlink" title="4.2 上采样实现方法–插值法"></a>4.2 上采样实现方法–插值法</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-16-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-16-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-16-32"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-18-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-18-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-18-13"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-26-45.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-26-45.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-26-45"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-27-49.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-27-49.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-27-49"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-31-39.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-31-39.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-31-39"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-36-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-36-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-36-24"></p><h2 id="4-3-典型的图像分割网络"><a href="#4-3-典型的图像分割网络" class="headerlink" title="4.3 典型的图像分割网络"></a>4.3 典型的图像分割网络</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-39-19.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-39-19.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-39-19"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语义分割上采样</title>
      <link href="/post/20250224-yu-yi-fen-ge-shang-cai-yang/"/>
      <url>/post/20250224-yu-yi-fen-ge-shang-cai-yang/</url>
      
        <content type="html"><![CDATA[<h2 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h2><blockquote><p>可以将较小的 feature 映射回一个较大的 feature map，这样的操作称为上采样，常用的上采样包括转置卷积，反池化，插值等操作。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-54-56.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-54-56.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-54-56"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-55-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-55-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-55-44"></p><p><strong>小特征图 -&gt; 大特征图</strong></p><p>可学习的上采样：转置卷积</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_15-01-27.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_15-01-27.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_15-01-27"></p><p>转置卷积，也叫反卷积，它并不是正向卷积的完全逆过程，用一句话来解释：</p><blockquote><p>反卷积是一种特殊的正向卷积，先按照一定的比例通过补 0 来扩大输入图像的尺寸，再进行普通的卷积。</p></blockquote><blockquote><p>卷积核大小：kernelsize</p><p>卷积步长：stride</p><p>特征图填充宽度：padding</p></blockquote><p>对于普通的 kernelsize&#x3D;(3,3),strides&#x3D;(2,2) 的卷积，其过程如下图所示</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides.gif" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides.gif" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="padding_strides"></p><p>kernelsize&#x3D;(3,3),strides&#x3D;(1,1) 情况下的反卷积则体现为：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides_transposed.gif" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides_transposed.gif" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="padding_strides_transposed"></p><p>我们可以发现，对于反卷积而言，补 0 主要是通过输入边缘的 padding 和输入内部插 0 实现。</p><p>具体的说，padding 的层数为 $kernelsize−stride$，而对于输入的内部插 0，其插入的 0 的数量为$stride−1$。</p><p><strong>接口与参数说明</strong></p><p><strong>torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride&#x3D;1, padding&#x3D;0, output_padding&#x3D;0, groups&#x3D;1, bias&#x3D;True, dilation&#x3D;1)</strong></p><ol><li><strong>in_channels</strong>(int) – 输入信号的通道数</li><li><strong>out_channels</strong> (int) – 卷积产生的通道数</li><li><strong>kerner_size</strong> (int or tuple) - 卷积核的大小</li><li><strong>stride</strong> (int or tuple,optional) - 卷积步长，即要将输入扩大的倍数。</li><li><strong>padding</strong> (int or tuple, optional) - 输入的每一条边补充0的层数，高宽都增加2*padding</li><li><strong>output_padding</strong> (int or tuple, optional) - 输出边补充0的层数，高宽都增加padding</li><li><strong>groups</strong> (int, optional) – 从输入通道到输出通道的阻塞连接数</li><li><strong>bias</strong> (bool, optional) - 如果bias&#x3D;True，添加偏置</li><li><strong>dilation</strong> (int or tuple, optional) – 卷积核元素之间的间距</li></ol><p>输出尺寸的计算公式为:<br>$$<br>H_{out}&#x3D;(H_{in}-1)<em>stride[0]-2</em>padding[0]+kernelsize[0]+outputpadding[0]\W_{out}&#x3D;(W_{in}-1)<em>stride[1]-2</em>padding[1]+kernelsize[1]+outputpadding[1]<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语义分割概述</title>
      <link href="/post/20250224-yu-yi-fen-ge-gai-shu/"/>
      <url>/post/20250224-yu-yi-fen-ge-gai-shu/</url>
      
        <content type="html"><![CDATA[<h2 id="1-什么是语义分割"><a href="#1-什么是语义分割" class="headerlink" title="1.什么是语义分割"></a>1.什么是语义分割</h2><blockquote><p>所谓的分割，就是从像素层面上对图像进行描述，即某一个像素属于哪一类物体</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-03-50.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-03-50.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-03-50"></p><p>分类：<strong>图片级</strong>，比如区分一张图片是猫还是狗</p><p>检测：<strong>区域级</strong>，比如检测一个区域是猫还是狗</p><p>分割：<strong>像素级</strong>，比如区分一个像素是猫还是狗</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-09-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-09-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-09-11"></p><p><strong>语义分割</strong>：也称为全像素语义分割，它是将每个像素分类为属于对象类的过程。</p><p><strong>实例分割</strong>：是语义分割或全像素语义分割的子类型，它将每个像素分类为属于对象类以及该类的实体 ID。<br>简单的说，语义分割只需要对像素进行分类就行了，而实例分割不仅要对像素进行分类，同一类的不同物体也要进行分割。</p><p><strong>全景分割</strong>： 是语义分割和实例分割的结合，即要对所有目标都检测出来，又要区分出同个类别中的不同实例。</p><h2 id="2-模型的输入和输出"><a href="#2-模型的输入和输出" class="headerlink" title="2.模型的输入和输出"></a>2.模型的输入和输出</h2><blockquote><p>我们先要对模型有一个大的了解。模型的输入是什么？很显然，是一张张的图片；那么输出是什么呢？</p></blockquote><p>我们假设模型的分类数量为 <code>n</code>，输入的图像大小为$W\times H$的 <code>RGB</code> 三通道的图像，那么实际上模型的输出为$W\times D \times n$，这要我们就可以把每一个像素的预测看成是一个分类任务，回顾一下之前的全连接网络的分类模型，对于 <code>n</code> 分类模型，输出节点数为 <code>n</code>。</p><p>所以，在语义分割而言，可以看成是$W\times H$个分类任务，每个像素的分类类别均为 <code>n</code>，所以得到的输出 <code>shape</code> 为$W\times D \times n$。事实上，输出也可能会比实际输入更大或者更小一些，因为网络的设计未必那么完美，这时候需要对边缘进行裁切或者补零</p><h2 id="3-常见的分割模型"><a href="#3-常见的分割模型" class="headerlink" title="3.常见的分割模型"></a>3.常见的分割模型</h2><p>语义分割模型：FCN，RetinaNet，RefineNet，Deeplab 等等</p><p>实例分割模型：Mask RCNN，DeepMask等等</p><h2 id="4-语义分割的思路"><a href="#4-语义分割的思路" class="headerlink" title="4.语义分割的思路"></a>4.语义分割的思路</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-23-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-23-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-23-53"></p><p>滑动窗口的思路可以概括如下：</p><ol><li>将图像分割成一个个小的区域；</li><li>遍历一些小区域；</li><li>对这些小区域做分类。</li></ol><p>但是这个思想一个很重要的问题就是效率低，重叠区域的特征反复计算</p><h2 id="5-评价指标"><a href="#5-评价指标" class="headerlink" title="5.评价指标"></a>5.评价指标</h2><p>一般来说，分割的指标主要有两个：像素分类精度（<code>accuracy</code>）和区域的交并比（<code>IoU</code>），下面给出具体的计算公式。</p><p>说明：</p><ul><li><p>$n_{ij}$表示像素点属于分类$i$被预测为分类$j$的像素点数量，</p></li><li><p>$t_i&#x3D;\sum_{j}n_{ij}$表示标签中所有分类为$i$的像素点数量</p></li><li><p>总共有$n_{cl}$个不同的分类。</p></li><li><p>像素分类准确率 (<code>pixel accurarcy</code>)</p></li></ul><p>$$<br>pacc&#x3D;\frac{\sum_{i}n_{ii}}{\sum_{i}t_{i}}<br>$$</p><p>该指标表示所有像素中分类正确的比例</p><ul><li><p>平均准确率 (<code>mean accuracy</code>)：<br>$$<br>macc&#x3D;\frac1{n_{c1}}\sum_{i}\frac{n_{ii}}{t_{i}}<br>$$<br>计算每一类分类正确的像素点数和该类的所有像素点数的比例然后求平均</p></li><li><p>平均交并比 (<code>mean IoU</code>)：<br>$$<br>mIoU&#x3D;\frac{1}{n_{cl}}\sum_{i}\frac{n_{ii}}{(t_{i}+\sum_{j}n_{ji}-n_{ii})}<br>$$<br>其中 IoU 的概念与目标检测中的 IoU 概念一致，表示两个区域的交并比，只不过相比较目标检测的矩形框，在分割任务中，区域为不规则的物体边界。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>标题</title>
      <link href="/post/mo-ban/"/>
      <url>/post/mo-ban/</url>
      
        <content type="html"><![CDATA[<p>作者单位</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>精读模板</title>
      <link href="/post/jing-du-mo-ban/"/>
      <url>/post/jing-du-mo-ban/</url>
      
        <content type="html"><![CDATA[<p>作者单位</p><table><thead><tr><th>标题</th><th>XXXXXXXXXXXXXXXXXXXXX</th></tr></thead><tbody><tr><td>翻译</td><td>XXXXXXXXXXXXXXXXXXXXXXX</td></tr><tr><td>主题</td><td>XXXXXXXX</td></tr><tr><td>方法</td><td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</td></tr></tbody></table><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><blockquote><p>第一句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第二句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第三句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第四句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第五句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第六句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第七句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第八句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第九句：</p></blockquote><blockquote><p>翻译：</p></blockquote><h1 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h1><p><strong>研究背景</strong></p><blockquote><p>第一句：随着数字媒体产业的快速发展，由各种资源捕捉到的海量视频数据集正以爆炸式的速度增长。  </p><p>第二句：大多数监控视频只包含有限的重要事件。</p></blockquote><p><strong>提出视频摘要的概念</strong></p><blockquote><p>第三句：用户将视频中的重要事件浓缩转发  </p><p>第四句：提出了视频摘要的概念</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习环境配置2——windows下的torch=1.2.0环境配置</title>
      <link href="/post/windows-xia-de-torch-1.2.0-huan-jing-pei-zhi/"/>
      <url>/post/windows-xia-de-torch-1.2.0-huan-jing-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Anaconda安装"><a href="#一、Anaconda安装" class="headerlink" title="一、Anaconda安装"></a>一、Anaconda安装</h3><p><strong>Anaconda的安装主要是为了方便环境管理，可以同时在一个电脑上安装多种环境，不同环境放置不同框架：pytorch、tensorflow、keras可以在不同的环境下安装，只需要使用conda create –n创建新环境即可。</strong></p><h4 id="1、Anaconda的下载"><a href="#1、Anaconda的下载" class="headerlink" title="1、Anaconda的下载"></a>1、Anaconda的下载</h4><p>同学们可以选择安装新版Anaconda和旧版的Anaconda，安装步骤没有什么区别。</p><p><strong>旧版本anaconda的下载：</strong><br><strong>新版本的Anaconda没有VSCODE，如果大家为了安装VSCODE方便可以直接安装旧版的Anaconda，百度网盘连接如下。也可以装新版然后分开装VSCODE。</strong><br>链接: <a href="https://pan.baidu.com/s/12tW0Oad_Tqn7jNs8RNkvFA">https://pan.baidu.com/s/12tW0Oad_Tqn7jNs8RNkvFA</a> 提取码: i83n</p><p><strong>新版本anaconda的下载：</strong><br>如果想要安装最新的Anaconda，首先登录Anaconda的官网：<a href="https://www.anaconda.com/distribution/">https://www.anaconda.com/distribution/</a>。直接下载对应安装包就可以。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ccda457f3e2c14fa490e5dee510e15ff.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ccda457f3e2c14fa490e5dee510e15ff.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/79729ea1f6363089a7b848e2bbb41119.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/79729ea1f6363089a7b848e2bbb41119.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>一般是下载64位的，下载完成后打开。</p><h4 id="2、Anaconda的安装"><a href="#2、Anaconda的安装" class="headerlink" title="2、Anaconda的安装"></a>2、Anaconda的安装</h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b04e1b9b3c820f4212c77e872f721ff0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b04e1b9b3c820f4212c77e872f721ff0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>选择安装的位置，可以不安装在C盘。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cf41baaf1550d7d707c56da7997bf467.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cf41baaf1550d7d707c56da7997bf467.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>我选择了Add Anaconda to my PATH environment variable，这样会自动将anaconda装到系统的环境变量中，配置会更加方便一些。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/34a7c27d1eb9256186f88e6e610ffbd5.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/34a7c27d1eb9256186f88e6e610ffbd5.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>等待安装完之后，Anaconda的安装就结束了。</p><h3 id="二、Cudnn和CUDA的下载和安装"><a href="#二、Cudnn和CUDA的下载和安装" class="headerlink" title="二、Cudnn和CUDA的下载和安装"></a>二、Cudnn和CUDA的下载和安装</h3><p><strong>我这里使用的是torch&#x3D;1.2.0，官方推荐的Cuda版本是10.0，因此会用到cuda10.0，与cuda10.0对应的cudnn是7.4.1。</strong></p><h4 id="1、Cudnn和CUDA的下载"><a href="#1、Cudnn和CUDA的下载" class="headerlink" title="1、Cudnn和CUDA的下载"></a>1、Cudnn和CUDA的下载</h4><p><strong>网盘下载：</strong><br>链接: <a href="https://pan.baidu.com/s/1znYSRDtLNFLufAuItOeoyQ">https://pan.baidu.com/s/1znYSRDtLNFLufAuItOeoyQ</a><br>提取码: 8ggr</p><p><strong>官网下载：</strong><br>cuda10.0官网的地址是：<br><a href="https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal">cuda10.0官网地址</a><br>cudnn官网的地址是：需要大家进去后寻找7.4.1.5。<br><a href="https://developer.nvidia.com/cudnn">cudnn官网地址</a></p><p>下载完之后得到这两个文件。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/829e732c6e6228e02d96c3b7bd115d9b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/829e732c6e6228e02d96c3b7bd115d9b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8d273ca827020e1e079a78743bd000c5.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8d273ca827020e1e079a78743bd000c5.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h4 id="2、Cudnn和CUDA的安装"><a href="#2、Cudnn和CUDA的安装" class="headerlink" title="2、Cudnn和CUDA的安装"></a>2、Cudnn和CUDA的安装</h4><p>下载好之后可以打开exe文件进行安装。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c1fa30103f2316fc350436a8815d54e0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c1fa30103f2316fc350436a8815d54e0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>这里选择自定义。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bf16ce7629339969e0830a1630bd182.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bf16ce7629339969e0830a1630bd182.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="不"><br>然后直接点下一步就行了。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/d833eb6e1fa90ca9f621eb1072fe25aa.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/d833eb6e1fa90ca9f621eb1072fe25aa.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>安装完后在C盘这个位置可以找到根目录。<br>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0<br>然后大家把Cudnn的内容进行解压。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0ae6fdd762c2435ef118a642b341d4ba.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0ae6fdd762c2435ef118a642b341d4ba.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>把这里面的内容直接复制到C盘的根目录下就可以了。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a88d013177374fcfecfec1e3865e3c5e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a88d013177374fcfecfec1e3865e3c5e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h3 id="三、配置torch环境"><a href="#三、配置torch环境" class="headerlink" title="三、配置torch环境"></a>三、配置torch环境</h3><h4 id="1、pytorch环境的创建与激活"><a href="#1、pytorch环境的创建与激活" class="headerlink" title="1、pytorch环境的创建与激活"></a>1、pytorch环境的创建与激活</h4><p>Win+R启动cmd，在命令提示符内输入以下命令：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create –n pytorch python=3.6</span><br></pre></td></tr></table></figure><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate pytorch</span><br></pre></td></tr></table></figure><p>这里一共存在两条指令：<br>前面一条指令用于创建一个名为pytorch的环境，该环境的python版本为3.6。<br>后面一条指令用于激活一个名为pytorch的环境。</p><h4 id="2、pytorch库的安装"><a href="#2、pytorch库的安装" class="headerlink" title="2、pytorch库的安装"></a>2、pytorch库的安装</h4><p>由于我们所有的操作都要在对应环境中进行，所以在进行库的安装前需要先激活环境。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate pytorch </span><br></pre></td></tr></table></figure><p>此时cmd窗口的样子为：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e67dbf5cfc4f4125fedbffcb3bd85b77.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e67dbf5cfc4f4125fedbffcb3bd85b77.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h5 id="a、官方推荐安装方法（推荐）"><a href="#a、官方推荐安装方法（推荐）" class="headerlink" title="a、官方推荐安装方法（推荐）"></a>a、官方推荐安装方法（推荐）</h5><p>打开pytorch的官方安装方法：<br><a href="https://pytorch.org/get-started/previous-versions/">https://pytorch.org/get-started/previous-versions/</a><br>官网推荐的安装代码如下，我使用的是Cuda10的版本，不太懂为什么要写3个&#x3D;才能正确定位，两个&#x3D;会定位到cuda92的whl：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># CUDA 10.0</span><br><span class="line">pip install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure><p>这是pytorch官方提供的指令，用于安装torch和torchvision。</p><h5 id="b、先下载whl后安装"><a href="#b、先下载whl后安装" class="headerlink" title="b、先下载whl后安装"></a>b、先下载whl后安装</h5><p>需要注意的是，直接这样安装似乎特别慢，因此我们可以进入如下网址:<br><a href="https://download.pytorch.org/whl/torch_stable.html">https://download.pytorch.org/whl/torch_stable.html</a><br>找到自己需要的轮子下载。下载的时候使用迅雷下载就行了，速度还是比较快的！<br><img src="https://i-blog.csdnimg.cn/blog_migrate/08b8a756b9d7d214ce81f10bb5b73758.png#pic_center" class="lazyload placeholder" data-srcset="https://i-blog.csdnimg.cn/blog_migrate/08b8a756b9d7d214ce81f10bb5b73758.png#pic_center" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"  /><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/737ffc86979d1e7ceda0d98b5ddcef41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/737ffc86979d1e7ceda0d98b5ddcef41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>下载完成后找到安装路径：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1eae8e7fae0ea98cc5559e6287059451.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1eae8e7fae0ea98cc5559e6287059451.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>在cmd定位过来后利用文件全名进行安装就行了！<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/32c32f513e9be037243f885cd6f4ef11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/32c32f513e9be037243f885cd6f4ef11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>这里我也传一个百度网盘的版本。<br>链接: <a href="https://pan.baidu.com/s/14-QVk7Kb_CVwaVZxVPIgtw">https://pan.baidu.com/s/14-QVk7Kb_CVwaVZxVPIgtw</a><br>提取码: rg2e<br><strong>全部安装完成之后重启电脑。</strong></p><h4 id="3、其它依赖库的安装"><a href="#3、其它依赖库的安装" class="headerlink" title="3、其它依赖库的安装"></a>3、其它依赖库的安装</h4><p>但如果想要跑深度学习模型，还有一些其它的依赖库需要安装。具体如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scipy==1.2.1</span><br><span class="line">numpy==1.17.0</span><br><span class="line">matplotlib==3.1.2</span><br><span class="line">opencv_python==4.1.2.30</span><br><span class="line">torch==1.2.0</span><br><span class="line">torchvision==0.4.0</span><br><span class="line">tqdm==4.60.0</span><br><span class="line">Pillow==8.2.0</span><br><span class="line">h5py==2.10.0</span><br></pre></td></tr></table></figure><p>如果想要更便捷的安装可以在桌面或者其它地方创建一个requirements.txt文件，复制上述内容到txt文件中。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/7f76e6ad79f6bed1e2f4676b627354d3.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/7f76e6ad79f6bed1e2f4676b627354d3.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>使用如下指令安装即可。<strong>下述指令中，requirements.txt前方的路径是我将文件放在桌面的路径，各位同学根据自己的电脑修改。</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -r C:\Users\33232\Desktop\requirements.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="4、安装较慢请注意换源"><a href="#4、安装较慢请注意换源" class="headerlink" title="4、安装较慢请注意换源"></a>4、安装较慢请注意换源</h4><p>需要注意的是，如果在pip中下载安装比较慢可以换个源，可以到用户文件夹下，创建一个pip文件夹，然后在pip文件夹里创建一个txt文件。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/28006284902c6a57318e718daccee1a8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/28006284902c6a57318e718daccee1a8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>修改txt文件的内容，并且把后缀改成ini</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = http://pypi.mirrors.ustc.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">use-mirrors =true</span><br><span class="line">mirrors =http://pypi.mirrors.ustc.edu.cn/simple/</span><br><span class="line">trusted-host =pypi.mirrors.ustc.edu.cn</span><br></pre></td></tr></table></figure><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/783a72953baad1fd9de83303701cbaf8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/783a72953baad1fd9de83303701cbaf8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8bd41332dee625e1c6e182608acb9a29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8bd41332dee625e1c6e182608acb9a29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><strong>全部安装完成之后重启电脑。</strong></p><h3 id="四、安装VSCODE"><a href="#四、安装VSCODE" class="headerlink" title="四、安装VSCODE"></a>四、安装<a href="https://so.csdn.net/so/search?q=VSCODE&spm=1001.2101.3001.7020">VSCODE</a></h3><p><strong>我个人喜欢VSCODE，所以就安装它啦。其它的编辑软件也可以，个人喜好罢了。</strong></p><h4 id="1、下载安装包安装（推荐）"><a href="#1、下载安装包安装（推荐）" class="headerlink" title="1、下载安装包安装（推荐）"></a>1、下载安装包安装（推荐）</h4><p><strong>最新版本的Anaconda没有VSCODE因此可以直接百度VSCODE进行安装。</strong></p><h5 id="a、VSCODE的下载"><a href="#a、VSCODE的下载" class="headerlink" title="a、VSCODE的下载"></a>a、VSCODE的下载</h5><p>直接加载VSCODE的官网<a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a>，点击Download for Windows即可下载。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f69abbceb271a9dc5d12142e76df4ebc.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f69abbceb271a9dc5d12142e76df4ebc.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h5 id="b、VSCODE的安装"><a href="#b、VSCODE的安装" class="headerlink" title="b、VSCODE的安装"></a>b、VSCODE的安装</h5><p>首先同意协议，点一下步。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a80d57bcc63ffc394fb5b59aed099347.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a80d57bcc63ffc394fb5b59aed099347.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>其他里面的几个勾要打起来，因为这样就可以右键文件夹用VSCODE打开，非常方便。下一步。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4156c82700455d0ab65ea5bb8f68eeb3.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4156c82700455d0ab65ea5bb8f68eeb3.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>继续下一步安装即可。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6523e522b685fc8512cacaedfb1934d8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6523e522b685fc8512cacaedfb1934d8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p><strong>安装完成后在左下角更改自己的环境就行了。</strong><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4a6e8c3ce2dec68836338fdcc57a0dc1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4a6e8c3ce2dec68836338fdcc57a0dc1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h4 id="2、anaconda上安装"><a href="#2、anaconda上安装" class="headerlink" title="2、anaconda上安装"></a>2、anaconda上安装</h4><p>打开anaconda，切换环境。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/361dd496e006335bd418e8b03e91354e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/361dd496e006335bd418e8b03e91354e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>安装VSCODE，安装完就可以launch一下了，之后就可以把VScode固定到任务栏上，方便打开。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f781c276062dded3ab0d4c9f37aef3bf.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f781c276062dded3ab0d4c9f37aef3bf.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> CUDA </tag>
            
            <tag> cudnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vscode配置latex环境</title>
      <link href="/post/visual-studio-code/"/>
      <url>/post/visual-studio-code/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>LaTeX</strong> 作为一种强大的排版系统，对于理工科，特别是公式比较多的数学专业（秃头专业），其重要性自不必多说，不在本文探讨范围之内。</p><p>而选择一个比较好的编译器是很重要的，至少对笔者而言是如此。笔者前期使用的是<strong>TeXstudio</strong>进行文档的编译的，但是其编译速度比较慢，并且页面不是很美观。最让人头疼的是，当公式比较长的时候，使用的括号就比较多，但<strong>Texstudio</strong>的代码高亮功能实在是…（它对于括号根本就没有高亮，头秃）</p><p>而<strong>Visual Studio Code</strong>呢？话不多说，直接上图！<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5b90636b6875472ff796fe988e98826e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5b90636b6875472ff796fe988e98826e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="展示图"></p><p>可以看到，它不仅能够对代码高亮，不同级别括号用不同颜色标注了，颜值也很高。且 vscode 最突出的特点就是其强大的插件功能，每个使用者都能够根据自己的需求和想法下载相应的插件，从而将之配置为高度个性化的编辑器。可以这么说，每个使用者的 vscode 都不一样，为其专属定制编辑器。</p><p>笔者配置了好久，找了很多资料，很多博主也只是贴上了配置代码，没有详细的介绍说明。为了让更多人能够有一个比较清晰的了解，以此可以随时对自己的配置代码进行更改，故笔者写下了此文。希望能够对大家有所帮助。</p><p>注 1： 本文使用图片均为笔者自身编辑器截图或笔者朋友的编辑器截图（经过对方同意），且所有引用在文中或文末注明了来源，其余均为原创内容（代码不算哈哈哈）。</p><p>注 2： 若您的 vscode 页面和笔者所用图片中展示的页面有略微不同，均为笔者所安装的其余插件以及其余设置所致，并不影响本文中所说的所有配置，您无需担心，只需按照图片中所指向图标进行配置即可。</p><p>注 3： 文末有完整的个人配置代码（有的地方需要更改路径，有具体说明）。</p><h2 id="1-TeX-Live-下载与安装"><a href="#1-TeX-Live-下载与安装" class="headerlink" title="1 TeX Live 下载与安装"></a>1 TeX Live 下载与安装</h2><p>笔者选用的 Tex 系统是 TeX Live ，如果您想了解 TeX Live 和 MiKTeX 的区别，可以查看此篇文章：<a href="https://www.cnblogs.com/liuliang1999/p/12656706.html">https://www.cnblogs.com/liuliang1999/p/12656706.html</a></p><p>接下来是 TeX Live 的<strong>下载与安装说明</strong>：</p><p>① 通过网址 ：<a href="http://tug.org/texlive/acquire-iso.html">http://tug.org/texlive/acquire-iso.html</a> 进入 ISO 下载页面，点击图示红框圈画位置进入随机的镜像网站。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b238c6ecae7799a007459ab0cacddfdc.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b238c6ecae7799a007459ab0cacddfdc.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="下载网址"><br>② 可以看到的是，笔者进入了清华大学镜像网站，点击红框圈画链接进行 TeX Live 下载。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4cea74b28cf59bd59bd28cb03bdcb6e4.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4cea74b28cf59bd59bd28cb03bdcb6e4.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="下载"></p><p>③ 如果<strong>下载速度过慢</strong>，可以返回前一页面，进行重新点击，随机进入另一镜像网站进行下载尝试，直到下载速度在您的可接受范围内即可。或者在前一页面，点击 <strong>“mirror list”</strong> 进入镜像列表</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2a9a2b2fa4447587774d4f2f4eb1c9fb.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2a9a2b2fa4447587774d4f2f4eb1c9fb.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="mirror list"></p><p>然后手动选择某一镜像网站进行下载：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/45f4adf32411091223ef279f96fda7ed.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/45f4adf32411091223ef279f96fda7ed.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="镜像列表选择"></p><p>④ 找到下载好的压缩包，右键，在打开方式中选择**“Windows 资源管理器”**打开</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/db2bbda5fb583717e095af7edf5465a6.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/db2bbda5fb583717e095af7edf5465a6.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="资源管理器打开"></p><p>⑤ 找到 <strong>“install-tl-windows”</strong> 文件，为了后面不必要的麻烦，右键<strong>以管理员身份运行</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b666c4f9426f2fdf0a4a69bc86413ada.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b666c4f9426f2fdf0a4a69bc86413ada.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="以管理员身份运行"></p><p>⑥ 会先出现下图，无需理会，等会儿会消失</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/94aa279e1d0f6a59c34d95f4e85e0666.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/94aa279e1d0f6a59c34d95f4e85e0666.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="出现狮子"></p><p>⑦ **基本更改：**出现下图后，需要进行路径的更改；由于 TeX Live 自带的 TeXworks 不怎么好用，并且此文主要将 vscode 作为 LaTeX 的编辑器，故而取消 <strong>安装 TeXworks 前端</strong>的选项，再点击安装</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5382a34df2373e3c3febc4172ad4af1d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5382a34df2373e3c3febc4172ad4af1d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="修改路径"></p><p>⑧ <strong>个性化安装：</strong> 如果您需要个性化程度高的话，那么可以点击上图左下角的 <strong>Advancde</strong> ，根据您的需要进行相应的更改，但<strong>建议</strong>在不明白各个选项的作用时，不要对其进行修改，以免后期使用产生奇怪的问题。要注意的是，<strong>Adjust searchpath</strong> 这个选项一定要选中，将之添加到环境变量，否则后期手动添加比较麻烦；<br>而对于我们大部分人来说，只需要更改下图框选出的部分即可，也就是上图所完成的功能，再点击<strong>安装</strong>即可</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/64e11c5a3bfce6dae1620833aa528df6.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/64e11c5a3bfce6dae1620833aa528df6.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="advanced"></p><p>⑨ **进行安装：**接着就会出现下图，具体的安装指标已在下图标明，可根据其数字来判断安装所需时间。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/22a9567ff88b850c6de3365fd1bd570e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/22a9567ff88b850c6de3365fd1bd570e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="安装ing"></p><p>当上面标示的时间安装完之后，会出现一些配置文件的安装运行写入，进行等待即可，几分钟左右：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4bd5a634521b8a18328976fa6884d688.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4bd5a634521b8a18328976fa6884d688.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="安装后续"></p><p>当出现下图所示弹窗时，说明安装完毕，点击关闭即可。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/dd26e23808c26c27614803e4fbca04ca.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/dd26e23808c26c27614803e4fbca04ca.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="欢迎进入"></p><p>⑩ <strong>检查安装是否正常：</strong> 按win + R 打开<strong>运行</strong>，输入<code>cmd</code>，打开命令行窗口；然后输入命令<code>xelatex -v</code> ，如下图</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4042a6e9ec2aa56e6b1442fa1d4c96a0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4042a6e9ec2aa56e6b1442fa1d4c96a0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="安装检查"></p><p>如上图所示，若输出了一些版本信息，则安装正常。</p><h2 id="2-vscode下载与安装"><a href="#2-vscode下载与安装" class="headerlink" title="2 vscode下载与安装"></a>2 vscode下载与安装</h2><p>官网下载： <a href="https://code.visualstudio.com/">Click here to download Visual Studio Code</a>.</p><p>点进去之后就可以进行下载了。具体安装过程与常见的软件安装过程一致，这里就不作赘述。笔者只对几个要点进行提及：</p><p>① 记得<strong>修改安装路径</strong></p><p>② 根据个人想法可以选择是否在开始菜单文件夹创建 vscode 的快捷方式</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b52103381640e671ae6421fd329a600e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b52103381640e671ae6421fd329a600e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p>③ 一定要选上”添加到PATH”这个选项，能省很多麻烦。其余如图所示，自行选择。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c721d489a01c6fde012b9777eaae64ea.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c721d489a01c6fde012b9777eaae64ea.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p>安装好之后，打开 vscode，应如下图页面所示：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/56e8910ef21f541e6a699bb00f0a4f57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/56e8910ef21f541e6a699bb00f0a4f57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="启动页面"></p><h2 id="3-中文语言环境配置"><a href="#3-中文语言环境配置" class="headerlink" title="3 中文语言环境配置"></a>3 中文语言环境配置</h2><p>vscode的中文环境需要下载插件来进行支持。如下图所示：</p><p>① 点击拓展图标，打开拓展；</p><p>② 输入”Chinese”，选择第一个Chinese (Simplified) Language Pack for Visual Studio Code插件；</p><p>③ 点击”install”进行安装，等待安装完成；</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e74e7680283789d9bec131c5f69234d7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e74e7680283789d9bec131c5f69234d7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="下载中文插件"></p><p>④ 点击页面右下角跳出窗口中的”Restart now”，进行 vscode 重启。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/02a0f5f7d9a509bab220dfc4fd2b26b9.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/02a0f5f7d9a509bab220dfc4fd2b26b9.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="vscode重启"></p><p>⑤ 完成中文环境配置，显示如下：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57e75ca8310629f3fc4e2daf27cc8610.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57e75ca8310629f3fc4e2daf27cc8610.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="中文环境启动页面"></p><h2 id="4-LaTeX的支持插件-LaTeX-Workshop安装"><a href="#4-LaTeX的支持插件-LaTeX-Workshop安装" class="headerlink" title="4 LaTeX的支持插件 LaTeX Workshop安装"></a>4 LaTeX的支持插件 LaTeX Workshop安装</h2><p>① 点击拓展图标，打开拓展；</p><p>② 输入”latex workshop”，选择第一个LaTeX Workshop插件；</p><p>③ 点击”install”进行安装，等待安装完成；</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/254d1f24e8a90b349a77958b5c192d4d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/254d1f24e8a90b349a77958b5c192d4d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="latex workshop安装"></p><p>④ 若在安装完该插件之后在 vscode 页面右下角跳出如下弹窗，无需在意，只是提醒该插件已经更新到了8.11.1版本。若您想要了解新版本增加的功能，可以点击”Change log”进行查看；若不想了解，点击 “Disable this message” 即可。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c96cdbe7e01da3ff326da9df306918d1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c96cdbe7e01da3ff326da9df306918d1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="弹窗"></p><h2 id="5-打开LaTeX环境设置页面"><a href="#5-打开LaTeX环境设置页面" class="headerlink" title="5 打开LaTeX环境设置页面"></a>5 打开LaTeX环境设置页面</h2><p>① 点击设置图标</p><p>② 点击设置</p><p>③ 转到 UI 设置页面</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2ee6e759bdd67de85e8380d332db1a1a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2ee6e759bdd67de85e8380d332db1a1a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="打开设置"></p><p>④ 点击下图 1 处打开 json 文件，进入代码设置页面</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b8d742a52940e90d31777f7efd9943f1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b8d742a52940e90d31777f7efd9943f1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="json设置"></p><p>注 4 ： UI 设置页面和代码设置页面均为设置页面，其功能是一样的。不同的是，UI 设置页面交互能力较强，但一些设置需要去寻找，比较麻烦；而代码设置页面虽然相对 UI 而言不那么直观，但却可以对自己想要的功能直接进行代码编写，且代码设置可以直接克隆别人的代码到自己的编辑器中，从而直接完成相应设置，比较便捷。</p><p>注 5 ： 可以直接按Ctrl + ，进入设置页面。</p><h2 id="6-LaTeX环境的代码配置"><a href="#6-LaTeX环境的代码配置" class="headerlink" title="6 LaTeX环境的代码配置"></a>6 LaTeX环境的代码配置</h2><h3 id="6-1-LaTeX配置代码展示"><a href="#6-1-LaTeX配置代码展示" class="headerlink" title="6.1 LaTeX配置代码展示"></a>6.1 LaTeX配置代码展示</h3><p>先给出效果图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43a5acbf6be4a170f4a9dbdbbcff2209.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43a5acbf6be4a170f4a9dbdbbcff2209.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="LaTeX代码配置"></p><p>LaTeX 配置代码如下（不包含外部 pdf 查看器设置）：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;latex-workshop.latex.autoBuild.run&quot;: &quot;never&quot;,</span><br><span class="line">    &quot;latex-workshop.showContextMenu&quot;: true,</span><br><span class="line">    &quot;latex-workshop.intellisense.package.enabled&quot;: true,</span><br><span class="line">    &quot;latex-workshop.message.error.show&quot;: false,</span><br><span class="line">    &quot;latex-workshop.message.warning.show&quot;: false,</span><br><span class="line">    &quot;latex-workshop.latex.tools&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;-pdf&quot;,</span><br><span class="line">                &quot;-outdir=%OUTDIR%&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.latex.recipes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;XeLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;PDFLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;BibTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;bibtex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;LaTeXmk&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;latexmk&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex -&gt; bibtex -&gt; xelatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.latex.clean.fileTypes&quot;: [</span><br><span class="line">        &quot;*.aux&quot;,</span><br><span class="line">        &quot;*.bbl&quot;,</span><br><span class="line">        &quot;*.blg&quot;,</span><br><span class="line">        &quot;*.idx&quot;,</span><br><span class="line">        &quot;*.ind&quot;,</span><br><span class="line">        &quot;*.lof&quot;,</span><br><span class="line">        &quot;*.lot&quot;,</span><br><span class="line">        &quot;*.out&quot;,</span><br><span class="line">        &quot;*.toc&quot;,</span><br><span class="line">        &quot;*.acn&quot;,</span><br><span class="line">        &quot;*.acr&quot;,</span><br><span class="line">        &quot;*.alg&quot;,</span><br><span class="line">        &quot;*.glg&quot;,</span><br><span class="line">        &quot;*.glo&quot;,</span><br><span class="line">        &quot;*.gls&quot;,</span><br><span class="line">        &quot;*.ist&quot;,</span><br><span class="line">        &quot;*.fls&quot;,</span><br><span class="line">        &quot;*.log&quot;,</span><br><span class="line">        &quot;*.fdb_latexmk&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.latex.autoClean.run&quot;: &quot;onFailed&quot;,</span><br><span class="line">    &quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;,</span><br><span class="line">    &quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注 6 ： 若您不想要配置外部查看器以及了解内部查看和外部查看之间切换操作，可以直接复制上述代码至 json 文件中，即可完成 LaTeX 的配置，从而可以对 LaTeX 代码进行编译。</p><p>注 7 ： 根据 json 文件编写规则，每个代码语句（除了代码块儿最后一句）都需要加上英文状态下的<code>,</code>，否则就会报错；而每个代码块儿的最后一句是不需要加上<code>,</code>的。从上文整个代码块儿可以看出此规则。</p><p>如果您日后需要在上述代码之后再添加其他代码，请记得在最后一句</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;</span><br></pre></td></tr></table></figure><p>后添加上<code>,</code>，即变为</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;,</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>其中的<code>......</code>为您添加的其余代码。</p><p><strong>切记！</strong></p><h3 id="6-2-LaTeX配置代码解读"><a href="#6-2-LaTeX配置代码解读" class="headerlink" title="6.2 LaTeX配置代码解读"></a>6.2 LaTeX配置代码解读</h3><p>如果您对此不感兴趣，可以跳过该小节。下面进行代码<strong>注释解读</strong>：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.autoBuild.run&quot;: &quot;never&quot;</span><br></pre></td></tr></table></figure><p>设置何时使用默认的(第一个)编译链自动构建 LaTeX 项目，即什么时候自动进行代码的编译。有三个选项：</p><ol><li><strong>onFileChange</strong>：在检测任何依赖项中的文件更改(甚至被其他应用程序修改)时构建项目，即当检测到代码被更改时就自动编译tex文件；</li><li><strong>onSave</strong> : 当代码被保存时自动编译文件；</li><li><strong>never</strong>: 从不自动编译，即需编写者手动编译文档</li></ol><p>此项笔者设置为<strong>never</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.showContextMenu&quot;: true</span><br></pre></td></tr></table></figure><p>启用上下文LaTeX菜单。此菜单默认状态下停用，即变量设置为<strong>false</strong>，因为它可以通过新的 LaTeX 标记使用（新的 LaTeX 标记能够编译文档，将在下文提及）。只需将此变量设置为<strong>true</strong>即可恢复菜单。即此命令设置是否将编译文档的选项出现在鼠标右键的菜单中。</p><p>下图展示两者区别，左边为设置<strong>false</strong>情况，右边为设置<strong>true</strong>情况。可以看到的是，设置为<strong>true</strong>时，菜单中多了两个选项，其中多出来的第一个选项为进行tex文件的编译，而第二个选项为进行正向同步，即从代码定位到编译出来的 pdf 文件相应位置，下文会进行提及。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/80cef33dcac33ac0e86c82c101461f3b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/80cef33dcac33ac0e86c82c101461f3b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="无"><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bfedcbe76bb0f0ee3a27db3f6e4d538.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bfedcbe76bb0f0ee3a27db3f6e4d538.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="有"><br>笔者觉得菜单多了此选项较方便，故此项笔者设置为<strong>true</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.intellisense.package.enabled&quot;: true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>设置为<strong>true</strong>，则该拓展能够从使用的宏包中自动提取命令和环境，从而补全正在编写的代码。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.message.error.show&quot;  : false,</span><br><span class="line">&quot;latex-workshop.message.warning.show&quot;: false</span><br></pre></td></tr></table></figure><p>这两个命令是设置当文档编译错误时是否弹出显示出错和警告的弹窗。因为这些错误和警告信息能够从终端中获取，且弹窗弹出比较烦人，故而笔者设置均设置为<strong>false</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.tools&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;-pdf&quot;,</span><br><span class="line">                &quot;-outdir=%OUTDIR%&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>这些代码是定义在下文 recipes 编译链中被使用的编译命令，此处为默认配置，不需要进行更改。其中的<code>name</code>为这些命令的标签，用作下文 recipes 的引用；而<code>command</code>为在该拓展中的编译方式。</p><p>可以更改的代码为，将编译方式: pdflatex 、 xelatex 和 latexmk 中的<code>%DOCFILE</code>更改为<code>%DOC</code>。<code>%DOCFILE</code>表明编译器访问没有扩展名的根文件名，而<code>%DOC</code>表明编译器访问的是没有扩展名的根文件完整路径。这就意味着，使用<code>%DOCFILE</code>可以将文件所在路径设置为中文，但笔者不建议这么做，因为毕竟涉及到代码，当其余编译器引用时该 tex 文件仍需要根文件完整路径，且需要为英文路径。笔者此处设置为<code>%DOCFILE</code>仅是因为之前使用 TeXstudio，导致路径已经是中文了。</p><p>更多详情可以访问 github 中 LaTeX-Workshop 的 Wiki: <a href="https://github.com/James-Yu/LaTeX-Workshop/wiki/Compile#placeholders">Click here for more details about this.</a></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.recipes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;XeLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;PDFLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;BibTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;bibtex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;LaTeXmk&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;latexmk&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex -&gt; bibtex -&gt; xelatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>此串代码是对编译链进行定义，其中<code>name</code>是标签，也就是出现在工具栏中的链名称；<code>tool</code>是<code>name</code>标签所对应的编译顺序，其内部编译命令来自上文<code>latex-workshop.latex.recipes</code>中内容。</p><p>定义完成后，能够在 vscode 编译器中能够看到的编译顺序，具体看下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/bfcdea57459bf5f1687f3a4c548e868a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/bfcdea57459bf5f1687f3a4c548e868a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="编译链"></p><p>可以看到的是，在编译链中定义的命令出现在了vscode右侧的工具栏中。</p><p>注 8 ： <strong>PDFLaTeX</strong> 编译模式与 <strong>XeLaTeX</strong> 区别如下：</p><blockquote><ol><li><p>PDFLaTeX 使用的是TeX的标准字体，所以生成PDF时，会将所有的非 TeX 标准字体进行替换，其生成的 PDF 文件默认嵌入所有字体；而使用 XeLaTeX 编译，如果说论文中有很多图片或者其他元素没有嵌入字体的话，生成的 PDF<br>文件也会有些字体没有嵌入。</p></li><li><p>XeLaTeX 对应的 XeTeX 对字体的支持更好，允许用户使用操作系统字体来代替 TeX 的标准字体，而且对非拉丁字体的支持更好。</p></li><li><p>PDFLaTeX 进行编译的速度比 XeLaTeX 速度快。</p></li></ol></blockquote><p>注 9 ： 编译链的存在是为了更方便编译，因为如果涉及到**.bib**文件，就需要进行多次不同命令的转换编译，比较麻烦，而编译链就解决了这个问题。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.clean.fileTypes&quot;: [</span><br><span class="line">        //&quot;*.aux&quot;,</span><br><span class="line">       // &quot;*.bbl&quot;,</span><br><span class="line">        &quot;*.blg&quot;,</span><br><span class="line">        &quot;*.idx&quot;,</span><br><span class="line">        &quot;*.ind&quot;,</span><br><span class="line">        &quot;*.lof&quot;,</span><br><span class="line">        &quot;*.lot&quot;,</span><br><span class="line">        &quot;*.out&quot;,</span><br><span class="line">        &quot;*.toc&quot;,</span><br><span class="line">        &quot;*.acn&quot;,</span><br><span class="line">        &quot;*.acr&quot;,</span><br><span class="line">        &quot;*.alg&quot;,</span><br><span class="line">        &quot;*.glg&quot;,</span><br><span class="line">        &quot;*.glo&quot;,</span><br><span class="line">        &quot;*.gls&quot;,</span><br><span class="line">        &quot;*.ist&quot;,</span><br><span class="line">        &quot;*.fls&quot;,</span><br><span class="line">        &quot;*.log&quot;,</span><br><span class="line">        &quot;*.fdb_latexmk&quot;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>这串命令则是设置编译完成后要清除掉的辅助文件类型，若无特殊需求，无需进行更改。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.autoClean.run&quot;: &quot;onFailed&quot;</span><br></pre></td></tr></table></figure><p>这条命令是设置什么时候对上文设置的辅助文件进行清除。其变量有：</p><ol><li><strong>onBuilt</strong> : 无论是否编译成功，都选择清除辅助文件；</li><li><strong>onFailed</strong> : 当编译失败时，清除辅助文件；</li><li><strong>never</strong> : 无论何时，都不清除辅助文件。</li></ol><p>由于 tex 文档编译有时需要用到辅助文件，比如编译目录和编译参考文献时，如果使用<code>onBuilt</code>命令，则会导致编译不出完整结果甚至编译失败；</p><p>而有时候将 tex 文件修改后进行编译时，可能会导致 pdf 文件没有正常更新的情况，这个时候可能就是由于辅助文件没有进行及时更新的缘故，需要清除辅助文件了，而<code>never</code>命令做不到这一点；</p><p>故而笔者使用了<code>onFailed</code>，同时解决了上述两个问题。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;</span><br></pre></td></tr></table></figure><p>该命令的作用为设置 vscode 编译 tex 文档时的默认编译链。有两个变量：</p><ol><li><strong>first</strong> : 使用<code>latex-workshop.latex.recipes</code>中的第一条编译链，故而您可以根据自己的需要更改编译链顺序；</li><li><strong>lastUsed</strong> : 使用最近一次编译所用的编译链。</li></ol><p>笔者选择使用<strong>lastUsed</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;</span><br></pre></td></tr></table></figure><p>用于反向同步（即从编译出的 pdf 文件指定位置跳转到 tex 文件中相应代码所在位置）的内部查看器的快捷键绑定。变量有两种：</p><ol><li><strong>ctrl-click</strong> ： 为默认选项，使用Ctrl&#x2F;cmd+鼠标左键单击</li><li><strong>double-click</strong> : 使用鼠标左键双击</li></ol><p>此处笔者使用的为<strong>double-click</strong>。</p><h2 id="7-tex文件编译"><a href="#7-tex文件编译" class="headerlink" title="7 tex文件编译"></a>7 tex文件编译</h2><h3 id="7-1-tex测试文件下载"><a href="#7-1-tex测试文件下载" class="headerlink" title="7.1 tex测试文件下载"></a>7.1 tex测试文件下载</h3><p>为了测试 vscode 功能是否比较完整，笔者编写了一份简单的 tex 文件，以此测试其是否支持中英文，能否编译目录，能否插入图片，能否进行引用，能否编译参考文献（编译bixtex文件）等功能。</p><p>测试所用的 tex 文件可以从 github 下载：<a href="https://github.com/Ali-loner/Ali-loner.github.io">Click here to download the LaTeX testfile for vscode</a></p><p><strong>下载步骤</strong>如图：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6aca9b730b7c4ee2946a88fbe6ac40ad.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6aca9b730b7c4ee2946a88fbe6ac40ad.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="测试文件下载"><br>将之下载后，进行解压。</p><p>注 10 ： 若因网络原因无法连接到github导致无法下载，可以使用自己的tex文件进行测试，或者复制以下代码进行文档的简单编译测试，但其只能测试一部分功能：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[a4paper]&#123;article&#125;</span><br><span class="line">\usepackage[margin=1in]&#123;geometry&#125; % 设置边距，符合Word设定</span><br><span class="line">\usepackage&#123;ctex&#125;</span><br><span class="line">\usepackage&#123;lipsum&#125;</span><br><span class="line">\title&#123;\heiti\zihao&#123;2&#125; This is a test for vscode&#125;</span><br><span class="line">\author&#123;\songti Ali-loner&#125;</span><br><span class="line">\date&#123;2020.08.02&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">\maketitle</span><br><span class="line">\begin&#123;abstract&#125;</span><br><span class="line">\lipsum[2]</span><br><span class="line">\end&#123;abstract&#125;</span><br><span class="line">\tableofcontents</span><br><span class="line">\section&#123;This is a section&#125;</span><br><span class="line">Hello world! 你好，世界 ！</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><h3 id="7-2-tex-测试文件编译"><a href="#7-2-tex-测试文件编译" class="headerlink" title="7.2 tex 测试文件编译"></a>7.2 tex 测试文件编译</h3><p><strong>① 打开测试文件所在文件夹</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/927b15521ea5f9c20d3ba6c426e098ba.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/927b15521ea5f9c20d3ba6c426e098ba.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="打开文件夹"></p><p><strong>② 点击选中 tex 文件</strong>，进行文件内容查看</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6a28c240c45d77c6972a7e2fb6cd62a8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6a28c240c45d77c6972a7e2fb6cd62a8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="tex文件查看"></p><p><strong>③ 开始编译文件。</strong> 由于进行测试的文件中涉及参考文献的引用（<strong>.bib</strong>的编译），故而选择<code>xelatex -&gt; bibtex -&gt; xelatex*2</code>编译链。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/306b76bae450c39ddd5a7cea4074c84a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/306b76bae450c39ddd5a7cea4074c84a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="开始编译"></p><p>注 11 ： 为了更方便进行编译，可对其设置快捷键，设置快捷键步骤如下：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/24c26cd24b87c3f1e59568b8b8e8bb00.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/24c26cd24b87c3f1e59568b8b8e8bb00.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="快捷键绑定"></p><p>笔者将快捷键设置为Ctrl+Alt+R。</p><p><strong>选中tex文件的代码页面</strong>（若未选中，则无法进行编译），然后按下该快捷键，在编辑器页面上端进行编译链选择，如下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cbaed92ca564f9dbbb4204e0d9c2fee7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cbaed92ca564f9dbbb4204e0d9c2fee7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="快捷键编译"></p><p><strong>④ 编译成功</strong></p><p>当发现页面下方出现 <strong>√</strong> 符号时，说明编译成功，相反，如果出现 <strong>×</strong> 符号，说明编译失败，就要找失败原因了。</p><p><strong>a.</strong> 左侧工具栏</p><p>当编译成功后，选中 tex 文件中任意的代码，以此来选中 tex 文件，然后进行图示操作。其中侧边栏所展现的就是上文提及的新的 LaTeX 标记。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/04f02e79054c25953a761251a72e8682.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/04f02e79054c25953a761251a72e8682.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="pdf查看"></p><p><strong>b.</strong> 快捷键</p><p>选中 tex 文件中任意的代码，然后按Ctrl+Alt+V，出现编译好的 pdf 页面。该快捷键为默认设置。若您想要更改，可以根据上文进行配置。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/518090c31940a4a61b55f75a31f7c923.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/518090c31940a4a61b55f75a31f7c923.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="编译成功"></p><p>注意到，现在编译的结果为内部查看器查看。</p><p><strong>⑤ 正向同步测试</strong>，即从代码定位到 pdf 页面相应位置。有以下三种方法：</p><p><strong>a.</strong> 使用侧边工具栏</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/20bc13379b5e1d69eecb53c98175e896.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/20bc13379b5e1d69eecb53c98175e896.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="正向同步1"></p><p><strong>b.</strong> 使用右键菜单</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ac759bac5a92854811260f5ea5f56f0f.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ac759bac5a92854811260f5ea5f56f0f.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="正向同步2"><br><strong>c.</strong> 使用快捷键</p><p>选中需要跳转的代码所在行，按Ctrl+Alt+J，右侧就会跳转到相应行。这里的快捷键为默认设置，可自行通过上文方式设置为您想要的快捷键。</p><p><strong>⑥ 反向同步测试</strong>,即从 pdf 页面定位到代码相应位置</p><p>在编译生成的 pdf 上，选中想要跳转行，鼠标左键双击或ctrl+鼠标左键单击，跳转到对应代码。此处快捷键的选择为上文设置，若使用笔者的代码，则为鼠标左键双击。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bc30efe92862befcd35000da3b3eb93.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bc30efe92862befcd35000da3b3eb93.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="反向同步"></p><h2 id="8-SumatraPDF-安装设置（可选）"><a href="#8-SumatraPDF-安装设置（可选）" class="headerlink" title="8 SumatraPDF 安装设置（可选）"></a>8 SumatraPDF 安装设置（可选）</h2><p>您可自行选择是否需要设置此部分内容。</p><p>有的时候，由于想要看到 pdf 文件的完整展现效果，使用内置查看器已无法满足需求，这时可以使用外部查看器进行查看。</p><p>外部查看器的优势是能够看到 pdf 文件在查看器中的目录，可以实时进行跳转；且根据笔者使用来看，外部查看器展示出来的 pdf 默认会放大一些，使得字体变大，要更加让人舒服一些。</p><p>笔者选择 <strong>SumatraPDF</strong> 作为外部查看器，该软件的优点在于在具有 pdf 阅读功能的同时很轻量，安装包不到 10MB 大小，且支持双向同步功能。通过调整其与 vscode 的窗口位置，能够在拥有这些优势的同时，达到与内置 pdf 查看具有相同的效果。</p><h3 id="8-1-SumatraPDF下载与安装"><a href="#8-1-SumatraPDF下载与安装" class="headerlink" title="8.1 SumatraPDF下载与安装"></a>8.1 SumatraPDF下载与安装</h3><p>官网下载：<a href="https://www.sumatrapdfreader.org/download-free-pdf-viewer.html">Click here to download SumatraPDF</a></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/46ef16263fbbb2396ceb7029c1963a32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/46ef16263fbbb2396ceb7029c1963a32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="sumatraPDF"></p><p>其安装很简单，与通用软件安装过程一致，记得更改安装路径并记住，下文配置需要使用其路径。</p><h3 id="8-2-使用SumatraPDF查看的代码配置"><a href="#8-2-使用SumatraPDF查看的代码配置" class="headerlink" title="8.2 使用SumatraPDF查看的代码配置"></a>8.2 使用SumatraPDF查看的代码配置</h3><h4 id="8-2-1-代码展示"><a href="#8-2-1-代码展示" class="headerlink" title="8.2.1 代码展示"></a>8.2.1 代码展示</h4><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;,</span><br><span class="line">    &quot;latex-workshop.view.pdf.ref.viewer&quot;:&quot;auto&quot;,</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.args&quot;: [</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.args&quot;: [</span><br><span class="line">        &quot;-forward-search&quot;,</span><br><span class="line">        &quot;%TEX%&quot;,</span><br><span class="line">        &quot;%LINE%&quot;,</span><br><span class="line">        &quot;-reuse-instance&quot;,</span><br><span class="line">        &quot;-inverse-search&quot;,</span><br><span class="line">        &quot;code \&quot;F:/Microsoft VS Code/resources/app/out/cli.js\&quot; -r -g \&quot;%f:%l\&quot;&quot;, // 注意修改路径</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此代码仅为展示所用，让您进行查看，为下文解读之用。如需写入到 json 文件内，可直接完整复制文末笔者的个人配置到自己的编译器内。</p><h4 id="8-2-2-代码解读"><a href="#8-2-2-代码解读" class="headerlink" title="8.2.2 代码解读"></a>8.2.2 代码解读</h4><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;</span><br></pre></td></tr></table></figure><p>设置默认的pdf查看器，有三种变量参数：</p><ol><li><strong>tab</strong> : 使用 vscode 内置 pdf 查看器；</li><li><strong>browser</strong> : 使用电脑默认浏览器进行 pdf 查看；</li><li><strong>external</strong> : 使用外部 pdf 查看器查看。</li></ol><p>此处选择 <strong>external</strong> 参数，使用外部查看器。</p><p>注 12 ： 此参数为下文进行pdf内部查看和外部查看进行切换的关键参数。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.ref.viewer&quot;:&quot;auto&quot;</span><br></pre></td></tr></table></figure><p>设置PDF查看器用于在 <strong>\ref</strong> 命令上的[View on PDF]链接，此命令作用于 <strong>\ref</strong> 引用查看。有三个参数变量：</p><ol><li><strong>auto</strong> : 由编辑器根据情况自动设置；</li><li><strong>tabOrBrowser</strong> : 使用vscode内置pdf查看器或使用电脑默认浏览器进行pdf查看；</li><li><strong>external</strong> : 使用外部pdf查看器查看。</li></ol><p>此处设置为<strong>auto</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.viewer.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;// 注意修改路径</span><br></pre></td></tr></table></figure><p>使用外部查看器时要执行的命令，设置外部查看器启动文件<strong>SumatraPDF.exe</strong>文件所在位置，此处需要您根据自身情况进行路径更改，正常情况下只需更改磁盘盘符即可。</p><p><strong>请注意</strong>中间为 “ &#x2F; “ 而不是” \ “ ，不然会报错。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.viewer.args&quot;: [</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>此代码是设置使用外部查看器时，<code>latex-workshop.view.pdf.external.view .command</code>的参数。<code>%PDF%</code>是用于生成PDF文件的绝对路径的占位符。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.synctex.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot; // 注意修改路径</span><br></pre></td></tr></table></figure><p>此命令是将生成的辅助文件 <strong>.synctex.gz</strong> 转发到外部查看器时要执行的命令,设置其位置参数，您注意更改路径，此路径为 <strong>SumatraPDF.exe</strong> 文件路径。与上文相同。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.synctex.args&quot;: [</span><br><span class="line">        &quot;-forward-search&quot;,</span><br><span class="line">        &quot;%TEX%&quot;,</span><br><span class="line">        &quot;%LINE%&quot;,</span><br><span class="line">        &quot;-reuse-instance&quot;,</span><br><span class="line">        &quot;-inverse-search&quot;,</span><br><span class="line">        &quot;code \&quot;F:/Microsoft VS Code/resources/app/out/cli.js\&quot; -r -g \&quot;%f:%l\&quot;&quot;// 注意修改路径</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>设置当 <strong>.synctex.gz</strong> 文件同步到外部查看器时<code>latex-workshop.view.pdf.external.synctex</code>的参数。<code>%LINE%</code>是行号，<code>%PDF%</code>是生成PDF文件的绝对路径的占位符，<code>%TEX%</code>是当触发syncTeX被触发时，扩展名为 <strong>.tex</strong> 的 LaTeX 文件路径。</p><p>上面代码串中记得进行 <strong>Microsoft VS Code</strong> 路径修改，修改如下图:</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43de3a7d084a4b2b2eae6a259ba24f33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43de3a7d084a4b2b2eae6a259ba24f33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="路径修改"></p><h2 id="9-SumatraPDF-的使用"><a href="#9-SumatraPDF-的使用" class="headerlink" title="9 SumatraPDF 的使用"></a>9 SumatraPDF 的使用</h2><p>将完整代码复制到自己的 json 文件内后，即可使用 SumatraPDF作为自己的 pdf 外部查看器了。以下为具体操作：</p><p>① 点击编辑页面任意位置来选中 tex 文件；<br>② 按Ctrl+Alt+V，打开编译出的 pdf 文件；<br>③ 出现如下图页面。可以看到的是，原本内嵌输出的 pdf 变为了在 SumatraPDF 上查看，且侧面带有书签：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e5003691bd206012a74ecddf6c7bfa82.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e5003691bd206012a74ecddf6c7bfa82.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="sumatrapdf查看"></p><p>④ 为了出现和内嵌输出具有相同的效果，可以将 vscode 和 SumatraPDF 进行分屏，且根据需要关闭标签，如下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57974febf6f2c77c5b9bb9e22235887d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57974febf6f2c77c5b9bb9e22235887d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="分屏查看"></p><p>⑤ 且同样支持双向同步（正向同步和反向同步），其操作步骤与内嵌输出 pdf 时操作步骤相同，此处就不再赘述。查看效果图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6118b9cb1ea3e22d80a5a04edd5fb6a7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6118b9cb1ea3e22d80a5a04edd5fb6a7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="双向同步"></p><h2 id="10-pdf-内部查看与外部查看的切换"><a href="#10-pdf-内部查看与外部查看的切换" class="headerlink" title="10 pdf 内部查看与外部查看的切换"></a>10 pdf 内部查看与外部查看的切换</h2><p>以下展示由外部查看转为内部查看的操作，由内转外操作相同。</p><p>共有两种操作方式：<strong>UI界面设置</strong> 或 <strong>Json界面设置</strong> 。具体见下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8fde188378a7f0d0d15e09941768c364.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8fde188378a7f0d0d15e09941768c364.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="内外切换"></p><p>您可根据个人适应选择相应的方法。</p><h2 id="11-个人完整配置"><a href="#11-个人完整配置" class="headerlink" title="11 个人完整配置"></a>11 个人完整配置</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> //------------------------------LaTeX 配置----------------------------------</span><br><span class="line">    // 设置是否自动编译</span><br><span class="line">    &quot;latex-workshop.latex.autoBuild.run&quot;:&quot;never&quot;,</span><br><span class="line">    //右键菜单</span><br><span class="line">    &quot;latex-workshop.showContextMenu&quot;:true,</span><br><span class="line">    //从使用的包中自动补全命令和环境</span><br><span class="line">    &quot;latex-workshop.intellisense.package.enabled&quot;: true,</span><br><span class="line">    //编译出错时设置是否弹出气泡设置</span><br><span class="line">    &quot;latex-workshop.message.error.show&quot;: false,</span><br><span class="line">    &quot;latex-workshop.message.warning.show&quot;: false,</span><br><span class="line">    // 编译工具和命令</span><br><span class="line">    &quot;latex-workshop.latex.tools&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;-pdf&quot;,</span><br><span class="line">                &quot;-outdir=%OUTDIR%&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    // 用于配置编译链</span><br><span class="line">    &quot;latex-workshop.latex.recipes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;XeLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;PDFLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;BibTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;bibtex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;LaTeXmk&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;latexmk&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex -&gt; bibtex -&gt; xelatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    //文件清理。此属性必须是字符串数组</span><br><span class="line">    &quot;latex-workshop.latex.clean.fileTypes&quot;: [</span><br><span class="line">        &quot;*.aux&quot;,</span><br><span class="line">        &quot;*.bbl&quot;,</span><br><span class="line">        &quot;*.blg&quot;,</span><br><span class="line">        &quot;*.idx&quot;,</span><br><span class="line">        &quot;*.ind&quot;,</span><br><span class="line">        &quot;*.lof&quot;,</span><br><span class="line">        &quot;*.lot&quot;,</span><br><span class="line">        &quot;*.out&quot;,</span><br><span class="line">        &quot;*.toc&quot;,</span><br><span class="line">        &quot;*.acn&quot;,</span><br><span class="line">        &quot;*.acr&quot;,</span><br><span class="line">        &quot;*.alg&quot;,</span><br><span class="line">        &quot;*.glg&quot;,</span><br><span class="line">        &quot;*.glo&quot;,</span><br><span class="line">        &quot;*.gls&quot;,</span><br><span class="line">        &quot;*.ist&quot;,</span><br><span class="line">        &quot;*.fls&quot;,</span><br><span class="line">        &quot;*.log&quot;,</span><br><span class="line">        &quot;*.fdb_latexmk&quot;</span><br><span class="line">    ],</span><br><span class="line">    //设置为onFaild 在构建失败后清除辅助文件</span><br><span class="line">    &quot;latex-workshop.latex.autoClean.run&quot;: &quot;onFailed&quot;,</span><br><span class="line">    // 使用上次的recipe编译组合</span><br><span class="line">    &quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;,</span><br><span class="line">    // 用于反向同步的内部查看器的键绑定。ctrl/cmd +点击(默认)或双击</span><br><span class="line">    &quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //使用 SumatraPDF 预览编译好的PDF文件</span><br><span class="line">    // 设置VScode内部查看生成的pdf文件</span><br><span class="line">    &quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;,</span><br><span class="line">    // PDF查看器用于在\ref上的[View on PDF]链接</span><br><span class="line">    &quot;latex-workshop.view.pdf.ref.viewer&quot;:&quot;auto&quot;,</span><br><span class="line">    // 使用外部查看器时要执行的命令。此功能不受官方支持。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    // 使用外部查看器时，latex-workshop.view.pdf.external.view .command的参数。此功能不受官方支持。%PDF%是用于生成PDF文件的绝对路径的占位符。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.args&quot;: [</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ],</span><br><span class="line">    // 将synctex转发到外部查看器时要执行的命令。此功能不受官方支持。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    // latex-workshop.view.pdf.external.synctex的参数。当同步到外部查看器时。%LINE%是行号，%PDF%是生成PDF文件的绝对路径的占位符，%TEX%是触发syncTeX的扩展名为.tex的LaTeX文件路径。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.args&quot;: [</span><br><span class="line">        &quot;-forward-search&quot;,</span><br><span class="line">        &quot;%TEX%&quot;,</span><br><span class="line">        &quot;%LINE%&quot;,</span><br><span class="line">        &quot;-reuse-instance&quot;,</span><br><span class="line">        &quot;-inverse-search&quot;,</span><br><span class="line">        &quot;code \&quot;F:/Microsoft VS Code/resources/app/out/cli.js\&quot; -r -g \&quot;%f:%l\&quot;&quot;, // 注意修改路径</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>写在最后</strong> ： 笔者也只是一个初学者，文中如果出现错误的地方，欢迎您在评论区批评指正，笔者会虚心接受这些产生错误的地方，争取以后学得更扎实再编写这些文字。</p><p>另：若您感觉此文写得勉强还行，希望您能够不吝点赞，给笔者一点小小的激励，以此来进行更多更好的文字编写。非常感谢！！！</p><p>注：转载自<a href="https://zhuanlan.zhihu.com/p/166523064">https://zhuanlan.zhihu.com/p/166523064</a></p>]]></content>
      
      
      <categories>
          
          <category> YOLO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vscode </tag>
            
            <tag> latex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv8的训练自己的数据集</title>
      <link href="/post/article-1/"/>
      <url>/post/article-1/</url>
      
        <content type="html"><![CDATA[<h2 id="一、YOLOv8的简介"><a href="#一、YOLOv8的简介" class="headerlink" title="一、YOLOv8的简介"></a>一、YOLOv8的简介</h2><p>YOLO（You Only Look Once）系列算法因其高效、准确等特点而备受瞩目。由2023年Ultralytics公司发布了YOLO的<strong>最新版本YOLOv8是结合前几代YOLO的基础上的一个融合改进版</strong>。</p><p>本文YOLOv8网络结构&#x2F;环境搭建&#x2F;数据集获取&#x2F;训练&#x2F;推理&#x2F;验证&#x2F;导出&#x2F;部署，从网络结构的讲解从模型的网络结构讲解到模型的部署都有详细介绍，同时在本专栏中还包括YOLOv8模型系列的改进包<strong>括个人提出的创新点，传统卷积、注意力机制、损失函数的修改教程，能够帮助你的论文获得创新点。</strong></p><h2 id="二、YOLOv8相对于Yolov5的核心改动"><a href="#二、YOLOv8相对于Yolov5的核心改动" class="headerlink" title="二、YOLOv8相对于Yolov5的核心改动"></a>二、YOLOv8相对于Yolov5的核心改动</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/39fa749365bc4a6e87a8e63563bca5cc.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/39fa749365bc4a6e87a8e63563bca5cc.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="39fa749365bc4a6e87a8e63563bca5cc.png"></p><hr><p>从YOLOv8的网络结构可以看出,其延用了YOLOv5的网络结构思想，<strong>包括基于CSP（紧凑和分离）的骨干网络(backbone)和Neck部分的设计，以及对于不同尺度模型的考虑。</strong></p><p><strong>改进总结：</strong></p><blockquote><ol><li><p>Backbone的改进：使用C2f模块代替C3模块，进一步轻量化，同时保持了CSP的思想，同时采用了SPPF模块。</p></li><li><p>PAN-FPN的改进：保留了PAN的思想，但删除了上采样阶段中的卷积结构，同时将C3模块替换为C2f模块。</p></li><li><p>Decoupled-Head的引入：采用了Decoupled-Head的思想，使得网络的训练和推理更加高效。</p></li><li><p>Anchor-Free的思想：抛弃了Anchor-Base，采用了Anchor-Free的思想。</p></li><li><p>损失函数的改进：采用VFL Loss作为分类损失，同时使用DFL Loss和CIOU Loss作为回归损失。</p></li><li><p>样本匹配方式的改进：采用了Task-Aligned Assigner匹配方式。</p></li></ol></blockquote><p>这些改进使得YOLOv8在目标检测方面具有更高的精度和更快的速度，同时保持了轻量化的特点。</p><p><strong>具体来说</strong>，YOLOv8的Backbone部分使用了C2f模块来替代了YOLOv5中的C3模块，实现了进一步的轻量化。同时，它也保留了YOLOv5等架构中使用的SPPF（空间金字塔池化）模块。</p><p>在PAN-FPN（路径聚合网络-特征金字塔网络）部分，虽然YOLOv8依旧采用了PAN的思想，但是在结构上，它删除了YOLOv5中PAN-FPN上采样阶段中的卷积结构，并将C3模块替换为了C2f模块。</p><p>这些改进使得YOLOv8在保持了YOLOv5网络结构的优点的同时，进行了更加精细的调整和优化，提高了模型在不同场景下的性能。</p><h2 id="三、YOLOv8的网络结构"><a href="#三、YOLOv8的网络结构" class="headerlink" title="三、YOLOv8的网络结构"></a><strong>三、YOLOv8的网络结构</strong></h2><p>YOLOv8的网络结构主要由以下三个大部分组成：</p><blockquote><ol><li><p>Backbone：它采用了一系列卷积和反卷积层来提取特征，同时也使用了残差连接和瓶颈结构来减小网络的大小和提高性能。该部分采用了C2f模块作为基本构成单元，与YOLOv5的C3模块相比，C2f模块具有更少的参数量和更优秀的特征提取能力。</p></li><li><p>Neck：它采用了多尺度特征融合技术，将来自Backbone的不同阶段的特征图进行融合，以增强特征表示能力。具体来说，YOLOv8的Neck部分包括一个SPPF模块、一个PAA模块和两个PAN模块。</p></li><li><p>Head：它负责最终的目标检测和分类任务，包括一个检测头和一个分类头。检测头包含一系列卷积层和反卷积层，用于生成检测结果；分类头则采用全局平均池化来对每个特征图进行分类。</p></li></ol></blockquote><p>下面我们来针对于YOLOv8的三个组成部分进行详细讲解。</p><h4 id="3-1-Backbone"><a href="#3-1-Backbone" class="headerlink" title="3.1 Backbone"></a>3.1 Backbone</h4><p>由最上面的YOLOv8网络结构图我们可以看出在其中的Backbone部分，由5个卷积模块和4个C2f模块和一个SPPF模块组成，</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a744491cd5a14bcf9b33015b18c6c6c8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a744491cd5a14bcf9b33015b18c6c6c8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="a744491cd5a14bcf9b33015b18c6c6c8.png"></p><p>(其中浅蓝色为卷积模块,黄色为C2f模块深蓝色为SPPF模块 )</p><p>如果上图看的不够直观,我们来看一下YOLOv8的文件中的yaml文件,看一下它backbone部分的结构组成部分，会更加直观。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">backbone:</span><br><span class="line">  <span class="comment"># [from, repeats, module, args]</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">64</span>, <span class="number">3</span>, <span class="number">2</span>]]  <span class="comment"># 0-P1/2</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">128</span>, <span class="number">3</span>, <span class="number">2</span>]]  <span class="comment"># 1-P2/4</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">128</span>, <span class="literal">True</span>]]</span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">256</span>, <span class="number">3</span>, <span class="number">2</span>]]  <span class="comment"># 3-P3/8</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">6</span>, C2f, [<span class="number">256</span>, <span class="literal">True</span>]]</span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">512</span>, <span class="number">3</span>, <span class="number">2</span>]]  <span class="comment"># 5-P4/16</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">6</span>, C2f, [<span class="number">512</span>, <span class="literal">True</span>]]</span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">1024</span>, <span class="number">3</span>, <span class="number">2</span>]]  <span class="comment"># 7-P5/32</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">1024</span>, <span class="literal">True</span>]]</span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, SPPF, [<span class="number">1024</span>, <span class="number">5</span>]]  <span class="comment"># 9</span></span><br></pre></td></tr></table></figure><p>上面的部分就是YOLOv8的yaml文件的Backbone部分，可以看到其由5个Conv模块，四个C2f模块以及一个SPPF模块组成，<strong>下面我们来根据每个模块的组成来进行讲解。</strong></p><h5 id="3-1-1-卷积模块-Conv"><a href="#3-1-1-卷积模块-Conv" class="headerlink" title="3.1.1 卷积模块(Conv)"></a>3.1.1 卷积模块(Conv)</h5><p>在其中卷积模块的结构主要为下图</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/56694f12be0d4664905561c9438e2850.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/56694f12be0d4664905561c9438e2850.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="56694f12be0d4664905561c9438e2850.png"></p><p>在其中主要结构为一个2D的卷积一个BatchNorm2d和一个SiLU激活函数，整个<strong>卷积模块</strong>的作用为：</p><ol><li><strong>降采样：每个卷积模块中的卷积层都采用步长为2的卷积核进行降采样操作，以减小特征图的尺寸并增加通道数。</strong></li><li><strong>非线性表示：每个卷积层之后都添加了Batch Normalization（批标准化）层和ReLU激活函数，以增强模型的非线性表示能力。</strong></li></ol><blockquote><p>在其中Batch Normalization（批标准化）是深度学习中常用的一种技术，用于加速神经网络的训练。Batch Normalization通过对每个小批量数据进行标准化，使得神经网络在训练过程中更加稳定，可以使用更高的学习率，并且减少了对初始化权重的依赖。Batch Normalization的基本思想是：对每个小批量数据进行标准化，使得每个特征的均值为0，方差为1，然后再通过一个可学习的缩放因子和平移因子来调整数据的分布，从而使得神经网络更容易训练。</p></blockquote><h5 id="3-1-2-C2f模块"><a href="#3-1-2-C2f模块" class="headerlink" title="3.1.2 C2f模块"></a><strong>3.1.2 C2f模块</strong></h5><p>在YOLOv8的网络结构中C2f模块算是YOLOv8的一个较大的改变，与YOLOv5的C3模块相比，C2f模块具有更少的参数量和更优秀的特征提取能力。<strong>下图为C2f的内部网络结构图。</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3fa4855f3d38447b93e5faf40cd59169.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3fa4855f3d38447b93e5faf40cd59169.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="3fa4855f3d38447b93e5faf40cd59169.png"></p><p>在C2f模块中我们可以看到输入首先经过<strong>一个k&#x3D;1，s&#x3D;1，p&#x3D;0，c&#x3D;c_out</strong>的卷积模块进行了处理，然后经过一个split处理**(在这里split和后面的concat的组成其实就是所谓的残差模块处理)**经过数量为n的DarknetBottleneck模块处理以后将残差模块和主干模块的结果进行Concat拼接在经过一个卷积模块处理进行输出。 </p><blockquote><p>在其中提到的残差连接（residual connections）是一种用于构建深层神经网络的技术。它的核心思想是通过跳过层级连接来传递残差或误差。</p><p>在传统的神经网络中，信息流通过一层层的网络层，每一层都通过非线性激活函数进行转换和提取特征。然而，随着神经网络的加深，可能会出现”梯度消失”或”梯度爆炸”的问题，导致网络收敛困难或性能下降。</p><p>残差连接通过引入跨层级的连接，将输入的原始信息直接传递到后续层级，以解决梯度消失和爆炸问题。具体而言，它将网络的输入与中间层的输出相加，形成了一个”捷径”或”跳跃连接”，从而允许梯度更容易地传播。</p><p>数学上，假设我们有一个输入x，通过多个网络层进行处理后得到预测值H(x)。那么残差连接的表达式为：</p><p>F(x) &#x3D; H(x) + x</p><p>其中，F(x)为残差块的输出，H(x)为经过一系列网络层处理后得到的特征表示，x为输入直接连接到残差块中的跳跃连接。</p><p>通过残差连接，网络可以更容易地学习残差或误差，从而使网络更深层次的特征表达更准确。这对于训练深层神经网络非常有用，可以提高网络的性能和收敛速度。</p></blockquote><blockquote><p> 在C2f模块中用到的DarknetBottleneck模块其中使用多个3x3卷积核进行卷积操作，提取特征信息。同时其具有add是否进行残差链接的选项。</p></blockquote><p> <strong>其实整个C2f模块就是一个改良版本的Darknet</strong></p><ol><li><p>首先，使用1x1卷积核将输入通道数减少到原来的1&#x2F;2，以减少计算量和内存消耗。</p></li><li><p>然后，使用多个3x3卷积核进行卷积操作，提取特征信息。</p></li><li><p>接着，使用残差链接，将输入直接加到输出中，从而形成了一条跨层连接。</p></li><li><p>接着，再次使用1x1卷积核恢复特征图的通道数。</p></li></ol><p>SPPF模块 </p><p>YOLOv8的SPPF模块相对于YOLOv5的SPPF模块并没有任何的改变。</p><h4 id="3-2-Neck"><a href="#3-2-Neck" class="headerlink" title="3.2 Neck"></a>3.2 Neck</h4><p>YOLOv8的Neck部分是该模型中的一个关键组件，<strong>它在特征提取和融合方面起着重要作用</strong>。Neck的详细描述如下：</p><p>Neck部分主要起到一个特征融合的操作, YOLOv8的Neck部分依然采用PAN-FPN的思想，下图的a，b，c为一个Neck部分的流程示意图。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a3aeff6d8f0542a79efbd5e95c0b10b9.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a3aeff6d8f0542a79efbd5e95c0b10b9.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="a3aeff6d8f0542a79efbd5e95c0b10b9.png"></p><blockquote><p>整个Neck部分的步骤如下：：将特征提取网络(Backbone)的输出P3，P4，P5输入进PAN-FPN网络结构，使得多个尺度的特征图进行融合；将P5经过上采样与P4进行融合得到F1，将F1经过C2f层和一次上采样与P3进行融合得到T1，将T1经过一次卷积层与F1经过融合得到F2，将F2经过一次C2f层得到T2，将T2经过一次卷积层与P5融合得到F3，将F3经过一次C2f层得到T3，最终得到T1、T2、T3就是整个Neck的产物； </p></blockquote><p>上述过程可以描述为下图，我在图片上做了一些标准方便理解。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4b480765acd947879588f6d132704eb8.jpeg" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4b480765acd947879588f6d132704eb8.jpeg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="4b480765acd947879588f6d132704eb8.jpeg"></p><p>上述的过程可以在代码部分看到,我们同样看YOLOv8的yaml文件，能够更直观的看到这个步骤,大家可以看代码同时对应图片来进行分析:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">head:</span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, nn.Upsample, [<span class="literal">None</span>, <span class="number">2</span>, <span class="string">&#x27;nearest&#x27;</span>]]</span><br><span class="line">  - [[-<span class="number">1</span>, <span class="number">6</span>], <span class="number">1</span>, Concat, [<span class="number">1</span>]]  <span class="comment"># cat backbone P4</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">512</span>]]  <span class="comment"># 12</span></span><br><span class="line"> </span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, nn.Upsample, [<span class="literal">None</span>, <span class="number">2</span>, <span class="string">&#x27;nearest&#x27;</span>]]</span><br><span class="line">  - [[-<span class="number">1</span>, <span class="number">4</span>], <span class="number">1</span>, Concat, [<span class="number">1</span>]]  <span class="comment"># cat backbone P3</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">256</span>]]  <span class="comment"># 15 (P3/8-small)</span></span><br><span class="line"> </span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">256</span>, <span class="number">3</span>, <span class="number">2</span>]]</span><br><span class="line">  - [[-<span class="number">1</span>, <span class="number">12</span>], <span class="number">1</span>, Concat, [<span class="number">1</span>]]  <span class="comment"># cat head P4</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">512</span>]]  <span class="comment"># 18 (P4/16-medium)</span></span><br><span class="line"> </span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">512</span>, <span class="number">3</span>, <span class="number">2</span>]]</span><br><span class="line">  - [[-<span class="number">1</span>, <span class="number">9</span>], <span class="number">1</span>, Concat, [<span class="number">1</span>]]  <span class="comment"># cat head P5</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">1024</span>]]  <span class="comment"># 21 (P5/32-large)</span></span><br></pre></td></tr></table></figure><p><strong>Neck部分的整体功能的详细分析如下:</strong></p><p>1. Neck的作用：<br>Neck部分在YOLOv8模型中<strong>负责对来自Backbone的特征进行进一步处理和融合</strong>，以提高目标检测的准确性和鲁棒性。它通过引入不同的结构和技术，将多尺度的特征图进行融合，以便更好地捕捉不同尺度目标的信息。</p><p>2. 特征金字塔网络（Feature Pyramid Network, FPN）：<br>YOLOv8的Neck部分通常采用特征金字塔网络结构，用于处理来自Backbone的多尺度特征图。<strong>FPN通过在不同层级上建立特征金字塔</strong>，使得模型能够在不同尺度上进行目标检测。它通过上采样和下采样操作，将低层级的细节特征与高层级的语义特征进行融合，以获取更全面和丰富的特征表示。</p><p>3. 特征融合（Feature Fusion）：<br>Neck部分还包括特征融合的操作，<strong>用于将来自不同层级的特征进行融合</strong>。这种特征融合有助于提高模型对目标的检测准确性，尤其是对于不同尺度的目标。</p><p>4. 上采样和下采样：<br>Neck部分通常会使用上采样和下采样操作，以调整特征图的尺度和分辨率。上采样操作可以将低分辨率的特征图放大到与高分辨率特征图相同的尺寸，<strong>以保留更多的细节信息</strong>。而下采样操作则可以将高分辨率的特征图降低尺寸，<strong>以减少计算量和内存消耗</strong>。</p><p>YOLOv8的Neck部分通过特征金字塔网络和特征融合等操作，<strong>有效地提取和融合多尺度的特征</strong>，从而提高了目标检测的性能和鲁棒性。这使得模型能够更好地适应不同尺度和大小的目标，并在复杂场景下取得更准确的检测结果。</p><blockquote><p>PAN-FPN（具有特征金字塔网络的路径聚合网络）是一种用于计算机视觉中对象检测的神经网络架构。它将特征金字塔网络（FPN）与路径聚合网络（PAN）相结合，以提高目标检测的准确性和效率。</p><p>FPN 用于从不同比例的图像中提取特征，而 PAN 用于跨网络的不同层聚合这些特征。这允许网络检测不同大小和分辨率的对象，并处理具有多个对象的复杂场景。</p></blockquote><h4 id="3-3-Head"><a href="#3-3-Head" class="headerlink" title="3.3 Head"></a>3.3 Head</h4><p>如果Backbone和Neck部分可以理解为准备工作，那么Head部分就是收获的部分，经过前面的准备工作我们得到了Neck部分的输出T1、T2、T3分别代表不同层级的特征图，<strong>Head部分就是对这三个特征图进行处理以产生模型的的输出结果的一个过程。</strong></p><p>YOLOv8的Head部分我们先来看一下它的网络结构。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bc05b2293026433985d4152e8a116634.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bc05b2293026433985d4152e8a116634.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="bc05b2293026433985d4152e8a116634.png"></p><p>可以看到在YOLOv8的Head部分，体现了最核心的改动——&gt;解耦头(Decoupled-Head)，顾名思义就是将原先的一个检测头分解成两个部分。</p><p>在Head部分的三个解耦头分别对应着Neck部分的特征图输出T1、T2、T3。、</p><p><strong>解耦头的工作流程是：</strong></p><blockquote><p>将网络得到的特征图T1，T2，T3分别输入解耦头头进行预测，检测头的结构如下图所示其中包含4个3×3卷积与2个1×1卷积，同时在检测头的回归分支中添加WIOU损失函数如图4所示，回归头部需要计算预测框与真实框之间的位置偏移量，然后将偏移量送入回归头部进行损失计算，然后输出一个四维向量，分别表示目标框的左上角坐标x、y和右下角坐标x、y。分类头部针对于每个Anchor Free提取的候选框对其进行RoI Pooling和卷积操作得到一个分类器输出张量每个位置上的值表示该候选框属于每个类别的概率，在最后通过极大值抑制方式筛选出最终的检测结果 </p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ddf8b2f464a348868513ba3488ece02b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ddf8b2f464a348868513ba3488ece02b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="ddf8b2f464a348868513ba3488ece02b.png"></p><p>我们再从YOLOv8的yaml文件来看Head部分的作用</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/16099ed50b934e5ba7100ab8a381c1cd.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/16099ed50b934e5ba7100ab8a381c1cd.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="16099ed50b934e5ba7100ab8a381c1cd.png"></p><p><strong>可以看到检测头部分的输出为15,18，21分别对应着Neck部分的三个输出特征图。</strong> </p><p>到此YOLOv8的网络结构部分讲解就已经完成，下面我们来看如何利用YOLOv8进行训练操作。  </p><hr><h2 id="四、环境搭建"><a href="#四、环境搭建" class="headerlink" title="四、环境搭建"></a>四、环境搭建</h2><p>在我们配置好环境之后，在之后模型获取完成之后，我们可以进行配置的安装我们可以在命令行下输入如下命令进行环境的配置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>输入如上命令之后我们就可以看到命令行在安装模型所需的库了。 </p><h2 id="五、数据集获取"><a href="#五、数据集获取" class="headerlink" title="五、数据集获取"></a>五、数据集获取</h2><p>我在上面随便下载了一个 数据集用它导出yolov8的数据集，以及自动给转换成txt的格式yaml文件也已经配置好了，我们直接用就可以。 </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8673527d34eb42348770158c69de678f.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8673527d34eb42348770158c69de678f.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="8673527d34eb42348770158c69de678f.png"></p><hr><h2 id="六、模型获取"><a href="#六、模型获取" class="headerlink" title="六、模型获取"></a>六、模型获取</h2><p>到这里假设你已经搭建好了环境和有了数据集，那么我们就可以进行模型的下载，因为yolov8目前还存在BUG并不稳定随时都有可能进行更新，所以不推荐大家通过其它的途径下载，最好通过下面的方式进行下载。</p><p>我们可以直接在终端命令下</p><p><strong>(PS：这里需要注意的是我们需要在你总项目文件目录下输入这个命令，因为他会下载到当前目录下)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install ultralytics</span><br></pre></td></tr></table></figure><p> 如果大家去github上直接下载zip文件到本地可能会遇到报错如下，识别不了yolo命令，所以推荐大家用这种方式下载，</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c89d06161cd149c0ac0488e90188bcfc.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c89d06161cd149c0ac0488e90188bcfc.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="c89d06161cd149c0ac0488e90188bcfc.png"></p><hr><h2 id="七、模型训练"><a href="#七、模型训练" class="headerlink" title="七、模型训练"></a>七、模型训练</h2><p>我们来看一下主要的ultralytics目录结构，</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a900c1d0d16f45e2b8c3829aec6c2499.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a900c1d0d16f45e2b8c3829aec6c2499.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="a900c1d0d16f45e2b8c3829aec6c2499.png"></p><p> 我门打开cfg目录下的default.yaml文件可以配置模型的参数，</p><p>在其中和模型训练有关的参数及其解释如下:</p><table><thead><tr><th></th><th>参数名</th><th>输入类型</th><th>参数解释</th></tr></thead><tbody><tr><td>0</td><td>task</td><td>str</td><td>YOLO模型的任务选择，选择你是要进行检测、分类等操作</td></tr><tr><td>1</td><td>mode</td><td>str</td><td>YOLO模式的选择，选择要进行训练、推理、输出、验证等操作</td></tr><tr><td>2</td><td>model</td><td>str&#x2F;optional</td><td>模型的文件，可以是官方的预训练模型，也可以是训练自己模型的yaml文件</td></tr><tr><td>3</td><td>data</td><td>str&#x2F;optional</td><td>模型的地址，可以是文件的地址，也可以是配置好地址的yaml文件</td></tr><tr><td>4</td><td>epochs</td><td>int</td><td>训练的轮次，将你的数据输入到模型里进行训练的次数</td></tr><tr><td>5</td><td>patience</td><td>int</td><td>早停机制，当你的模型精度没有改进了就提前停止训练</td></tr><tr><td>6</td><td>batch</td><td>int</td><td>我们输入的数据集会分解为多个子集，一次向模型里输入多少个子集</td></tr><tr><td>7</td><td>imgsz</td><td>int&#x2F;list</td><td>输入的图片的大小，可以是整数就代表图片尺寸为int*int，或者list分别代表宽和高[w，h]</td></tr><tr><td>8</td><td>save</td><td>bool</td><td>是否保存模型以及预测结果</td></tr><tr><td>9</td><td>save_period</td><td>int</td><td>在训练过程中多少次保存一次模型文件,就是生成的pt文件</td></tr><tr><td>10</td><td>cache</td><td>bool</td><td>参数cache用于控制是否启用缓存机制。</td></tr><tr><td>11</td><td>device</td><td>int&#x2F;str&#x2F;list&#x2F;optional</td><td>GPU设备的选择：cuda device&#x3D;0 or device&#x3D;0,1,2,3 or device&#x3D;cpu</td></tr><tr><td>12</td><td>workers</td><td>int</td><td>工作的线程，Windows系统一定要设置为0否则很可能会引起线程报错</td></tr><tr><td>13</td><td>name</td><td>str&#x2F;optional</td><td>模型保存的名字，结果会保存到’project&#x2F;name’ 目录下</td></tr><tr><td>14</td><td>exist_ok</td><td>bool</td><td>如果模型存在的时候是否进行覆盖操作</td></tr><tr><td>15</td><td>prepetrained</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr></tbody></table><h3 id="7-1-训练的三种方式"><a href="#7-1-训练的三种方式" class="headerlink" title="7.1 训练的三种方式"></a>7.1 训练的三种方式</h3><h4 id="7-1-1-方式一"><a href="#7-1-1-方式一" class="headerlink" title="7.1.1 方式一"></a>7.1.1 方式一</h4><p>我们可以通过命令直接进行训练在其中指定参数，但是这样的方式，我们每个参数都要在其中打出来。命令如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo task=detect mode=train model=yolov8n.pt data=data.yaml batch=<span class="number">16</span> epochs=<span class="number">100</span> imgsz=<span class="number">640</span> workers=<span class="number">0</span> device=<span class="number">0</span></span><br></pre></td></tr></table></figure><p>需要注意的是如果你是Windows系统的电脑其中的Workers最好设置成0否则容易报线程的错误。</p><h4 id="7-1-2-方式二（推荐）"><a href="#7-1-2-方式二（推荐）" class="headerlink" title="7.1.2 方式二（推荐）"></a><strong>7.1.2 方式二（推荐）</strong></h4><p>通过指定cfg直接进行训练，我们配置好ultralytics&#x2F;cfg&#x2F;default.yaml这个文件之后，可以直接执行这个文件进行训练，这样就不用在命令行输入其它的参数了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo cfg=ultralytics/cfg/default.yaml</span><br></pre></td></tr></table></figure><h4 id="7-1-3-方式三"><a href="#7-1-3-方式三" class="headerlink" title="7.1.3 方式三"></a><strong>7.1.3 方式三</strong></h4><p> 我们可以通过创建py文件来进行训练，这样的好处就是不用在终端上打命令，这也能省去一些工作量，我们在根目录下创建一个名字为run.py的文件，在其中输入代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO model = YOLO(<span class="string">&quot;权重的地址&quot;</span>) data = <span class="string">&quot;文件的地址&quot;</span> model.train(data=data, epochs=<span class="number">100</span>, batch=<span class="number">16</span>)</span><br></pre></td></tr></table></figure><p> 无论通过上述的哪一种方式在控制台输出如下图片的内容就代表着开始训练成功了！</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4f1fbc25c60f44bd980ee215b5866d12.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4f1fbc25c60f44bd980ee215b5866d12.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="4f1fbc25c60f44bd980ee215b5866d12.png"></p><hr><h2 id="八、模型验证-测试"><a href="#八、模型验证-测试" class="headerlink" title="八、模型验证&#x2F;测试 "></a>八、模型验证&#x2F;测试 </h2><table><thead><tr><th></th><th>参数名</th><th>类型</th><th>参数讲解</th></tr></thead><tbody><tr><td>1</td><td>val</td><td>bool</td><td>用于控制是否在训练过程中进行验证&#x2F;测试。</td></tr><tr><td>2</td><td>split</td><td>str</td><td>用于指定用于验证&#x2F;测试的数据集划分。可以选择 ‘val’、’test’ 或 ‘train’ 中的一个作为验证&#x2F;测试数据集</td></tr><tr><td>3</td><td>save_json</td><td>bool</td><td>用于控制是否将结果保存为 JSON 文件</td></tr><tr><td>4</td><td>save_hybird</td><td>bool</td><td>用于控制是否保存标签和附加预测结果的混合版本</td></tr><tr><td>5</td><td>conf</td><td>float&#x2F;optional</td><td>用于设置检测时的目标置信度阈值</td></tr><tr><td>6</td><td>iou</td><td>float</td><td>用于设置非极大值抑制（NMS）的交并比（IoU）阈值。</td></tr><tr><td>7</td><td>max_det</td><td>int</td><td>用于设置每张图像的最大检测数。</td></tr><tr><td>8</td><td>half</td><td>bool</td><td>用于控制是否使用半精度（FP16）进行推断。</td></tr><tr><td>9</td><td>dnn</td><td>bool</td><td>，用于控制是否使用 OpenCV DNN 进行 ONNX 推断。</td></tr><tr><td>10</td><td>plots</td><td>bool</td><td>用于控制在训练&#x2F;验证过程中是否保存绘图结果。</td></tr></tbody></table><p> 验证我们划分的验证集&#x2F;测试集的情况，也就是评估我们训练出来的best.pt模型好与坏</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo task=detect mode=val model=best.pt data=data.yaml device=<span class="number">0</span></span><br></pre></td></tr></table></figure><hr><h2 id="九、模型推理"><a href="#九、模型推理" class="headerlink" title="九、模型推理"></a>九、模型推理</h2><p>我们训练好自己的模型之后，都会生成一个模型文件,保存在你设置的目录下,当我们再次想要实验该模型的效果之后就可以调用该模型进行推理了，我们也可以用官方的预训练权重来进行推理。</p><p>推理的方式和训练一样我们这里就选一种来进行举例其它的两种方式都是一样的操作只是需要改一下其中的一些参数即可:</p><p><strong>参数讲解</strong></p><table><thead><tr><th></th><th>参数名</th><th>类型</th><th>参数讲解</th></tr></thead><tbody><tr><td>0</td><td>source</td><td>str&#x2F;optinal</td><td>用于指定图像或视频的目录</td></tr><tr><td>1</td><td>show</td><td>bool</td><td>用于控制是否在可能的情况下显示结果</td></tr><tr><td>2</td><td>save_txt</td><td>bool</td><td>用于控制是否将结果保存为 <code>.txt</code> 文件</td></tr><tr><td>3</td><td>save_conf</td><td>bool</td><td>用于控制是否在保存结果时包含置信度分数</td></tr><tr><td>4</td><td>save_crop</td><td>bool</td><td>用于控制是否将带有结果的裁剪图像保存下来</td></tr><tr><td>5</td><td>show_labels</td><td>bool</td><td>用于控制在绘图结果中是否显示目标标签</td></tr><tr><td>6</td><td>show_conf</td><td>bool</td><td>用于控制在绘图结果中是否显示目标置信度分数</td></tr><tr><td>7</td><td>vid_stride</td><td>int&#x2F;optional</td><td>用于设置视频的帧率步长</td></tr><tr><td>8</td><td>stream_buffer</td><td>bool</td><td>用于控制是否缓冲所有流式帧（True）或返回最新的帧（False）</td></tr><tr><td>9</td><td>line_width</td><td>int&#x2F;list[int]&#x2F;optional</td><td>用于设置边界框的线宽度，如果缺失则自动设置</td></tr><tr><td>10</td><td>visualize</td><td>bool</td><td>用于控制是否可视化模型的特征</td></tr><tr><td>11</td><td>augment</td><td>bool</td><td>用于控制是否对预测源应用图像增强</td></tr><tr><td>12</td><td>agnostic_nms</td><td>bool</td><td>用于控制是否使用无关类别的非极大值抑制（NMS）</td></tr><tr><td>13</td><td>classes</td><td>int&#x2F;list[int]&#x2F;optional</td><td>用于按类别筛选结果</td></tr><tr><td>14</td><td>retina_masks</td><td>bool</td><td>用于控制是否使用高分辨率分割掩码</td></tr><tr><td>15</td><td>boxes</td><td>bool</td><td>用于控制是否在分割预测中显示边界框。</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo task=detect mode=predict model=best.pt source=images device=<span class="number">0</span></span><br></pre></td></tr></table></figure><p> 这里需要需要注意的是我们用模型进行推理的时候可以选择照片也可以选择一个视频的格式都可以。支持的视频格式有 </p><blockquote><ul><li><p>MP4（.mp4）：这是一种常见的视频文件格式，通常具有较高的压缩率和良好的视频质量</p></li><li><p>AVI（.avi）：这是一种较旧但仍广泛使用的视频文件格式。它通常具有较大的文件大小</p></li><li><p>MOV（.mov）：这是一种常见的视频文件格式，通常与苹果设备和QuickTime播放器相关</p></li><li><p>MKV（.mkv）：这是一种开放的多媒体容器格式，可以容纳多个视频、音频和字幕轨道</p></li><li><p>FLV（.flv）：这是一种用于在线视频传输的流式视频文件格式</p></li></ul></blockquote><hr><h2 id="十、模型输出"><a href="#十、模型输出" class="headerlink" title="十、模型输出"></a>十、模型输出</h2><p>当我们进行部署的时候可以进行文件导出，然后在进行部署。</p><p>YOLOv8支持的输出格式有如下</p><blockquote><p>1. ONNX（Open Neural Network Exchange）：ONNX 是一个开放的深度学习模型表示和转换的标准。它允许在不同的深度学习框架之间共享模型，并支持跨平台部署。导出为 ONNX 格式的模型可以在支持 ONNX 的推理引擎中进行部署和推理。</p><p>2. TensorFlow SavedModel：TensorFlow SavedModel 是 TensorFlow 框架的标准模型保存格式。它包含了模型的网络结构和参数，可以方便地在 TensorFlow 的推理环境中加载和使用。</p><p>3. PyTorch JIT（Just-In-Time）：PyTorch JIT 是 PyTorch 的即时编译器，可以将 PyTorch 模型导出为优化的 Torch 脚本或 Torch 脚本模型。这种格式可以在没有 PyTorch 环境的情况下进行推理，并且具有更高的性能。</p><p>4. Caffe Model：Caffe 是一个流行的深度学习框架，它使用自己的模型表示格式。导出为 Caffe 模型的文件可以在 Caffe 框架中进行部署和推理。</p><p>5. TFLite（TensorFlow Lite）：TFLite 是 TensorFlow 的移动和嵌入式设备推理框架，支持在资源受限的设备上进行高效推理。模型可以导出为 TFLite 格式，以便在移动设备或嵌入式系统中进行部署。</p><p>6. Core ML（Core Machine Learning）：Core ML 是苹果的机器学习框架，用于在 iOS 和 macOS 上进行推理。模型可以导出为 Core ML 格式，以便在苹果设备上进行部署。</p><p>这些格式都提供了不同的优势和适用场景。选择合适的导出格式应该考虑到目标平台和部署环境的要求，以及所使用的深度学习框架的支持情况。</p></blockquote><p>模型输出的参数有如下</p><table><thead><tr><th></th><th>参数名</th><th>类型</th><th>参数解释</th></tr></thead><tbody><tr><td>0</td><td>format</td><td>str</td><td>导出模型的格式</td></tr><tr><td>1</td><td>keras</td><td>bool</td><td>表示是否使用Keras</td></tr><tr><td>2</td><td>optimize</td><td>bool</td><td>用于在导出TorchScript模型时进行优化，以便在移动设备上获得更好的性能</td></tr><tr><td>3</td><td>int8</td><td>bool</td><td>用于在导出CoreML或TensorFlow模型时进行INT8量化</td></tr><tr><td>4</td><td>dynamic</td><td>bool</td><td>用于在导出CoreML或TensorFlow模型时进行INT8量化</td></tr><tr><td>5</td><td>simplify</td><td>bool</td><td>用于在导出ONNX模型时进行模型简化</td></tr><tr><td>6</td><td>opset</td><td>int&#x2F;optional</td><td>用于指定导出ONNX模型时的opset版本</td></tr><tr><td>7</td><td>workspace</td><td>int</td><td>用于指定TensorRT模型的工作空间大小，以GB为单位</td></tr><tr><td>8</td><td>nms</td><td>bool</td><td>用于在导出CoreML模型时添加非极大值抑制（NMS）</td></tr></tbody></table><p>命令行命令如下: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo task=detect mode=export model=best.pt <span class="built_in">format</span>=onnx  </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>恒源云</title>
      <link href="/post/article-2/"/>
      <url>/post/article-2/</url>
      
        <content type="html"><![CDATA[<h1 id="恒源云"><a href="#恒源云" class="headerlink" title="恒源云"></a>恒源云</h1><p>为当涉及到深度学习的训练任务时，GPU的计算能力是不可或缺的。相对于传统的中央处理器（CPU），图形处理器（GPU）具有更强大的并行计算能力，能够显著加速深度学习模型的训练过程。深度学习算法通常涉及大量的矩阵运算和张量操作，而GPU的并行计算架构使得它们能够高效地执行这些计算，从而加速模型训练的速度。</p><p>恒源云是一个经济高效的云计算平台，您可以通过恒源云的控制台或者命令行界面来管理实例、上传和下载数据、执行训练任务等。恒源云还提供了高度可定制的实例规格，您可以根据自己的需求选择适合的实例类型和配置，以最大程度地优化性能和成本。</p><p>另一个恒源云的优势是其<strong>经济实惠的价格</strong>。相对于购买和维护专门的GPU设备，利用恒源云进行云端模型训练可以大大节省成本。恒源云提供了多种付费模式，包括按需付费和预付费套餐，使您能够根据自己的预算和需求进行灵活选择。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3d1ee5ffbd434e55b5d844b892b57423.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3d1ee5ffbd434e55b5d844b892b57423.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h2 id="上传数据集"><a href="#上传数据集" class="headerlink" title="上传数据集"></a>上传数据集</h2><p>在恒源云中我们需要通过终端来上传数据集文件，当在本地处理好了数据集文件以后，我们将其解压缩成zip文件的格式当然tar压缩包等格式的都可以。 </p><p>这里推荐大家用OSS命令上传数据集,可以支持大规模的数据上传。</p><p>在利用OSS进行上传之前我们需要下载一个文件，下载方式如下。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2e6a644b805c4ab491ffc9a06b4d0acc.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2e6a644b805c4ab491ffc9a06b4d0acc.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>完成之后，我们点击下载好的文件，会弹出命令行。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c9daf9ea8fef41edb01f7dfd6a420e28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c9daf9ea8fef41edb01f7dfd6a420e28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>在这里我们可以输入指令,我们先来输入version来检验下我们是否安装成功。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/490f8eb1388b4ccd9d79a464de20960c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/490f8eb1388b4ccd9d79a464de20960c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>当我们安装成功之后，我们先远程登录我们的账号和密码，输入Login</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">login</span><br></pre></td></tr></table></figure><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8c9b62f6cf8a47789acca1a7079fb06f.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8c9b62f6cf8a47789acca1a7079fb06f.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p> 当我们登录成功之后,我们就远程登录了我们的恒源云账号和密码,我们就可以在我们的账号下面建立存储我们数据的文件了。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/059efdcfd75548f9912456c145124dc7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/059efdcfd75548f9912456c145124dc7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"> 按照下图操作即可。<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bf26c45c981f4f3a824245a7cf03a354.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bf26c45c981f4f3a824245a7cf03a354.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>当我们上传好一个文件之后,该文件就保存到我们的系统内了,我们可以随时在该终端页面下载该数据到我们后面步骤中创建的任何实例当中，利用如下命令</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0910b808ea634c318316c93eaf7e694a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0910b808ea634c318316c93eaf7e694a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"> (PS:最后一步需要我们经过下面的’利用云端训练YOLOv8模型’之后才可以进行，在我们创建完实例之后进行的操作步骤)</p><h2 id="利用云端训练YOLOv8模型"><a href="#利用云端训练YOLOv8模型" class="headerlink" title="利用云端训练YOLOv8模型"></a>利用云端训练YOLOv8模型</h2><p>首先进入恒源云的官方网站</p><p><a href="https://www.gpushare.com/" title="恒源云官方网站">恒源云官方网站</a></p><p>然后进行注册和登录操作此步骤省略</p><p> 当我们注册和登录之后会进到控制台界面,然后点击创建实例进入到如下界面。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0f55027db1194e14abc71c9bcf5bfa0d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0f55027db1194e14abc71c9bcf5bfa0d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>在其中根据你的需求选择你的GPU型号,</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/dc99d3b2734e493aa405e8b80dc69dae.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/dc99d3b2734e493aa405e8b80dc69dae.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"> 之后在同页面的最下面有一个实例镜像，可以在其中的下拉滚动条中选择你需要的PyTorch、TensorFlow或者其它框架的版本</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8f54226cfeaa498594522102f26048a5.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8f54226cfeaa498594522102f26048a5.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>然后之后我们创建实例即可。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5f764bb992ca476689322218d7c84146.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5f764bb992ca476689322218d7c84146.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"> 首先开始时需要创建一会,然后才可以进行操作，等待一会创建成功后就会变成如下图的状态情况。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/da777e8327cd43cd8c9962b2d1307e17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/da777e8327cd43cd8c9962b2d1307e17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"> 我们按照图片的操作点击其中的”JupyterLab” 然后会弹出新的网页如下图。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/aa99c7769ca1457da7300c73277351e4.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/aa99c7769ca1457da7300c73277351e4.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>在其中hy-tmp是一个存放我们文件的文件夹,我们点击进去点击图片上的上传本地文件操作即可上传你的模型文件。终端就是一个输入命令的地方，<strong>我们点击终端命令，如下图所示。</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4fc533b4a44645a19c95b04380f76afb.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4fc533b4a44645a19c95b04380f76afb.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>我们初始的时候是在系统的根目录下面,我们进行模型训练等操作进入hy-tmp目录也就是你上传文件的目录下面。</p><p>我们利用cd 命令进入hy-tmp目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> hy-tmp</span><br></pre></td></tr></table></figure><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bec73eccac7a47dbbd6af209ab67edee.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bec73eccac7a47dbbd6af209ab67edee.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>进入其中以后，上传我们的文件。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8e52743f93b343bab86ff0e68b67c5dd.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8e52743f93b343bab86ff0e68b67c5dd.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"> 可以看到我把YOLOv8的官方下载的压缩包上传了进去，其为zip格式的压缩包。</p><p>此时在命令行输入命令解压缩该文件</p><p>输入unzip 文件名.zip解压文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip 文件名.<span class="built_in">zip</span></span><br></pre></td></tr></table></figure><p>cd到该文件目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> 文件名</span><br></pre></td></tr></table></figure><p>输入ll 看文件目录下的结构 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll</span><br></pre></td></tr></table></figure><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a7ed129b0bf440e3b596b1962f61336a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a7ed129b0bf440e3b596b1962f61336a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>这里我们演示的是利用YOLOv8进行目标检测时候的训练流程进行演示,我们进入ultralytics\cfg文件目录利用cd进入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ultralytics\cfg</span><br></pre></td></tr></table></figure><p>同理我们输入ll看该文件下的目录结构</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/1835b338dc514718ae30ecd9bf9229bc.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/1835b338dc514718ae30ecd9bf9229bc.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>可以看到其中有一个default.yaml文件,该文件就是我们进行训练模型的文件,我们可以在左侧的目录下看该文件的代码。 </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/536cbabd66be4ad9aa74729c5ff09b7c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/536cbabd66be4ad9aa74729c5ff09b7c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>当然其中的配置,我就不在这里描述了,如果有需要可以看我的YOLOv8详细训练教程里面有具体的配置以及教程。当我们配置好了数据集以及选择的模型之后就可以在官方的模型基础上进行训练了de。 </p><p>此时我们需要退到ultralytics-main的目录下面执行下面的文件就可以进行训练了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo task=detect mode=train model=datasets/yolo8n.yaml  data=datasets/data.yaml epochs=<span class="number">100</span> batch=<span class="number">64</span> device=<span class="number">0</span> single_cls=<span class="literal">True</span> pretrained=yolov8n.pt</span><br></pre></td></tr></table></figure><p>PS：在我们的系统中python解释器已经默认帮我们配置好了,如果你想要执行一个py格式文件，我们只需要输入python  文件名.py文件即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python 文件名.py</span><br></pre></td></tr></table></figure><p>到此本教程就结束,希望对你有所帮助。大家如有任何问题可以在评论区进行提问。 </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 恒源云 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据集</title>
      <link href="/post/article-13/"/>
      <url>/post/article-13/</url>
      
        <content type="html"><![CDATA[<p>YoloV8。YoloV8是一种高效的目标检测算法，它的训练需要高质量的数据集。然而，获取高质量的数据集是一项耗时且费力的任务。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ad4f0d35d5a24785bdf9b5d0517be144.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ad4f0d35d5a24785bdf9b5d0517be144.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>YoloV8官方推荐了一个数据集网站，就是Roboflow。Roboflow是一个数据集管理平台，提供了免费的数据集，同时也支持上传自己的数据集进行格式转换。使用Roboflow，开发者可以方便地获取所需格式的数据集，无需手动转换格式。此外，Roboflow还提供了多种数据预处理、数据增强等功能，可帮助开发者更好地优化训练数据，<strong>从下面官方获取的图片上来YOLOv8官方指定的数据集获取网站就是Roboflow。</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f1d1f09f485f423ba47df662c9c4f451.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f1d1f09f485f423ba47df662c9c4f451.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>因此，如果你正在使用YoloV8进行目标检测算法的训练，Roboflow是一个非常好的选择，可以帮助你快速获取高质量的数据集，从而加快训练效率。同时，Roboflow也是YoloV8官方推荐的数据集网站，保证了数据集的质量和可靠性。</p><h3 id="下面首先分享Roboflow的官方地址"><a href="#下面首先分享Roboflow的官方地址" class="headerlink" title="下面首先分享Roboflow的官方地址"></a><strong>下面首先分享Roboflow的官方地址</strong></h3><p><strong><a href="https://roboflow.com/" title="请点击此处跳转">请点击此处跳转</a></strong></p><h3 id="搜索自己想要的数据集流程"><a href="#搜索自己想要的数据集流程" class="headerlink" title="搜索自己想要的数据集流程"></a>搜索自己想要的数据集流程</h3><p>成功利用神秘力量以后并点击连接后会跳转下面的网址,</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2330ab877e144be58a4264f69457bce1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2330ab877e144be58a4264f69457bce1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>点击Get Started之后如果你没有之前登录过该网站就会跳转登录和注册的页面</p><p><img src="https://img-blog.csdnimg.cn/a054b6098518427d8ecb840d46f7cf63.png" class="lazyload placeholder" data-srcset="https://img-blog.csdnimg.cn/a054b6098518427d8ecb840d46f7cf63.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"> 这里有三个登录方式我推荐的是用Google账号也就是第一个选项，点击之后跳转如下界面,</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/83af5512f3b048189a971c3ca94e4bed.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/83af5512f3b048189a971c3ca94e4bed.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>  <strong>如果你有Google账户那么就直接登录即可,如果你没有Google账户那么就需要创建一个账号，点击Create account按照操作流程输入即可需要注意的是这里需要一个手机号验证相比于chatgpt的不同这里的Google账户是可以输入中国的手机号的，但是你用中国手机号注册的账户的Google是登陆不了chatgpt的。</strong></p><p><strong>当注册完账号并登录之后就跳转以下界面</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4116ba4cba7a43feaa3ae597e950bf19.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4116ba4cba7a43feaa3ae597e950bf19.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>在开始之前我们需要创建一个工作组,就是左上角所显示的Workspaces流程如下图所示 </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/18f99ea2585240a68a9be1f457693ef3.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/18f99ea2585240a68a9be1f457693ef3.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p> 按照上面操作之后就建立完成了我们的Workspaces</p><p>点击下面图示选项</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/725df3223aaf49aa8f065a60aca9db97.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/725df3223aaf49aa8f065a60aca9db97.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>跳转如下界面</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5b62b21f8616430088b28f006e8ff810.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5b62b21f8616430088b28f006e8ff810.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p> 在上图所示位置输入你想要搜索的数据集**(需要注意的是这里需要输入英文的名称不像Github那样你输入中文名字它可以给你对应搜索中文的，)** </p><p>这里假如我想搜索mask(口罩数据集)他就会跳出一堆数据集提供给你选择</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f9b8fb22d99a48b99393ee39f6eed8a8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f9b8fb22d99a48b99393ee39f6eed8a8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>我们随便选择一个点击进去</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/d39aeb842f7f47f5a3e411b62f1727cd.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/d39aeb842f7f47f5a3e411b62f1727cd.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"> 然后跳出选项框操作流程如下图所示</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/682b82d399054b718f8e0a5c0ca0ac7c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/682b82d399054b718f8e0a5c0ca0ac7c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"> 根据你想要的格式进行选择,然后记得选择Zip格式进行下载到本地(下载到你的浏览器默认下载地址)跳出第三个框即代表你下载完成了。图二中的第二个选项是你进行一些在线训练时候所用的code代码,如果有需要我后期也会出教程，除此之外roboflow还有需要强大的功能如下：</p><ol><li><p>数据增强：Roboflow支持多种数据增强的选项，如旋转、翻转、缩放等，可以帮助开发者扩充数据集，提高模型的泛化能力和准确率。</p></li><li><p>数据预处理：Roboflow提供了多种数据预处理的选项，如去除背景、裁剪、缩放等，可以帮助开发者更好地优化训练数据，提高模型的准确率。</p></li><li><p>数据集管理：Roboflow可以帮助开发者管理数据集，包括上传、下载、删除等操作，方便管理和使用数据集。</p></li><li><p>API支持：Roboflow提供了API支持，可以与其他工具和平台进行集成，方便开发者在不同的应用场景中使用数据集。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> YOLO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> Ultralytics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv8目录结构</title>
      <link href="/post/article-3/"/>
      <url>/post/article-3/</url>
      
        <content type="html"><![CDATA[<h1 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h1><h2 id="一、本文介绍"><a href="#一、本文介绍" class="headerlink" title="一、本文介绍"></a>一、本文介绍</h2><p>Hello，大家好这次给大家带来的不是改进，<strong>是整个YOLOv8项目的分析</strong>，<strong>整个系列大概会更新7-10篇左右的文章</strong>，从项目的目录到每一个功能代码的都会进行详细的讲解，同时YOLOv8改进系列也突破了三十篇文章，最后预计本专栏持续更新会在年底更新上百篇的改进教程， 所以大家如果没有订阅专栏可以提前订阅以下。下面开始进行YOLOv8逐行解析的第一篇——<strong>项目目录构造分析</strong></p><h2 id="二、项目目录构造分析"><a href="#二、项目目录构造分析" class="headerlink" title="二、项目目录构造分析"></a>二、项目目录构造分析</h2><p>开始之前先把源代码的地址分析给大家-&gt;</p><blockquote><p><strong>官方代码地址：</strong><a href="https://github.com/ultralytics/ultralytics" title="YOLO仓库下载地址">YOLO仓库下载地址</a></p></blockquote><p>下面的图片是我们从仓库上下载整个打开之后的图片，左边的部分是文件，右面呢就是展示窗口。 </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/df2b0cc18959403993dcc63056305aa2.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/df2b0cc18959403993dcc63056305aa2.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p><strong>下面的是文件部分的清晰截图-&gt;</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b56eceb48b814fb89370fa557e6da0b6.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b56eceb48b814fb89370fa557e6da0b6.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p><strong>下面我们来逐个分析左边的文件各个都是什么作用-&gt;</strong></p><hr><h3 id="2-1-github"><a href="#2-1-github" class="headerlink" title="2.1 .github"></a><strong>2.1 .github</strong></h3><p><strong>该目录包含以下内容：</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b6f4dfdb894a451289b00660e468521e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b6f4dfdb894a451289b00660e468521e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><blockquote><p>ISSUE_TEMPLATE：提供不同类型的问题报告模板，包括 bug-report.yml、config.yml、feature-request.yml和 question.yml。这些模板帮助用户以结构化的方式报告错误、提出功能请求或提问。  </p></blockquote><blockquote><p>workflows：包含多个工作流文件，如 ci.yml（持续集成）、cla.yml（贡献者许可协议）、codeql.yml（代码质量检查）、docker.yml（Docker配置）、greetings.yml（自动问候新贡献者）、links.yml、publish.yml（自动发布）、stale.yml（处理陈旧问题）。</p></blockquote><p>dependabot.yml（自动依赖更新）</p><p>这些文件共同支持项目的自动化管理，包括代码质量保证、持续集成和部署、社区互动和依赖项维护。</p><hr><h3 id="2-2-docker"><a href="#2-2-docker" class="headerlink" title="2.2  docker"></a>2.2  docker</h3><p><strong>该目录包含以下内容：</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/1b9c076093ff49e6abf177bca6b1ee6b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/1b9c076093ff49e6abf177bca6b1ee6b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>docker 目录包含多个 Dockerfile，每个文件都是为不同环境或平台配置的，例如：</p><ul><li>Dockerfile: 主要的Docker配置文件，用于构建项目的默认Docker镜像。</li><li>Dockerfile-arm64: 针对ARM64架构的设备（如某些类型的服务器或高级嵌入式设备）定制的Docker配置。</li><li>Dockerfile-conda: 使用Conda包管理器配置环境的Docker配置文件。</li><li>Dockerfile-cpu: 为不支持GPU加速的环境配置的Docker配置文件。</li><li>Dockerfile-jetson: 专为NVIDIA Jetson平台定制的Docker配置。</li><li>Dockerfile-python: 可能是针对纯Python环境的简化Docker配置。</li><li>Dockerfile-runner: 可能用于配置持续集成&#x2F;持续部署（CI&#x2F;CD）运行环境的Docker配置。</li></ul><p>这些配置文件是用来部署用的，用户可以根据自己的需要选择合适的环境来部署和运行项目。</p><hr><h3 id="2-3-docs"><a href="#2-3-docs" class="headerlink" title="2.3 docs"></a>2.3 docs</h3><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6895d5eb8af94fa7af4e0d571d845203.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6895d5eb8af94fa7af4e0d571d845203.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>docs目录通常用于存放文档资料，包括多种语言的翻译。例如，此目录下有多个文件夹，每个文件夹代表一种语言（如en代表英语文档）。除此之外，还有几个重要的Python脚本和配置文件给大家说一下：</p><blockquote><p>build_docs.py：一个Python脚本，用于自动化构建和编译文档的过程。<br>mkdocs.yml：MkDocs配置文件，用于指定文档网站的结构和设置。</p></blockquote><p>以mkdocs_es.yml为例，这是用于构建西班牙语文档的MkDocs配置文件。类似的，mkdocs_zh.yml用于构建中文文档。所以这些文档其实和我们学习YOLOv8没啥太大的关系，<strong>大家了解以下就可以了</strong>。</p><hr><h3 id="2-4-examples"><a href="#2-4-examples" class="headerlink" title="2.4 examples"></a>2.4 examples</h3><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8d4825a9f0e84cdb8e7b664860d1d342.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8d4825a9f0e84cdb8e7b664860d1d342.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>在examples文件夹中，大家可以找到不同编程语言和平台的YOLOv8实现示例：</p><p>YOLOv8-CPP-Inference：包含C++语言实现的YOLOv8推理示例，内有CMakeLists.txt（用于项目构建的CMake配置文件），inference.cpp和inference.h（推理相关的源代码和头文件），main.cpp（主程序入口）以及README.md（使用说明）。</p><p>YOLOv8-ONNXRuntime：提供Python语言与ONNX Runtime结合使用的YOLOv8推理示例，其中main.py是主要的脚本文件，README.md提供了如何使用该示例的指南。</p><p>YOLOv8-ONNXRuntime-CPP：与上述ONNX Runtime类似，但是是用C++编写的，包含了相应的CMakeLists.txt，inference.cpp，inference.h和main.cpp文件，以及用于解释如何运行示例的README.md。</p><p>每个示例都配有相应的文档，是当我们进行模型部署的时候在不同环境中部署和使用YOLOv8的示例。</p><hr><h3 id="2-5-tests"><a href="#2-5-tests" class="headerlink" title="2.5 tests"></a>2.5 tests</h3><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f0bdd6022b9043bbaad7e85d9b2c2979.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f0bdd6022b9043bbaad7e85d9b2c2979.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>tests目录包含了项目的自动化测试脚本，每个脚本针对项目的不同部分进行测试：</p><p>conftest.py：包含测试配置选项或共享的测试助手函数。<br>test_cli.py：用于测试命令行界面（CLI）的功能和行为。<br>test_cuda.py：专门测试项目是否能正确使用NVIDIA的CUDA技术，确保GPU加速功能正常。<br>test_engine.py：测试底层推理引擎，如模型加载和数据处理等。<br>test_integrations.py：测试项目与其他服务或库的集成是否正常工作。<br>test_python.py：用于测试项目的Python API接口是否按预期工作。</p><p>这些测试脚本确保大家在改进了文件之后更新或添加的新功能后仍能运行的文件。</p><hr><h3 id="2-6-runs"><a href="#2-6-runs" class="headerlink" title="2. 6 runs"></a>2. 6 runs</h3><p>这个文件我们在上面目录构造没有看到是因为，这是我们成功训练了一次模型之后生成的文件，里面保存我们每一次训练之后的各种信息。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2bb4fc2a037047629dc81e480bf7331b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2bb4fc2a037047629dc81e480bf7331b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>下面的是训练成功之后的一个完整保存文件:</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bcccc7ee5d5144c08788e20d33166cc1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bcccc7ee5d5144c08788e20d33166cc1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><hr><h3 id="2-6-utlralytics-重点"><a href="#2-6-utlralytics-重点" class="headerlink" title="2.6 utlralytics(重点)"></a>2.6 utlralytics(重点)</h3><p>上面讲的大部分文件其实对于大部分读者都用不上，这里的<strong>utralytics文件才是重点</strong>，包含了YOLOv8的所有功能都集成在这个文件目录下面，这里我只介绍每一个目录的功能，每一个文件的内部代码我会在接下来的几个博客里面详细的讲到。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0e171e9dca9c48fcb96510e2e6cd8726.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0e171e9dca9c48fcb96510e2e6cd8726.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h4 id="2-6-1-assets"><a href="#2-6-1-assets" class="headerlink" title="2.6.1 assets"></a><strong>2.6.1 assets</strong></h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/87f3ca08eac94fca888baa2f09e60a54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/87f3ca08eac94fca888baa2f09e60a54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>这个文件下面保存了YOLO历史上可以说最最最经典的两张图片了，这个是大家用来基础推理时候的图片，给大家测试用的。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e624e36a10ae49b891b8045333ab6f5c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e624e36a10ae49b891b8045333ab6f5c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h4 id="2-6-2-cfg（重点）"><a href="#2-6-2-cfg（重点）" class="headerlink" title="2.6.2 cfg（重点）"></a>2.6.2 cfg（重点）</h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0c08501716d94128a14644311c8accf7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0c08501716d94128a14644311c8accf7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>这个文件下面保存了我们的模型配置文件，cfg目录是项目配置的集中地，其中包括：</p><p><strong>datasets文件夹</strong>：包含数据集的配置文件，如数据路径、类别信息等（就是我们训练YOLO模型的时候需要一个数据集，这里面就保存部分数据集的yaml文件，如果我们训练的时候没有指定数据集则会自动下载其中的数据集文件，但是很容易失败！）。<br><strong>models文件夹</strong>：存放模型配置文件，定义了模型结构和训练参数等，这个是我们改进或者就基础版本的一个yaml文件配置的地方，截图如下:</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c7117aced6e44708a1df19915508c59d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c7117aced6e44708a1df19915508c59d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>models文件夹中的每个.yaml文件代表了不同的YOLOv8模型配置，具体包括：</p><p><strong>yolov8.yaml:</strong>   这是YOLOv8模型的标准配置文件，定义了模型的基础架构和参数。<br><strong>yolov8-cls.yaml:</strong> 配置文件调整了YOLOv8模型，专门用于图像分类任务。<br><strong>yolov8-ghost.yaml:</strong> 应用Ghost模块的YOLOv8变体，旨在提高计算效率。<br><strong>yolov8-ghost-p2.yaml 和 yolov8-ghost-p6.yaml:</strong> 这些文件是针对特定大小输入的Ghost模型变体配置。<br><strong>yolov8-p2.yaml和 yolov8-p6.yaml:</strong> 针对不同处理级别（例如不同的输入分辨率或模型深度）的YOLOv8模型配置。<br><strong>yolov8-pose.yaml:</strong> 为姿态估计任务定制的YOLOv8模型配置。<br><strong>yolov8-pose-p6.yaml:</strong> 针对更大的输入分辨率或更复杂的模型架构姿态估计任务。<br><strong>yolov8-rtdetr.yaml:</strong> 可能表示实时检测和跟踪的YOLOv8模型变体。<br><strong>yolov8-seg.yaml 和 yolov8-seg-p6.yaml:</strong> 这些是为语义分割任务定制的YOLOv8模型配置。</p><p>这些配置文件是模型训练和部署的核心，同时大家如果进行改进也是修改其中的对应文件来优化 网络结构。</p><p><strong>trackers文件夹</strong>：用于追踪算法的配置。<br><strong>__init__.py文件</strong>：表明`cfg`是一个Python包。<br><strong>default.yaml</strong>：项目的默认配置文件，包含了被多个模块共享的通用配置项。</p><p>这个文件就是配置训练的时候进行用的然后一些任务选择部分</p><h4 id="2-6-3-data"><a href="#2-6-3-data" class="headerlink" title="2.6.3 data"></a>2.6.3 data</h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/dc76024ccabc4de3982b1e7f37620708.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/dc76024ccabc4de3982b1e7f37620708.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>在data&#x2F;scripts文件夹中，包括了一系列脚本和Python文件：</p><p>- download_weights.sh: 用来下载预训练权重的脚本。<br>- get_coco.sh, get_coco128.sh, get_imagenet.sh: 用于下载COCO数据集完整版、128张图片版以及ImageNet数据集的脚本。<br>  <br><strong>在data文件夹中，包括：</strong></p><p><strong>annotator.py:</strong> 用于数据注释的工具。<br><strong>augment.py:</strong> 数据增强相关的函数或工具。<br><strong>base.py, build.py, converter.py:</strong> 包含数据处理的基础类或函数、构建数据集的脚本以及数据格式转换工具。<br><strong>dataset.py:</strong> 数据集加载和处理的相关功能。<br><strong>loaders.py:</strong> 定义加载数据的方法。<br><strong>utils.py:</strong> 各种数据处理相关的通用工具函数。</p><h4 id="2-6-4-engine"><a href="#2-6-4-engine" class="headerlink" title="2.6.4  engine"></a>2.6.4  engine</h4><p>engine文件夹包含与模型训练、评估和推理有关的核心代码：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7800024fbeae41cc8f009745c4101fc5.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7800024fbeae41cc8f009745c4101fc5.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p><strong>exporter.py:</strong> 用于将训练好的模型导出到其他格式，例如ONNX或TensorRT。<br><strong>model.py:</strong> 包含模型定义，还包括模型初始化和加载的方法。<br><strong>predictor.py:</strong> 包含推理和预测的逻辑，如加载模型并对输入数据进行预测。<br><strong>results.py:</strong> 用于存储和处理模型输出的结果。<br><strong>trainer.py:</strong> 包含模型训练过程的逻辑。<br><strong>tuner.py:</strong> 用于模型超参数调优。<br><strong>validator.py:</strong> 包含模型验证的逻辑，如在验证集上评估模型性能。</p><h4 id="2-6-5-hub"><a href="#2-6-5-hub" class="headerlink" title="2.6.5 hub"></a>2.6.5 hub</h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5942a5cb41504caaa71e9d1017c6b0bb.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5942a5cb41504caaa71e9d1017c6b0bb.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>hub文件夹通常用于处理与平台或服务集成相关的操作，包括：</p><p><strong>auth.py:</strong> 处理认证流程，如API密钥验证或OAuth流程。<br><strong>session.py:</strong> 管理会话，包括创建和维护持久会话。<br><strong>utils.py:</strong> 包含一些通用工具函数，可能用于支持认证和会话管理功能。</p><h4 id="2-6-6-models-重点"><a href="#2-6-6-models-重点" class="headerlink" title="2.6.6 models(重点)"></a>2.6.6 models(重点)</h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e29f4a22943e4ce7ad1cc84b247d99e3.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e29f4a22943e4ce7ad1cc84b247d99e3.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>这个目录下面是YOLO仓库包含的一些模型的方法实现，我们这里之说YOLO的，同时这里只是简单介绍，后面的博客针对于其中的任意一个都会进行单独的讲解。</p><p>这个models&#x2F;yolo目录中包含了YOLO模型的不同任务特定实现：</p><p><strong>classify:</strong> 这个目录可能包含用于图像分类的YOLO模型。<br><strong>detect:</strong> 包含用于物体检测的YOLO模型。<br><strong>pose:</strong> 包含用于姿态估计任务的YOLO模型。<br><strong>segment:</strong> 包含用于图像分割的YOLO模型，</p><h4 id="2-6-7-nn-重点"><a href="#2-6-7-nn-重点" class="headerlink" title="2.6.7 nn(重点)"></a>2.6.7 nn(重点)</h4><p>这个文件目录下的所有文件，就是定义我们模型中的一些组成构建，之后我们进行改进和优化，增加其它结构的时候都要在对应的文件下面进行改动。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/22a54c754e424b2d808cc532e7a23c47.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/22a54c754e424b2d808cc532e7a23c47.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p><strong>modules文件夹:</strong><br>   <strong>__init__.py:</strong> 表明此目录是Python包。<br>   <strong>block.py:</strong> 包含定义神经网络中的基础块，如残差块或瓶颈块。<br>   <strong>conv.py:</strong> 包含卷积层相关的实现。<br>   <strong>head.py:</strong> 定义网络的头部，用于预测。<br>   <strong>transformer.py:</strong> 包含Transformer模型相关的实现。<br>   <strong>utils.py:</strong> 提供构建神经网络时可能用到的辅助函数。</p><p><strong>__init__.py:</strong> 同样标记这个目录为Python包。</p><p><strong>autobackend.py:</strong> 用于自动选择最优的计算后端。</p><p><strong>tasks.py</strong>: 定义了使用神经网络完成的不同任务的流程，例如分类、检测或分割，所有的流程基本上都定义在这里，定义模型前向传播都在这里。</p><h4 id="2-6-8-solutions"><a href="#2-6-8-solutions" class="headerlink" title="2.6.8 solutions"></a>2.6.8 solutions</h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b4b0a147cae94419b8e094c2d0c2c6e9.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b4b0a147cae94419b8e094c2d0c2c6e9.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p><strong>__init__.py:</strong> 标识这是一个Python包。<br><strong>ai_gym.py:</strong> 与强化学习相关，例如在OpenAI Gym环境中训练模型的代码。<br><strong>heatmap.py:</strong> 用于生成和处理热图数据，这在物体检测和事件定位中很常见。<br><strong>object_counter.py:</strong> 用于物体计数的脚本，包含从图像中检测和计数实例的逻辑。</p><h4 id="2-6-9-trackers"><a href="#2-6-9-trackers" class="headerlink" title="2.6.9 trackers"></a>2.6.9 trackers</h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/db2b42c30d7c4db29f9fc02b3c114c47.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/db2b42c30d7c4db29f9fc02b3c114c47.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p><strong>trackers</strong>文件夹包含了实现目标跟踪功能的脚本和模块：</p><p><strong>__init__.py:</strong> 指示该文件夹是一个Python包。<br><strong>basetrack.py:</strong> 包含跟踪器的基础类或方法。<br><strong>bot_sort.py:</strong> 实现了SORT算法（Simple Online and Realtime Tracking）的版本。<br><strong>byte_tracker.py:</strong> 是一个基于深度学习的跟踪器，使用字节为单位跟踪目标。<br><strong>track.py:</strong> 包含跟踪单个或多个目标的具体逻辑。<br><strong>README.md:</strong> 提供该目录内容和用法的说明。</p><h4 id="2-6-10-utils"><a href="#2-6-10-utils" class="headerlink" title="2.6.10 utils"></a>2.6.10 utils</h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7267573c5a5940a796e373dbdbcda494.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7267573c5a5940a796e373dbdbcda494.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>这个utils目录包含了多个Python脚本，每个脚本都有特定的功能：</p><p><strong>callbacks.py:</strong> 包含在训练过程中被调用的回调函数。<br><strong>autobatch.py:</strong> 用于实现批处理优化，以提高训练或推理的效率。<br><strong>benchmarks.py:</strong> 包含性能基准测试相关的函数。<br><strong>checks.py</strong>: 用于项目中的各种检查，如参数验证或环境检查。<br><strong>dist.py:</strong> 涉及分布式计算相关的工具。<br><strong>downloads.py:</strong> 包含下载数据或模型等资源的脚本。<br><strong>errors.py:</strong> 定义错误处理相关的类和函数。<br><strong>files.py:</strong> 包含文件操作相关的工具函数。<br><strong>instance.py:</strong> 包含实例化对象或模型的工具。<br><strong>loss.py:</strong> 定义损失函数。<br><strong>metrics.py:</strong> 包含评估模型性能的指标计算函数。<br><strong>ops.py:</strong> 包含自定义操作，如特殊的数学运算或数据转换。<br><strong>patches.py:</strong> 用于实现修改或补丁应用的工具。<br><strong>plotting.py:</strong> 包含数据可视化相关的绘图工具。<br><strong>tal.py:</strong> 一些损失函数的功能应用<br><strong>torch_utils.py:</strong> 提供PyTorch相关的工具和辅助函数，包括GFLOPs的计算。<br><strong>triton.py:</strong> 可能与NVIDIA Triton Inference Server集成相关。<br><strong>tuner.py:</strong> 包含模型或算法调优相关的工具。</p><p><strong>到这里重点的ultralytics文件目录下的所有功能都介绍完毕了，这里只是简单的介绍，后面的博客会详细的介绍一些重要的功能。</strong></p><hr><h3 id="2-7-同级目录下的文件"><a href="#2-7-同级目录下的文件" class="headerlink" title="2.7 同级目录下的文件"></a>2.7 同级目录下的文件</h3><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/91ff9a17779d4019ada156f9fca35155.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/91ff9a17779d4019ada156f9fca35155.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p><strong>这个里是项目的根本配置和文档文件：</strong></p><p><strong>.gitignore:</strong> Git配置文件，指定了Git版本控制要忽略的文件。<br><strong>.pre-commit-config.yaml:</strong> 预提交钩子的配置文件，用于在提交前自动执行代码质量检查。<br><strong>CITATION.cff:</strong> 提供了如何引用该项目的格式说明。<br><strong>CONTRIBUTING.md:</strong> 说明如何为项目贡献代码的指南。<br><strong>LICENSE:</strong> 包含了项目的许可证信息。<br><strong>MANIFEST.in:</strong> 列出了在构建和分发Python包时需要包含的文件。<br><strong>README.md 和 README.zh-CN.md:</strong> 项目的说明文件，分别为英文和中文版本。<br><strong>requirements.txt:</strong> 列出了项目运行所需的Python依赖。<br><strong>setup.cfg 和 setup.py:</strong> 包含了设置项目安装和分发的脚本。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv8文件分析</title>
      <link href="/post/article-4/"/>
      <url>/post/article-4/</url>
      
        <content type="html"><![CDATA[<h1 id="文件分析"><a href="#文件分析" class="headerlink" title="文件分析"></a>文件分析</h1><h2 id="一、本文介绍"><a href="#一、本文介绍" class="headerlink" title="一、本文介绍"></a>一、本文介绍</h2><p>本文给大家带来的是<strong>YOLOv8项目的解读</strong>，之前给大家分析了YOLOv8的项目文件分析，这一篇文章给大家带来的是模型训练从我们的yaml文件定义到模型的定义部分的讲解，我们一般只知道如何去训练模型，和配置yaml文件，但是对于yaml文件是如何输入到模型里，模型如何将yaml文件解析出来的确是不知道的，本文的内容接上一篇的代码逐行解析(一) 项目目录分析，本文对于小白来说非常友好，非常推荐大家进行阅读，深度的了解模型的工作原理已经流程，下面我们从yaml文件来讲解。</p><p>本文的讲解全部在代码的对应位置进行注释介绍非常详细，<strong>以下为部分内容的截图。</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/591c0efe630e4e51b8059bf9e8b197c6.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/591c0efe630e4e51b8059bf9e8b197c6.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="591c0efe630e4e51b8059bf9e8b197c6.png"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/af9d4d78be4d448a98554c9265832fb0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/af9d4d78be4d448a98554c9265832fb0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="af9d4d78be4d448a98554c9265832fb0.png"></p><hr><h2 id="二、yaml文件的定义"><a href="#二、yaml文件的定义" class="headerlink" title="二、yaml文件的定义"></a>二、yaml文件的定义</h2><p>我们训练模型的第一步是需要配置yaml文件，我们的讲解第一步也从yaml文件来开始讲解，YOLOv8的yaml文件存放在我们的如下目录内’ultralytics&#x2F;cfg&#x2F;models&#x2F;v8’，在其中我们可以定义各种模型配置的文件组合不同的模块，我们拿最基础的YOLOv8yaml文件来讲解一下。</p><p><strong>注释部分的内容我就不介绍了，我只介绍一下其中有用的部分，我已经在代码中对应的位置注释上了解释，大家可以看这样看起来也直观一些。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ultralytics YOLO 🚀, AGPL-3.0 license</span></span><br><span class="line"><span class="comment"># YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">nc: <span class="number">80</span>  <span class="comment"># 数据集的类别数，我们默认的数据COCO是80类别（YOLOv8提供的权重也是由此数据集训练出来的），有的读者喜欢修改nc此处其实不需要修改，</span></span><br><span class="line">        <span class="comment"># 模型会自动根据我们数据集的yaml文件获取此处的数量，同时我们8.1版本之前的ultralytics仓库打印两边的网络结构，唯一的区别就是nc的数量不一样（实际运行的是第二遍的网络结构）。</span></span><br><span class="line"> </span><br><span class="line">scales:  <span class="comment"># model compound scaling constants, i.e. &#x27;model=yolov8n.yaml&#x27; will call yolov8.yaml with scale &#x27;n&#x27;</span></span><br><span class="line">         <span class="comment"># 此处的含义大概就是如果我们在训练的指令时候使用model=yolov8.yaml 则对应的是v8n，如果使用model=yolov8s.yaml则对应的是v8s</span></span><br><span class="line">         <span class="comment"># 当然了大家如果不想使用上面的方式指定模型，我们只需要将下面想要使用的模型移到最前端即可，或者将其余不想用的注释掉都可以。</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment"># [depth, width, max_channels]</span></span><br><span class="line">  n: [<span class="number">0.33</span>, <span class="number">0.25</span>, <span class="number">1024</span>]  <span class="comment"># YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs</span></span><br><span class="line">  s: [<span class="number">0.33</span>, <span class="number">0.50</span>, <span class="number">1024</span>]  <span class="comment"># YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs</span></span><br><span class="line">  m: [<span class="number">0.67</span>, <span class="number">0.75</span>, <span class="number">768</span>]   <span class="comment"># YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs</span></span><br><span class="line">  l: [<span class="number">1.00</span>, <span class="number">1.00</span>, <span class="number">512</span>]   <span class="comment"># YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs</span></span><br><span class="line">  x: [<span class="number">1.00</span>, <span class="number">1.25</span>, <span class="number">512</span>]   <span class="comment"># YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># YOLOv8.0n backbone (主干部分的配置)</span></span><br><span class="line">backbone:</span><br><span class="line">  <span class="comment"># [from, repeats, module, args]</span></span><br><span class="line">  <span class="comment"># 这里需要多介绍一下，from, repeats, module, args</span></span><br><span class="line">  <span class="comment"># from 此处有三种可能的值分别是 -1、具体的数值、list存放数值。分别含义如下  (1)、-1的含义就是代表此层的输入就是上一层的输出，</span></span><br><span class="line">  <span class="comment">#                                                                (2)、如果是具体的某个数字比如4那么则代表本层的输入来自于模型的第四层，</span></span><br><span class="line">  <span class="comment">#                                                                (3)、有的层是list存放两个值也可能是多个值，则代表对应两个值的输出为本层的输入</span></span><br><span class="line">  <span class="comment"># repeats 这个参数是为了C2f设置的其它的模块都用不到，代表着C2f当中Bottleneck重复的次数，比如当我们的模型用的是l的时候，那么repeats=3那么则代表C2f当中的Bottleneck串行3个。</span></span><br><span class="line">  <span class="comment"># module 此处则代表模型的名称</span></span><br><span class="line">  <span class="comment"># args 此处代表输入到对应模块的参数，此处和parse_model函数中的定义方法有关，对于C2f来说传入的参数-&gt;第一个参数是上一个模型的输出通道数，第二个参数就是args的第一个参数，然后以此类推。</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">64</span>, <span class="number">3</span>, <span class="number">2</span>]]  <span class="comment"># 0-P1/2</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">128</span>, <span class="number">3</span>, <span class="number">2</span>]]  <span class="comment"># 1-P2/4</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">128</span>, <span class="literal">True</span>]]</span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">256</span>, <span class="number">3</span>, <span class="number">2</span>]]  <span class="comment"># 3-P3/8</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">6</span>, C2f, [<span class="number">256</span>, <span class="literal">True</span>]]</span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">512</span>, <span class="number">3</span>, <span class="number">2</span>]]  <span class="comment"># 5-P4/16</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">6</span>, C2f, [<span class="number">512</span>, <span class="literal">True</span>]]</span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">1024</span>, <span class="number">3</span>, <span class="number">2</span>]]  <span class="comment"># 7-P5/32</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">1024</span>, <span class="literal">True</span>]]</span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, SPPF, [<span class="number">1024</span>, <span class="number">5</span>]]  <span class="comment"># 9</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># YOLOv8.0n head</span></span><br><span class="line">head:</span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, nn.Upsample, [<span class="literal">None</span>, <span class="number">2</span>, <span class="string">&#x27;nearest&#x27;</span>]]</span><br><span class="line">  - [[-<span class="number">1</span>, <span class="number">6</span>], <span class="number">1</span>, Concat, [<span class="number">1</span>]]  <span class="comment"># cat backbone P4</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">512</span>]]  <span class="comment"># 12</span></span><br><span class="line"> </span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, nn.Upsample, [<span class="literal">None</span>, <span class="number">2</span>, <span class="string">&#x27;nearest&#x27;</span>]]</span><br><span class="line">  - [[-<span class="number">1</span>, <span class="number">4</span>], <span class="number">1</span>, Concat, [<span class="number">1</span>]]  <span class="comment"># cat backbone P3</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">256</span>]]  <span class="comment"># 15 (P3/8-small)</span></span><br><span class="line"> </span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">256</span>, <span class="number">3</span>, <span class="number">2</span>]]</span><br><span class="line">  - [[-<span class="number">1</span>, <span class="number">12</span>], <span class="number">1</span>, Concat, [<span class="number">1</span>]]  <span class="comment"># cat head P4</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">512</span>]]  <span class="comment"># 18 (P4/16-medium)</span></span><br><span class="line"> </span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">1</span>, Conv, [<span class="number">512</span>, <span class="number">3</span>, <span class="number">2</span>]]</span><br><span class="line">  - [[-<span class="number">1</span>, <span class="number">9</span>], <span class="number">1</span>, Concat, [<span class="number">1</span>]]  <span class="comment"># cat head P5</span></span><br><span class="line">  - [-<span class="number">1</span>, <span class="number">3</span>, C2f, [<span class="number">1024</span>]]  <span class="comment"># 21 (P5/32-large)</span></span><br><span class="line"> </span><br><span class="line">  - [[<span class="number">15</span>, <span class="number">18</span>, <span class="number">21</span>], <span class="number">1</span>, Detect, [nc]]  <span class="comment"># Detect(P3, P4, P5)</span></span><br></pre></td></tr></table></figure><p>其中的Conv和C2f的结构我就不过多解释了，网上教程已经很多了，其中详细的结构在下图中都能够看到。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/de838aaf62ff43df9cf3a6786cb1ec8f.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/de838aaf62ff43df9cf3a6786cb1ec8f.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="de838aaf62ff43df9cf3a6786cb1ec8f.png"></p><hr><h2 id="三、yaml文件的输入"><a href="#三、yaml文件的输入" class="headerlink" title="三、yaml文件的输入 "></a>三、yaml文件的输入 </h2><p>上面我们解释了yaml文件中的参数含义，然后提供了一个结构图（其中能够获取到每个模块的详细结构，该结构图来源于官方）。然后我们下一步介绍当定义好了一个ymal文件其是如何传入到模型的内部的，模型的开始在哪里。</p><h3 id="3-1-模型的定义"><a href="#3-1-模型的定义" class="headerlink" title="3.1 模型的定义"></a>3.1 模型的定义</h3><p>我们通过命令行的命令或者创建py文件运行模型之后，模型最开始的工作是模型的定义操作。模型存放于文件’ultralytics&#x2F;engine&#x2F;model.py’内部，首先需要通过’__init__‘来定义模型的一些变量。</p><p><strong>此处我将模型的定义部分的代码解释了一下，大家有兴趣的可以和自己的文件对比着看。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        一个统一所有模型API的基类。</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            model (str, Path): 要加载或创建的模型文件的路径。</span></span><br><span class="line"><span class="string">            task (Any, 可选): YOLO模型的任务类型。默认为None。</span></span><br><span class="line"><span class="string">        属性:</span></span><br><span class="line"><span class="string">            predictor (Any): 预测器对象。</span></span><br><span class="line"><span class="string">            model (Any): 模型对象。</span></span><br><span class="line"><span class="string">            trainer (Any): 训练器对象。</span></span><br><span class="line"><span class="string">            task (str): 模型任务类型。</span></span><br><span class="line"><span class="string">            ckpt (Any): 如果从*.pt文件加载的模型，则为检查点对象。</span></span><br><span class="line"><span class="string">            cfg (str): 如果从*.yaml文件加载的模型，则为模型配置。</span></span><br><span class="line"><span class="string">            ckpt_path (str): 检查点文件路径。</span></span><br><span class="line"><span class="string">            overrides (dict): 训练器对象的覆盖。</span></span><br><span class="line"><span class="string">            metrics (Any): 用于度量的数据。</span></span><br><span class="line"><span class="string">        方法:</span></span><br><span class="line"><span class="string">            __call__(source=None, stream=False, **kwargs):</span></span><br><span class="line"><span class="string">                预测方法的别名。</span></span><br><span class="line"><span class="string">            _new(cfg:str, verbose:bool=True) -&gt; None:</span></span><br><span class="line"><span class="string">                初始化一个新模型，并从模型定义中推断任务类型。</span></span><br><span class="line"><span class="string">            _load(weights:str, task:str=&#x27;&#x27;) -&gt; None:</span></span><br><span class="line"><span class="string">                初始化一个新模型，并从模型头中推断任务类型。</span></span><br><span class="line"><span class="string">            _check_is_pytorch_model() -&gt; None:</span></span><br><span class="line"><span class="string">                如果模型不是PyTorch模型，则引发TypeError。</span></span><br><span class="line"><span class="string">            reset() -&gt; None:</span></span><br><span class="line"><span class="string">                重置模型模块。</span></span><br><span class="line"><span class="string">            info(verbose:bool=False) -&gt; None:</span></span><br><span class="line"><span class="string">                记录模型信息。</span></span><br><span class="line"><span class="string">            fuse() -&gt; None:</span></span><br><span class="line"><span class="string">                为了更快的推断，融合模型。</span></span><br><span class="line"><span class="string">            predict(source=None, stream=False, **kwargs) -&gt; List[ultralytics.engine.results.Results]:</span></span><br><span class="line"><span class="string">                使用YOLO模型进行预测。</span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            list(ultralytics.engine.results.Results): 预测结果。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model: <span class="type">Union</span>[<span class="built_in">str</span>, Path] = <span class="string">&quot;yolov8n.pt&quot;</span>, task=<span class="literal">None</span>, verbose=<span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initializes the YOLO model.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            model (Union[str, Path], optional): Path or name of the model to load or create. Defaults to &#x27;yolov8n.pt&#x27;.</span></span><br><span class="line"><span class="string">            task (Any, optional): Task type for the YOLO model. Defaults to None.</span></span><br><span class="line"><span class="string">            verbose (bool, optional): Whether to enable verbose mode.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        此处为上面的解释</span></span><br><span class="line"><span class="string">               初始化 YOLO 模型。</span></span><br><span class="line"><span class="string">               参数:</span></span><br><span class="line"><span class="string">                   model (Union[str, Path], 可选): 要加载或创建的模型的路径或名称。默认为&#x27;yolov8n.pt&#x27;。</span></span><br><span class="line"><span class="string">                   task (Any, 可选): YOLO 模型的任务类型。默认为 None。</span></span><br><span class="line"><span class="string">                   verbose (bool, 可选): 是否启用详细模式。</span></span><br><span class="line"><span class="string">               &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot;此处就是读取我们的yaml文件的地方，callbacks.get_default_callbacks()会将我们的yaml文件进行解析然后将名称返回回来存放在self.callbacks中&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.callbacks = callbacks.get_default_callbacks()</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 下面的部分就是一些模型的参数定义，我大概解释了一下，大家其实也不用太了解，一篇文章也介绍不了太多&quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line">        <span class="variable language_">self</span>.predictor = <span class="literal">None</span>  <span class="comment"># 重用预测器</span></span><br><span class="line">        <span class="variable language_">self</span>.model = <span class="literal">None</span>  <span class="comment"># 模型对象</span></span><br><span class="line">        <span class="variable language_">self</span>.trainer = <span class="literal">None</span>  <span class="comment"># 训练器对象</span></span><br><span class="line">        <span class="variable language_">self</span>.ckpt = <span class="literal">None</span>  <span class="comment"># 如果从*.pt文件加载的检查点对象</span></span><br><span class="line">        <span class="variable language_">self</span>.cfg = <span class="literal">None</span>  <span class="comment"># 如果从*.yaml文件加载的模型配置</span></span><br><span class="line">        <span class="variable language_">self</span>.ckpt_path = <span class="literal">None</span>  <span class="comment"># 检查点文件路径</span></span><br><span class="line">        <span class="variable language_">self</span>.overrides = &#123;&#125;  <span class="comment"># 训练器对象的覆盖设置</span></span><br><span class="line">        <span class="variable language_">self</span>.metrics = <span class="literal">None</span>  <span class="comment"># 验证/训练指标</span></span><br><span class="line">        <span class="variable language_">self</span>.session = <span class="literal">None</span>  <span class="comment"># HUB 会话</span></span><br><span class="line">        <span class="variable language_">self</span>.task = task  <span class="comment"># 任务类型</span></span><br><span class="line">        <span class="variable language_">self</span>.model_name = model = <span class="built_in">str</span>(model).strip()  <span class="comment"># 去除空格</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 检查是否为来自 https://hub.ultralytics.com 的 Ultralytics HUB 模型</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.is_hub_model(model):</span><br><span class="line">            <span class="comment"># 从 HUB 获取模型</span></span><br><span class="line">            checks.check_requirements(<span class="string">&quot;hub-sdk&gt;0.0.2&quot;</span>)</span><br><span class="line">            <span class="variable language_">self</span>.session = <span class="variable language_">self</span>._get_hub_session(model)</span><br><span class="line">            model = <span class="variable language_">self</span>.session.model_file</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 检查是否为 Triton 服务器模型</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.is_triton_model(model):</span><br><span class="line">            <span class="variable language_">self</span>.model = model</span><br><span class="line">            <span class="variable language_">self</span>.task = task</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 加载或创建新的 YOLO 模型</span></span><br><span class="line">        model = checks.check_model_file_from_stem(model)  <span class="comment"># 添加后缀，例如 yolov8n -&gt; yolov8n.pt</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; 此处比较重要,如果我们没有指定模型的权重.pt那么模型会根据yaml文件创建一个新的模型，如果指定了权重那么模型这回加载pt文件中的模型&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> Path(model).suffix <span class="keyword">in</span> (<span class="string">&quot;.yaml&quot;</span>, <span class="string">&quot;.yml&quot;</span>):</span><br><span class="line">            <span class="variable language_">self</span>._new(model, task=task, verbose=verbose)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>._load(model, task=task)</span><br><span class="line"> </span><br><span class="line">        <span class="variable language_">self</span>.model_name = model <span class="comment"># 返回的模型则保存在self.model_name中</span></span><br></pre></td></tr></table></figure><hr><h3 id="3-2-模型的训练"><a href="#3-2-模型的训练" class="headerlink" title="3.2 模型的训练"></a>3.2 模型的训练</h3><p>我们上面讲完了模型的定义，然后模型就会根据你指定的参数来进行调用对应的函数，比如我这里指定的是detect，和train，如下图所示，然后模型就会根据指定的参数进行对应任务的训练。</p><p><strong>图片来源于文件’ultralytics&#x2F;cfg&#x2F;default.yaml’ 截图。</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ba833c58a10243b7b3a30fdc9c137c3e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ba833c58a10243b7b3a30fdc9c137c3e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="ba833c58a10243b7b3a30fdc9c137c3e.png"></p><p>此处执行的是ultralytics&#x2F;engine&#x2F;model.py’文件中class Model(nn.Module):类别的def train(self, trainer&#x3D;None, **kwargs):函数，具体的解释我已经在代码中标记了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, trainer=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">在给定的数据集上训练模型。</span></span><br><span class="line"><span class="string">参数:</span></span><br><span class="line"><span class="string">    trainer (BaseTrainer, 可选): 自定义的训练器。</span></span><br><span class="line"><span class="string">    **kwargs (Any): 表示训练配置的任意数量的参数。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="variable language_">self</span>._check_is_pytorch_model()  <span class="comment"># 检查模型是否为 PyTorch 模型</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>.session, <span class="string">&quot;model&quot;</span>) <span class="keyword">and</span> <span class="variable language_">self</span>.session.model.<span class="built_in">id</span>:  <span class="comment"># Ultralytics HUB session with loaded model</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">any</span>(kwargs):</span><br><span class="line">        LOGGER.warning(<span class="string">&quot;WARNING ⚠️ 使用 HUB 训练参数，忽略本地训练参数。&quot;</span>)</span><br><span class="line">    kwargs = <span class="variable language_">self</span>.session.train_args  <span class="comment"># 覆盖 kwargs</span></span><br><span class="line"> </span><br><span class="line">checks.check_pip_update_available()  <span class="comment"># 检查 pip 是否有更新</span></span><br><span class="line"> </span><br><span class="line">overrides = yaml_load(checks.check_yaml(kwargs[<span class="string">&quot;cfg&quot;</span>])) <span class="keyword">if</span> kwargs.get(<span class="string">&quot;cfg&quot;</span>) <span class="keyword">else</span> <span class="variable language_">self</span>.overrides</span><br><span class="line">custom = &#123;<span class="string">&quot;data&quot;</span>: DEFAULT_CFG_DICT[<span class="string">&quot;data&quot;</span>] <span class="keyword">or</span> TASK2DATA[<span class="variable language_">self</span>.task]&#125;  <span class="comment"># 方法的默认设置</span></span><br><span class="line">args = &#123;**overrides, **custom, **kwargs, <span class="string">&quot;mode&quot;</span>: <span class="string">&quot;train&quot;</span>&#125;  <span class="comment"># 最高优先级的参数在右侧</span></span><br><span class="line"><span class="keyword">if</span> args.get(<span class="string">&quot;resume&quot;</span>):</span><br><span class="line">    args[<span class="string">&quot;resume&quot;</span>] = <span class="variable language_">self</span>.ckpt_path</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 实例化或加载训练器</span></span><br><span class="line"><span class="string">&quot;&quot;&quot; 此处将一些参数加载到模型的内部&quot;&quot;&quot;</span></span><br><span class="line"><span class="variable language_">self</span>.trainer = (trainer <span class="keyword">or</span> <span class="variable language_">self</span>._smart_load(<span class="string">&quot;trainer&quot;</span>))(overrides=args, _callbacks=<span class="variable language_">self</span>.callbacks)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> args.get(<span class="string">&quot;resume&quot;</span>):  <span class="comment"># 仅在不续训的时候手动设置模型</span></span><br><span class="line">    <span class="comment"># 获取模型并设置训练器</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    此处比较重要,为开始定义我们的对应任务的模型了比如我这里task设置的为Detect,那么此处会实例化DetectModel模型。</span></span><br><span class="line"><span class="string">    模型存放在ultralytics/nn/tasks.py内（就是我们修改模型时候的用到的那个task.py文件）</span></span><br><span class="line"><span class="string">    此处就会跳转到&#x27;ultralytics/nn/tasks.py&#x27;文化内的class DetectionModel(BaseModel):类中进行初始化和模型的定义工作</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="variable language_">self</span>.trainer.model = <span class="variable language_">self</span>.trainer.get_model(weights=<span class="variable language_">self</span>.model <span class="keyword">if</span> <span class="variable language_">self</span>.ckpt <span class="keyword">else</span> <span class="literal">None</span>, cfg=<span class="variable language_">self</span>.model.yaml)</span><br><span class="line">    <span class="variable language_">self</span>.model = <span class="variable language_">self</span>.trainer.model</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> SETTINGS[<span class="string">&quot;hub&quot;</span>] <span class="keyword">is</span> <span class="literal">True</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="variable language_">self</span>.session:</span><br><span class="line">        <span class="comment"># 如果开启了 HUB 并且没有 HUB 会话</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 创建一个 HUB 中的模型</span></span><br><span class="line">            <span class="variable language_">self</span>.session = <span class="variable language_">self</span>._get_hub_session(<span class="variable language_">self</span>.model_name)</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.session:</span><br><span class="line">                <span class="variable language_">self</span>.session.create_model(args)</span><br><span class="line">                <span class="comment"># 检查模型是否创建成功</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">getattr</span>(<span class="variable language_">self</span>.session.model, <span class="string">&quot;id&quot;</span>, <span class="literal">None</span>):</span><br><span class="line">                    <span class="variable language_">self</span>.session = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">except</span> (PermissionError, ModuleNotFoundError):</span><br><span class="line">            <span class="comment"># 忽略 PermissionError 和 ModuleNotFoundError，表示 hub-sdk 未安装</span></span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将可选的 HUB 会话附加到训练器</span></span><br><span class="line"><span class="variable language_">self</span>.trainer.hub_session = <span class="variable language_">self</span>.session</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 进行模型训练</span></span><br><span class="line"><span class="variable language_">self</span>.trainer.train()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 训练结束后更新模型和配置信息</span></span><br><span class="line"><span class="keyword">if</span> RANK <span class="keyword">in</span> (-<span class="number">1</span>, <span class="number">0</span>):</span><br><span class="line">    ckpt = <span class="variable language_">self</span>.trainer.best <span class="keyword">if</span> <span class="variable language_">self</span>.trainer.best.exists() <span class="keyword">else</span> <span class="variable language_">self</span>.trainer.last</span><br><span class="line">    <span class="variable language_">self</span>.model, _ = attempt_load_one_weight(ckpt)</span><br><span class="line">    <span class="variable language_">self</span>.overrides = <span class="variable language_">self</span>.model.args</span><br><span class="line">    <span class="variable language_">self</span>.metrics = <span class="built_in">getattr</span>(<span class="variable language_">self</span>.trainer.validator, <span class="string">&quot;metrics&quot;</span>, <span class="literal">None</span>)  <span class="comment"># <span class="doctag">TODO:</span> DDP 模式下没有返回指标</span></span><br><span class="line"><span class="keyword">return</span> <span class="variable language_">self</span>.metrics</span><br></pre></td></tr></table></figure><hr><h3 id="3-3-模型的网络结构打印"><a href="#3-3-模型的网络结构打印" class="headerlink" title="3.3 模型的网络结构打印"></a>3.3 模型的网络结构打印</h3><p>第三步比较重要的就是来到了’ultralytics&#x2F;nn&#x2F;tasks.py’（就是我们改进模型时候的那个文件）文化内的class DetectionModel(BaseModel):类中进行初始化和模型的定义工作。</p><p>这里涉及到了模型的定义和校验工作（在模型的正式开始训练之前检测模型是否能够运行的工作！）。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DetectionModel</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;YOLOv8 目标检测模型。&quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg=<span class="string">&quot;yolov8n.yaml&quot;</span>, ch=<span class="number">3</span>, nc=<span class="literal">None</span>, verbose=<span class="literal">True</span></span>):  <span class="comment"># model, input channels, number of classes</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;使用给定的配置和参数初始化 YOLOv8 目标检测模型。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.yaml = cfg <span class="keyword">if</span> <span class="built_in">isinstance</span>(cfg, <span class="built_in">dict</span>) <span class="keyword">else</span> yaml_model_load(cfg)  <span class="comment"># cfg 字典</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 定义模型</span></span><br><span class="line">        ch = <span class="variable language_">self</span>.yaml[<span class="string">&quot;ch&quot;</span>] = <span class="variable language_">self</span>.yaml.get(<span class="string">&quot;ch&quot;</span>, ch)  <span class="comment"># 输入通道数</span></span><br><span class="line">        <span class="keyword">if</span> nc <span class="keyword">and</span> nc != <span class="variable language_">self</span>.yaml[<span class="string">&quot;nc&quot;</span>]:</span><br><span class="line">            LOGGER.info(<span class="string">f&quot;覆盖 model.yaml nc=<span class="subst">&#123;self.yaml[<span class="string">&#x27;nc&#x27;</span>]&#125;</span> 为 nc=<span class="subst">&#123;nc&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="variable language_">self</span>.yaml[<span class="string">&quot;nc&quot;</span>] = nc  <span class="comment"># 覆盖 YAML 中的值</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; 此处最为重要，涉及到了我们修改模型的配置的那个函数parse_model,</span></span><br><span class="line"><span class="string">            这里返回了我们的每一个模块的定义，也就是self.model保存了我们的ymal文件所有模块的实例化模型</span></span><br><span class="line"><span class="string">            self.save保存列表 | 也就是除了from部分为-1的部分比如from为4那么就将第四层的索引保存这里留着后面备用，</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.model, <span class="variable language_">self</span>.save = parse_model(deepcopy(<span class="variable language_">self</span>.yaml), ch=ch, verbose=verbose)  <span class="comment"># 模型，保存列表</span></span><br><span class="line">        <span class="variable language_">self</span>.names = &#123;i: <span class="string">f&quot;<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.yaml[<span class="string">&quot;nc&quot;</span>])&#125;  <span class="comment"># 默认名称字典</span></span><br><span class="line">        <span class="variable language_">self</span>.inplace = <span class="variable language_">self</span>.yaml.get(<span class="string">&quot;inplace&quot;</span>, <span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 构建步长</span></span><br><span class="line">        m = <span class="variable language_">self</span>.model[-<span class="number">1</span>]  <span class="comment"># Detect()</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, (Detect, Segment, Pose, Detect_AFPN4, Detect_AFPN3, Detect_ASFF, Detect_FRM, Detect_dyhead,</span><br><span class="line">                          CLLAHead, Detect_dyhead3, Detect_DySnakeConv, Segment_DySnakeConv,</span><br><span class="line">                          Segment_DBB, Detect_DBB, Pose_DBB, OBB, Detect_FASFF)):</span><br><span class="line">            s = <span class="number">640</span>  <span class="comment"># 2x 最小步长</span></span><br><span class="line">            m.inplace = <span class="variable language_">self</span>.inplace</span><br><span class="line">            forward = <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.forward(x)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, (Segment, Segment_DySnakeConv, Pose, Pose_DBB, Segment_DBB, OBB)) <span class="keyword">else</span> <span class="variable language_">self</span>.forward(x)</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                m.stride = torch.tensor([s / x.shape[-<span class="number">2</span>] <span class="keyword">for</span> x <span class="keyword">in</span> forward(torch.zeros(<span class="number">1</span>, ch, s, s))])  <span class="comment"># 在 CPU 上进行前向传播</span></span><br><span class="line">            <span class="keyword">except</span> RuntimeError:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="variable language_">self</span>.model.to(torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">                    m.stride = torch.tensor([s / x.shape[-<span class="number">2</span>] <span class="keyword">for</span> x <span class="keyword">in</span> forward(</span><br><span class="line">                        torch.zeros(<span class="number">1</span>, ch, s, s).to(torch.device(<span class="string">&#x27;cuda&#x27;</span>)))])  <span class="comment"># 在 CUDA 上进行前向传播</span></span><br><span class="line">                <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> error:</span><br><span class="line">                    <span class="keyword">raise</span> error</span><br><span class="line">            <span class="variable language_">self</span>.stride = m.stride</span><br><span class="line">            m.bias_init()  <span class="comment"># 仅运行一次</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.stride = torch.Tensor([<span class="number">32</span>])  <span class="comment"># 默认步长，例如 RTDETR</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 初始化权重和偏置</span></span><br><span class="line">        initialize_weights(<span class="variable language_">self</span>)</span><br><span class="line">        <span class="keyword">if</span> verbose: <span class="comment"># 此处为获取模型参数量和打印的地方。</span></span><br><span class="line">            <span class="variable language_">self</span>.info()</span><br><span class="line">            LOGGER.info(<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="3-4-parse-model的解析"><a href="#3-4-parse-model的解析" class="headerlink" title="3.4 parse_model的解析"></a>3.4 parse_model的解析</h3><p>这里涉及到yaml文件中模块的定义和，通道数放缩的地方，此处大家可以仔细看看比较重要涉及到模块的改动。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse_model</span>(<span class="params">d, ch, verbose=<span class="literal">True</span></span>):  <span class="comment"># model_dict, input_channels(3)</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;解析 YOLO 模型.yaml 字典为 PyTorch 模型。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">import</span> ast</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 参数设置</span></span><br><span class="line">    max_channels = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>) <span class="comment"># 设置一个最大的通道数inf,防止后面的通道数有的超出了范围，没什么作用其实。</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下面一行代码比较重要，为获取我们yaml文件中的参数,nc=类别数（前面解释过了） act=激活函数， scales=模型的大小&quot;&quot;&quot;</span></span><br><span class="line">    nc, act, scales = (d.get(x) <span class="keyword">for</span> x <span class="keyword">in</span> (<span class="string">&quot;nc&quot;</span>, <span class="string">&quot;activation&quot;</span>, <span class="string">&quot;scales&quot;</span>))</span><br><span class="line">    <span class="string">&quot;&quot;&quot;此处为获取模型的通道数放缩比例假如  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;那么此处对应的就是 0.33 , 0.25, 1024&quot;&quot;&quot;</span></span><br><span class="line">    depth, width, kpt_shape = (d.get(x, <span class="number">1.0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> (<span class="string">&quot;depth_multiple&quot;</span>, <span class="string">&quot;width_multiple&quot;</span>, <span class="string">&quot;kpt_shape&quot;</span>))</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下面这个判断主要的功能就是我们指定yaml文件的时候如果不指定n或者其它模型尺度则默认用n然后提出一个警告，细心的读者应该会遇到过这个警告，群里也有人问过&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> scales:</span><br><span class="line">        scale = d.get(<span class="string">&quot;scale&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> scale:</span><br><span class="line">            scale = <span class="built_in">tuple</span>(scales.keys())[<span class="number">0</span>]</span><br><span class="line">            LOGGER.warning(<span class="string">f&quot;WARNING ⚠️ 没有传递模型比例。假定 scale=&#x27;<span class="subst">&#123;scale&#125;</span>&#x27;。&quot;</span>)</span><br><span class="line">        depth, width, max_channels = scales[scale]</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> act:</span><br><span class="line">        Conv.default_act = <span class="built_in">eval</span>(act)  <span class="comment"># 重新定义默认激活函数，例如 Conv.default_act = nn.SiLU()</span></span><br><span class="line">        <span class="keyword">if</span> verbose:</span><br><span class="line">            LOGGER.info(<span class="string">f&quot;<span class="subst">&#123;colorstr(<span class="string">&#x27;activation:&#x27;</span>)&#125;</span> <span class="subst">&#123;act&#125;</span>&quot;</span>)  <span class="comment"># 打印</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> verbose:</span><br><span class="line">        LOGGER.info(<span class="string">f&quot;\n<span class="subst">&#123;<span class="string">&#x27;&#x27;</span>:&gt;<span class="number">3</span>&#125;</span><span class="subst">&#123;<span class="string">&#x27;from&#x27;</span>:&gt;<span class="number">20</span>&#125;</span><span class="subst">&#123;<span class="string">&#x27;n&#x27;</span>:&gt;<span class="number">3</span>&#125;</span><span class="subst">&#123;<span class="string">&#x27;params&#x27;</span>:&gt;<span class="number">10</span>&#125;</span>  <span class="subst">&#123;<span class="string">&#x27;module&#x27;</span>:&lt;<span class="number">45</span>&#125;</span><span class="subst">&#123;<span class="string">&#x27;arguments&#x27;</span>:&lt;<span class="number">30</span>&#125;</span>&quot;</span>)</span><br><span class="line">    ch = [ch] <span class="comment"># 存放第一个输入的通道数,这个ch后面会存放所有层的通道数，第一层为通道数是ch=3也就是对应我们一张图片的RGB图片的三基色三个通道，分别对应红绿蓝！</span></span><br><span class="line">    layers, save, c2 = [], [], ch[-<span class="number">1</span>]  <span class="comment"># 提前定义一些之后存放的容器分别为，模型层，保存列表，输出通道数</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下面开始正式解析模型的yaml文件然后进行定义的操作用for训练便利yaml文件&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i, (f, n, m, args) <span class="keyword">in</span> <span class="built_in">enumerate</span>(d[<span class="string">&quot;backbone&quot;</span>] + d[<span class="string">&quot;head&quot;</span>]):  <span class="comment"># from, number, module, args</span></span><br><span class="line">        m = <span class="built_in">getattr</span>(torch.nn, m[<span class="number">3</span>:]) <span class="keyword">if</span> <span class="string">&quot;nn.&quot;</span> <span class="keyword">in</span> m <span class="keyword">else</span> <span class="built_in">globals</span>()[m]  <span class="comment"># 获取模块</span></span><br><span class="line">        <span class="keyword">for</span> j, a <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(a, <span class="built_in">str</span>):</span><br><span class="line">                <span class="keyword">with</span> contextlib.suppress(ValueError):</span><br><span class="line">                    args[j] = <span class="built_in">locals</span>()[a] <span class="keyword">if</span> a <span class="keyword">in</span> <span class="built_in">locals</span>() <span class="keyword">else</span> ast.literal_eval(a)</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 此处为repeat那个参数的放缩操作,不过多解释了,最小的n是1（就是是说你yaml文件里定义的是3，然后和放缩系数相乘然后和1比那个小取那个）&quot;&quot;&quot;</span></span><br><span class="line">        n = n_ = <span class="built_in">max</span>(<span class="built_in">round</span>(n * depth), <span class="number">1</span>) <span class="keyword">if</span> n &gt; <span class="number">1</span> <span class="keyword">else</span> n</span><br><span class="line">        <span class="string">&quot;&quot;&quot;下面是一些具体模块的定义操作了&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> m <span class="keyword">in</span> (Classify, Conv, ConvTranspose, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, Focus,</span><br><span class="line">                 BottleneckCSP, C1, C2, C2f, C2fAttn, C3, C3TR, C3Ghost, nn.ConvTranspose2d, DWConvTranspose2d, C3x, RepC3):</span><br><span class="line">            c1, c2 = ch[f], args[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> c2 != nc:  <span class="comment"># 如果 c2 不等于类别数（即 Classify() 输出）</span></span><br><span class="line">                <span class="string">&quot;&quot;&quot; 绝大多数情况下都不等，我们放缩通道数，也就是为什么不同大小的模型参数量不一致的地方因为参数量主要由通道数决定，GFLOPs主要有图像的宽和高决定&quot;&quot;&quot;</span></span><br><span class="line">                c2 = make_divisible(<span class="built_in">min</span>(c2, max_channels) * width, <span class="number">8</span>)</span><br><span class="line">            <span class="keyword">if</span> m <span class="keyword">is</span> C2fAttn:</span><br><span class="line">                args[<span class="number">1</span>] = make_divisible(<span class="built_in">min</span>(args[<span class="number">1</span>], max_channels // <span class="number">2</span>) * width, <span class="number">8</span>)  <span class="comment"># 嵌入通道数</span></span><br><span class="line">                args[<span class="number">2</span>] = <span class="built_in">int</span>(</span><br><span class="line">                    <span class="built_in">max</span>(<span class="built_in">round</span>(<span class="built_in">min</span>(args[<span class="number">2</span>], max_channels // <span class="number">2</span> // <span class="number">32</span>)) * width, <span class="number">1</span>) <span class="keyword">if</span> args[<span class="number">2</span>] &gt; <span class="number">1</span> <span class="keyword">else</span> args[<span class="number">2</span>]</span><br><span class="line">                )  <span class="comment"># 头部数量</span></span><br><span class="line">            <span class="string">&quot;&quot;&quot;此处需要解释一下，大家需要仔细注意此处&quot;&quot;&quot;</span></span><br><span class="line">            <span class="string">&quot;&quot;&quot; 这个args就是传入到我们模型的参数,C1就是上一层的或者指定层的输出的通道数，C2就是本层的输出通道数， *args[1:]就是其它的一些参数比如卷积核步长什么的&quot;&quot;&quot;</span></span><br><span class="line">            <span class="string">&quot;&quot;&quot; 此处和注意力机制不同的是，为什么注意力机制不在此处添加因为注意力机制不改变模型的维度，所以一般只需要指定一个输入通道数就行，</span></span><br><span class="line"><span class="string">                所以这也是为什么我们在后面定义注意力需要额外添加代码的原因有兴趣的读者可以对比一下&quot;&quot;&quot;</span></span><br><span class="line">            args = [c1, c2, *args[<span class="number">1</span>:]]</span><br><span class="line">            <span class="string">&quot;&quot;&quot; 此处就是涉及的上面求出的实际的n然后插入的参数列表中去，然后准备在最下面进行传参&quot;&quot;&quot;</span></span><br><span class="line">            <span class="keyword">if</span> m <span class="keyword">in</span> (BottleneckCSP, C1, C2, C2f, C2fAttn, C3, C3TR, C3Ghost, C3x, RepC3):</span><br><span class="line">                args.insert(<span class="number">2</span>, n)  <span class="comment"># 重复次数</span></span><br><span class="line">                n = <span class="number">1</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;这些都是一些具体的模块定义的方法，不多解释了&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> m <span class="keyword">is</span> AIFI:</span><br><span class="line">            args = [ch[f], *args]</span><br><span class="line">        <span class="keyword">elif</span> m <span class="keyword">in</span> (HGStem, HGBlock):</span><br><span class="line">            c1, cm, c2 = ch[f], args[<span class="number">0</span>], args[<span class="number">1</span>]</span><br><span class="line">            args = [c1, cm, c2, *args[<span class="number">2</span>:]]</span><br><span class="line">            <span class="keyword">if</span> m <span class="keyword">is</span> HGBlock:</span><br><span class="line">                args.insert(<span class="number">4</span>, n)  <span class="comment"># 重复次数</span></span><br><span class="line">                n = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> m <span class="keyword">is</span> ResNetLayer:</span><br><span class="line">            c2 = args[<span class="number">1</span>] <span class="keyword">if</span> args[<span class="number">3</span>] <span class="keyword">else</span> args[<span class="number">1</span>] * <span class="number">4</span></span><br><span class="line">        <span class="keyword">elif</span> m <span class="keyword">is</span> nn.BatchNorm2d:</span><br><span class="line">            args = [ch[f]]</span><br><span class="line">        <span class="keyword">elif</span> m <span class="keyword">is</span> Concat:</span><br><span class="line">            c2 = <span class="built_in">sum</span>(ch[x] <span class="keyword">for</span> x <span class="keyword">in</span> f)</span><br><span class="line">        <span class="keyword">elif</span> m <span class="keyword">in</span> (Detect, WorldDetect, Segment, Pose, OBB, ImagePoolingAttn):</span><br><span class="line">            args.append([ch[x] <span class="keyword">for</span> x <span class="keyword">in</span> f])</span><br><span class="line">            <span class="keyword">if</span> m <span class="keyword">is</span> Segment:</span><br><span class="line">                args[<span class="number">2</span>] = make_divisible(<span class="built_in">min</span>(args[<span class="number">2</span>], max_channels) * width, <span class="number">8</span>)</span><br><span class="line">        <span class="keyword">elif</span> m <span class="keyword">is</span> RTDETRDecoder:  <span class="comment"># 特殊情况，channels 参数必须在索引 1 中传递</span></span><br><span class="line">            args.insert(<span class="number">1</span>, [ch[x] <span class="keyword">for</span> x <span class="keyword">in</span> f])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            c2 = ch[f]</span><br><span class="line">        <span class="string">&quot;&quot;&quot;此处就是模型的正式定义和传参的操作&quot;&quot;&quot;</span></span><br><span class="line">        m_ = nn.Sequential(*(m(*args) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n))) <span class="keyword">if</span> n &gt; <span class="number">1</span> <span class="keyword">else</span> m(*args)  <span class="comment"># 模块</span></span><br><span class="line">        t = <span class="built_in">str</span>(m)[<span class="number">8</span>:-<span class="number">2</span>].replace(<span class="string">&quot;__main__.&quot;</span>, <span class="string">&quot;&quot;</span>)  <span class="comment"># 模块类型</span></span><br><span class="line">        m.np = <span class="built_in">sum</span>(x.numel() <span class="keyword">for</span> x <span class="keyword">in</span> m_.parameters())  <span class="comment"># 参数数量</span></span><br><span class="line">        m_.i, m_.f, m_.<span class="built_in">type</span> = i, f, t  <span class="comment"># 附加索引，&#x27;from&#x27; 索引，类型</span></span><br><span class="line">        <span class="keyword">if</span> verbose:</span><br><span class="line">            LOGGER.info(<span class="string">f&quot;<span class="subst">&#123;i:&gt;<span class="number">3</span>&#125;</span><span class="subst">&#123;<span class="built_in">str</span>(f):&gt;<span class="number">20</span>&#125;</span><span class="subst">&#123;n_:&gt;<span class="number">3</span>&#125;</span><span class="subst">&#123;m.np:<span class="number">10.0</span>f&#125;</span>  <span class="subst">&#123;t:&lt;<span class="number">45</span>&#125;</span><span class="subst">&#123;<span class="built_in">str</span>(args):&lt;<span class="number">30</span>&#125;</span>&quot;</span>)  <span class="comment"># 打印</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;此处就是保存一些索引通道数涉及到from的部分，此处文字很难解释的清楚有兴趣可以自己debug看一下就明白了&quot;&quot;&quot;</span></span><br><span class="line">        save.extend(x % i <span class="keyword">for</span> x <span class="keyword">in</span> ([f] <span class="keyword">if</span> <span class="built_in">isinstance</span>(f, <span class="built_in">int</span>) <span class="keyword">else</span> f) <span class="keyword">if</span> x != -<span class="number">1</span>)  <span class="comment"># 添加到保存列表</span></span><br><span class="line">        layers.append(m_)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            ch = []</span><br><span class="line">        ch.append(c2)</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers), <span class="built_in">sorted</span>(save)</span><br></pre></td></tr></table></figure><hr><h2 id="四、模型的结构打印"><a href="#四、模型的结构打印" class="headerlink" title="四、模型的结构打印"></a>四、模型的结构打印</h2><p>经过上面的分析之后，我们就会打印了模型的结构，图片如下所示，然后到此本篇文章的分析就到这里了，剩下的下一篇文章讲解。</p><p><strong>（需要注意的是上面的讲解整体是按照顺序但是是以递归的形式介绍，比如3.2是3.1当中的某一行代码的功能而不是结束之后才允许的3.2，而是3.1运行的过程中运行了3.2。）</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/d3d9d7580362433ba08904db10be9ea4.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/d3d9d7580362433ba08904db10be9ea4.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="d3d9d7580362433ba08904db10be9ea4.png"></p>]]></content>
      
      
      <categories>
          
          <category> 写作 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>说明文档</title>
      <link href="/post/shuo-ming-wen-dang/"/>
      <url>/post/shuo-ming-wen-dang/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这篇文档主要介绍《基于YOLOv8的农田病虫害检测与分析》的代码实现部分，整篇论文的目的主要是改进YOLOv8的网络结构，使其在检测病虫害的精度和实时性上有所提升。接下来，我将介绍如何从零开始搭建起本项目。</p></blockquote><h1 id="安装Python"><a href="#安装Python" class="headerlink" title="安装Python"></a>安装Python</h1><p>到python的官方网站：<a href="https://www.python.org/">https://www.python.org/</a>下载，安装</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-10-42.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-10-42.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>安装完成后，在命令行窗口运行：python，查看安装的结果，如下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-14-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-14-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>至此，Python安装完成，接下来还需要安装anaconda，这是一个python虚拟环境，特别适合管理python的环境。</p><h1 id="安装anaconda"><a href="#安装anaconda" class="headerlink" title="安装anaconda"></a>安装anaconda</h1><p>到anaconda的官方网站：<a href="https://www.anaconda.com/download/success">https://www.anaconda.com/download/success</a>下载，并安装：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-17-10.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-17-10.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>安装成功后，会在开始菜单出现如下图所示：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-19-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-19-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>anaconda安装完成，接下来安装pycharm，主要用来编写代码。</p><h1 id="安装Pycharm"><a href="#安装Pycharm" class="headerlink" title="安装Pycharm"></a>安装Pycharm</h1><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-23-47.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-23-47.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>学生可以申请教育版</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-24-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-24-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>支持，所有的软件安装完成。</p><h1 id="YOLOv8目录结构介绍"><a href="#YOLOv8目录结构介绍" class="headerlink" title="YOLOv8目录结构介绍"></a>YOLOv8目录结构介绍</h1><p>首先介绍整个项目的目录：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-27-47.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-27-47.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-28-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-28-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>和原来的YOLOv8相比，根目录新增一些训练的脚本和测试的脚本，比如train.py和Detect.py，当然也可以直接通过命令行的方式来实现，两者效果都是一样的。</p><blockquote><p><strong>重点是ultralytics&#x2F;nn目录，所有的改进模块都是在这里进行，在这里我新建了一个Addmodules的目录，里面是改进的各种模块，包括主干网络，颈部网络和检测头的改进。</strong></p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-36-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-36-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>需要修改的部分我都已经作了修改，不用再做其他的改动</p><blockquote><p><strong>还有一个重要的目录：ultralytics&#x2F;cfg&#x2F;models&#x2F;Add，这里面放的都是yaml文件，其中改进的yaml文件都已经写好，不需要改动。</strong></p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-38-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-38-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>以下是一个yaml文件的示例，其它的都是类似的结构，只是参数不同：</p><h1 id="安装项目的环境（非常重要）"><a href="#安装项目的环境（非常重要）" class="headerlink" title="安装项目的环境（非常重要）"></a>安装项目的环境（非常重要）</h1><blockquote><p>环境配置非常重要，我当时配环境换了一周左右的时间，中间经历了各种报错，软件包不兼容的问题和显卡驱动匹配的问题，总之就是不好搞。为了方面复现工作，我已经把anaconda的环境导出为environment.yml，位于项目的根目录里面，创建虚拟环境的时候直接使用就可以</p></blockquote><h2 id="anaconda虚拟环境"><a href="#anaconda虚拟环境" class="headerlink" title="anaconda虚拟环境"></a>anaconda虚拟环境</h2><p>再anaconda prompt终端输入conda env create -f environment.yml，就可以根据environment.yml文件创建虚拟环境，创建好后，通过conda env list查看环境是否存在，如下图所示就表明创建成功：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_16-35-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_16-35-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>如果安装的时候出现torch相关的错误，大概率是你的显卡驱动和这里面的torch包版本不匹配，这个问题需要自行修改即可，网上关于这方面的资料很多。</p><h2 id="使用虚拟环境"><a href="#使用虚拟环境" class="headerlink" title="使用虚拟环境"></a>使用虚拟环境</h2><p>虚拟环境创建完成之后，就可以在pycharm中使用，点击右下角，切换conda环境，选择刚才创建的虚拟环境。如果到了这一步还没有报错的话，恭喜你，已经完成了80%的工作。</p><p>运行Detect.py脚本，测试检测效果，如果没有报错，接下来就是训练模型。</p><h1 id="训练脚本train-py"><a href="#训练脚本train-py" class="headerlink" title="训练脚本train.py"></a>训练脚本train.py</h1><p>找到根目录的train.py文件，注释已经写的很清楚，如下图：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = YOLO(<span class="string">&#x27;yolov8-HSFPN.yaml&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model.load(&#x27;yolov8n.pt&#x27;) # 是否加载预训练权重,科研不建议大家加载否则很难提升精度</span></span><br><span class="line"></span><br><span class="line">    model.train(data=<span class="string">r&#x27;D:/Downloads/YOLOv8/datasets/data.yaml&#x27;</span>,</span><br><span class="line">                <span class="comment"># 如果大家任务是其它的&#x27;ultralytics/cfg/default.yaml&#x27;找到这里修改task可以改成detect, segment, classify, pose</span></span><br><span class="line">                cache=<span class="literal">False</span>,</span><br><span class="line">                imgsz=<span class="number">640</span>,</span><br><span class="line">                epochs=<span class="number">150</span>,</span><br><span class="line">                single_cls=<span class="literal">False</span>,  <span class="comment"># 是否是单类别检测</span></span><br><span class="line">                batch=<span class="number">4</span>,</span><br><span class="line">                close_mosaic=<span class="number">10</span>,</span><br><span class="line">                workers=<span class="number">0</span>,</span><br><span class="line">                device=<span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">                optimizer=<span class="string">&#x27;SGD&#x27;</span>, <span class="comment"># using SGD</span></span><br><span class="line">                <span class="comment"># resume=&#x27;runs/train/exp21/weights/last.pt&#x27;, # 如过想续训就设置last.pt的地址</span></span><br><span class="line">                amp=<span class="literal">True</span>,  <span class="comment"># 如果出现训练损失为Nan可以关闭amp</span></span><br><span class="line">                project=<span class="string">&#x27;runs/train&#x27;</span>,</span><br><span class="line">                name=<span class="string">&#x27;exp&#x27;</span>,</span><br><span class="line">                )</span><br></pre></td></tr></table></figure><p>model &#x3D; YOLO(‘yolov8-HSFPN.yaml’)，把里面的yaml文件换成自己的yaml文件，我这里用的是yolov8-HSFPN.yaml，data&#x3D;r’D:&#x2F;Downloads&#x2F;YOLOv8&#x2F;datasets&#x2F;data.yaml，同理，换成自己数据集的yaml文件，我这里的数据集是yolo格式。其它的参数可以按照自己的任务自行调整。</p><p>还有一个检测的脚本，Detect.py:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = YOLO(<span class="string">&#x27;D:/Downloads/YOLOv8/result/result_8_HSFPN/train/exp/weights/best.pt&#x27;</span>) <span class="comment"># select your model.pt path</span></span><br><span class="line">    model.predict(source=<span class="string">&#x27;D:/Downloads/YOLOv8/ultralytics/assets&#x27;</span>,</span><br><span class="line">                  imgsz=<span class="number">640</span>,</span><br><span class="line">                  project=<span class="string">&#x27;runs/detect&#x27;</span>,</span><br><span class="line">                  name=<span class="string">&#x27;exp&#x27;</span>,</span><br><span class="line">                  save=<span class="literal">True</span>,</span><br><span class="line">                )</span><br></pre></td></tr></table></figure><p>同理，把best.pt换成你自己训练好的模型，source里面输入检测图片的路径，运行该脚本就可以开始检测，结果保存在runs&#x2F;detect目录。</p><h1 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h1><p>准备好数据集，最好是yolo格式的，我的数据集项目里自带了，不需要重新下载：</p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-55-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-55-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" style="zoom:67%;" /><p>datasets目录里面就是我的数据集：有train，test，valid三个目录，分别存放训练集，测试集和验证集的图像和标签：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-58-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-58-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-58-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-58-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>准备这些之后，运行train.py文件，开始训练。如果报错的话，请自行上网查找，无非就是找不到数据集，某个包的版本不对，或者是GPU用不了，只能用CPU。</p><h1 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h1><blockquote><p>训练结果会保存在runs&#x2F;train目录下，exp1,exp2,exp3的顺序，表示每一次的训练结果。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_16-04-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_16-04-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>上图就是训练完成后目录的结构，weights目录里面就是我们需要的模型：best.pts是效果最好的，最后也是需要这个，last.pt是最后一次的训练结果。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_16-05-47.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_16-05-47.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><strong>整个项目的改进工作我已经做好，复现的话只需装好对应的环境，修改train.py的参数，运行train.py就可以开始训练；修改Detect.py的参数，就可以检测。目前项目只针对检测任务，对于分割和分类没有做改进。</strong></p><h1 id="经验之谈"><a href="#经验之谈" class="headerlink" title="经验之谈"></a>经验之谈</h1><p><strong>（1）以下为两个重要库的版本，必须对应下载，否则会报错</strong></p><blockquote><p>python &#x3D;&#x3D; 3.9.7<br>pytorch &#x3D;&#x3D; 1.12.1<br>timm &#x3D;&#x3D; 0.9.12  # 此安装包必须要<br>mmcv-full &#x3D;&#x3D; 1.6.2  # 不安装此包部分关于dyhead的代码运行不了以及Gold-YOLO</p></blockquote><p><strong>（2）mmcv-full会安装失败是因为自身系统的编译工具有问题，也有可能是环境之间安装的有冲突</strong></p><pre><code>推荐大家离线安装的形式,下面的地址中大家可以找找自己的版本,下载到本地进行安装。https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.htmlhttps://download.openmmlab.com/mmcv/dist/index.html</code></pre><p><strong>（3）basicsr安装失败原因,通过pip install basicsr 下载如果失败,大家可以去百度搜一下如何换下载镜像源就可以修复</strong></p><h2 id="针对一些报错的解决办法在这里说一下"><a href="#针对一些报错的解决办法在这里说一下" class="headerlink" title="针对一些报错的解决办法在这里说一下"></a>针对一些报错的解决办法在这里说一下</h2><p><strong>(1)训练过程中loss出现Nan值.</strong><br>   可以尝试关闭AMP混合精度训练.</p><p><strong>(2)多卡训练问题,修改模型以后不能支持多卡训练可以尝试下面的两行命令行操作，两个是不同的操作，是代表不同的版本现尝试第一个不行用第二个</strong></p><pre><code>python -m torch.distributed.run --nproc_per_node 2 train.pypython -m torch.distributed.launch --nproc_per_node 2 train.py</code></pre><p><strong>(3) 针对运行过程中的一些报错解决</strong><br>    1.如果训练的过程中验证报错了(主要是一些形状不匹配的错误这是因为验证集的一些特殊图片导致)<br>    找到ultralytics&#x2F;models&#x2F;yolo&#x2F;detect&#x2F;train.py的DetectionTrainer class中的build_dataset函数中的rect&#x3D;mode &#x3D;&#x3D; ‘val’改为rect&#x3D;False</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2.</span>推理的时候运行detect.py文件报了形状不匹配的错误</span><br><span class="line">找到ultralytics/engine/predictor.py找到函数<span class="keyword">def</span> <span class="title function_">pre_transform</span>(<span class="params">self, im</span>),在LetterBox中的auto改为<span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="number">3.</span>训练的过程中报错类型不匹配的问题</span><br><span class="line">找到<span class="string">&#x27;ultralytics/engine/validator.py&#x27;</span>文件找到 <span class="string">&#x27;class BaseValidator:&#x27;</span> 然后在其<span class="string">&#x27;__call__&#x27;</span>中</span><br><span class="line"><span class="variable language_">self</span>.args.half = <span class="variable language_">self</span>.device.<span class="built_in">type</span> != <span class="string">&#x27;cpu&#x27;</span>  <span class="comment"># force FP16 val during training的一行代码下面加上self.args.half = False</span></span><br></pre></td></tr></table></figure><p><strong>(4) 针对yaml文件中的nc修改</strong><br>    不用修改，模型会自动根据你数据集的配置文件获取。<br>    这也是模型打印两次的区别，第一次打印出来的就是你选择模型的yaml文件结构，第二次打印的就是替换了你数据集的yaml文件，模型使用的是第二种。</p><p><strong>(5) 针对环境的问题</strong><br>    环境的问题每个人遇见的都不一样，可自行上网查找。</p>]]></content>
      
      
      <categories>
          
          <category> YOLO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>评估</title>
      <link href="/post/article-11/"/>
      <url>/post/article-11/</url>
      
        <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>这篇博客，<strong>主要给大家讲解我们在训练yolov8时生成的结果文件中各个图片及其中指标的含义</strong>，帮助大家更深入的理解，以及我们在评估模型时和发表论文时主要关注的参数有那些。本文通过举例训练过程中的某一时间的结果来帮助大家理解，大家阅读过程中如有任何问题可以在评论区提问出来，我会帮助大家解答。首先我们来看一个在一次训练完成之后都能生成多少个文件如下图所示，下面的文章讲解都会围绕这个结果文件来介绍。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bdf99b744c6646f6a82b2be30e3e9d92.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bdf99b744c6646f6a82b2be30e3e9d92.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><hr><h2 id="二、评估用的数据集"><a href="#二、评估用的数据集" class="headerlink" title="二、评估用的数据集 "></a><strong>二、评估用的数据集</strong> </h2><blockquote><p>上面的训练结果，是根据一个检测飞机的数据集训练得来，其中只有个标签就是飞机，对于这种单标签的数据集，其实我们可以将其理解为一个二分类任务，</p><p><strong>一种情况-&gt;检测为飞机，另一种情况-&gt;不是飞机。</strong></p></blockquote><hr><h2 id="三、结果分析"><a href="#三、结果分析" class="headerlink" title="三、结果分析 "></a>三、结果分析 </h2><p>我们可以从结果文件中看到其中<strong>共有文件24个</strong>，后12张图片是根据我们训练过程中的一些检测结果图片，用于我们可以观察检测结果，有哪些被检测出来了，那些没有被检测出来，其不作为指标评估的文件。         </p><h3 id="Weights文件夹"><a href="#Weights文件夹" class="headerlink" title="Weights文件夹"></a>Weights文件夹</h3><p>我们先从第一个weights文件夹来分析，其中有两个文件，分别是<strong>best.pt、last.pt</strong>,其分别为训练过程中的损失最低的结果和模型训练的最后一次结果保存的模型。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3986c306bb3b4e9893da7f89d2994a88.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3986c306bb3b4e9893da7f89d2994a88.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h3 id="args-yaml"><a href="#args-yaml" class="headerlink" title="args.yaml"></a>args.yaml</h3><p>第二个文件是args.yaml文件,其中主要保存一些我们训练时指定的参数，内容如下所示。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f464e438dd6f4f0a9c52e7246439295c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f464e438dd6f4f0a9c52e7246439295c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h3 id="混淆矩阵-ConfusionMatrix"><a href="#混淆矩阵-ConfusionMatrix" class="headerlink" title="混淆矩阵(ConfusionMatrix)"></a>混淆矩阵(ConfusionMatrix)</h3><p>第三个文件就是混淆矩阵，大家都应该听过这个名字，其是一种用于评估分类模型性能的表格形式。它以实际类别（真实值）和模型预测类别为基础，将样本分类结果进行统计和汇总。</p><blockquote><p>对于二分类问题，混淆矩阵通常是一个2×2的矩阵，包括真阳性（True Positive, TP）、真阴性（True Negative, TN）、假阳性（False Positive, FP）和假阴性（False Negative, FN）四个元素。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ae117a5a660142f3a44b52834fa04ec3.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ae117a5a660142f3a44b52834fa04ec3.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True_Label = [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1 ,0, 1, 0 , 1 , 0, 0 , 1]Predict_Label = [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1 ,0 , 0 , 1 , 0, 0 , 1, 0]</span><br></pre></td></tr></table></figure><p>我们来分析这个图，其每个格子代表的含义我在图片上标注了出来**,下面我们来拿一个例子来帮助大家来理解这个混淆矩阵。**</p><p>假设我们的数据集预测为飞机标记为数字0、预测不为飞机标记为1，<strong>现在假设我们在模型的训练的某一批次种预测了20次其真实结果和预测结果如下所示。</strong> </p><p>其中True_Label代表真实的标签，Predict_Label代表我们用模型预测的标签。</p><p>那么我们可以进行对比产生如下分析</p><blockquote><ul><li>6个样本的真实标签和预测标签都是0（真阴性，True Negative）。</li><li>1个样本的真实标签是0，但预测标签是1（假阳性，False Positive）。</li><li>8个样本的真实标签是1，但预测标签是0（假阴性，False Negative）。</li><li>5个样本的真实标签和预测标签都是1（真阳性，True Positive）。</li></ul></blockquote><p>下面根据我们的分析结果，我们就能够画出这个预测的混淆矩阵，</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/36c503208f654d06a1ad585e772364a8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/36c503208f654d06a1ad585e772364a8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>由此我们就能得到那一批次的混淆矩阵，<strong>我们的最终结果生成的混淆矩阵可以理解为多个混淆矩阵的统计结果。</strong> </p><h3 id="混淆矩阵归一化-Confusion-Matrix-Normal"><a href="#混淆矩阵归一化-Confusion-Matrix-Normal" class="headerlink" title="混淆矩阵归一化(Confusion Matrix Normal)"></a>混淆矩阵归一化(Confusion Matrix Normal)</h3><p>这个混淆矩阵的归一化，就是对混淆矩阵做了一个归一化处理，对混淆矩阵进行归一化可以将每个单元格的值除以该类别实际样本数，从而得到表示分类准确率的百分比。这种标准化使得我们可以直观地比较类别间的分类准确率，并识别出模型在哪些类别上表现较好或较差。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4642ed3defe146a3b93999ffbd5d5129.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4642ed3defe146a3b93999ffbd5d5129.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p><strong>我们可以看到是对于列进行了归一化处理，0.9 + 0.1 &#x3D; 1，1 + 0 &#x3D; 1。</strong> </p><h3 id="计算mAP、Precision、Recall"><a href="#计算mAP、Precision、Recall" class="headerlink" title="计算mAP、Precision、Recall"></a><strong>计算mAP、Precision、Recall</strong></h3><p>在讲解其它的图片之前我们需要来计算三个比较重要的参数，这是其它图片的基础，这里的计算还是利用上面的某一批次举例的分析结果。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/09e2217c78ab4e6ab49eeb2b8f128fed.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/09e2217c78ab4e6ab49eeb2b8f128fed.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><ol><li><p>精确度（Precision）：<strong>预测为正的样本中有多少是正确的</strong>，Precision &#x3D; TP &#x2F; (TP + FP) &#x3D; 5 &#x2F; (5 + 1) &#x3D; 5&#x2F;6 ≈ 0.833</p></li><li><p>召回率（Recall）：真实为正的样本中有多少被正确预测为正，Recall &#x3D; TP &#x2F; (TP + FN) &#x3D; 5 &#x2F; (5 + 8) ≈ 0.385</p></li><li><p>F1值（F1-Score）：**综合考虑精确度和召回率的指标，**F1 &#x3D; 2 * (Precision * Recall) &#x2F; (Precision + Recall) &#x3D; 2 * (0.833 * 0.385) &#x2F; (0.833 + 0.385) ≈ 0.526</p></li><li><p>准确度（Accuracy）：**所有样本中模型正确预测的比例，**Accuracy &#x3D; (TP + TN) &#x2F; (TP + TN + FP + FN) &#x3D; (5 + 6) &#x2F; (5 + 6 + 1 + 8) ≈ 0.565</p></li><li><p>平均精确度（Average Precision, AP）：**用于计算不同类别的平均精确度，对于二分类问题，AP等于精确度。**AP &#x3D; Precision &#x3D; 0.833</p></li><li><p>平均精确度（Mean Average Precision, mAP）：<strong>多类别问题的平均精确度，对于二分类问题，mAP等于AP（精确度）</strong>，所以mAP &#x3D; AP &#x3D; 0.833</p></li></ol><p>这里需要讲解的主要是AP和MAP如果是多分类的问题，AP和mAP怎么计算，首先我们要知道AP的全称就是Average Precision，平均精度所以我们AP的计算公式如下？</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5a9f270d50ce4bbfb76e800f4553200c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5a9f270d50ce4bbfb76e800f4553200c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>mAP就是Mean Average Precision，计算如下，计算每一个没别的AP进行求平均值处理就是mAP。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a7ef1cd0cb924112acfa07084524a7a8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a7ef1cd0cb924112acfa07084524a7a8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h3 id="F1-Curve"><a href="#F1-Curve" class="headerlink" title="F1_Curve"></a>F1_Curve</h3><p>F1_Curve这个文件，我们点击去的图片的标题是F1-Confidence Curve它显示了在不同分类阈值下的F1值变化情况。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/97f8c2e20dd24d59a954bd43c4644c0f.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/97f8c2e20dd24d59a954bd43c4644c0f.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>我们可以这么理解，先看它的横纵坐标，横坐标是置信度，纵坐标是F1-Score，F1-Score在前面我们以及讲解过了，那什么是置信度？</p><p>**置信度(Confidence)-&gt;**在我们模型的识别过程中会有一个概率，就是模型判定一个物体并不是百分百判定它是属于某一个分类，它会给予它以个概率，Confidence就是我们设置一个阈值，如果超过这个概率那么就确定为某一分类，<strong>假如我模型判定一个物体由0.7的概率属于飞机，此时我们设置的阈值如果为0.7以下那么模型就会输出该物体为飞机，如果我们设置的阈值大于0.7那么模型就不会输出该物体为飞机。</strong></p><p><strong>F1-Confidence Curve就是随着F1-Score随着Confience的逐渐增高而变化的一个曲线。</strong></p><h3 id="Labels"><a href="#Labels" class="headerlink" title="Labels"></a>Labels</h3><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/521ff0b11be64fcbbbd711c3de43ddcd.jpeg" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/521ff0b11be64fcbbbd711c3de43ddcd.jpeg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>Labels图片代表每个检测到的目标的类别和边界框信息。每个目标都由一个矩形边界框和一个类别标签表示，<strong>我们逆时针来看这个图片！！！</strong></p><ol><li>目标类别：该像素点所检测到的目标类别，例如飞机等。</li><li>目标位置：该像素点所检测到的目标在图像中的位置，即该像素点在图像中的坐标。</li><li>目标大小：该像素点所检测到的目标的大小，即该像素点所覆盖的区域的大小。</li><li>其他信息：例如目标的旋转角度等其他相关信息。</li></ol><h3 id="labels-correlogram"><a href="#labels-correlogram" class="headerlink" title="labels_correlogram"></a>labels_correlogram</h3><p>labels_correlogram是一个在**机器学习领域中使用的术语，**它指的是一种图形，<strong>用于显示目标检测算法在训练过程中预测标签之间的相关性</strong>。</p><p>具体来说，labels_correlogram是一张<strong>颜色矩阵图</strong>，它展示了训练集数据标签之间的相关性。它可以帮助我们理解目标检测算法在训练过程中的行为和表现，以及预测标签之间的相互影响。</p><p>通过观察labels_correlogram，我们可以了解到目标检测算法在不同类别之间的区分能力，以及对于不同类别的预测精度。此外，我们还可以通过比较不同算法或不同数据集labels_correlogram，来评估算法的性能和数据集的质量。</p><p>总之，labels_correlogram是一种有用的工具，可以帮助我们更好地理解目标检测算法在训练过程中的行为和表现，以及评估算法的性能和数据集的质量。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0f1e5f82a532423dae3a4e8b897e6165.jpeg" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0f1e5f82a532423dae3a4e8b897e6165.jpeg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h3 id="P-curve"><a href="#P-curve" class="headerlink" title="P_curve"></a>P_curve</h3><p>这个图的分析和F1_Curve一样，不同的是关于的是Precision和Confidence之间的关系，<strong>可以看出我们随着置信度的越来越高检测的准确率按理来说是越来越高的。</strong> </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7ac794c6f34b418c95dfc7951382171c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7ac794c6f34b418c95dfc7951382171c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h3 id="R-curve"><a href="#R-curve" class="headerlink" title="R_curve"></a>R_curve</h3><p>这个图的分析和F1_Curve一样，不同的是关于的是Recall和Confidence之间的关系，<strong>可以看出我们随着置信度的越来越高召回率的准确率按理来说是越来越低的。</strong> </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e72c4546e65d445c9831567e12d55df0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e72c4546e65d445c9831567e12d55df0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h3 id="PR-curve"><a href="#PR-curve" class="headerlink" title="PR_curve"></a>PR_curve</h3><p>它显示了在不同分类阈值下模型的精确度（Precision）和召回率（Recall）之间的关系。</p><p><strong>PR曲线越靠近坐标轴的右上角，模型性能越好，越能够正确识别正样本，正确分类正样本的Precision值越高，而靠近右侧则说明模型对正样本的识别能力较差，即召回能力较差。</strong></p><blockquote><p>PR曲线的特点是随着分类阈值的变化，精确度和召回率会有相应的改变。通常情况下，当分类模型能够同时保持较高的精确度和较高的召回率时，PR曲线处于较高的位置。当模型偏向于高精确度或高召回率时，曲线则相应地向低精确度或低召回率的方向移动。</p><p>PR曲线可以帮助我们评估模型在不同阈值下的性能，并选择适当的阈值来平衡精确度和召回率。对于模型比较或选择，我们可以通过比较PR曲线下方的面积（称为平均精确度均值，Average Precision, AP）来进行定量评估。AP值越大，模型的性能越好。</p><p>总结：PR曲线是一种展示分类模型精确度和召回率之间关系的可视化工具，通过绘制精确度-召回率曲线，我们可以评估和比较模型在不同分类阈值下的性能，并计算平均精确度均值（AP）来定量衡量模型的好坏。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c00378b5866f44978bf907f4b92d6a2c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c00378b5866f44978bf907f4b92d6a2c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h3 id="results-csv"><a href="#results-csv" class="headerlink" title="results.csv"></a>results.csv</h3><p>results.csv记录了一些我们训练过程中的参数信息，包括损失和学习率等，这里没有什么需要理解大家可以看一看，我们后面的results图片就是根据这个文件绘画出来的。</p><h3 id="results"><a href="#results" class="headerlink" title="results"></a><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/9af828676f704aada0b9b18797ba75ce.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/9af828676f704aada0b9b18797ba75ce.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp">results</h3><p>这个图片就是生成结果的最后一个了，我们可以看出其中标注了许多小的图片包括训练过程在的各种损失，我们主要看的其实就是后面的四幅图mAP50、mAP50-95、metrics&#x2F;precision、metrics&#x2F;recall四张图片。 </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0108b195b2e04b46811b44dc9f5f351f.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0108b195b2e04b46811b44dc9f5f351f.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><blockquote><ol><li>mAP50：mAP是mean Average Precision的缩写，表示在多个类别上的平均精度。mAP50表示在50%的IoU阈值下的mAP值。</li><li>mAP50-95：这是一个更严格的评价指标，它计算了在50-95%的IoU阈值范围内的mAP值，然后取平均。这能够更准确地评估模型在不同IoU阈值下的性能。</li><li>metrics&#x2F;precision：精度（Precision）是评估模型预测正确的正样本的比例。在目标检测中，如果模型预测的边界框与真实的边界框重合，则认为预测正确。</li><li>metrics&#x2F;recall：召回率（Recall）是评估模型能够找出所有真实正样本的比例。在目标检测中，如果真实的边界框与预测的边界框重合，则认为该样本被正确召回。</li></ol></blockquote><h3 id="检测效果图"><a href="#检测效果图" class="headerlink" title="检测效果图"></a>检测效果图</h3><p> 最后的十四张图片就是检测效果图了，给大家看一下这里没什么好讲解的了。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7dbceef24f184435b49dd7480b2cc2b3.jpeg" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7dbceef24f184435b49dd7480b2cc2b3.jpeg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><hr><h2 id="四、其它参数"><a href="#四、其它参数" class="headerlink" title="四、其它参数"></a>四、其它参数</h2><p>FPS和IoU是目标检测领域中使用的两个重要指标，分别表示每秒处理的图片数量和交并比。</p><blockquote><ol><li>FPS：全称为Frames Per Second，即每秒帧率。它用于评估模型在给定硬件上的处理速度，即每秒可以处理的图片数量。该指标对于实现实时检测非常重要，因为只有处理速度快，才能满足实时检测的需求。</li><li>IoU：全称为Intersection over Union，表示交并比。在目标检测中，它用于衡量模型生成的候选框与原标记框之间的重叠程度。IoU值越大，表示两个框之间的相似性越高。通常，当IoU值大于0.5时，认为可以检测到目标物体。这个指标常用于评估模型在特定数据集上的检测准确度。</li></ol></blockquote><p>在目标检测领域中，处理速度和准确度是两个重要的性能指标。在实际应用中，我们需要根据具体需求来平衡这两个指标。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 评估 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>绘图</title>
      <link href="/post/article-12/"/>
      <url>/post/article-12/</url>
      
        <content type="html"><![CDATA[<h2 id="一、本文介绍"><a href="#一、本文介绍" class="headerlink" title="一、本文介绍"></a>一、本文介绍</h2><p>本文给大家带来的是<strong>YOLOv8系列的绘图功能</strong>，我将向大家介绍YOLO系列的绘图功能。我们在进行实验时，经常需要比较多个结果，针对这一问题，我写了点代码来解决这个问题，它可以根据训练结果绘制损失(loss)和mAP（平均精度均值）的对比图。这个工具不仅支持多个文件的对比分析，还允许大家在现有代码的基础上进行修，从而达到数据可视化的功能，大家也可以将对比图来放在论文中进行对比也是非常不错的选择。</p><p><strong>先展示一下效果图-&gt;</strong> </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/059f9bb891c3424aaf012d14e370287b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/059f9bb891c3424aaf012d14e370287b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>  </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/059f9bb891c3424aaf012d14e370287b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/059f9bb891c3424aaf012d14e370287b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp">​</p><p><strong>损失对比图象-&gt;</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0b93636a9f29432fadb3f0aeacd3406c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0b93636a9f29432fadb3f0aeacd3406c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp">​</p><hr><h2 id="二、绘图工具核心代码"><a href="#二、绘图工具核心代码" class="headerlink" title="二、绘图工具核心代码 "></a>二、绘图工具核心代码 </h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_metrics_and_loss</span>(<span class="params">experiment_names, metrics_info, loss_info, metrics_subplot_layout, loss_subplot_layout,</span></span><br><span class="line"><span class="params">                          metrics_figure_size=(<span class="params"><span class="number">15</span>, <span class="number">10</span></span>), loss_figure_size=(<span class="params"><span class="number">15</span>, <span class="number">10</span></span>), base_directory=<span class="string">&#x27;runs/train&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># Plot metrics</span></span><br><span class="line">    plt.figure(figsize=metrics_figure_size)</span><br><span class="line">    <span class="keyword">for</span> i, (metric_name, title) <span class="keyword">in</span> <span class="built_in">enumerate</span>(metrics_info):</span><br><span class="line">        plt.subplot(*metrics_subplot_layout, i + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> experiment_names:</span><br><span class="line">            file_path = os.path.join(base_directory, name, <span class="string">&#x27;results.csv&#x27;</span>)</span><br><span class="line">            data = pd.read_csv(file_path)</span><br><span class="line">            column_name = [col <span class="keyword">for</span> col <span class="keyword">in</span> data.columns <span class="keyword">if</span> col.strip() == metric_name][<span class="number">0</span>]</span><br><span class="line">            plt.plot(data[column_name], label=name)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">        plt.title(title)</span><br><span class="line">        plt.legend()</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    metrics_filename = <span class="string">&#x27;metrics_curves.png&#x27;</span></span><br><span class="line">    plt.savefig(metrics_filename)</span><br><span class="line">    plt.show()</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Plot loss</span></span><br><span class="line">    plt.figure(figsize=loss_figure_size)</span><br><span class="line">    <span class="keyword">for</span> i, (loss_name, title) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loss_info):</span><br><span class="line">        plt.subplot(*loss_subplot_layout, i + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> experiment_names:</span><br><span class="line">            file_path = os.path.join(base_directory, name, <span class="string">&#x27;results.csv&#x27;</span>)</span><br><span class="line">            data = pd.read_csv(file_path)</span><br><span class="line">            column_name = [col <span class="keyword">for</span> col <span class="keyword">in</span> data.columns <span class="keyword">if</span> col.strip() == loss_name][<span class="number">0</span>]</span><br><span class="line">            plt.plot(data[column_name], label=name)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">        plt.title(title)</span><br><span class="line">        plt.legend()</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    loss_filename = <span class="string">&#x27;loss_curves.png&#x27;</span></span><br><span class="line">    plt.savefig(loss_filename)</span><br><span class="line">    plt.show()</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> metrics_filename, loss_filename</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># Metrics to plot</span></span><br><span class="line">metrics_info = [</span><br><span class="line">    (<span class="string">&#x27;metrics/precision(B)&#x27;</span>, <span class="string">&#x27;Precision&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;metrics/recall(B)&#x27;</span>, <span class="string">&#x27;Recall&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;metrics/mAP50(B)&#x27;</span>, <span class="string">&#x27;mAP at IoU=0.5&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;metrics/mAP50-95(B)&#x27;</span>, <span class="string">&#x27;mAP for IoU Range 0.5-0.95&#x27;</span>)</span><br><span class="line">]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Loss to plot</span></span><br><span class="line">loss_info = [</span><br><span class="line">    (<span class="string">&#x27;train/box_loss&#x27;</span>, <span class="string">&#x27;Training Box Loss&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;train/cls_loss&#x27;</span>, <span class="string">&#x27;Training Classification Loss&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;train/dfl_loss&#x27;</span>, <span class="string">&#x27;Training DFL Loss&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;val/box_loss&#x27;</span>, <span class="string">&#x27;Validation Box Loss&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;val/cls_loss&#x27;</span>, <span class="string">&#x27;Validation Classification Loss&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;val/dfl_loss&#x27;</span>, <span class="string">&#x27;Validation DFL Loss&#x27;</span>)</span><br><span class="line">]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Plot the metrics and loss from multiple experiments</span></span><br><span class="line">metrics_filename, loss_filename = plot_metrics_and_loss(</span><br><span class="line">    experiment_names=[<span class="string">&#x27;exp294&#x27;</span>, <span class="string">&#x27;exp297&#x27;</span>, <span class="string">&#x27;exp293&#x27;</span>, <span class="string">&#x27;exp291&#x27;</span>, <span class="string">&#x27;exp287&#x27;</span>],</span><br><span class="line">    metrics_info=metrics_info,</span><br><span class="line">    loss_info=loss_info,</span><br><span class="line">    metrics_subplot_layout=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    loss_subplot_layout=(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><hr><h2 id="三、使用讲解"><a href="#三、使用讲解" class="headerlink" title="三、使用讲解 "></a>三、使用讲解 </h2><p>使用方式非常简单，我们首先创建一个文件，将核心代码粘贴进去，其中experiment_names这个参数就代表我们的每个训练结果的名字， 我们只需要修改这个即可，我这里就是五个结果进行对比，修改完成之后大家运行该文件即可。<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e7d9ce0ce8004e178f1386272eb6c319.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e7d9ce0ce8004e178f1386272eb6c319.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp">​</p><h2 id="五、热力图代码"><a href="#五、热力图代码" class="headerlink" title="五、热力图代码 "></a>五、热力图代码 </h2><p>使用方式我会单独更一篇，这个热力图代码的进阶版，这里只是先放一下。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">warnings.simplefilter(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> torch, yaml, cv2, os, shutil</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> trange</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> ultralytics.nn.tasks <span class="keyword">import</span> DetectionModel <span class="keyword">as</span> Model</span><br><span class="line"><span class="keyword">from</span> ultralytics.utils.torch_utils <span class="keyword">import</span> intersect_dicts</span><br><span class="line"><span class="keyword">from</span> ultralytics.utils.ops <span class="keyword">import</span> xywh2xyxy</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam <span class="keyword">import</span> GradCAMPlusPlus, GradCAM, XGradCAM</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam.utils.image <span class="keyword">import</span> show_cam_on_image</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam.activations_and_gradients <span class="keyword">import</span> ActivationsAndGradients</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">letterbox</span>(<span class="params">im, new_shape=(<span class="params"><span class="number">640</span>, <span class="number">640</span></span>), color=(<span class="params"><span class="number">114</span>, <span class="number">114</span>, <span class="number">114</span></span>), auto=<span class="literal">True</span>, scaleFill=<span class="literal">False</span>, scaleup=<span class="literal">True</span>, stride=<span class="number">32</span></span>):</span><br><span class="line">    <span class="comment"># Resize and pad image while meeting stride-multiple constraints</span></span><br><span class="line">    shape = im.shape[:<span class="number">2</span>]  <span class="comment"># current shape [height, width]</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(new_shape, <span class="built_in">int</span>):</span><br><span class="line">        new_shape = (new_shape, new_shape)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Scale ratio (new / old)</span></span><br><span class="line">    r = <span class="built_in">min</span>(new_shape[<span class="number">0</span>] / shape[<span class="number">0</span>], new_shape[<span class="number">1</span>] / shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> scaleup:  <span class="comment"># only scale down, do not scale up (for better val mAP)</span></span><br><span class="line">        r = <span class="built_in">min</span>(r, <span class="number">1.0</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Compute padding</span></span><br><span class="line">    ratio = r, r  <span class="comment"># width, height ratios</span></span><br><span class="line">    new_unpad = <span class="built_in">int</span>(<span class="built_in">round</span>(shape[<span class="number">1</span>] * r)), <span class="built_in">int</span>(<span class="built_in">round</span>(shape[<span class="number">0</span>] * r))</span><br><span class="line">    dw, dh = new_shape[<span class="number">1</span>] - new_unpad[<span class="number">0</span>], new_shape[<span class="number">0</span>] - new_unpad[<span class="number">1</span>]  <span class="comment"># wh padding</span></span><br><span class="line">    <span class="keyword">if</span> auto:  <span class="comment"># minimum rectangle</span></span><br><span class="line">        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  <span class="comment"># wh padding</span></span><br><span class="line">    <span class="keyword">elif</span> scaleFill:  <span class="comment"># stretch</span></span><br><span class="line">        dw, dh = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">        new_unpad = (new_shape[<span class="number">1</span>], new_shape[<span class="number">0</span>])</span><br><span class="line">        ratio = new_shape[<span class="number">1</span>] / shape[<span class="number">1</span>], new_shape[<span class="number">0</span>] / shape[<span class="number">0</span>]  <span class="comment"># width, height ratios</span></span><br><span class="line"> </span><br><span class="line">    dw /= <span class="number">2</span>  <span class="comment"># divide padding into 2 sides</span></span><br><span class="line">    dh /= <span class="number">2</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> shape[::-<span class="number">1</span>] != new_unpad:  <span class="comment"># resize</span></span><br><span class="line">        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)</span><br><span class="line">    top, bottom = <span class="built_in">int</span>(<span class="built_in">round</span>(dh - <span class="number">0.1</span>)), <span class="built_in">int</span>(<span class="built_in">round</span>(dh + <span class="number">0.1</span>))</span><br><span class="line">    left, right = <span class="built_in">int</span>(<span class="built_in">round</span>(dw - <span class="number">0.1</span>)), <span class="built_in">int</span>(<span class="built_in">round</span>(dw + <span class="number">0.1</span>))</span><br><span class="line">    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  <span class="comment"># add border</span></span><br><span class="line">    <span class="keyword">return</span> im, ratio, (dw, dh)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">yolov8_heatmap</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, weight, cfg, device, method, layer, backward_type, conf_threshold, ratio</span>):</span><br><span class="line">        device = torch.device(device)</span><br><span class="line">        ckpt = torch.load(weight)</span><br><span class="line">        model_names = ckpt[<span class="string">&#x27;model&#x27;</span>].names</span><br><span class="line">        csd = ckpt[<span class="string">&#x27;model&#x27;</span>].<span class="built_in">float</span>().state_dict()  <span class="comment"># checkpoint state_dict as FP32</span></span><br><span class="line">        model = Model(cfg, ch=<span class="number">3</span>, nc=<span class="built_in">len</span>(model_names)).to(device)</span><br><span class="line">        csd = intersect_dicts(csd, model.state_dict(), exclude=[<span class="string">&#x27;anchor&#x27;</span>])  <span class="comment"># intersect</span></span><br><span class="line">        model.load_state_dict(csd, strict=<span class="literal">False</span>)  <span class="comment"># load</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Transferred <span class="subst">&#123;<span class="built_in">len</span>(csd)&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(model.state_dict())&#125;</span> items&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        target_layers = [<span class="built_in">eval</span>(layer)]</span><br><span class="line">        method = <span class="built_in">eval</span>(method)</span><br><span class="line"> </span><br><span class="line">        colors = np.random.uniform(<span class="number">0</span>, <span class="number">255</span>, size=(<span class="built_in">len</span>(model_names), <span class="number">3</span>)).astype(np.<span class="built_in">int</span>)</span><br><span class="line">        <span class="variable language_">self</span>.__dict__.update(<span class="built_in">locals</span>())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">post_process</span>(<span class="params">self, result</span>):</span><br><span class="line">        logits_ = result[:, <span class="number">4</span>:]</span><br><span class="line">        boxes_ = result[:, :<span class="number">4</span>]</span><br><span class="line">        <span class="built_in">sorted</span>, indices = torch.sort(logits_.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">0</span>], descending=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.transpose(logits_[<span class="number">0</span>], dim0=<span class="number">0</span>, dim1=<span class="number">1</span>)[indices[<span class="number">0</span>]], torch.transpose(boxes_[<span class="number">0</span>], dim0=<span class="number">0</span>, dim1=<span class="number">1</span>)[indices[<span class="number">0</span>]], xywh2xyxy(torch.transpose(boxes_[<span class="number">0</span>], dim0=<span class="number">0</span>, dim1=<span class="number">1</span>)[indices[<span class="number">0</span>]]).cpu().detach().numpy()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">draw_detections</span>(<span class="params">self, box, color, name, img</span>):</span><br><span class="line">        xmin, ymin, xmax, ymax = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">list</span>(box)))</span><br><span class="line">        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), <span class="built_in">tuple</span>(<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> color), <span class="number">2</span>)</span><br><span class="line">        cv2.putText(img, <span class="built_in">str</span>(name), (xmin, ymin - <span class="number">5</span>), cv2.FONT_HERSHEY_SIMPLEX, <span class="number">0.8</span>, <span class="built_in">tuple</span>(<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> color), <span class="number">2</span>, lineType=cv2.LINE_AA)</span><br><span class="line">        <span class="keyword">return</span> img</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img_path, save_path</span>):</span><br><span class="line">        <span class="comment"># remove dir if exist</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(save_path):</span><br><span class="line">            shutil.rmtree(save_path)</span><br><span class="line">        <span class="comment"># make dir if not exist</span></span><br><span class="line">        os.makedirs(save_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># img process</span></span><br><span class="line">        img = cv2.imread(img_path)</span><br><span class="line">        img = letterbox(img)[<span class="number">0</span>]</span><br><span class="line">        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">        img = np.float32(img) / <span class="number">255.0</span></span><br><span class="line">        tensor = torch.from_numpy(np.transpose(img, axes=[<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])).unsqueeze(<span class="number">0</span>).to(<span class="variable language_">self</span>.device)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># init ActivationsAndGradients</span></span><br><span class="line">        grads = ActivationsAndGradients(<span class="variable language_">self</span>.model, <span class="variable language_">self</span>.target_layers, reshape_transform=<span class="literal">None</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># get ActivationsAndResult</span></span><br><span class="line">        result = grads(tensor)</span><br><span class="line">        activations = grads.activations[<span class="number">0</span>].cpu().detach().numpy()</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># postprocess to yolo output</span></span><br><span class="line">        post_result, pre_post_boxes, post_boxes = <span class="variable language_">self</span>.post_process(result[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> trange(<span class="built_in">int</span>(post_result.size(<span class="number">0</span>) * <span class="variable language_">self</span>.ratio)):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">float</span>(post_result[i].<span class="built_in">max</span>()) &lt; <span class="variable language_">self</span>.conf_threshold:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">            <span class="variable language_">self</span>.model.zero_grad()</span><br><span class="line">            <span class="comment"># get max probability for this prediction</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;class&#x27;</span> <span class="keyword">or</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;all&#x27;</span>:</span><br><span class="line">                score = post_result[i].<span class="built_in">max</span>()</span><br><span class="line">                score.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;box&#x27;</span> <span class="keyword">or</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;all&#x27;</span>:</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    score = pre_post_boxes[i, j]</span><br><span class="line">                    score.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># process heatmap</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;class&#x27;</span>:</span><br><span class="line">                gradients = grads.gradients[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">elif</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;box&#x27;</span>:</span><br><span class="line">                gradients = grads.gradients[<span class="number">0</span>] + grads.gradients[<span class="number">1</span>] + grads.gradients[<span class="number">2</span>] + grads.gradients[<span class="number">3</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                gradients = grads.gradients[<span class="number">0</span>] + grads.gradients[<span class="number">1</span>] + grads.gradients[<span class="number">2</span>] + grads.gradients[<span class="number">3</span>] + grads.gradients[<span class="number">4</span>]</span><br><span class="line">            b, k, u, v = gradients.size()</span><br><span class="line">            weights = <span class="variable language_">self</span>.method.get_cam_weights(<span class="variable language_">self</span>.method, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, activations, gradients.detach().numpy())</span><br><span class="line">            weights = weights.reshape((b, k, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            saliency_map = np.<span class="built_in">sum</span>(weights * activations, axis=<span class="number">1</span>)</span><br><span class="line">            saliency_map = np.squeeze(np.maximum(saliency_map, <span class="number">0</span>))</span><br><span class="line">            saliency_map = cv2.resize(saliency_map, (tensor.size(<span class="number">3</span>), tensor.size(<span class="number">2</span>)))</span><br><span class="line">            saliency_map_min, saliency_map_max = saliency_map.<span class="built_in">min</span>(), saliency_map.<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> (saliency_map_max - saliency_map_min) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            saliency_map = (saliency_map - saliency_map_min) / (saliency_map_max - saliency_map_min)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># add heatmap and box to image</span></span><br><span class="line">            cam_image = show_cam_on_image(img.copy(), saliency_map, use_rgb=<span class="literal">True</span>)</span><br><span class="line">            cam_image = <span class="variable language_">self</span>.draw_detections(post_boxes[i], <span class="variable language_">self</span>.colors[<span class="built_in">int</span>(post_result[i, :].argmax())], <span class="string">f&#x27;<span class="subst">&#123;self.model_names[<span class="built_in">int</span>(post_result[i, :].argmax())]&#125;</span> <span class="subst">&#123;<span class="built_in">float</span>(post_result[i].<span class="built_in">max</span>()):<span class="number">.2</span>f&#125;</span>&#x27;</span>, cam_image)</span><br><span class="line">            cam_image = Image.fromarray(cam_image)</span><br><span class="line">            cam_image.save(<span class="string">f&#x27;<span class="subst">&#123;save_path&#125;</span>/<span class="subst">&#123;i&#125;</span>.png&#x27;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>():</span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;weight&#x27;</span>: <span class="string">&#x27;yolov8n.pt&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;cfg&#x27;</span>: <span class="string">&#x27;ultralytics/cfg/models/v8/yolov8n.yaml&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;device&#x27;</span>: <span class="string">&#x27;cuda:0&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;method&#x27;</span>: <span class="string">&#x27;GradCAM&#x27;</span>, <span class="comment"># GradCAMPlusPlus, GradCAM, XGradCAM</span></span><br><span class="line">        <span class="string">&#x27;layer&#x27;</span>: <span class="string">&#x27;model.model[9]&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;backward_type&#x27;</span>: <span class="string">&#x27;all&#x27;</span>, <span class="comment"># class, box, all</span></span><br><span class="line">        <span class="string">&#x27;conf_threshold&#x27;</span>: <span class="number">0.6</span>, <span class="comment"># 0.6</span></span><br><span class="line">        <span class="string">&#x27;ratio&#x27;</span>: <span class="number">0.02</span> <span class="comment"># 0.02-0.1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = yolov8_heatmap(**get_params())</span><br><span class="line">    model(<span class="string">r&#x27;ultralytics/assets/bus.jpg&#x27;</span>, <span class="string">&#x27;result&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 写作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>报错</title>
      <link href="/post/article-14/"/>
      <url>/post/article-14/</url>
      
        <content type="html"><![CDATA[<h2 id="一、本文介绍"><a href="#一、本文介绍" class="headerlink" title="一、本文介绍"></a>一、本文介绍</h2><p>本文为专栏内读者和我个人在训练<strong>YOLOv8时遇到的各种错误解决方案</strong>，你遇到的问题本文基本上都能够解决。</p><h2 id="二、-报错问题"><a href="#二、-报错问题" class="headerlink" title="二、 报错问题 "></a>二、 报错问题 </h2><p># 以下为两个重要库的版本，大家可以对应下载，使用教程我会更新，时间还没来得及大家可以先看视频使用。</p><blockquote><p><strong>项目环境：</strong></p><p>python &#x3D;&#x3D; 3.9.7</p><p>pytorch &#x3D;&#x3D; 1.12.1</p><p>timm &#x3D;&#x3D; 0.9.12</p><p>mmcv-full &#x3D;&#x3D; 1.6.2</p></blockquote><hr><h3 id="1-训练过程中loss出现Nan值"><a href="#1-训练过程中loss出现Nan值" class="headerlink" title="(1)训练过程中loss出现Nan值."></a>(1)训练过程中loss出现Nan值.</h3><p>可以尝试关闭AMP混合精度训练，如何关闭amp呢找到如下文件’ultralytics&#x2F;cfg&#x2F;default.yaml’，其中有一个参数是</p><p>amp: False  # (bool) Automatic Mixed Precision (AMP) training, choices&#x3D;[True, False], True runs AMP check</p><p>我们将其设置为False即可，默认时为True。</p><p>.</p><h3 id="2-多卡训练问题-修改模型以后不能支持多卡训练可以尝试下面的两行命令行操作，两个是不同的操作，是代表不同的版本现尝试第一个不行用第二个"><a href="#2-多卡训练问题-修改模型以后不能支持多卡训练可以尝试下面的两行命令行操作，两个是不同的操作，是代表不同的版本现尝试第一个不行用第二个" class="headerlink" title="(2)多卡训练问题,修改模型以后不能支持多卡训练可以尝试下面的两行命令行操作，两个是不同的操作，是代表不同的版本现尝试第一个不行用第二个"></a>(2)多卡训练问题,修改模型以后不能支持多卡训练可以尝试下面的两行命令行操作，两个是不同的操作，是代表不同的版本现尝试第一个不行用第二个</h3><p>    python -m torch.distributed.run –nproc_per_node 2 train.py</p><p>    python -m torch.distributed.launch –nproc_per_node 2 train.py</p><hr><h3 id="3-针对运行过程中的一些报错解决"><a href="#3-针对运行过程中的一些报错解决" class="headerlink" title="(3) 针对运行过程中的一些报错解决"></a>(3) 针对运行过程中的一些报错解决</h3><p>    <strong>1.如果训练的过程中验证报错了(主要是一些形状不匹配的错误这是因为验证集的一些特殊图片导致)</strong></p><p>就是有这种训练第一个epochs完成后开始验证的时候报错，下面的方法基本百分之九十都能够解决。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e61c95278a244aebbe4ac67f07f90466.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e61c95278a244aebbe4ac67f07f90466.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><p>    找到ultralytics&#x2F;models&#x2F;yolo&#x2F;detect&#x2F;train.py的DetectionTrainer class中的build_dataset函数中的rect&#x3D;mode &#x3D;&#x3D; ‘val’改为rect&#x3D;False</p><p>    <strong>2.推理的时候运行detect.py文件报了形状不匹配的错误</strong></p><p>    找到ultralytics&#x2F;engine&#x2F;predictor.py找到函数def pre_transform(self, im),在LetterBox中的auto改为False</p><p>    <strong>3.训练的过程中报错类型不匹配的问题</strong></p><p>    找到’ultralytics&#x2F;engine&#x2F;validator.py’文件找到 ‘class BaseValidator:’ 然后在其’__call__‘中</p><p>    self.args.half &#x3D; self.device.type !&#x3D; ‘cpu’  # force FP16 val during training的一行代码下面加上self.args.half &#x3D; False</p><hr><h3 id="4-针对yaml文件中的nc修改"><a href="#4-针对yaml文件中的nc修改" class="headerlink" title="(4) 针对yaml文件中的nc修改"></a>(4) 针对yaml文件中的nc修改</h3><p>    不用修改，模型会自动根据你数据集的配置文件获取。</p><p>    这也是模型打印两次的区别，第一次打印出来的就是你选择模型的yaml文件结构，第二次打印的就是替换了你数据集的yaml文件，模型使用的是第二种。</p><hr><h3 id="5-针对环境的问题"><a href="#5-针对环境的问题" class="headerlink" title="(5) 针对环境的问题"></a>(5) 针对环境的问题</h3><p>    环境的问题我实在解决不过来，所以大家可以自行在网上搜索解决方案。  </p><hr><h3 id="6-训练过程中不打印GFLOpS"><a href="#6-训练过程中不打印GFLOpS" class="headerlink" title="(6) 训练过程中不打印GFLOpS"></a>(6) 训练过程中不打印GFLOpS</h3><p>计算的GFLOPs计算异常不打印，所以需要额外修改一处， 我们找到如下文件’ultralytics&#x2F;utils&#x2F;torch_utils.py’文件内有如下的代码按照如下的图片进行修改，大家看好函数就行，其中红框的640可能和你的不一样， 然后用我给的代码替换掉整个代码即可。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/24068f6039b94ceeb91e98642c00e594.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/24068f6039b94ceeb91e98642c00e594.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_flops</span>(<span class="params">model, imgsz=<span class="number">640</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a YOLO model&#x27;s FLOPs.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        model = de_parallel(model)</span><br><span class="line">        p = <span class="built_in">next</span>(model.parameters())</span><br><span class="line">        <span class="comment"># stride = max(int(model.stride.max()), 32) if hasattr(model, &#x27;stride&#x27;) else 32  # max stride</span></span><br><span class="line">        stride = <span class="number">640</span></span><br><span class="line">        im = torch.empty((<span class="number">1</span>, <span class="number">3</span>, stride, stride), device=p.device)  <span class="comment"># input image in BCHW format</span></span><br><span class="line">        flops = thop.profile(deepcopy(model), inputs=[im], verbose=<span class="literal">False</span>)[<span class="number">0</span>] / <span class="number">1E9</span> * <span class="number">2</span> <span class="keyword">if</span> thop <span class="keyword">else</span> <span class="number">0</span>  <span class="comment"># stride GFLOPs</span></span><br><span class="line">        imgsz = imgsz <span class="keyword">if</span> <span class="built_in">isinstance</span>(imgsz, <span class="built_in">list</span>) <span class="keyword">else</span> [imgsz, imgsz]  <span class="comment"># expand if int/float</span></span><br><span class="line">        <span class="keyword">return</span> flops * imgsz[<span class="number">0</span>] / stride * imgsz[<span class="number">1</span>] / stride  <span class="comment"># 640x640 GFLOPs</span></span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><hr><h3 id="7-mmcv安装的解决方法"><a href="#7-mmcv安装的解决方法" class="headerlink" title="(7) mmcv安装的解决方法"></a>(7) mmcv安装的解决方法</h3><p>有的读者mmcv-full会安装失败是因为自身系统的编译工具有问题，也有可能是环境之间安装的有冲突 推荐大家离线安装的形式,下面的地址中大家可以找找自己的版本,下载到本地进行安装。 <a href="https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html">https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html</a> <a href="https://download.openmmlab.com/mmcv/dist/index.html">https://download.openmmlab.com/mmcv/dist/index.html</a> </p>]]></content>
      
      
      <categories>
          
          <category> YOLO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 论文 </tag>
            
            <tag> Ultralytics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可视化热力图</title>
      <link href="/post/article-10/"/>
      <url>/post/article-10/</url>
      
        <content type="html"><![CDATA[<h2 id="一、本文介绍"><a href="#一、本文介绍" class="headerlink" title="一、本文介绍"></a>一、本文介绍</h2><p>本文给大家带来的机制是的<strong>可视化热力图功能</strong>，热力图)作为我们论文当中的必备一环，可以展示出我们呈现机制的有效性，本文的内容支持YOLOv8最新版本，同时支持视频讲解，本文的内容是根据检测头的输出内容，然后来绘图，<strong>产生6300张预测图片，从中选取出有效的热力图来绘图。</strong></p><p>在开始之前给大家推荐一下我的专栏，本专栏每周更新3-10篇最新前沿机制 | 包括二次创新全网无重复，以及融合改进(大家拿到之后添加另外一个改进机制在你的数据集上实现涨点即可撰写论文)，还有各种前沿顶会改进机制 |，更有包含我所有附赠的文件（文件内集成我所有的改进机制全部注册完毕可以直接运行）和交流群和视频讲解提供给大家。  </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0a203fd9e3cd4f89b95e419e4fc22f1f.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0a203fd9e3cd4f89b95e419e4fc22f1f.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><hr><h2 id="二、项目完整代码"><a href="#二、项目完整代码" class="headerlink" title="二、项目完整代码 "></a>二、项目完整代码 </h2><p><strong>我们将这个代码，复制粘贴到我们YOLOv8的仓库里然后创建一个py文件存放进去即可。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">warnings.simplefilter(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> torch, yaml, cv2, os, shutil</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> trange</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> ultralytics.nn.tasks <span class="keyword">import</span> DetectionModel <span class="keyword">as</span> Model</span><br><span class="line"><span class="keyword">from</span> ultralytics.utils.torch_utils <span class="keyword">import</span> intersect_dicts</span><br><span class="line"><span class="keyword">from</span> ultralytics.utils.ops <span class="keyword">import</span> xywh2xyxy</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam <span class="keyword">import</span> GradCAMPlusPlus, GradCAM, XGradCAM</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam.utils.image <span class="keyword">import</span> show_cam_on_image</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam.activations_and_gradients <span class="keyword">import</span> ActivationsAndGradients</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">letterbox</span>(<span class="params">im, new_shape=(<span class="params"><span class="number">640</span>, <span class="number">640</span></span>), color=(<span class="params"><span class="number">114</span>, <span class="number">114</span>, <span class="number">114</span></span>), auto=<span class="literal">True</span>, scaleFill=<span class="literal">False</span>, scaleup=<span class="literal">True</span>, stride=<span class="number">32</span></span>):</span><br><span class="line">    <span class="comment"># Resize and pad image while meeting stride-multiple constraints</span></span><br><span class="line">    shape = im.shape[:<span class="number">2</span>]  <span class="comment"># current shape [height, width]</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(new_shape, <span class="built_in">int</span>):</span><br><span class="line">        new_shape = (new_shape, new_shape)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Scale ratio (new / old)</span></span><br><span class="line">    r = <span class="built_in">min</span>(new_shape[<span class="number">0</span>] / shape[<span class="number">0</span>], new_shape[<span class="number">1</span>] / shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> scaleup:  <span class="comment"># only scale down, do not scale up (for better val mAP)</span></span><br><span class="line">        r = <span class="built_in">min</span>(r, <span class="number">1.0</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Compute padding</span></span><br><span class="line">    ratio = r, r  <span class="comment"># width, height ratios</span></span><br><span class="line">    new_unpad = <span class="built_in">int</span>(<span class="built_in">round</span>(shape[<span class="number">1</span>] * r)), <span class="built_in">int</span>(<span class="built_in">round</span>(shape[<span class="number">0</span>] * r))</span><br><span class="line">    dw, dh = new_shape[<span class="number">1</span>] - new_unpad[<span class="number">0</span>], new_shape[<span class="number">0</span>] - new_unpad[<span class="number">1</span>]  <span class="comment"># wh padding</span></span><br><span class="line">    <span class="keyword">if</span> auto:  <span class="comment"># minimum rectangle</span></span><br><span class="line">        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  <span class="comment"># wh padding</span></span><br><span class="line">    <span class="keyword">elif</span> scaleFill:  <span class="comment"># stretch</span></span><br><span class="line">        dw, dh = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">        new_unpad = (new_shape[<span class="number">1</span>], new_shape[<span class="number">0</span>])</span><br><span class="line">        ratio = new_shape[<span class="number">1</span>] / shape[<span class="number">1</span>], new_shape[<span class="number">0</span>] / shape[<span class="number">0</span>]  <span class="comment"># width, height ratios</span></span><br><span class="line"> </span><br><span class="line">    dw /= <span class="number">2</span>  <span class="comment"># divide padding into 2 sides</span></span><br><span class="line">    dh /= <span class="number">2</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> shape[::-<span class="number">1</span>] != new_unpad:  <span class="comment"># resize</span></span><br><span class="line">        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)</span><br><span class="line">    top, bottom = <span class="built_in">int</span>(<span class="built_in">round</span>(dh - <span class="number">0.1</span>)), <span class="built_in">int</span>(<span class="built_in">round</span>(dh + <span class="number">0.1</span>))</span><br><span class="line">    left, right = <span class="built_in">int</span>(<span class="built_in">round</span>(dw - <span class="number">0.1</span>)), <span class="built_in">int</span>(<span class="built_in">round</span>(dw + <span class="number">0.1</span>))</span><br><span class="line">    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  <span class="comment"># add border</span></span><br><span class="line">    <span class="keyword">return</span> im, ratio, (dw, dh)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">yolov8_heatmap</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, weight, cfg, device, method, layer, backward_type, conf_threshold, ratio</span>):</span><br><span class="line">        device = torch.device(device)</span><br><span class="line">        ckpt = torch.load(weight)</span><br><span class="line">        model_names = ckpt[<span class="string">&#x27;model&#x27;</span>].names</span><br><span class="line">        csd = ckpt[<span class="string">&#x27;model&#x27;</span>].<span class="built_in">float</span>().state_dict()  <span class="comment"># checkpoint state_dict as FP32</span></span><br><span class="line">        model = Model(cfg, ch=<span class="number">3</span>, nc=<span class="built_in">len</span>(model_names)).to(device)</span><br><span class="line">        csd = intersect_dicts(csd, model.state_dict(), exclude=[<span class="string">&#x27;anchor&#x27;</span>])  <span class="comment"># intersect</span></span><br><span class="line">        model.load_state_dict(csd, strict=<span class="literal">False</span>)  <span class="comment"># load</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Transferred <span class="subst">&#123;<span class="built_in">len</span>(csd)&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(model.state_dict())&#125;</span> items&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        target_layers = [<span class="built_in">eval</span>(layer)]</span><br><span class="line">        method = <span class="built_in">eval</span>(method)</span><br><span class="line"> </span><br><span class="line">        colors = np.random.uniform(<span class="number">0</span>, <span class="number">255</span>, size=(<span class="built_in">len</span>(model_names), <span class="number">3</span>)).astype(np.int32)</span><br><span class="line">        <span class="variable language_">self</span>.__dict__.update(<span class="built_in">locals</span>())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">post_process</span>(<span class="params">self, result</span>):</span><br><span class="line">        logits_ = result[:, <span class="number">4</span>:]</span><br><span class="line">        boxes_ = result[:, :<span class="number">4</span>]</span><br><span class="line">        <span class="built_in">sorted</span>, indices = torch.sort(logits_.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">0</span>], descending=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.transpose(logits_[<span class="number">0</span>], dim0=<span class="number">0</span>, dim1=<span class="number">1</span>)[indices[<span class="number">0</span>]], torch.transpose(boxes_[<span class="number">0</span>], dim0=<span class="number">0</span>, dim1=<span class="number">1</span>)[indices[<span class="number">0</span>]], xywh2xyxy(torch.transpose(boxes_[<span class="number">0</span>], dim0=<span class="number">0</span>, dim1=<span class="number">1</span>)[indices[<span class="number">0</span>]]).cpu().detach().numpy()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">draw_detections</span>(<span class="params">self, box, color, name, img</span>):</span><br><span class="line">        xmin, ymin, xmax, ymax = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">list</span>(box)))</span><br><span class="line">        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), <span class="built_in">tuple</span>(<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> color), <span class="number">2</span>)</span><br><span class="line">        cv2.putText(img, <span class="built_in">str</span>(name), (xmin, ymin - <span class="number">5</span>), cv2.FONT_HERSHEY_SIMPLEX, <span class="number">0.8</span>, <span class="built_in">tuple</span>(<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> color), <span class="number">2</span>, lineType=cv2.LINE_AA)</span><br><span class="line">        <span class="keyword">return</span> img</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img_path, save_path</span>):</span><br><span class="line">        <span class="comment"># remove dir if exist</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(save_path):</span><br><span class="line">            shutil.rmtree(save_path)</span><br><span class="line">        <span class="comment"># make dir if not exist</span></span><br><span class="line">        os.makedirs(save_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># img process</span></span><br><span class="line">        img = cv2.imread(img_path)</span><br><span class="line">        img = letterbox(img)[<span class="number">0</span>]</span><br><span class="line">        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">        img = np.float32(img) / <span class="number">255.0</span></span><br><span class="line">        tensor = torch.from_numpy(np.transpose(img, axes=[<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])).unsqueeze(<span class="number">0</span>).to(<span class="variable language_">self</span>.device)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># init ActivationsAndGradients</span></span><br><span class="line">        grads = ActivationsAndGradients(<span class="variable language_">self</span>.model, <span class="variable language_">self</span>.target_layers, reshape_transform=<span class="literal">None</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># get ActivationsAndResult</span></span><br><span class="line">        result = grads(tensor)</span><br><span class="line">        activations = grads.activations[<span class="number">0</span>].cpu().detach().numpy()</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># postprocess to yolo output</span></span><br><span class="line">        post_result, pre_post_boxes, post_boxes = <span class="variable language_">self</span>.post_process(result[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> trange(<span class="built_in">int</span>(post_result.size(<span class="number">0</span>) * <span class="variable language_">self</span>.ratio)):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">float</span>(post_result[i].<span class="built_in">max</span>()) &lt; <span class="variable language_">self</span>.conf_threshold:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">            <span class="variable language_">self</span>.model.zero_grad()</span><br><span class="line">            <span class="comment"># get max probability for this prediction</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;class&#x27;</span> <span class="keyword">or</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;all&#x27;</span>:</span><br><span class="line">                score = post_result[i].<span class="built_in">max</span>()</span><br><span class="line">                score.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;box&#x27;</span> <span class="keyword">or</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;all&#x27;</span>:</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    score = pre_post_boxes[i, j]</span><br><span class="line">                    score.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># process heatmap</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;class&#x27;</span>:</span><br><span class="line">                gradients = grads.gradients[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">elif</span> <span class="variable language_">self</span>.backward_type == <span class="string">&#x27;box&#x27;</span>:</span><br><span class="line">                gradients = grads.gradients[<span class="number">0</span>] + grads.gradients[<span class="number">1</span>] + grads.gradients[<span class="number">2</span>] + grads.gradients[<span class="number">3</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                gradients = grads.gradients[<span class="number">0</span>] + grads.gradients[<span class="number">1</span>] + grads.gradients[<span class="number">2</span>] + grads.gradients[<span class="number">3</span>] + grads.gradients[<span class="number">4</span>]</span><br><span class="line">            b, k, u, v = gradients.size()</span><br><span class="line">            weights = <span class="variable language_">self</span>.method.get_cam_weights(<span class="variable language_">self</span>.method, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, activations, gradients.detach().numpy())</span><br><span class="line">            weights = weights.reshape((b, k, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            saliency_map = np.<span class="built_in">sum</span>(weights * activations, axis=<span class="number">1</span>)</span><br><span class="line">            saliency_map = np.squeeze(np.maximum(saliency_map, <span class="number">0</span>))</span><br><span class="line">            saliency_map = cv2.resize(saliency_map, (tensor.size(<span class="number">3</span>), tensor.size(<span class="number">2</span>)))</span><br><span class="line">            saliency_map_min, saliency_map_max = saliency_map.<span class="built_in">min</span>(), saliency_map.<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> (saliency_map_max - saliency_map_min) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            saliency_map = (saliency_map - saliency_map_min) / (saliency_map_max - saliency_map_min)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># add heatmap and box to image</span></span><br><span class="line">            cam_image = show_cam_on_image(img.copy(), saliency_map, use_rgb=<span class="literal">True</span>)</span><br><span class="line">            <span class="string">&quot;不想在图片中绘画出边界框和置信度，注释下面的一行代码即可&quot;</span></span><br><span class="line">            cam_image = <span class="variable language_">self</span>.draw_detections(post_boxes[i], <span class="variable language_">self</span>.colors[<span class="built_in">int</span>(post_result[i, :].argmax())], <span class="string">f&#x27;<span class="subst">&#123;self.model_names[<span class="built_in">int</span>(post_result[i, :].argmax())]&#125;</span> <span class="subst">&#123;<span class="built_in">float</span>(post_result[i].<span class="built_in">max</span>()):<span class="number">.2</span>f&#125;</span>&#x27;</span>, cam_image)</span><br><span class="line">            cam_image = Image.fromarray(cam_image)</span><br><span class="line">            cam_image.save(<span class="string">f&#x27;<span class="subst">&#123;save_path&#125;</span>/<span class="subst">&#123;i&#125;</span>.png&#x27;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>():</span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;weight&#x27;</span>: <span class="string">&#x27;yolov8n.pt&#x27;</span>,   <span class="comment"># 训练出来的权重文件</span></span><br><span class="line">        <span class="string">&#x27;cfg&#x27;</span>: <span class="string">&#x27;ultralytics/cfg/models/v8/yolov8n.yaml&#x27;</span>,  <span class="comment"># 训练权重对应的yaml配置文件</span></span><br><span class="line">        <span class="string">&#x27;device&#x27;</span>: <span class="string">&#x27;cuda:0&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;method&#x27;</span>: <span class="string">&#x27;GradCAM&#x27;</span>, <span class="comment"># GradCAMPlusPlus, GradCAM, XGradCAM , 使用的热力图库文件不同的效果不一样可以多尝试</span></span><br><span class="line">        <span class="string">&#x27;layer&#x27;</span>: <span class="string">&#x27;model.model[9]&#x27;</span>,  <span class="comment"># 想要检测的对应层</span></span><br><span class="line">        <span class="string">&#x27;backward_type&#x27;</span>: <span class="string">&#x27;all&#x27;</span>, <span class="comment"># class, box, all</span></span><br><span class="line">        <span class="string">&#x27;conf_threshold&#x27;</span>: <span class="number">0.01</span>, <span class="comment"># 0.6  # 置信度阈值，有的时候你的进度条到一半就停止了就是因为没有高于此值的了</span></span><br><span class="line">        <span class="string">&#x27;ratio&#x27;</span>: <span class="number">0.02</span> <span class="comment"># 0.02-0.1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = yolov8_heatmap(**get_params())</span><br><span class="line">    model(<span class="string">r&#x27;ultralytics/assets/bus.jpg&#x27;</span>, <span class="string">&#x27;result&#x27;</span>)  <span class="comment"># 第一个是检测的文件, 第二个是保存的路径</span></span><br></pre></td></tr></table></figure><h2 id="三、参数解析"><a href="#三、参数解析" class="headerlink" title=" 三、参数解析 "></a> 三、参数解析 </h2><p><strong>下面上面项目核心代码的参数解析，共有7个，能够起到作用的参数并不多。</strong> </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6fcc9bb6ec814ab1961efb0c7db5f1a1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6fcc9bb6ec814ab1961efb0c7db5f1a1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><table><thead><tr><th></th><th>参数名</th><th>参数类型</th><th>参数讲解</th></tr></thead><tbody><tr><td>0</td><td>weights</td><td>str</td><td>用于检测视频的权重文件地址（可以是你训练好的，也可以是官方提供的）</td></tr><tr><td>1</td><td>cfg</td><td>str</td><td>你选择的权重对应的yaml配置文件，请注意一定要对应否则会报错和不显示图片</td></tr><tr><td>2</td><td>device</td><td>str</td><td>设备的选择可以用GPU也可以用CPU</td></tr><tr><td>3</td><td>method</td><td>str</td><td>使用的热力图第三方库的版本，不同的版本效果也不一样。</td></tr><tr><td>4</td><td>layer</td><td>str</td><td>想要检测的对应层，比如这里设置的是9那么检测的就是第九层</td></tr><tr><td>4</td><td>backward_type</td><td>str</td><td>检测的类别</td></tr><tr><td>5</td><td>conf_threshold</td><td>str</td><td>置信度阈值，有的时候你的进度条没有满就是因为没有大于这个阈值的图片了</td></tr><tr><td>6</td><td>ratio</td><td>int</td><td>YOLOv8一次产生6300张预测框，选择多少比例的图片绘画热力图。</td></tr></tbody></table><h2 id="四、项目的使用教程"><a href="#四、项目的使用教程" class="headerlink" title="四、项目的使用教程"></a>四、项目的使用教程</h2><h3 id="4-1-步骤一"><a href="#4-1-步骤一" class="headerlink" title="4.1 步骤一"></a>4.1 步骤一</h3><p>我们在Yolo仓库的目录下创建一个py文件将代码存放进去，如下图所示。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6c4bed4cef9a488d8cda62492efa80c7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6c4bed4cef9a488d8cda62492efa80c7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><hr><h3 id="4-2-步骤二"><a href="#4-2-步骤二" class="headerlink" title="4.2 步骤二"></a>4.2 步骤二</h3><p><strong>我们按照参数解析部分的介绍填好大家的参数，主要配置的有两个一个就是权重文件地址另一个就是图片的地址。</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2bd70b825206498bb8688b90cfa3e12c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2bd70b825206498bb8688b90cfa3e12c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><hr><h3 id="4-3-步骤三"><a href="#4-3-步骤三" class="headerlink" title="4.3 步骤三"></a>4.3 步骤三</h3><p>我们挺好之后运行文件即可，图片就会保存在同级目录下的新的文件夹result内。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0a203fd9e3cd4f89b95e419e4fc22f1f.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0a203fd9e3cd4f89b95e419e4fc22f1f.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><hr><h3 id="4-4-置信度和检测框"><a href="#4-4-置信度和检测框" class="headerlink" title="4.4 置信度和检测框"></a>4.4 置信度和检测框</h3><p>看下下面的说明就行。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/26206bf8e8cb4da4b2f230ac932cd577.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/26206bf8e8cb4da4b2f230ac932cd577.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><hr>]]></content>
      
      
      <categories>
          
          <category> YOLO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 热力图 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
