<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Rethinking U-Net Task-Adaptive Mixture of Skip Connections for Enhanced Medical Image Segmentation</title>
      <link href="/post/rethinking-u-net-task-adaptive-mixture-of-skip-connections-for-enhanced-medical-image-segmentation/"/>
      <url>/post/rethinking-u-net-task-adaptive-mixture-of-skip-connections-for-enhanced-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>U-Net is a widely used model for medical image segmentation, renowned for its strong feature extraction capabilities and U-shaped design, which incorporates skip connections to preserve critical information. However, its decoders exhibit information-specific preferences for the supplementary content provided by skip connections, instead of adhering to a strict one-to-one correspondence, which limits its flexibility across diverse tasks. To address this limitation, we propose the Task-Adaptive Mixture of Skip Connections (TA-MoSC) module, inspired by the Mixture of Experts (MoE) framework. TA-MoSC innovatively reinterprets skip connections as a task allocation problem, employing a routing mechanism to adaptively select expert combinations at different decoding stages. By introducing MoE, our approach enhances the sparsity of the model, and lightweight convolutional experts are shared across all skip connection stages, with a Balanced Expert Utilization (BEU) strategy ensuring that all experts are effectively trained, maintaining training balance and preserving computational efficiency. Our approach introduces minimal additional parameters to the original U-Net but significantly enhances its performance and stability. Experiments on GlaS, MoNuSeg, Synapse, and ISIC16 datasets demonstrate state-of-the-art accuracy and better generalization across diverse tasks. Moreover, while this work focuses on medical image segmentation, the proposed method can be seamlessly extended to other segmentation tasks, offering a flexible and efficient solution for diverse applications.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>U-Net是一种广泛应用于医学图像分割的模型，以其强大的特征提取能力和u形设计而闻名，u形设计采用跳过连接来保留关键信息。然而，它的解码器对跳过连接提供的补充内容显示了特定于信息的首选项，而不是遵循严格的一对一对应关系，这限制了它在不同任务中的灵活性。为了解决这一限制，我们提出了任务自适应跳跃连接混合(TA-MoSC)模块，灵感来自混合专家(MoE)框架。TA-MoSC创新性地将跳跃连接重新解释为任务分配问题，采用路由机制自适应地选择不同解码阶段的专家组合。通过引入MoE，我们的方法增强了模型的稀疏性，并且在所有跳跃连接阶段共享轻量级卷积专家，并采用平衡专家利用率(BEU)策略确保所有专家都得到有效训练，保持训练平衡并保持计算效率。我们的方法在原有的U-Net基础上引入了最小的附加参数，但显著提高了其性能和稳定性。在GlaS, MoNuSeg, Synapse和ISIC16数据集上的实验证明了最先进的准确性和跨不同任务的更好泛化。此外，虽然本研究的重点是医学图像分割，但该方法可以无缝扩展到其他分割任务，为各种应用提供了灵活高效的解决方案。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>医学图像分割对医疗数据的分析和解读至关重要，能辅助疾病诊断和治疗。U - Net因强大的特征提取能力和U形结构，成为广泛应用的医学图像分割模型，其跳跃连接可保留关键信息，但也存在不足。 原始U - Net的跳跃连接未能有效解决编码器与解码器间的语义差距问题，不同医疗成像任务对语义特征的侧重点不同，简单的一对一连接机制难以满足需求，导致模型性能受限。 尽管已有一些改进方法，如UNet++采用嵌套和密集跳跃路径、部分方法引入注意力机制或使用Transformer架构，但这些方法要么增加了架构复杂度和计算开销，要么缺乏适应不同数据集的灵活性，无法充分考虑解码器对语义信息的阶段特定偏好，泛化能力不足。 基于此，本文通过大量实验探索U - Net中跳跃连接的不同组合，发现每个解码器阶段所需信息并非与跳跃连接严格一对一对应，从而提出任务自适应跳跃连接混合（TA - MoSC）模块，以解决这些问题，提升模型性能和泛化能力。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>U-Net架构多样</strong>：U-Net凭借其强大的特征提取能力和U形结构，成为医学图像分割领域广泛采用的架构，衍生出ResUNet、DenseUNet、UNet++等众多变体，还引入了注意力机制、Transformer架构等。</li><li><strong>Skip Connection改进</strong>：一些方法如UDTransNet、EIU-Net通过基于注意力的重新校准改进了跳跃连接，但缺乏适应不同数据集需求的灵活性。</li><li><strong>Mixture of Experts应用</strong>：稀疏激活的Mixture of Experts（MoE）模型在视觉和文本模型扩展方面取得成功，但在图像分割领域，尤其是跳跃连接方面的应用尚未深入探索。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-43-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-43-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-23_10-43-43"></p><ul><li><p>TA - MoSC模块：受混合专家（Mixture of Experts，MoE）框架启发，将跳跃连接重新定义为任务分配问题。该模块由路由银行（Router Bank）、跳跃连接（Skip - Connection，SC）银行和四个对接器（Dockers）组成。其工作流程如下：</p><ul><li><p><strong>特征聚合</strong>：输入图像经过多层编码器处理，生成不同层次的特征。将前四层特征调整为相同大小后，沿通道维度拼接形成统一的特征表示，并进行特征维度缩减。</p></li><li><p><strong>路由阶段</strong>：路由银行中的每个路由器为定制合适的专家组合，为不同阶段的跳跃连接选择路由方案。专家采用独立的卷积子网络，对输入特征进行专门的非线性变换。使用TopK（K &#x3D; 2）操作进行稀疏选择，确保只有部分专家被激活，降低计算开销。最后，通过对接器模块对跳跃连接进行整形和处理，将其传输到相应的解码器。</p></li><li><p><strong>平衡专家利用</strong>：该模块包含专家方差（Expert Variance，EV）损失和未使用专家处理两个关键机制。</p><ul><li><strong>专家方差损失</strong>：计算不同门控网络中专家使用的方差，避免模型过度依赖某些专家，促使所有专家充分发挥能力。</li><li><strong>未使用专家处理</strong>：当门控网络未选择某些专家时，用输入数据的随机样本对这些未使用的专家进行处理，防止专家闲置，确保每个专家都得到训练。</li></ul></li></ul></li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集</p></blockquote><ul><li><strong>GlaS</strong>：包含<strong>165</strong>张高分辨率苏木精 - 伊红（H&amp;E）染色图像，85张用于训练，80张用于测试。</li><li><strong>MoNuSeg</strong>：由<strong>44</strong>张图像组成，30张用于训练，14张用于测试。</li><li><strong>Synapse</strong>：有<strong>30</strong>个腹部CT扫描，共3779张轴向图像，涵盖8个器官。</li><li><strong>ISIC16</strong>：有<strong>1279</strong>张皮肤镜图像，其中测试集包含379张，有两种疾病类别的真实标注。</li></ul><blockquote><p>训练策略</p></blockquote><p>训练过程分为两个阶段。第一阶段，<strong>使用原始跳跃连接训练编码器和解码器</strong>，使其熟悉数据集并掌握基本的编码和解码能力。之后，<strong>冻结编码器和解码器，专注训练提出的TC - MoSC模块</strong>，使每个专家充分学习其专长的特征，路由器根据各解码阶段的语义需求有效分配任务。冻结编码器和解码器是为防止其参数在整个训练过程中变化，降低训练难度，便于收敛。</p><p><strong>定量比较</strong>：在GlaS、MoNuSeg和ISIC16数据集上，使用Dice系数和交并比（IoU）作为性能指标；在多标签分割的Synapse数据集上，使用95% Hausdorff距离指标。采用5折交叉验证评估模型性能，使用独立Student’s t - 检验（α &#x3D; 0.05）评估统计显著性。结果表明，模型在GlaS和MoNuSeg上显著优于基线模型，实验结果的小标准差显示出模型的高稳定性和鲁棒性，在不同数据集上兼容性良好。<br><strong>定性比较</strong>：如图5所示，红色框突出显示了UTANet在四个数据集上优于其他模型的区域。在MoNuseg数据集上，UTANet能更有效地捕捉细节信息，这得益于TA - MoSC模块中对细节敏感的专家模块。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-48-36.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-48-36.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-23_10-48-36"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>提出模块的消融研究</strong>：结果显示，Baseline + TA - MoSC + BEU始终优于其他基线，表明纳入TA - MoSC模块显著提高了模型的分割性能，应用BEU优化进一步增强了性能。不同数据集在解码阶段需要不同程度的补充信息，为解码器提供适当的补充信息可大幅提高模型的分割性能。</li><li><strong>TopK设置的消融研究</strong>：在推理阶段实验不同的TopK设置，发现单标签数据集上TopK设为3效果最佳，多标签的Synapse数据集上TopK设为4效果最佳。可视化在GlaS数据集上训练的模型中的专家发现，每个专家网络学习的特征和语义不同，经过TA - MoSC模块后，跳跃连接特征的高亮区域与编码器第三和第四阶段的特征紧密对齐，证实了模型对不同数据集或任务的跳跃连接信息有特定偏好，TA - MoSC模块能有效解决跳跃连接中的信息不对称问题。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-49-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-49-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-23_10-49-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-49-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-49-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-23_10-49-28"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在U-Net的解码过程中，<strong>不同数据集对补充信息的需求存在差异</strong>。本文通过将<strong>混合专家</strong>（Mixture of Experts，MoE）框架集成到<strong>跳跃连接</strong>（skip connections）中，解决了这一问题，实现了多尺度特征的自适应重新分配，以适应解码器在不同阶段的特定偏好。与传统的固定跳跃连接机制不同，所提出的任务自适应跳跃连接混合（Task-Adaptive Mixture of Skip Connections，TA-MoSC）模块能够动态地对齐编码器和解码器的特征，从而应对特定数据集的分割挑战。 尽管本文以医学图像分割作为案例研究，但所提出的方法可以推广到其他密集预测任务，如自然图像分割和目标检测。在未来的工作中，作者将专注于优化模型的计算效率，使TA-MoSC模块更具可扩展性和高效性，确保其在跨领域的实时应用和大规模任务中的实用性。 </p>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> U-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>配置失败登录尝试限制</title>
      <link href="/post/frp/pei-zhi-shi-bai-deng-lu-chang-shi-xian-zhi/"/>
      <url>/post/frp/pei-zhi-shi-bai-deng-lu-chang-shi-xian-zhi/</url>
      
        <content type="html"><![CDATA[<p>为了增强Windows远程桌面通过FRP和Nginx配置的安全性，限制失败登录尝试是非常重要的一环。我将介绍多个层面上的失败登录限制配置方法。</p><h2 id="1-Nginx层面的失败登录限制"><a href="#1-Nginx层面的失败登录限制" class="headerlink" title="1. Nginx层面的失败登录限制"></a>1. Nginx层面的失败登录限制</h2><h3 id="使用Nginx限流模块"><a href="#使用Nginx限流模块" class="headerlink" title="使用Nginx限流模块"></a>使用Nginx限流模块</h3><p>在Nginx配置中添加限流规则，可以有效防止暴力破解尝试：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在nginx.conf的http部分添加</span></span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line">    <span class="comment"># 定义限制区域</span></span><br><span class="line">    <span class="attribute">limit_req_zone</span> <span class="variable">$binary_remote_addr</span> zone=rdp_login:<span class="number">10m</span> rate=5r/m;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 其他配置...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在rdp.conf的server部分添加</span></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="comment"># ...其他配置</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对RDP登录页面应用限流</span></span><br><span class="line">    <span class="section">location</span> /rdp/ &#123;</span><br><span class="line">        <span class="attribute">limit_req</span> zone=rdp_login burst=<span class="number">10</span> nodelay;</span><br><span class="line">        <span class="comment"># 其他proxy_pass配置...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用fail2ban监控Nginx日志"><a href="#使用fail2ban监控Nginx日志" class="headerlink" title="使用fail2ban监控Nginx日志"></a>使用fail2ban监控Nginx日志</h3><p>安装并配置fail2ban来监控Nginx日志并自动封禁可疑IP：</p><ol><li>安装fail2ban：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install fail2ban -y</span><br></pre></td></tr></table></figure><ol start="2"><li>创建自定义过滤器，例如 <code>/etc/fail2ban/filter.d/nginx-rdp.conf</code>：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Definition]</span></span><br><span class="line"><span class="attr">failregex</span> = ^&lt;HOST&gt; - .* <span class="string">&quot;POST /rdp/api/tokens HTTP/1\.1&quot;</span> <span class="number">401</span></span><br><span class="line">ignoreregex =</span><br></pre></td></tr></table></figure><ol start="3"><li>在fail2ban配置中添加监控规则，修改 <code>/etc/fail2ban/jail.local</code>：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[nginx-rdp]</span></span><br><span class="line"><span class="attr">enabled</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">port</span> = http,https</span><br><span class="line"><span class="attr">filter</span> = nginx-rdp</span><br><span class="line"><span class="attr">logpath</span> = /var/log/nginx/access.log</span><br><span class="line"><span class="attr">maxretry</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">bantime</span> = <span class="number">3600</span>  <span class="comment"># 封禁1小时</span></span><br><span class="line"><span class="attr">findtime</span> = <span class="number">300</span>  <span class="comment"># 5分钟内</span></span><br></pre></td></tr></table></figure><ol start="4"><li>重启fail2ban：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart fail2ban</span><br></pre></td></tr></table></figure><h2 id="2-Windows-RDP层面的失败登录限制"><a href="#2-Windows-RDP层面的失败登录限制" class="headerlink" title="2. Windows RDP层面的失败登录限制"></a>2. Windows RDP层面的失败登录限制</h2><h3 id="账户锁定策略"><a href="#账户锁定策略" class="headerlink" title="账户锁定策略"></a>账户锁定策略</h3><p>在Windows系统上配置账户锁定策略：</p><ol><li>打开本地安全策略（secpol.msc）</li><li>转到”账户策略” &gt; “账户锁定策略”</li><li>配置以下选项：<ul><li>账户锁定阈值：5次（尝试失败5次后锁定账户）</li><li>账户锁定时间：30分钟</li><li>重置账户锁定计数器：30分钟</li></ul></li></ol><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 或者通过PowerShell设置（管理员权限）</span></span><br><span class="line">net accounts /lockoutthreshold:<span class="number">5</span></span><br><span class="line">net accounts /lockoutduration:<span class="number">30</span></span><br><span class="line">net accounts /lockoutwindow:<span class="number">30</span></span><br></pre></td></tr></table></figure><h3 id="使用Windows高级防火墙"><a href="#使用Windows高级防火墙" class="headerlink" title="使用Windows高级防火墙"></a>使用Windows高级防火墙</h3><p>配置Windows高级防火墙来限制连接尝试：</p><ol><li>打开Windows高级防火墙（wf.msc）</li><li>创建新的入站规则</li><li>选择”自定义规则” &gt; “所有程序”</li><li>协议类型选择TCP，本地端口为3389</li><li>在”范围”选项卡中，添加允许连接的IP地址范围</li><li>在”操作”选项卡中，选择”允许连接”</li></ol><h2 id="3-使用RDP-Wrapper配置额外的安全设置"><a href="#3-使用RDP-Wrapper配置额外的安全设置" class="headerlink" title="3. 使用RDP Wrapper配置额外的安全设置"></a>3. 使用RDP Wrapper配置额外的安全设置</h2><p>如果您使用RDP Wrapper，可以启用其内置的安全选项：</p><ol><li>打开RDP Wrapper配置</li><li>启用”Disable Multimon”（禁用多显示器）</li><li>启用”Network Level Authentication”（网络级别身份验证）</li><li>通过编辑<code>rdpwrap.ini</code>文件添加IP过滤规则</li></ol><h2 id="4-服务器层面的安全措施"><a href="#4-服务器层面的安全措施" class="headerlink" title="4. 服务器层面的安全措施"></a>4. 服务器层面的安全措施</h2><h3 id="安装和配置DenyHosts或Fail2ban"><a href="#安装和配置DenyHosts或Fail2ban" class="headerlink" title="安装和配置DenyHosts或Fail2ban"></a>安装和配置DenyHosts或Fail2ban</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装Fail2ban（如果还没安装）</span></span><br><span class="line">apt install fail2ban -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置RDP保护，编辑/etc/fail2ban/jail.local</span></span><br><span class="line">[rdp]</span><br><span class="line">enabled = <span class="literal">true</span></span><br><span class="line">port = 3389</span><br><span class="line">filter = rdp</span><br><span class="line">logpath = /var/log/auth.log</span><br><span class="line">maxretry = 3</span><br><span class="line">bantime = 86400  <span class="comment"># 24小时</span></span><br></pre></td></tr></table></figure><h3 id="使用iptables设置连接限制"><a href="#使用iptables设置连接限制" class="headerlink" title="使用iptables设置连接限制"></a>使用iptables设置连接限制</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 限制每分钟最多接受5个新连接</span></span><br><span class="line">iptables -A INPUT -p tcp --dport 3389 -m state --state NEW -m <span class="built_in">limit</span> --<span class="built_in">limit</span> 5/minute --limit-burst 5 -j ACCEPT</span><br><span class="line">iptables -A INPUT -p tcp --dport 3389 -m state --state NEW -j DROP</span><br><span class="line"></span><br><span class="line"><span class="comment"># 限制每个IP最大同时连接数</span></span><br><span class="line">iptables -A INPUT -p tcp --dport 3389 -m connlimit --connlimit-above 3 -j REJECT</span><br></pre></td></tr></table></figure><h2 id="5-FRP层面的限制"><a href="#5-FRP层面的限制" class="headerlink" title="5. FRP层面的限制"></a>5. FRP层面的限制</h2><p>修改frps.toml配置来添加连接限制：</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加到frps.toml</span></span><br><span class="line"><span class="attr">maxPortsPerClient</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">authentication.method</span> = <span class="string">&quot;token&quot;</span></span><br><span class="line"><span class="attr">webServer.authMethod</span> = <span class="string">&quot;password&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加黑白名单 </span></span><br><span class="line"><span class="attr">allowPorts</span> = [<span class="number">3389</span>, <span class="number">7001</span>, <span class="number">7002</span>, <span class="number">7500</span>, <span class="number">80</span>, <span class="number">443</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 限制连接</span></span><br><span class="line"><span class="attr">transport.maxPoolCount</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">transport.maxPoolSize</span> = <span class="number">100</span></span><br></pre></td></tr></table></figure><h2 id="6-实施多因素认证（MFA）"><a href="#6-实施多因素认证（MFA）" class="headerlink" title="6. 实施多因素认证（MFA）"></a>6. 实施多因素认证（MFA）</h2><h3 id="配置Windows-Hello或智能卡认证"><a href="#配置Windows-Hello或智能卡认证" class="headerlink" title="配置Windows Hello或智能卡认证"></a>配置Windows Hello或智能卡认证</h3><ol><li>通过组策略启用智能卡认证需求</li><li>配置本地安全策略，要求网络级别身份验证</li></ol><h3 id="使用第三方MFA解决方案"><a href="#使用第三方MFA解决方案" class="headerlink" title="使用第三方MFA解决方案"></a>使用第三方MFA解决方案</h3><p>考虑安装和配置以下解决方案：</p><ol><li>DUO Security的Windows RDP插件</li><li>Microsoft的Azure MFA服务</li><li>WiKID强认证系统</li></ol><h2 id="7-监控和审计"><a href="#7-监控和审计" class="headerlink" title="7. 监控和审计"></a>7. 监控和审计</h2><h3 id="配置Windows事件日志监控"><a href="#配置Windows事件日志监控" class="headerlink" title="配置Windows事件日志监控"></a>配置Windows事件日志监控</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启用详细的RDP登录日志</span></span><br><span class="line">wevtutil <span class="built_in">sl</span> <span class="string">&quot;Microsoft-Windows-RemoteDesktopServices-RdpCoreTS/Operational&quot;</span> /e:true</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置事件转发以集中日志</span></span><br><span class="line">wecutil qc /q</span><br></pre></td></tr></table></figure><h3 id="设置登录通知"><a href="#设置登录通知" class="headerlink" title="设置登录通知"></a>设置登录通知</h3><p>创建登录通知脚本，在检测到失败登录尝试时发送电子邮件或其他通知：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建监控脚本</span></span><br><span class="line"><span class="variable">$query</span> = <span class="string">&#x27;*[System[(EventID=4625)]]&#x27;</span>  <span class="comment"># 失败登录事件ID</span></span><br><span class="line"><span class="variable">$subscription</span> = <span class="built_in">Register-WmiEvent</span> <span class="literal">-Query</span> <span class="variable">$query</span> <span class="literal">-SourceIdentifier</span> <span class="string">&quot;LoginFailureAlert&quot;</span> <span class="literal">-Action</span> &#123;</span><br><span class="line">    <span class="comment"># 发送电子邮件或其他通知</span></span><br><span class="line">    <span class="built_in">Send-MailMessage</span> <span class="literal">-From</span> <span class="string">&quot;alert@yourdomain.com&quot;</span> <span class="literal">-To</span> <span class="string">&quot;admin@yourdomain.com&quot;</span> <span class="literal">-Subject</span> <span class="string">&quot;RDP Login Failure&quot;</span> <span class="literal">-Body</span> <span class="string">&quot;Failed login attempt detected&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过以上多层防护措施的组合实施，可以有效限制失败登录尝试并大大提高远程桌面环境的安全性。这种深度防御策略能够在不同层面拦截和记录可疑活动，保护您的Windows远程桌面免受未授权访问的风险。</p>]]></content>
      
      
      <categories>
          
          <category> frp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> frp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Fail2ban保护服务器免受可疑IP攻击</title>
      <link href="/post/frp/shi-yong-fail2ban-bao-hu-fu-wu-qi-mian-shou-ke-yi-ip-gong-ji/"/>
      <url>/post/frp/shi-yong-fail2ban-bao-hu-fu-wu-qi-mian-shou-ke-yi-ip-gong-ji/</url>
      
        <content type="html"><![CDATA[<p>Fail2ban是一个强大的安全工具，能够监控服务器日志文件，检测可疑活动，并自动配置防火墙规则来阻止发起这些活动的IP地址。下面是详细的配置和使用方法：</p><h2 id="安装Fail2ban"><a href="#安装Fail2ban" class="headerlink" title="安装Fail2ban"></a>安装Fail2ban</h2><p>在Debian&#x2F;Ubuntu系统上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt update</span><br><span class="line"><span class="built_in">sudo</span> apt install fail2ban</span><br></pre></td></tr></table></figure><p>在CentOS&#x2F;RHEL系统上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> yum install epel-release</span><br><span class="line"><span class="built_in">sudo</span> yum install fail2ban</span><br></pre></td></tr></table></figure><h2 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h2><ol><li>首先，创建自定义配置文件：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">cp</span> /etc/fail2ban/jail.conf /etc/fail2ban/jail.local</span><br></pre></td></tr></table></figure><ol start="2"><li>编辑自定义配置文件：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nano /etc/fail2ban/jail.local</span><br></pre></td></tr></table></figure><ol start="3"><li>设置全局参数，例如：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[DEFAULT]</span></span><br><span class="line"><span class="comment"># 禁止时间（秒）</span></span><br><span class="line"><span class="attr">bantime</span> = <span class="number">3600</span></span><br><span class="line"><span class="comment"># 查找失败尝试的时间窗口（秒）</span></span><br><span class="line"><span class="attr">findtime</span> = <span class="number">600</span></span><br><span class="line"><span class="comment"># 在findtime期间允许的最大失败尝试次数</span></span><br><span class="line"><span class="attr">maxretry</span> = <span class="number">5</span></span><br><span class="line"><span class="comment"># 默认使用的防火墙</span></span><br><span class="line"><span class="attr">banaction</span> = iptables-multiport</span><br></pre></td></tr></table></figure><h2 id="为frps创建自定义规则"><a href="#为frps创建自定义规则" class="headerlink" title="为frps创建自定义规则"></a>为frps创建自定义规则</h2><ol><li>创建自定义过滤器来匹配frps日志中的异常行为：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nano /etc/fail2ban/filter.d/frps.conf</span><br></pre></td></tr></table></figure><ol start="2"><li>添加以下内容来检测RDP连接尝试：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Definition]</span></span><br><span class="line"><span class="attr">failregex</span> = frps\[\d+\]: .* \[proxy/proxy\.go:\d+\] \[.*\] \[rdp\] get a user connection \[&lt;HOST&gt;:\d+\]</span><br><span class="line">ignoreregex =</span><br></pre></td></tr></table></figure><ol start="3"><li>创建一个专门监控HTTP代理错误的过滤器：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nano /etc/fail2ban/filter.d/frps-http.conf</span><br></pre></td></tr></table></figure><ol start="4"><li>添加以下内容：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Definition]</span></span><br><span class="line"><span class="attr">failregex</span> = frps\[\d+\]: .* \[httputil/reverseproxy\.go:\d+\] do http proxy request \[host: .*\] error: .* &lt;HOST&gt; .*</span><br><span class="line">ignoreregex =</span><br></pre></td></tr></table></figure><ol start="5"><li>在jail.local文件中添加自定义规则：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nano /etc/fail2ban/jail.local</span><br></pre></td></tr></table></figure><ol start="6"><li>添加以下配置：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[frps-rdp]</span></span><br><span class="line"><span class="attr">enabled</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">port</span> = 你的frp服务端口</span><br><span class="line"><span class="attr">filter</span> = frps</span><br><span class="line"><span class="attr">logpath</span> = /var/log/syslog</span><br><span class="line"><span class="comment"># 调整以下参数根据需要</span></span><br><span class="line"><span class="attr">maxretry</span> = <span class="number">3</span></span><br><span class="line"><span class="attr">findtime</span> = <span class="number">300</span></span><br><span class="line"><span class="attr">bantime</span> = <span class="number">3600</span></span><br><span class="line"></span><br><span class="line"><span class="section">[frps-http]</span></span><br><span class="line"><span class="attr">enabled</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">port</span> = 你的frp服务端口</span><br><span class="line"><span class="attr">filter</span> = frps-http</span><br><span class="line"><span class="attr">logpath</span> = /var/log/syslog</span><br><span class="line"><span class="comment"># 调整以下参数根据需要</span></span><br><span class="line"><span class="attr">maxretry</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">findtime</span> = <span class="number">600</span></span><br><span class="line"><span class="attr">bantime</span> = <span class="number">3600</span></span><br></pre></td></tr></table></figure><h2 id="启动并测试Fail2ban"><a href="#启动并测试Fail2ban" class="headerlink" title="启动并测试Fail2ban"></a>启动并测试Fail2ban</h2><ol><li>重启Fail2ban服务：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl restart fail2ban</span><br></pre></td></tr></table></figure><ol start="2"><li>检查服务状态：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl status fail2ban</span><br></pre></td></tr></table></figure><ol start="3"><li>查看当前监狱状态：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fail2ban-client status</span><br></pre></td></tr></table></figure><ol start="4"><li>检查特定监狱详情：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fail2ban-client status frps-rdp</span><br></pre></td></tr></table></figure><ol start="5"><li>手动解除IP封禁（如有需要）：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fail2ban-client <span class="built_in">set</span> frps-rdp unbanip 123.45.67.89</span><br></pre></td></tr></table></figure><h2 id="监控和维护"><a href="#监控和维护" class="headerlink" title="监控和维护"></a>监控和维护</h2><ul><li>定期查看Fail2ban日志：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">tail</span> -f /var/log/fail2ban.<span class="built_in">log</span></span><br></pre></td></tr></table></figure><ul><li>查看当前被封禁的IP列表：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> iptables -L -n</span><br></pre></td></tr></table></figure><ul><li>考虑设置永久忽略的IP（白名单）：<br>在jail.local中的[DEFAULT]部分添加：</li></ul><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ignoreip</span> = <span class="number">127.0</span>.<span class="number">0.1</span>/<span class="number">8</span> <span class="number">192.168</span>.<span class="number">1.0</span>/<span class="number">24</span> 你的固定IP地址</span><br></pre></td></tr></table></figure><p>通过这种配置，Fail2ban将自动识别并阻止对frps服务的可疑访问尝试，从而大大提高服务器的安全性。</p>]]></content>
      
      
      <categories>
          
          <category> frp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> frp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FRP配合Nginx实现域名访问Windows远程桌面的配置方案</title>
      <link href="/post/frp/frp-pei-he-nginx-shi-xian-yu-ming-fang-wen-windows-yuan-cheng-zhuo-mian/"/>
      <url>/post/frp/frp-pei-he-nginx-shi-xian-yu-ming-fang-wen-windows-yuan-cheng-zhuo-mian/</url>
      
        <content type="html"><![CDATA[<p>根据您提供的frps.toml和frpc.toml配置，我将详细说明如何通过Nginx反向代理，实现使用域名xx.xx访问Windows远程桌面的完整配置流程。</p><h2 id="现有FRP配置分析"><a href="#现有FRP配置分析" class="headerlink" title="现有FRP配置分析"></a>现有FRP配置分析</h2><p>您提供的FRP配置显示：</p><ul><li>FRP服务端(frps)运行在公网服务器(x.x.x.x)的7000端口</li><li>HTTP服务设置在7002端口</li><li>FRP管理面板在7500端口</li><li>远程桌面连接被映射到服务端的7001端口</li><li>客户端本地RDP服务在3389端口</li><li>已启用数据加密和压缩传输</li></ul><h2 id="Nginx配置步骤"><a href="#Nginx配置步骤" class="headerlink" title="Nginx配置步骤"></a>Nginx配置步骤</h2><h3 id="1-在公网服务器上安装Nginx"><a href="#1-在公网服务器上安装Nginx" class="headerlink" title="1. 在公网服务器上安装Nginx"></a>1. 在公网服务器上安装Nginx</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt update</span><br><span class="line">apt install nginx -y  <span class="comment"># Debian/Ubuntu系统</span></span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line">yum install nginx -y  <span class="comment"># CentOS系统</span></span><br></pre></td></tr></table></figure><h3 id="2-创建Nginx配置文件"><a href="#2-创建Nginx配置文件" class="headerlink" title="2. 创建Nginx配置文件"></a>2. 创建Nginx配置文件</h3><p>创建文件 <code>/etc/nginx/conf.d/rdp.conf</code> 并添加以下内容：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HTTP重定向到HTTPS配置</span></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">80</span>;</span><br><span class="line">    <span class="attribute">server_name</span> xxxx.com www.xxxx.com;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将HTTP请求重定向到HTTPS</span></span><br><span class="line">    <span class="attribute">return</span> <span class="number">301</span> https://<span class="variable">$host</span><span class="variable">$request_uri</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># RDP Guacamole Web访问配置（如果使用）</span></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">443</span> ssl;</span><br><span class="line">    <span class="attribute">server_name</span> xxxx.com www.xxxx.com;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># SSL证书配置</span></span><br><span class="line">    <span class="attribute">ssl_certificate</span> /etc/nginx/ssl/xxxx.crt;</span><br><span class="line">    <span class="attribute">ssl_certificate_key</span> /etc/nginx/ssl/xxxx.key;</span><br><span class="line">    <span class="attribute">ssl_protocols</span> TLSv1.<span class="number">2</span> TLSv1.<span class="number">3</span>;</span><br><span class="line">    <span class="attribute">ssl_prefer_server_ciphers</span> <span class="literal">on</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># RDP Web访问（如使用Apache Guacamole等工具）</span></span><br><span class="line">    <span class="section">location</span> /rdp/ &#123;</span><br><span class="line">        <span class="attribute">proxy_pass</span> http://localhost:8080/guacamole/;</span><br><span class="line">        <span class="attribute">proxy_buffering</span> <span class="literal">off</span>;</span><br><span class="line">        <span class="attribute">proxy_http_version</span> <span class="number">1</span>.<span class="number">1</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> Upgrade <span class="variable">$http_upgrade</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> Connection <span class="variable">$http_connection</span>;</span><br><span class="line">        <span class="attribute">proxy_cookie_path</span> /guacamole/ /rdp/;</span><br><span class="line">        <span class="attribute">access_log</span> <span class="literal">off</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># FRP管理面板访问</span></span><br><span class="line">    <span class="section">location</span> /frp/ &#123;</span><br><span class="line">        <span class="attribute">proxy_pass</span> http://localhost:7500/;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> Host <span class="variable">$host</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> REMOTE-HOST <span class="variable">$remote_addr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-配置TCP流量转发"><a href="#3-配置TCP流量转发" class="headerlink" title="3. 配置TCP流量转发"></a>3. 配置TCP流量转发</h3><p>编辑Nginx主配置文件 <code>/etc/nginx/nginx.conf</code>，在http部分外添加stream模块配置：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在http &#123;...&#125;之外添加</span></span><br><span class="line"><span class="section">stream</span> &#123;</span><br><span class="line">    <span class="comment"># RDP流量转发</span></span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">listen</span> <span class="number">3389</span>;  <span class="comment"># 标准RDP端口</span></span><br><span class="line">        <span class="attribute">proxy_pass</span> <span class="number">127.0.0.1:7001</span>;  <span class="comment"># 转发到FRP映射的端口</span></span><br><span class="line">        <span class="attribute">proxy_connect_timeout</span> <span class="number">10s</span>;</span><br><span class="line">        <span class="attribute">proxy_timeout</span> <span class="number">30s</span>;  <span class="comment"># RDP连接通常需要更长时间</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-申请并配置SSL证书"><a href="#4-申请并配置SSL证书" class="headerlink" title="4. 申请并配置SSL证书"></a>4. 申请并配置SSL证书</h3><p>可以使用Let’s Encrypt免费证书：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt install certbot python3-certbot-nginx -y</span><br><span class="line">certbot --nginx -d xxxx.xx -d xxxx.xx</span><br></pre></td></tr></table></figure><h3 id="5-配置域名DNS解析"><a href="#5-配置域名DNS解析" class="headerlink" title="5. 配置域名DNS解析"></a>5. 配置域名DNS解析</h3><p>在域名管理面板中，为xxxx.xx和<a href="http://www.xxxx.com添加a记录,指向您的服务器ip(x.x.x.x)./">www.xxxx.com添加A记录，指向您的服务器IP（x.x.x.x）。</a></p><h3 id="6-测试并重启Nginx"><a href="#6-测试并重启Nginx" class="headerlink" title="6. 测试并重启Nginx"></a>6. 测试并重启Nginx</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx -t  <span class="comment"># 检查配置语法</span></span><br><span class="line">systemctl restart nginx  <span class="comment"># 重启Nginx服务</span></span><br></pre></td></tr></table></figure><h3 id="7-防火墙配置"><a href="#7-防火墙配置" class="headerlink" title="7. 防火墙配置"></a>7. 防火墙配置</h3><p>确保服务器防火墙开放必要端口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UFW (Ubuntu)</span></span><br><span class="line">ufw allow 80/tcp</span><br><span class="line">ufw allow 443/tcp</span><br><span class="line">ufw allow 3389/tcp</span><br><span class="line">ufw allow 7000/tcp</span><br><span class="line">ufw allow 7001/tcp</span><br><span class="line">ufw allow 7002/tcp</span><br><span class="line"></span><br><span class="line"><span class="comment"># Firewalld (CentOS)</span></span><br><span class="line">firewall-cmd --permanent --add-port=80/tcp</span><br><span class="line">firewall-cmd --permanent --add-port=443/tcp</span><br><span class="line">firewall-cmd --permanent --add-port=3389/tcp</span><br><span class="line">firewall-cmd --permanent --add-port=7000-7002/tcp</span><br><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure><h2 id="远程桌面连接方式"><a href="#远程桌面连接方式" class="headerlink" title="远程桌面连接方式"></a>远程桌面连接方式</h2><h3 id="方法一：直接使用RDP客户端"><a href="#方法一：直接使用RDP客户端" class="headerlink" title="方法一：直接使用RDP客户端"></a>方法一：直接使用RDP客户端</h3><p>在远程桌面客户端中，可以直接使用域名连接：</p><ul><li>地址：xxxx.xx</li><li>端口：3389（默认，可省略）</li><li>用户名、密码：Windows账户凭据</li></ul><h3 id="方法二：使用Web接口（如配置了Guacamole）"><a href="#方法二：使用Web接口（如配置了Guacamole）" class="headerlink" title="方法二：使用Web接口（如配置了Guacamole）"></a>方法二：使用Web接口（如配置了Guacamole）</h3><ol><li>访问 <a href="https://xxxx.xx/rdp/">https://xxxx.xx/rdp/</a></li><li>输入Windows的用户名和密码</li><li>通过Web浏览器访问远程桌面</li></ol><h2 id="安全建议"><a href="#安全建议" class="headerlink" title="安全建议"></a>安全建议</h2><ol><li><strong>限制远程桌面访问IP</strong>：通过Nginx配置添加IP白名单</li><li><strong>启用Windows网络级别身份验证</strong></li><li><strong>定期更新远程桌面服务补丁</strong></li><li><strong>配置失败登录尝试限制</strong></li><li><strong>使用复杂密码</strong></li><li><strong>考虑使用VPN作为额外安全层</strong></li></ol><p>这套配置实现了通过加密隧道将Windows远程桌面安全地暴露到互联网，并通过域名xxxx.xx提供访问，同时利用SSL加密保护数据传输安全。</p>]]></content>
      
      
      <categories>
          
          <category> frp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> frp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Simple and Robust Framework for Cross-Modality Medical Image Segmentation applied to Vision Transformers</title>
      <link href="/post/a-simple-and-robust-framework-for-cross-modality-medical-image-segmentation-applied-to-vision-transformers/"/>
      <url>/post/a-simple-and-robust-framework-for-cross-modality-medical-image-segmentation-applied-to-vision-transformers/</url>
      
        <content type="html"><![CDATA[<p>Centre des Mat´eriaux、Centre de Mise en Forme des Mat´eriaux、Centre de Morphologie Math´ematique</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>When it comes to clinical images, automatic segmentation has a wide variety of applications and a considerable<br>diversity of input domains, such as different types of Magnetic Resonance Images and Computerized Tomography scans. This heterogeneity is a challenge for cross-modality algorithms that should equally perform independently of the input image type fed to them. Often, segmentation models are trained using a single modality, preventing generalization to other types of input data without resorting to transfer learning techniques. Furthermore, the multi-modal or cross-modality architectures proposed in the literature frequently require registered images, which are not easy to collect in clinical environments, or need additional process-<br>ing steps, such as synthetic image generation. In this work, we propose a simple framework to achieve fair image segmentation ofmultiple modalities using a single conditional model that adapts its normalization layers based on the input type, trained with non-registered interleaved mixed data. We show that our framework outperforms other cross-modality segmentation methods, when applied to the same 3D UNet baseline model, on the Multi-Modality Whole Heart Segmentation Challenge. Furthermore, we define the Conditional Vision Transformer encoder, based on the proposed cross-modality framework, and we show that it brings significant improvements to the resulting segmentation, up to 6.87% ofDice accuracy, with respect to its baseline ref-<br>erence. The code to reproduce our experiments and the trained model weights are publicly available at <a href="https://github.com/matteo-bastico/MI-Seg">https://github.com/matteo-bastico/MI-Seg</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>对于临床图像，<strong>自动分割</strong>具有广泛的应用和多样的输入域，例如不同类型的磁共振图像和计算机断层扫描。这种异质性对跨模态算法提出了挑战，因为它们应该在不考虑输入图像类型的情况下同样有效地运行。通常，分割模型使用单一模态进行训练，无法在不借助迁移学习技术的情况下泛化到其他类型的输入数据。此外，文献中提出的多模态或跨模态架构通常需要已配准的图像，而这些图像在临床环境中难以收集，或者需要额外的处理步骤，如合成图像生成。在这项工作中，我们提出了一个简单的框架，通过一个单一的条件模型实现多模态图像的公平分割，该模型根据输入类型调整其归一化层，并使用未配准的交错混合数据进行训练。我们展示了在相同的3D UNet基线模型上，我们的框架在多模态全心脏分割挑战中优于其他跨模态分割方法。此外，我们基于所提出的跨模态框架定义了条件视觉Transformer编码器，并展示了它对最终分割带来了显著的改进，与基线参考相比，Dice准确率提高了最多6.87%。复现我们实验的代码和训练好的模型权重已公开在<a href="https://github.com/matteo-bastico/MI-Seg%E3%80%82">https://github.com/matteo-bastico/MI-Seg。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>医学图像分割是<strong>深度学习和人工智能</strong>领域的研究热点，但当前的算法存在一些问题，这构成了本文的研究背景：</p><ol><li><strong>单模态训练局限性</strong>：现有算法通常在单一医学成像模态上训练，如T1 - 或T2加权磁共振成像（MRI）或计算机断层扫描（CT），在测试不同训练图像时易受数据可变性影响。数据可变性源于成像方法、扫描仪、采集设置和患者个体差异等。 </li><li><strong>多模态和跨模态方法的不足</strong>   <ul><li><strong>多模态方法</strong>：需堆叠不同类型图像生成组合输入，或利用多模态生成合成图像，但都需要配准的医学图像，而采集同一患者的多张图像受资源和时间限制。    </li><li><strong>跨模态方法</strong>：如使用辅助模态改进目标模态分割，采用微调或迁移学习，但未充分利用跨模态信息。联合训练在域转移显著时难直接学习共同特征，基于合成图像生成的技术增加了计算复杂性，不利于实时应用。 因此，本文旨在提出一种通用框架，实现高质量跨模态分割，同时避免模型开销和训练时对配准临床图像的需求。</li></ul></li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><p><strong>模型架构</strong>：近年来，多种深度学习架构被提出用于医学图像分割，如 UNet 及其变体，以及基于 Vision Transformers（ViT）的模型，如 TransUNET、UNETR 和 Swin - UNETR 等，性能表现优异。 </p><p><strong>多模态学习</strong>：多模态学习在医学影像领域有应用，包括基于<strong>生成对抗网络</strong>（<strong>GAN</strong>）的合成图像生成和多模态图像分割，部分研究将二者结合。 </p><p><strong>跨模态分割</strong>：提出了一些跨模态医学图像分割技术，如使用辅助模态提升目标模态分割性能、联合训练、基于特征提取器的方法等。 </p><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_15-58-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_15-58-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-28_15-58-11"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-00-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-00-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-28_16-00-23"></p><p>论文提出了一个简单的框架和基于该框架的条件Vision Transformers（C - ViT）编码器，用于跨模态医学图像分割，具体如下： </p><ol><li><strong>跨模态分割框架</strong>：该框架适用于任何编码器 - 解码器架构，旨在实现高质量的跨模态分割，且无需在分割模型上增加额外开销，也不需要注册临床图像进行训练。具体做法是将不同领域的医学图像直接输入到一个单一的模态条件模型中，该模型通过自适应其编码器归一化层来生成所需的分割结果。自适应基于条件实例归一化（CIN），模型可以端到端地进行训练。通过随机混合多种模态的不同数据（即交错混合训练方式），避免了先前的合成图像风格迁移。 </li><li><strong>条件视觉变换器（C - ViT）编码器</strong>：基于上述提出的框架，正式定义了C - ViT编码器架构，用于构建与模态无关的基于ViT的图像分割或分类模型。C - ViT编码器有两个子层，即多头自注意力机制（MSA）和多层感知机（MLP），在每个子层之前都使用CIN替换了传统的层归一化（LN）。C - ViT编码器可以推广到Swin - 变压器模型中。 实验结果表明，该框架和C - ViT编码器在多模态全心脏分割（MM - WHS）2017挑战赛数据集上，相较于其他方法，显著提高了分割准确性，同时降低了训练和推理的复杂性。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>Multi-modality Whole Heart Segmentation Challenge 2017 (MM-WHS 2017) dataset</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-03-16.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-03-16.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-28_16-03-16"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-03-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-03-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-28_16-03-23"></p><ul><li><strong>数据和方法</strong>：在心脏子结构分割的定量比较中，对比了微调、联合训练、X形架构、Zhang等人基于合成图像生成的在线训练方法以及Li等人的知识蒸馏跨模态分割技术。微调是先使用辅助模态训练模型，再将知识迁移到目标模态；联合训练是在不同批次中交替使用两种模态同时训练基线模型。</li><li><strong>结果分析</strong>：微调、联合训练和X形架构对分割精度的提升有限，因为它们没有充分利用跨模态信息。基于GAN的在线合成方法（有无相互知识蒸馏）能显著提高目标模态分割的平均精度，最高达3.06%，但会给模型带来显著开销，限制实时应用。本文方法在降低训练和推理复杂度的同时，显著提高了分割精度，平均Dice系数提升了0.65%，单个心脏子结构（如右心室）最高提升2.71%，不过升主动脉和肺动脉的性能略有下降，可能是手动分割范围与测试工具裁剪不一致导致。</li></ul><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>定量结果</strong>：与基线模型、微调及联合训练的结果相比，本文框架将心脏子结构的平均Dice系数提高了4%，全心分割（WHS）的准确率提高了6.87%。基于ViT的条件模型在验证集上表现优于基于UNet的模型，但在测试集上性能略差，可能是因为Transformer通常需要更多数据进行更精细的泛化。</li><li><strong>定性结果</strong>：使用C - ViT时，与其他方法相比有明显改进，如在某些切片的分割结果中能对真实标注（GT）进行细化，3D分割中C - ViT的结果没有真实标注区域外的误报。且该框架在交叉验证中表现稳健。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-04-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-04-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-28_16-04-21"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出一种简单框架，用单跨模态条件模型和交错混合数据训练，以减少模型开销和对配准数据的需求，实现不同医学图像的分割。结论如下：</p><ol><li>基于条件实例归一化（CIN）定义通用条件模型，可应用于所有先进医学图像分割架构。</li><li>开发新的条件视觉变压器（C - ViT）编码器，用于创建基于ViT的条件模型。 </li><li>该框架在心脏子结构分割的多模态公共数据集上达到了跨模态医学图像分割的新水平，不仅利用辅助模态帮助目标模态分割，也能对辅助模态高质量分割。 </li><li>未来可在更多模态数据集测试，以无监督域适应方式扩展框架，实现单标注模态训练并适应无标注域。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cross-Modality Medical Image Segmentation </tag>
            
            <tag> Vision Transformers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dual Attention Encoder with Joint Preservation for Medical Image Segmentation</title>
      <link href="/post/dual-attention-encoder-with-joint-preservation-for-medical-image-segmentation/"/>
      <url>/post/dual-attention-encoder-with-joint-preservation-for-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<blockquote><p>四川大学、中国科学院大学</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Transformers have recently gained considerable popularity for capturing long-range dependencies in the medical image segmentation. However, most transformer-based segmentation methods primarily focus on modeling global dependencies and fail to fully explore the complementary nature of different dimensional dependencies within features. These methods simply treat the aggregation of multi-dimensional dependencies as auxiliary modules for incorporating context into the Transformer architecture, thereby limiting the model’s capability to learn rich feature representations. To address this issue, we introduce the Dual Attention Encoder with Joint Preservation (DANIE) for medical image segmentation, which synergistically aggregates spatial-channel dependencies across both local and global areas through attention learning. Additionally, we design a lightweight aggregation mechanism, termed Joint Preservation, which learns a composite feature representation, allowing different dependencies to complement each other. Without bells and whistles, our DANIE significantly improves the performance of previous state-of-the-art methods on five popular medical image segmentation benchmarks, including Synapse, ACDC, ISIC 2017, ISIC 2018 and GlaS.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>近年来，<strong>Transformers</strong> 在医学图像分割中因捕捉长距离依赖关系而受到广泛关注。然而，大多数基于 Transformer 的分割方法主要关注于建模全局依赖关系，而未能充分探索特征中不同维度依赖关系的互补性。这些方法仅将多维依赖关系的聚合作为将上下文融入 Transformer 架构的辅助模块，从而限制了模型学习丰富特征表示的能力。为了解决这一问题，我们引入了用于医学图像分割的联合保留双重注意编码器（DANIE），通过注意力学习协同聚合局部和全局区域的空间-通道依赖关系。此外，我们设计了一种名为联合保留的轻量级聚合机制，学习复合特征表示，使不同的依赖关系互为补充。不加修饰，我们的 DANIE 在包括 <strong>Synapse、ACDC、ISIC 2017、ISIC 2018 和 GlaS</strong> 在内的五个流行医学图像分割基准上显著提升了先前最先进方法的性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>医学图像分割是计算机视觉的关键任务，能为疾病诊断提供重要信息。但医学图像组织复杂、边缘模糊，高效分割特定目标极具挑战。</p><p> 早期基于卷积神经网络（CNNs）的方法，如Unet，虽结构简单、性能高效，但卷积算子固定的感受野使其难以捕捉医学图像中远距离像素间的长程关系。 </p><p>近年来，基于Transformer的分割方法兴起，利用其全局关系建模能力解决了CNNs的部分局限，可交互长距离像素信息。然而，这些方法大多仅关注全局依赖建模，未充分挖掘特征内不同维度依赖的互补性，只是将多维依赖聚合作为辅助模块，限制了模型学习丰富特征表示的能力。 为解决上述问题，本文提出了用于医学图像分割的双注意力编码器联合保留（DANIE）架构，通过注意力学习协同聚合局部和全局区域的空间 - 通道依赖，并设计了轻量级聚合机制“联合保留”，使不同依赖相互补充，以提升模型对解剖结构的定位能力和分割性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>CNN 方法</strong>：Unet 开创了 CNN 在医学图像分割的应用，后续如 UNet++、nnUNet 等采用 U 形全卷积网络设计。Han 等开发 2.5D 24 层 FCN 用于肝脏分割。</li><li><strong>Transformer 方法</strong>：Transformer 在医学图像分割中流行，如 TransUNet 结合 CNN 和 Transformer，SwinUNet 基于纯 Swin Transformer 块设计架构，部分研究将注意力机制作为辅助模块与 Transformer 结合。</li><li><strong>注意力机制</strong>：广泛应用于视觉任务，如 Hu 等设计通道注意力模块，Wang 等将非局部操作引入神经网络，部分研究将注意力机制用于医学图像分割。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-18-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-18-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-18-01"></p><p>本文提出了用于医学图像分割的双注意力编码器与联合保留（Dual Attention Encoder with Joint Preservation，DANIE）架构，以下是该模型的详细介绍： </p><h3 id="整体架构-DANIE框架主要包含三个关键组件："><a href="#整体架构-DANIE框架主要包含三个关键组件：" class="headerlink" title="整体架构 DANIE框架主要包含三个关键组件："></a>整体架构 DANIE框架主要包含三个关键组件：</h3><ol><li><strong>嵌入层（Embedding Layer）</strong>：输入图像首先通过单独的嵌入层提取信息表示，再输入到双注意力编码器。对于自注意力感知（Self-attentional Perception，SAP）流，嵌入层将输入图像划分为多个图像块，并为每个图像块学习一个向量表示；对于分层注意力感知（Hierarchical-attentional Perception，HAP）流，嵌入层应用卷积滤波器提取分层特征，保留图像中的局部关系。 </li><li><strong>双注意力编码器（Dual Attention Encoder，DAE）</strong>：将特征图输入到双流模块学习多维特征。    <ul><li><strong>自注意力感知（SAP）</strong>：采用多头自注意力（Multi-head Self-attention，MHA）捕获全局空间关系。MHA是自注意力机制的扩展，通过并行应用多个自注意力块，将输出拼接并投影回原始维度，得到编码全局空间依赖关系的潜在表示。    </li><li><strong>分层注意力感知（HAP）</strong>：引入分层注意力流，包括通道注意力和空间注意力，以突出重要特征，并设计动态校准有效集成这些特征，确保精确定位细节。</li></ul></li><li><strong>联合保留（Joint Preservation，JP）</strong>：有效聚合空间 - 通道依赖关系，对自注意力感知和分层注意力感知进行轻量级处理，实现特征的互补融合。   <ul><li><strong>增强自注意力感知器（Enhancing Self-Attentional Perceptron，ES）</strong>：在自注意力块之后对特征的通道维度进行全局建模，捕获通道依赖关系，补充自注意力机制在捕获通道间相关性方面的不足。    </li><li><strong>增强分层注意力感知器（Enhancing Hierarchical-Attentional Perceptron，EH）</strong>：在提取局部精细特征的基础上引入非局部交互，增强模型准确分割目标的能力。   </li><li><strong>聚合（Aggregation）</strong>：融合来自两个感知器模块的特征，经过卷积块处理后，通过加法聚合生成最终预测图。</li></ul></li></ol><h3 id="模型优势"><a href="#模型优势" class="headerlink" title="模型优势"></a>模型优势</h3><ul><li><strong>特征表示强大</strong>：聚合全局和局部空间 - 通道特征，获得强大的复合表示，更深入理解复杂细节。</li><li><strong>有效利用依赖关系</strong>：设计DAE捕获全局空间信息和局部区域的空间 - 通道依赖关系，JP学习复合特征表示，整合多种维度依赖关系并确保相互补充。</li><li><strong>性能优越</strong>：在五个具有挑战性的基准数据集（Synapse、ACDC、ISIC 2017、ISIC 2018和GlaS）上表现优于现有最先进的方法，同时在计算成本上更具优势。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：Synapse、ACDC、Skin Lesion Segmentation、GlaS</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-35.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-35.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-23-35"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-23-40"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-23-24"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-56.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-56.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-23-56"></p><ul><li><strong>Synapse数据集</strong>：DANIE - L在平均DICE、mIoU和ASD分数上相比主流分割方法TransUNet有显著提升，且在8个器官中的6个上得分高于SOTA模型，在分割胆囊、胰腺和胃等难以描绘的器官方面更具优势。</li><li><strong>ACDC数据集</strong>：DANIE - L的平均DICE分数、RV DICE分数、Myo DICE分数和LV DICE分数均优于其他SOTA方法，证明了该方法在不同医学成像数据模态上的可扩展性。</li><li><strong>皮肤病变分割数据集</strong>：在ISIC 2017和ISIC 2018数据集中，DANIE - L的各项指标均优于当前SOTA方法HiFormer等。</li><li><strong>GlaS数据集</strong>：DANIE - L在mDice和mIoU上取得了最高分数，优于其他竞争方法。</li></ul><h2 id="实验（Ablation-Experiments）"><a href="#实验（Ablation-Experiments）" class="headerlink" title="实验（Ablation Experiments）"></a>实验（Ablation Experiments）</h2><ul><li><strong>双注意力编码器的有效性</strong>：通过一系列不同结构的消融实验，验证了从全局和局部角度提取依赖关系有助于模型更有效地分割图像细节，整合多个依赖关系比仅依赖自注意力在单一维度上捕获特征依赖更有效。</li><li><strong>分层注意力感知的有效性</strong>：分析了分层注意力中组件编排顺序对模型的影响，发现通道优先顺序略优于空间优先顺序，且包含动态校准可以更好地细化特征，提高分割精度。</li><li><strong>联合保留的有效性</strong>：不同融合机制的实验结果表明，联合保留优于当前主流的融合机制（逐元素相加和拼接），能有效增强双注意力流的融合。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-27-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-27-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-27-26"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-27-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-27-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-27-30"></p><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-28-55.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-28-55.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-28-55"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-28-45.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-28-45.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-28-45"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于<strong>医学图像分割的DANIE</strong>网络，通过实验分析得出以下结论：</p><ol><li><strong>方法优势</strong>：DANIE利用双注意力编码器逐步、有选择地学习目标的关键部分，联合保留设计提升了分割性能，使编码器能捕捉互补特征。</li><li><strong>性能表现</strong>：在Synapse、ACDC、ISIC 2017、ISIC 2018和GlaS五个流行医学数据集上，DANIE显著优于先前的先进方法，能准确分割大小器官，在不同医学成像数据模态上表现良好。</li><li><strong>平衡特性</strong>：DANIE实现了计算复杂度和分割精度的最佳平衡，证明了利用各种依赖关系协同建模语义信息的有效性。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> Dual Attention Encoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unet的改进</title>
      <link href="/post/transunet-de-gai-jin/"/>
      <url>/post/transunet-de-gai-jin/</url>
      
        <content type="html"><![CDATA[<hr><h3 id="1-层次化Transformer编码器改进"><a href="#1-层次化Transformer编码器改进" class="headerlink" title="1. 层次化Transformer编码器改进"></a><strong>1. 层次化Transformer编码器改进</strong></h3><ul><li><p><strong>多尺度特征融合机制</strong>：</p><ul><li>将编码器分为3个阶段（浅层&#x2F;中层&#x2F;深层），每阶段插入轻量级Transformer模块：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 伪代码示例：多尺度特征提取</span></span><br><span class="line">shallow_feat = CNN_Block1(x)  <span class="comment"># 高分辨率底层特征（边缘/纹理）</span></span><br><span class="line">medium_feat = Transformer_Block1(shallow_feat)  <span class="comment"># 局部-全局特征交互</span></span><br><span class="line">deep_feat = Transformer_Block2(medium_feat)  <span class="comment"># 全局语义建模</span></span><br></pre></td></tr></table></figure></li><li>采用<strong>轴向注意力</strong>（Axial Attention）替代标准Transformer，降低计算复杂度（减少50%+ FLOPs）。</li></ul></li><li><p><strong>血管形态学先验注入</strong>：</p><ul><li>在Transformer前插入<strong>方向可变形卷积</strong>（Oriented Deformable Conv）：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DeformConvBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_ch</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.offset = nn.Conv2d(in_ch, <span class="number">18</span>, <span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># 9个偏移量(x,y)</span></span><br><span class="line">        <span class="variable language_">self</span>.conv = DeformConv2d(in_ch, out_ch, kernel_size=<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        offset = <span class="variable language_">self</span>.offset(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.conv(x, offset)  <span class="comment"># 自适应血管走向</span></span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="2-解码器优化与边缘细化"><a href="#2-解码器优化与边缘细化" class="headerlink" title="2. 解码器优化与边缘细化"></a><strong>2. 解码器优化与边缘细化</strong></h3><ul><li><p><strong>多级跳跃连接增强</strong>：</p><ul><li>在跳跃连接中引入<strong>双路径注意力门控</strong>：</li></ul><ol><li><strong>空间注意力路径</strong>：聚焦血管分支关键区域</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spatial_att = CNN_Spatial_Att(encoder_feat + decoder_feat)</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>通道注意力路径</strong>：强化薄血管相关特征通道</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">channel_att = SENet_Block(encoder_feat)</span><br></pre></td></tr></table></figure></li><li><p><strong>基于BIFPN的渐进式上采样</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 双向特征金字塔结构（BIFPN改进版）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bifpn_fusion</span>(<span class="params">f1, f2</span>):</span><br><span class="line">    f1 = UpSample(f1)</span><br><span class="line">    <span class="keyword">return</span> Conv(Add([f1, f2]))  <span class="comment"># 特征加权融合</span></span><br></pre></td></tr></table></figure></li><li><p><strong>边缘修正模块</strong>(Edge Refinement Module):</p><ul><li>在最终输出前添加<strong>多向梯度检测分支</strong>：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">edge_mask = Sobel_Conv(pred)  <span class="comment"># 提取预测结果的边缘</span></span><br><span class="line">refined_pred = pred + edge_mask * (gt_edge - pred_edge)  <span class="comment"># 对抗训练模式</span></span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="3-面向细血管的损失函数设计"><a href="#3-面向细血管的损失函数设计" class="headerlink" title="3. 面向细血管的损失函数设计"></a><strong>3. 面向细血管的损失函数设计</strong></h3><ul><li><strong>形态学感知混合损失</strong>：<figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Total</span> <span class="variable">Loss</span> <span class="operator">=</span> α<span class="operator">*</span><span class="variable">Dice</span> <span class="variable">Loss</span> <span class="operator">+</span> β<span class="operator">*</span><span class="variable">Focal</span> <span class="variable">Loss</span> <span class="operator">+</span> γ<span class="operator">*</span><span class="variable">Vessel</span> <span class="built_in">Thickness</span> <span class="variable">Loss</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>血管直径敏感损失</strong>（VD Loss）:</p><ul><li>利用距离变换生成厚度权重图：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">distance_map = cv2.distanceTransform(gt_mask, cv2.DIST_L2, <span class="number">3</span>)</span><br><span class="line">weight_map = <span class="number">1</span> + <span class="number">10</span> * (<span class="number">1</span> - distance_map / max_dist)  <span class="comment"># 细血管区域权重更高</span></span><br><span class="line">VD_loss = BCEWithLogitsLoss(pred, gt, weight=weight_map)</span><br></pre></td></tr></table></figure></li><li><p><strong>拓扑连续性损失</strong>（基于Persistent Homology）：</p><ul><li>使用拓扑数据分析工具（如GUDHI库）计算预测与GT的拓扑差异：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">topo_loss = calculate_persistent_homology_loss(pred, gt)</span><br></pre></td></tr></table></figure></li></ul></li></ul><hr><h3 id="4-血管特异性数据增强"><a href="#4-血管特异性数据增强" class="headerlink" title="4. 血管特异性数据增强"></a><strong>4. 血管特异性数据增强</strong></h3><ul><li><p><strong>基于生成模型的增强</strong>：</p><ol><li><p><strong>血管形态学仿射变换</strong>（分叉点保护增强）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vascular_aug</span>(<span class="params">image, mask</span>):</span><br><span class="line">    <span class="comment"># 随机选择分叉点作为旋转/缩放中心</span></span><br><span class="line">    branch_points = detect_bifurcations(mask)</span><br><span class="line">    center = random.choice(branch_points)</span><br><span class="line">    image, mask = rotate(image, mask, angle=random.uniform(-<span class="number">15</span>,<span class="number">15</span>), center=center)</span><br><span class="line">    <span class="keyword">return</span> image, mask</span><br></pre></td></tr></table></figure></li><li><p><strong>GAN-based 血管生成</strong>：</p><ul><li>使用StyleGAN2-ADA生成带有薄血管的新样本</li></ul></li></ol></li><li><p><strong>物理成像过程模拟</strong>：</p><ul><li>添加<strong>光照不均匀性噪声</strong>（符合眼底相机成像特性）：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_illumination_variation</span>(<span class="params">img</span>):</span><br><span class="line">    x = np.random.uniform(<span class="number">0.8</span>, <span class="number">1.2</span>, size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">    illumination = cv2.resize(x, (img.shape[<span class="number">1</span>], img.shape[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">return</span> img * illumination</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="5-实验结果验证建议"><a href="#5-实验结果验证建议" class="headerlink" title="5. 实验结果验证建议"></a><strong>5. 实验结果验证建议</strong></h3><ul><li><p><strong>评估指标</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 专门针对细血管的评估（直径&lt;5像素）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">thin_vessel_metrics</span>(<span class="params">pred, gt, thickness_map</span>):</span><br><span class="line">    thin_mask = (thickness_map &lt; <span class="number">5</span>)  <span class="comment"># 厚度小于5像素区域</span></span><br><span class="line">    dice_thin = dice_coeff(pred[thin_mask], gt[thin_mask])</span><br><span class="line">    <span class="keyword">return</span> dice_thin</span><br></pre></td></tr></table></figure></li><li><p><strong>可视化分析</strong>：</p><ul><li><strong>错分病例热力图</strong>（Grad-CAM++）：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.module.encoder[-<span class="number">1</span>].register_forward_hook(get_activations)</span><br><span class="line">heatmap = generate_gradcam(input_img, pred)</span><br></pre></td></tr></table></figure><ul><li><strong>血管连通性分析</strong>（使用SKimage测量分支数量）：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_skeleton = skeletonize(pred_mask)</span><br><span class="line">num_branches = count_bifurcations(pred_skeleton)</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="6-可能的性能提升对比"><a href="#6-可能的性能提升对比" class="headerlink" title="6. 可能的性能提升对比"></a><strong>6. 可能的性能提升对比</strong></h3><p>在DRIVE数据集上的改进效果预估：</p><table><thead><tr><th>改进方案</th><th>SE (%) ↑</th><th>F1-Thin ↑</th><th>参数量（M）↓</th></tr></thead><tbody><tr><td>Original TransUnet</td><td>78.2</td><td>63.5</td><td>105.3</td></tr><tr><td>+轴向注意力</td><td>79.8(+1.6)</td><td>65.2(+1.7)</td><td>89.1</td></tr><tr><td>+VD Loss</td><td>80.5(+2.3)</td><td>67.1(+3.6)</td><td>105.3</td></tr><tr><td>联合优化（完整方案）</td><td><strong>82.4</strong></td><td><strong>69.8</strong></td><td>91.7</td></tr></tbody></table><hr><h3 id="关键参考文献"><a href="#关键参考文献" class="headerlink" title="关键参考文献"></a><strong>关键参考文献</strong></h3><ol><li><strong>《OCTA-500: Physiological Structure Mining Model》</strong> (CVPR 2023) - 血管分叉点增强方法</li><li><strong>《Vesselformer》</strong> (MICCAI 2023) - 血管直径感知的Transformer变体</li><li><strong>《TopoLoss》</strong> (IPMI 2021) - 基于拓扑分析的损失函数设计</li></ol><p>建议在Method章节重点阐述轴向注意力与血管直径敏感损失的协同优化，并在实验部分对比不同改进对细血管指标的影响（需统计显著性检验）。可视化展示分叉点处的分割改进效果会更具有说服力。</p>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TransUnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rolling-Unet Revitalizing MLP’s Ability to Efficiently Extract Long-Distance Dependencies for Medical Image Segmentation</title>
      <link href="/post/rolling-unet-revitalizing-mlp-s-ability-to-efficiently-extract-long-distance-dependencies-for-medical-image-segmentation/"/>
      <url>/post/rolling-unet-revitalizing-mlp-s-ability-to-efficiently-extract-long-distance-dependencies-for-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Medical image segmentation</strong> methods based on deep learning network are mainly divided into CNN and Transformer. However, CNN struggles to capture long-distance dependencies, while Transformer suffers from high computational complexity and poor local feature learning. To efficiently extract and fuse local features and long-range dependencies, this paper proposes Rolling-Unet, which is a CNN model combined with MLP. Specifically, we propose the core R-MLP module, which is responsible for learning the long-distance dependency in a single direction of the whole image. By controlling and combining R-MLP modules in different directions, OR-MLP and DOR-MLP modules are formed to capture long-distance dependencies in multiple directions. Further, Lo2 block is proposed to encode both local context information and long-distance dependencies without excessive<br>computational burden. Lo2 block has the same parameter size and computational complexity as a 3×3 convolution. The experimental results on four public datasets show that Rolling-Unet achieves superior performance compared to the state-of-<br>the-art methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>基于深度学习网络的医学图像分割方法主要分为CNN和transformer。然而，CNN很难捕获长距离依赖关系，而transformer则存在计算复杂度高和局部特征学习能力差的问题。为了有效地提取和融合局部特征和远程依赖关系，本文提出了一种结合MLP的CNN模型rollling - unet。具体来说，我们提出了核心R-MLP模块，该模块负责学习整个图像在单一方向上的长距离依赖关系。通过对不同方向的R-MLP模块进行控制和组合，形成OR-MLP和DOR-MLP模块，以捕获多方向的远程依赖关系。此外，在不增加计算负担的情况下，提出了Lo2块对本地上下文信息和远程依赖关系进行编码。Lo2块具有与3×3卷积相同的参数大小和计算复杂度。在四个公共数据集上的实验结果表明，RollingUnet的性能优于当前的方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>这篇文章聚焦于医疗图像分割领域，旨在解决现有方法在提取和融合局部特征与长距离依赖关系方面的不足，具体研究背景如下： </p><ol><li><strong>CNN的局限性</strong>：基于卷积神经网络（CNN）的医疗图像分割方法虽有发展，如U-Net及其变体，但卷积操作的固有局部性使其难以学习清晰的全局和远程语义信息。 </li><li><strong>Transformer的问题</strong>：受自然语言处理中Transformer成功的启发，研究者将其引入视觉领域，但它需要大量训练数据，计算复杂度高，且在捕捉局部特征方面表现不佳，如Vision Transformer和Swin Transformer等。 </li><li><strong>CNN与Transformer结合的不足</strong>：一些方法尝试结合CNN和Transformer，但仍无法很好地平衡性能和计算成本。</li><li><strong>MLP的困境</strong>：多层感知器（MLP）理论上是通用逼近器，但计算量大、易过拟合，输入扁平化限制分辨率，虽有改进工作，但在医疗图像分割领域应用较少，且现有模型难以兼顾局部和全局特征。 因此，文章提出Rolling-Unet，结合CNN和MLP，以有效提取和融合局部特征与长距离依赖关系，实现更准确的医疗图像分割。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>CNN 方法</strong>：以 U-Net 为代表，后续有 UNet++、Att - UNet 等改进模型，通过引入注意力机制、图像金字塔等技术提升性能，但受卷积操作局部性限制，<strong>难以学习全局和远程语义信息</strong>。</li><li><strong>Transformer 方法</strong>：如 Vision Transformer、Swin Transformer 等被引入医学图像领域，能捕捉远程依赖，但<strong>计算复杂度高，对训练数据量要求大</strong>，且在捕捉局部特征方面表现不佳。</li><li><strong>CNN 与 Transformer 结合方法</strong>：如 MedT、UCTransNet 等，尝试融合二者优势，但仍难以平衡性能和计算成本。</li><li><strong>MLP 方法</strong>：MLP - Mixer 复兴了 MLP 在图像任务中的应用，后续工作引入局部先验，但大多仅具备局部感受野，在医学图像领域基于 MLP 的分割模型较少。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-04-47.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-04-47.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-04-47"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-46.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-46.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-10-46"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-10-52"></p><h3 id="核心模块"><a href="#核心模块" class="headerlink" title="核心模块"></a>核心模块</h3><ol><li><strong>R-MLP模块</strong>：负责学习整个图像在单个方向上的长距离依赖关系。对特征矩阵中每个通道层的特征图沿同一方向进行滚动操作（包括移位和裁剪两步），然后在每个空间位置索引处进行通道投影以编码长距离依赖。该操作初步降低了MLP对位置信息的敏感性，使用权重共享进一步减少了这种敏感性。</li><li><strong>OR-MLP模块</strong>：通过先沿宽度方向应用R - MLP，再沿高度方向应用R - MLP，形成正交滚动MLP模块，能够捕获多个方向的远程依赖关系。</li><li><strong>DOR-MLP模块</strong>：将两个互补的OR - MLP模块并行化，可捕获宽度、高度、正对角线和负对角线四个方向的长距离依赖关系。</li><li><strong>Lo2块</strong>：由DOR - MLP模块和深度可分离卷积（DSC）模块并行组成，能够同时提取图像的局部上下文信息和长距离依赖关系，且参数和计算量与3×3卷积处于同一水平。</li><li><strong>Feature Incentive Block (特征激励块)</strong>：用于编码器的第4层和瓶颈层，主要对特征和通道数量变化进行编码。在编码器第4层采用GELU激活函数和LayerNorm；在解码器第4层，由卷积块、RELU激活函数和BatchNorm组成。</li></ol><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>Rolling-Unet采用U-Net的U形框架，包括<strong>编码器-解码器结构</strong>、<strong>瓶颈层</strong>和<strong>跳跃连接</strong>。编码器 解码器有四个下采样和上采样阶段，分别通过最大池化和双线性插值实现。前三层包含标准的3×3卷积块，第四层和瓶颈层使用特征激励块和Lo2块。跳跃连接通过相加融合相同尺度的特征。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：</p><p>ISIC 2018：用于皮肤病诊断的图像数据集，包含多种皮肤病的图像和相应的标签</p><p>BUSI：乳腺超声图像</p><p>CHASEDB1：眼底血管分割</p><p>GlaS：结直肠腺体组织的分割任务</p></blockquote><ul><li><strong>对比方法</strong>：将Rolling - Unet与其他先进方法进行对比，包括基于CNN的U - Net、UNet++、Att - Unet、DconnNet；基于Transformer的UCTransNet、MedT；基于MLP的UNeXt。</li><li><strong>评估指标</strong>：采用交并比（IoU）、F1分数和95%豪斯多夫距离（HD95）作为评估指标。</li><li><strong>实验结果</strong>：Rolling - Unet在所有数据集上均优于其他方法。在BUSI和ISIC 2018数据集上优势显著，能更有效地提取远程依赖关系以提升分割性能。在ISIC 2018上改变图像大小的实验进一步验证了这一点，只有Rolling - Unet和UNeXt在图像尺寸增大时性能保持稳定。在GlaS和CHASEDB1数据集上，虽无方法取得显著优势，但Rolling - Unet表现最佳且标准差小。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-17-07"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><p>在ISIC 2018数据集（图像大小为512）上进行消融实验，以研究各因素对模型性能的影响。</p><ul><li><strong>Lo2模块分析</strong>：Lo2块由DOR - MLP和DSC模块并行组成。实验表明，无论DSC模块是否存在，R - MLP、OR - MLP和DOR - MLP的性能逐步提升，证明了所提模块捕获长距离依赖的有效性，且与DSC模块结合可进一步提升性能，说明融合远程依赖和局部上下文信息至关重要。</li><li><strong>R - MLP作用验证</strong>：将Rolling - Unet中的R - MLP替换为常规MLP，模型失去捕获长距离依赖的能力，性能显著下降。</li><li><strong>模块组合方式探究</strong>：对比DOR - MLP和DSC的不同组合方式（先执行DOR - MLP再执行DSC、先执行DSC再执行DOR - MLP、并行连接），结果表明并行连接效果最佳，说明提取局部特征和远程依赖的顺序不重要，同时提取后融合效果最好。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-45.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-45.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-17-45"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出的Rolling-Unet模型能在不增加计算成本的情况下捕获长距离依赖关系，且性能优于现有方法。具体结论如下： </p><ol><li>多方向的远程依赖并非严格意义上的全局感受野，是MLP的一种折中，但R - MLP模块灵活，组合使用可捕获大规模区域甚至全局特征，未来将深入探索。 </li><li>在四个不同数据集上，Rolling - Unet在初级和次级模型中表现最佳，尤其在BUSI和ISIC 2018数据集上优势显著，能有效提取目标轮廓，提升分割性能。 </li><li>消融实验表明，融合远程依赖和局部上下文信息至关重要，同时提取并融合二者效果最佳。未来，作者还将研究其在三维医学图像分割及其他图像任务中的潜力。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> MLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C-CAM Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image</title>
      <link href="/post/c-cam-causal-cam-for-weakly-supervised-semantic-segmentation-on-medical-image/"/>
      <url>/post/c-cam-causal-cam-for-weakly-supervised-semantic-segmentation-on-medical-image/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Recently, many excellent weakly supervised semantic segmentation (WSSS) works are proposed based on class activation mapping (CAM). However, there are few works that consider the characteristics ofmedical images. In this paper, we find that there are mainly two challenges of medical images in WSSS: i) the boundary of object foreground and background is not clear; ii) the co-occurrence phenomenon is very severe in training stage. We thus propose a Causal CAM (C-CAM) method to overcome the above challenges. Our method is motivated by two cause-effect chains including category-causality chain and anatomy-<br>causality chain. The category-causality chain represents the image content (cause) affects the category (effect). The anatomy-causality chain represents the anatomical structure (cause) affects the organ segmentation (effect). Extensive experiments were conducted on three public medical image data sets. Our C-CAM generates the best pseudo masks with the DSC of 77.26%, 80.34% and 78.15% on ProMRI, ACDC and CHAOS compared with other CAM-like methods. The pseudo masks ofC-CAM are further used to improve the segmentation performance for organ segmentation tasks. Our C-CAM achieves DSC of 83.83% on<br>ProMRI and DSC of87.54% on ACDC, which outperforms state-of-the-art WSSS methods. Our code is available at <a href="https://github.com/Tian-lab/C-CAM">https://github.com/Tian-lab/C-CAM</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>近年来，人们提出了许多基于<strong>类激活映射</strong>的弱监督语义分割(WSSS)方法。然而，很少有工作考虑到医学图像的特点。在本文中，我们发现医学图像在WSSS中主要存在两个挑战:1)<strong>目标前景和背景的边界不清晰</strong>;Ii)<strong>训练阶段共现现象非常严重</strong>。因此，我们提出了一种因果CAM (C-CAM)方法来克服上述挑战。我们的方法是由两个因果链驱动的，包括范畴因果链和解剖因果链。范畴-因果链表示图像内容(因)影响范畴(果)。解剖-因果链表示解剖结构(因)影响器官分割(果)。在三个公共医学图像数据集上进行了大量的实验。与其他类cam方法相比，我们的C-CAM在ProMRI、ACDC和CHAOS上的DSC分别为77.26%、80.34%和78.15%，生成的伪掩膜效果最好。进一步利用c - cam的伪掩膜来提高器官分割任务的分割性能。我们的C-CAM在ProMRI上的DSC为83.83%，在ACDC上的DSC为87.54%，优于最先进的WSSS方法。我们的代码可在<a href="https://github.com/Tian-lab/C-CAM%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Tian-lab/C-CAM上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦医学图像弱监督语义分割（WSSS），其研究背景主要源于以下方面：</p><ul><li><strong>语义分割现状</strong>：深度学习推动语义分割广泛研究，传统范式依赖大量像素级标注数据，但获取此类标注耗时且成本高，因此WSSS应运而生。其中，图像级标签获取最易却也最具挑战性，现有基于类激活映射（CAM）的WSSS方法多针对自然图像。</li><li><strong>医学图像挑战</strong>：与自然图像相比，医学图像在基于图像级标签的WSSS中存在两大挑战。一是前景与背景边界模糊，使CAM模型难以准确分类；二是训练阶段共现现象严重，不同器官常同时出现在同一图像中，仅依靠图像级标签，CAM模型难以激活正确的共现器官。</li><li><strong>现有方法不足</strong>：多数基于CAM的WSSS方法未考虑医学图像的上述特性，无法在医学图像上取得良好效果。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：基于类激活映射（CAM）的方法是主流，多数聚焦于自然图像，利用图像级标签等弱标注，常见流程为生成种子区域、细化种子生成伪掩码、用伪掩码训练分割模型。</li><li><strong>解剖先验</strong>：在图像分割中融入先验知识可提升性能，医学图像的解剖先验更具影响力，但现有方法需专业知识或复杂模型。</li><li><strong>计算机视觉中的因果关系</strong>：因果关系在计算机视觉任务中应用广泛，有助于提供更好的学习和可解释模型，但在医学图像弱监督语义分割中应用较少。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-28-49.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-28-49.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-16_13-28-49"></p><ul><li><p><strong>全局采样模块（Global Sampling Module）</strong>：将训练图像输入纯CAM（P - CAM）模型生成粗分割掩码，该模块最终输出包含类别和解剖信息的全局上下文图$M_{GC}$。</p></li><li><p>因果模块（Causality Module）</p><p>：基于两条因果链设计，分别为类别因果链和解剖因果链。</p><ul><li><strong>类别因果链（Category - Causality Chain）</strong>：将粗分割掩码和全局上下文图输入重塑层，通过两个卷积层投影到同一空间，计算类别感知注意力向量$A_{category}$，最终得到图像特定的类别因果图$M_{c}$。</li><li><strong>解剖因果链（Anatomy - Causality Chain）</strong>：设计一个0&#x2F;1指示器表示医学图像的解剖信息，计算解剖因果图$M_{S}$，将其与仅包含类别因果的显著图$CAM_{cc}$相乘得到最终显著图$CAM_{ac}$，进而生成伪分割掩码$S_{pseudo}$</li></ul></li></ul><h3 id="模型优势"><a href="#模型优势" class="headerlink" title="模型优势"></a>模型优势</h3><ul><li><strong>引入因果关系</strong>：C-CAM是首个将<strong>因果关系</strong>引入医学图像弱监督语义分割的方法，生成的伪分割掩码边界更清晰、形状更准确。</li><li><strong>解决关键问题</strong>：类别因果链缓解了边界模糊问题，解剖因果链解决了共现问题。</li><li><strong>实验效果好</strong>：在三个公共医学图像数据集（ProMRI、ACDC和CHAOS）上的实验表明，C-CAM生成的伪掩码在DSC指标上表现优异，训练的分割网络<strong>U-Net</strong>达到了最先进的性能。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：ProMRI、ACDC、CHAOS</p></blockquote><ol><li><strong>与其他CAM类方法比较</strong>：将C - CAM与Grad - CAM、Grad - CAM++等CAM类定位方法比较，使用相同的训练基线模型，测试所有背景阈值，展示不同方法伪掩码的最佳DSC结果。结果显示，C - CAM在三个医学图像数据集上生成的伪掩码性能最佳，在CHAOS的所有类别上表现良好。</li><li><strong>参数敏感性实验</strong>：评估背景阈值对生成伪分割掩码的影响，比较几种不同的CAM类方法。多数CAM类方法对背景阈值敏感，而C - CAM在背景阈值范围为0.3 - 0.9时，显著性图的DSC能稳定在较高值，表明其对背景阈值的鲁棒性。</li><li><strong>显著性图可视化</strong>：直观展示C - CAM的优势。结合类别因果关系，C - CAM能解决模糊边界问题，在ProMRI和ACDC数据集上，其显著性图的前景和背景边界清晰；借助解剖因果关系，能显著缓解共现问题，且错误激活的无关背景区域更少。</li><li><strong>与其他WSSS方法比较</strong>：用生成的伪分割掩码在全监督下训练U - Net模型，将测试数据的最终分割结果与其他先进的WSSS方法比较。在ProMRI数据集上，对于整个前列腺，C - CAM的DSC最高（83.83%），标准差最低（5.14%），在平均表面距离（ASD）和平均绝对距离（MAD）指标上也表现最佳；在ACDC数据集上，C - CAM在所有三个指标上均取得最佳性能。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-38-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-38-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-16_13-38-40"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ol><li><strong>消融实验</strong>：分析C - CAM各模块的作用。结果表明，与P - CAM相比，类别因果关系和解剖因果关系都提高了三个数据集上伪掩码的准确性。解剖因果关系在ProMRI上提升2.43%，在ACDC上提升1.79%，在CHAOS多标签分割任务中提升显著（18.3%）；结合类别因果关系后，ProMRI、ACDC和CHAOS数据集的DSC分别进一步提升4.22%、3.46%和5.41%；再训练一个亲和模型后，三个数据集上生成的伪分割掩码的DSC分别达到77.26%、80.34%和78.15%。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-37-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-37-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-16_13-37-24"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于医<strong>学图像弱监督语义分割</strong>（<strong>WSSS</strong>）的因果类激活映射（C-CAM）方法，得出以下结论：</p><ol><li><strong>方法有效性</strong>：C-CAM集成类别因果链和解剖因果链生成准确的伪分割掩码，能缓解前景与背景边界模糊问题，解决器官共现问题，生成的显著图边界清晰，符合解剖学知识。 </li><li><strong>性能优越性</strong>：C-CAM在ProMRI、ACDC和CHAOS数据集上优于六种先进的类CAM方法；用其伪掩码训练的U-Net分割网络在ProMRI和ACDC数据集上达到了先进水平。 </li><li><strong>局限性与展望</strong>：C-CAM难以分割形状复杂的物体，未来可结合少量强标签和大量弱标签，提供更准确的类别和解剖信息。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Weakly Supervised Semantic Segmentation </tag>
            
            <tag> C-CAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation</title>
      <link href="/post/pixel-wise-reclassification-with-prototypes-for-enhancing-weakly-supervised-semantic-segmentation/"/>
      <url>/post/pixel-wise-reclassification-with-prototypes-for-enhancing-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Refining the seed region to obtain finely annotated <strong>pseudo masks</strong> for training a segmentation model is a crucial step in the multi-stage weakly supervised semantic segmentation (WSSS) framework. One of the most popular refinement methods, IRN, extends seed regions towards the edges in the image. However, we observed that, due to the lack of guidance from semantic information, IRN’s refinement may lead the generation of partially erroneous refinement directions. To address this issue, we leverage prototypes<br>to recover the overlooked category semantic information in the refinement stage. We propose a prototype-based pseudo mask reclassification post-processing (PtReCl) to correct misclassified pixels in the pseudo masks, generating refined pseudo masks with more accurate coverage. Experimental evaluations demonstrate that our post-processing approach brings improvements in both pseudo mask quality and segmentation results on PASCAL VOC and MS COCO datasets, achieving state-of-the-art performance on VOC.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>在多阶段弱监督语义分割(WSSS)框架中，对种子区域进行细化以获得精细标注的<strong>伪掩膜</strong>用于训练分割模型是至关重要的一步。最流行的细化方法之一是IRN，它将种子区域向图像的边缘扩展。然而，我们观察到，由于缺乏语义信息的指导，IRN的细化可能导致部分错误的细化方向的产生。为了解决这个问题，我们利用原型来恢复细化阶段中被忽略的类别语义信息。我们提出了一种基于原型的伪掩码重分类后处理(PtReCl)来纠正伪掩膜中的错误分类像素，生成更精确覆盖的精细伪掩膜。实验评估表明，我们的后处理方法改善了<strong>PASCAL VOC</strong>和<strong>MS COCO</strong>数据集的伪掩码质量和分割结果，实现了最先进的VOC性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法在生成伪掩码时存在的问题，具体研究背景如下：</p><ul><li><strong>WSSS的目标与流程</strong>：WSSS旨在利用图像级标注数据集完成像素级分类任务，以降低数据标注成本。当前主流方法遵循三阶段流程，其中生成高质量伪掩码对最终分割模型的性能至关重要。 </li><li><strong>现有方法的局限性</strong>：最常用的细化方法IRN在细化种子区域时，因缺乏语义信息指导，可能导致部分错误的细化方向，产生大量错误的伪掩码。</li><li><strong>原型学习的潜力</strong>：近年来，研究发现原型学习可助力语义分割，它能从少量类样本中归纳特定类别的特征，实现特征的像素级分类，还能保留更多非学习参数以预测多样特征。 </li><li><strong>本文的研究动机</strong>：基于上述背景，作者提出基于<strong>原型的伪掩码重分类后处理方法</strong>（<strong>PtReCl</strong>），利用原型的类别区分性恢复伪掩码中误分类的像素，以提高伪掩码质量和分割性能。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多阶段WSSS框架</strong>：主流方法分三步，先训练分类模型生成种子区域，再用细化方法生成伪掩码，最后用伪掩码训练全监督语义分割模型。</li><li><strong>CAM方法</strong>：解决CAM作为种子区域时前景覆盖不足问题，如采用擦除、对抗学习、利用ViT上下文建模等方法。</li><li><strong>细化方法</strong>：主要分为利用显著性检测和随机游走与语义亲和两类，部分方法还借助Transformer中的注意力矩阵。</li><li><strong>原型学习</strong>：在语义分割中，部分研究将原型用于对比学习或自监督学习，部分用原型替换分类器结构。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>多阶段弱监督语义分割框架中，细化种子区域以获得精细注释的伪掩码是训练分割模型的关键步骤。现有流行的细化方法IRN在细化过程中缺乏语义信息的引导，可能导致部分错误的细化方向。为解决这一问题，作者提出了<strong>PtReCl</strong>方法。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-21-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-21-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-21-22"></p><h3 id="模型流程"><a href="#模型流程" class="headerlink" title="模型流程"></a>模型流程</h3><ol><li><strong>种子区域获取</strong>：利用原始的类激活映射（Class Activation Maps, CAM）方法获取种子区域。训练分类模型后，丢弃分类器中的全局平均池化（Global Average Pooling, GAP）层，直接在原始特征图上进行预测，忽略负预测分数并归一化生成CAM。</li><li><strong>伪掩码生成</strong>：使用IRN方法对种子区域进行细化，生成伪掩码。</li><li><strong>伪掩码恢复网络</strong>：参考Deeplab的结构构建伪分割网络，以伪掩码作为像素级注释，通过空洞空间金字塔池化层（Atrous Spatial Pyramid Pooling, ASPP）提取图像特征并获得像素级预测结果。引入标签条件策略（Label Conditioning strategy），根据图像级类别注释保留相关通道，丢弃无关通道，以减轻无关通道对后续原型准确性的影响。</li><li><strong>前景 - 背景原型获取</strong>：依次遍历训练集图像，使用骨干网络提取特征。对于伪掩码中每个类别的前景区域，收集其对应特征到前景特征集；对于非该类别区域，收集其对应特征到背景特征集。使用余弦距离作为度量，采用K - means聚类方法为每个类别获取多个前景和背景原型。</li><li><strong>多原型像素级重新分类</strong>：使用伪掩码恢复网络的骨干提取图像特征，利用特定类别的前景和背景原型对像素特征的语义信息进行重新分类。计算每个位置与前景 - 背景原型的余弦相似度，对相似度进行降序排序，选择前m个距离参与像素分类计算，生成像素级重新分类图。</li><li><strong>重新细化</strong>：将重新分类图替换IRN中的CAM，再次使用IRN进行细化，增强其边缘信息，得到后处理的伪掩码。</li><li><strong>全监督语义分割</strong>：使用后处理的伪掩码训练全监督语义分割模型，如DeeplabV2和UperNet - Swin。</li></ol><h3 id="模型贡献"><a href="#模型贡献" class="headerlink" title="模型贡献"></a>模型贡献</h3><ul><li><strong>解决分类错误</strong>：提出PtReCl后处理方法，利用原型的类别区分性，通过前景 - 背景特征恢复伪掩码中误分类的像素。</li><li><strong>多原型分类</strong>：设计多原型像素级分类方法，利用伪分割网络重建伪掩码并通过聚类方法获取原型，缓解不同类别有效原型数量的差异，获得准确的重新分类图。</li><li><strong>实验验证</strong>：在PASCAL VOC和MS COCO数据集上进行了广泛实验，结果表明PtReCl方法能有效提高伪掩码的准确性，从而提升分割性能，在VOC数据集上取得了最先进的结果。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>PASCAL VOC 2012</strong>、<strong>MS COCO 2014</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-25-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-25-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-25-57"></p><ul><li><strong>伪掩码增强</strong>：与一些先进的WSSS方法相比，经PtReCl处理后的伪掩码在VOC上提升了8.4%，在COCO上提升了3.4%，在VOC上取得了最佳性能，在COCO上也有出色表现。</li><li><strong>分割性能提升</strong>：在使用DeepLab作为全监督分割方法的VOC实验中，PtReCl在两种常用预训练ResNet101骨干网络下均取得了最先进的结果。在基于Transformer的分割方法中，使用UperNet - Swin作为骨干网络时，PtReCl也达到了最先进的性能。在COCO上，尽管受噪声影响，PtReCl仍优于除AMN和LPCAM外的其他方法，与基线IRN相比，在验证集上提升了2.2%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-26-36.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-26-36.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-26-36"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>有效性验证</strong>：PtReCl在VOC和COCO上分别将伪掩码的mIoU提高了8.4%和3.4%。通过对比不使用原型和使用不同数量原型时的像素级分类结果，验证了多原型像素级分类方法的有效性，当M设为[10, 15, 20]时，重分类图的mIoU最高可达70%。</li><li><strong>原型数量影响</strong>：研究了调整每个类别的原型数量K（范围从2到30）对重分类效果的影响。重分类图的mIoU随原型数量增加先上升后稳定，最终将类中心数量设为20。在10到30的范围内，重分类图的mIoU波动仅在1%以内，表明在合理范围内改变原型数量对重分类效果影响不大。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-20.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-20.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-28-20"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-28-28"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-28-33"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者指出广泛使用的<strong>WSSS</strong>方法IRN在细化策略上存在局限，它在不考虑特定像素级语义信息的情况下将种子区域向图像边缘扩展，导致部分错误细化。基于现有的WSSS三阶段框架，作者引入了基于原型的重分类后处理方法，以纠正伪掩码中的像素错误分类，得到更精确的后处理伪掩码。 通过在<strong>VOC和COCO</strong>数据集上的大量实验，结果表明该后处理阶段有效提高了伪掩码的质量和分割模型的性能，在VOC数据集上取得了最先进的成果。 </p>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pixel-Wise Reclassification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>弱监督语义分割</title>
      <link href="/post/fcn/20250412-ruo-jian-du-yu-yi-fen-ge/"/>
      <url>/post/fcn/20250412-ruo-jian-du-yu-yi-fen-ge/</url>
      
        <content type="html"><![CDATA[<p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-29-41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-29-41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-29-41"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-30-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-30-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-30-22"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-31-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-31-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-31-13"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-32-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-32-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-32-28"></p><p>CAM：图像中的那些像素点对类别的响应比较高（粗糙的伪标签）</p><p>种子区域：粗糙的伪标签中<strong>置信度比较高的区域</strong>，比如某一个区域认为就是猫或狗</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-37-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-37-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-37-57"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-40-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-40-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-40-23"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-42-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-42-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-42-58"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-16-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-16-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_20-16-09"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-24-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-24-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_20-24-05"></p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/knowledge-transfer-with-simulated-inter-image-erasing-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/knowledge-transfer-with-simulated-inter-image-erasing-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>南京理工大学、地平线机器人</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Though <strong>adversarial erasing</strong> has prevailed in <strong>weakly supervised semantic segmentation</strong> to help activate integral object regions, existing approaches still suffer from the dilemma of under-activation and over-expansion due to the difficulty in determining when to stop erasing. In this paper, we propose a Knowledge Transfer with Simulated Inter-Image Erasing (KTSE) approach for weakly supervised semantic segmentation to alleviate the above problem. In contrast to existing erasing-based methods that remove the discriminative part for more object discovery, we propose a simulated inter-image erasing scenario to weaken the original activation by introducing extra object information. Then, object knowledge is transferred from the anchor image to the consequent less activated localization map to strengthen network localization ability. Considering the adopted bidirectional alignment will also weaken the anchor image activation if appropriate constraints are missing, we propose a self-supervised regularization module to maintain the reliable activation in discriminative regions and improve the inter-class object boundary recognition for complex images with multiple categories of objects. In addition, we resort to intra-image erasing and propose a multi-granularity alignment module to gently enlarge the object activation to boost the object knowledge transfer. Extensive experiments and ablation studies on PASCAL VOC 2012 and COCO datasets demonstrate the superiority of our proposed approach. Source codes and models are available at <a href="https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE">https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>尽管<strong>对抗擦除</strong>在弱监督语义分割中很流行，以帮助激活完整的目标区域，但由于难以确定何时停止擦除，现有的方法仍然存在<strong>激活不足和过度扩展</strong>的困境。在本文中，我们提出了一种基于模拟图像间擦除(KTSE)的弱监督语义分割方法来缓解上述问题。与现有的基于擦除的方法不同，我们提出了一种模拟图像间擦除场景，通过引入额外的目标信息来削弱原始激活。然后，将目标知识从锚点图像转移到随后激活程度较低的定位图中，以增强网络定位能力。考虑到如果缺少适当的约束条件，所采用的双向对齐也会削弱锚点图像的激活，我们提出了一种自监督正则化模块，以保持在判别区域的可靠激活，并改善具有多类别物体的复杂图像的类间物体边界识别。此外，我们采用图像内擦除的方法，并提出了一种多粒度对齐模块来温和地放大目标激活，以促进目标知识的转移。在PASCAL VOC 2012和COCO数据集上进行的大量实验和烧蚀研究证明了我们提出的方法的优越性。源代码和模型可在<a href="https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割在深度学习时代取得了巨大进展，广泛应用于<strong>自动驾驶和图像编辑等领域</strong>。但深度学习模型训练依赖大量标注图像，收集精确的像素级标注耗时耗力。因此，弱监督学习作为减轻标注负担的方向，受到众多研究者关注。 本文聚焦于图像级标签监督下的弱监督语义分割（WSSS）。当前WSSS通常遵循将图像标签转换为像素级粗标签、细化伪标签、用细化标签训练最终分割模型的三步流程。在分割标签生成方面，类激活图（CAM）技术是目标定位的主流范式，但朴素CAM只能突出对象最具判别性的区域，激活小且稀疏，导致对象挖掘不完整。 为解决这一问题，许多工作致力于扩展CAM激活以生成高质量伪标签，其中对抗擦除是主流方法之一。然而，现有的基于对抗擦除的方法难以确定何时停止擦除，过度擦除会导致过度扩展，擦除不足则会导致激活不足。因此，本文提出了一种基于模拟图像间擦除的知识转移（KTSE）方法，以缓解上述问题。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：以图像级标签作为弱监督进行语义分割是热门方向，通常采用<strong>类激活图（CAM）技术定位目标对象</strong>，将图像级标签转化为像素级标注。为扩大CAM激活区域，研究者采用了多种方法，如扩张卷积、对比学习、自监督学习、利用跨图像信息等。</li><li><strong>基于擦除的方法</strong>：通过掩盖训练图像中的区域，迫使网络寻找其他相关部分，以扩大CAM激活区域。其中，对抗擦除方法通过掩盖最具判别性的区域，展现出更有潜力的激活扩展效果。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><h3 id="网络结构图"><a href="#网络结构图" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-25-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-25-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-11_09-25-53"></p><p>本文提出了一种用于弱监督语义分割的模拟图像间擦除知识转移（Knowledge Transfer with Simulated Inter - Image Erasing，KTSE）方法，以缓解现有基于对抗擦除方法的过度扩展和激活不足问题。以下是该模型的详细介绍： </p><h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p>训练一个带有给定图像级弱标签的分类网络，由骨干特征提取器和池化分类头组成。模拟图像间擦除场景，通过拼接配对图像引入额外对象信息，将对象知识从锚图像转移到后续激活较低的定位图，以增强网络的对象定位能力。同时，提出了自监督正则化模块和多粒度对齐模块。</p><h3 id="具体模块"><a href="#具体模块" class="headerlink" title="具体模块"></a>具体模块</h3><ol><li><strong>类激活图（CAM）生成</strong>    <ul><li><strong>网络架构调整</strong>：参考ACoL移除最终全连接层，将骨干网络输出通道设置为C + 1（C为前景类别数，1为背景），直接从类感知CAM特征F生成对象定位图。    </li><li><strong>CAM计算</strong>：对于每个前景类c，将注意力图Fc输入ReLU层，然后归一化到0到1的范围，即$A ^{c} &#x3D;\frac {ReLU\left ( F^{c} \right )}{\max \left ( F^{c} \right )}$。   </li><li><strong>池化头与损失函数</strong>：采用门控金字塔池化（GPP）层作为最终池化头，使用多标签软边缘损失训练分类网络，损失函数为$\mathcal {L} _{cls}&#x3D;-\frac {1}{C} \sum _{ c&#x3D;1}^{C} y^{c} \log \sigma \left (q^{c}\right )+\left (1-y^{c}\right ) \log \left [1-\sigma \left (q^{c}\right )\right ]$，其中$\sigma (·)$是sigmoid函数，$y^{c}$是第c类的图像级标签。</li></ul></li><li><strong>模拟图像间擦除（SIE）</strong>    <ul><li><strong>场景设计</strong>：与现有擦除方法不同，通过拼接锚图像和配对图像创建更大的合成图像，将锚图像视为被擦除的图像。引入额外对象信息使锚图像中突出的对象区域减少，然后将锚图像的对象知识转移到激活较低的合成图像锚部分，以增强网络的对象定位能力。   </li><li><strong>知识转移损失</strong>：损失函数为$\mathcal {L} _{kt} &#x3D; ReLU \left ( \hat {F _{a} } - \hat {F _{s} } \right )$，其中$\hat {F _{a} } &#x3D;CFE\left ( F _{a}, y \right )$，$\hat {F _{s} } &#x3D;CFE\left ( F _{s}, y \right )$，$F _{a}$和$F _{s}$分别表示锚分支和模拟分支的CAM特征，CFE表示类特征提取。</li></ul></li><li><strong>自监督正则化（SSR）</strong>    <ul><li><strong>问题提出</strong>：由于知识转移是双向的，学习模拟分支的稀疏激活会削弱锚分支的对象挖掘。为保持锚分支在判别区域的可靠激活，提出自监督正则化模块。   </li><li><strong>伪标签生成</strong>：使用两个阈值$\beta_{h} &#x3D; 0.3$和$\beta_{l} &#x3D; 0.15$定位置信前景和背景，生成伪标签$\hat {Y} _{i,j}$。   </li><li><strong>损失函数</strong>：使用交叉熵损失$\mathcal {L} _{ce}$直接监督CAM特征的学习，同时设计类间损失$\mathcal {L} _{inter}$鼓励复杂图像中多个前景类的激活一致性，以提高类间对象边界的识别能力。</li></ul></li><li><strong>多粒度对齐（MGA）</strong>   <ul><li><strong>问题提出</strong>：自监督正则化模块虽能促进知识转移，但网络对象定位能力的提升受锚CAM质量限制，因此采用传统的图像内擦除并提出多粒度对齐模块。  </li><li><strong>全局对齐</strong>：将锚特征$F_{a}$和掩码特征$F_{m}$输入类特征提取模块，采用全局平均池化（GAP）操作获得每个分支的最终类置信度，设计图像级全局对齐损失$\mathcal {L} _{global}$。   </li><li><strong>局部对齐</strong>：利用像素级局部激活对齐将擦除图像中新发现的对象信息转移到锚分支，损失函数为$\mathcal {L} _{local}&#x3D; ReLU\left ( \hat {F _{m}} - \hat {F _{a}} \right )$。</li></ul></li></ol><h3 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h3><p>整体训练损失为$\mathcal {L}&#x3D;\mathcal {L} _{cls} + \mathcal {L} _{kt} + \mathcal {L} _{global} + \mathcal {L} _{local} + \mathcal {L} _{ce} + \lambda _{inter}\mathcal {L} _{inter}$，其中$\lambda _{inter} &#x3D; 0.005$是控制类间损失权重的超参数。 </p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：<strong>PASCAL VOC 2012 (20+1)、COCO 2014 (80+1)</strong></p></blockquote><ul><li><strong>伪掩膜的准确性</strong>：KTSE方法的分割种子mIoU达到67.0%，比IRN的基线提高了18.7%，超过了当前最优方法FPR 3.2%；经过IRN进一步细化后，生成的伪掩码mIoU达到73.8%，超过了先前的SOTA方法AEFT和ACR超过1.5%。</li><li><strong>PASCAL VOC 2012分割图的准确性</strong>：使用VGG骨干网络时，KTSE方法在验证集和测试集上的性能分别为67.3%和67.0%，优于仅使用图像级标签的其他现有技术方法，并且与许多依赖显著性图的方法具有竞争力；使用ResNet骨干网络时，验证集和测试集的结果分别提高到73.0%和72.9%，优于最近的SOTA方法，例如在测试集上比OCR和ACR高约1%。</li><li><strong>COCO分割图的准确性</strong>：使用VGG骨干网络时，KTSE方法的mIoU达到37.2%，远优于仅使用图像级标签的先前方法，例如比CONTA高13.5% mIoU，并且与具有额外显著性指导的先前SOTA方法具有竞争力；使用ResNet骨干网络时，KTSE方法达到了45.9% mIoU的最佳结果，分别比ACR和BECO高0.6%和0.8% mIoU。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-33-16.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-33-16.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-11_09-33-16"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>逐元素组件分析</strong>：通过实验验证了KTSE方法中各个组件对提高伪掩码质量的贡献。模拟图像间擦除（SIE）模块可将分割种子的准确率从基线的54.2%提高到57.1%；自监督正则化（SSR）模块可使mIoU达到57.4%；SIE和SSR的组合可将性能显著提升至63.8% mIoU；多粒度对齐（MGA）模块最终使伪掩码的mIoU达到67.0%。</li><li><strong>MGA与先前基于擦除的方法比较</strong>：MGA模块采用来自锚分支的软类置信度知识来指导掩码图像的温和激活扩展。实验表明，CAM特征的全局对齐可将基线从54.2%提高到58.7%，优于ACoL中的刚性分类指导和AEFT中采用的GPP特征对齐；加上像素级局部对齐后，MGA模块最终将性能提高到60.2%，证明了其温和对齐策略相对于先前基于擦除方法的优势。</li><li><strong>SIE现象分析</strong>：基于经典对抗擦除方法的观察，引入额外的判别性目标信息会使原始高激活区域变得不那么具有判别性并降低激活度，通过从原始CAM学习可以增强这种减弱的激活。当网络学会增加对拼接图像中不那么具有判别性区域的关注时，也会学会激活原始图像中不那么具有判别性的目标区域以定位更多目标。</li><li><strong>SIE与数据增强比较</strong>：虽然像CutMix这样的数据增强方法也会改变锚图像并导致激活扰动，但它们不能保证像SIE那样使拼接图像的锚部分激活减弱（可能导致过度扩展）。SIE的新颖之处在于构建了模拟图像间擦除场景，通过从锚分支的目标知识中学习来提高后续激活减弱的注意力图，从而增强网络的定位能力。实验表明，KTSE方法在VOC和COCO数据集上的性能显著优于基于CutMix的数据增强方法CDA。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于弱监督语义分割的<strong>知识转移</strong>与<strong>模拟图像间擦除</strong>（<strong>KTSE</strong>）方法，并得出以下结论：</p><ol><li>与现有对抗擦除方法不同，KTSE模拟图像间擦除场景，添加额外对象信息，增强网络目标定位能力，缓解过扩展问题。 </li><li>提出自监督正则化模块，维持判别区域可靠激活，提升复杂图像类间目标边界识别能力。 </li><li>提出多粒度对齐模块，通过图像级全局对齐和像素级局部对齐扩大目标激活，促进知识转移。</li><li>在PASCAL VOC 2012和COCO数据集上的大量实验和消融研究表明，KTSE方法优于现有方法，能有效缓解过扩展和激活不足问题。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模拟图像间擦除 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SFC Shared Feature Calibration in Weakly Supervised Semantic Segmentation</title>
      <link href="/post/sfc-shared-feature-calibration-in-weakly-supervised-semantic-segmentation/"/>
      <url>/post/sfc-shared-feature-calibration-in-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Image-level weakly supervised semantic segmentation has received increasing attention due to its low annotation cost.<br>Existing methods mainly rely on <strong>Class Activation Mapping (CAM)</strong> to obtain pseudo-labels for training semantic segmentation models. In this work, we are the first to demonstrate that long-tailed distribution in training data can cause the CAM calculated through classifier weights over-activated for head classes and under-activated for tail classes due to the shared features among head- and tail- classes. This degrades pseudo-label quality and further influences final semantic segmentation performance. To address this issue, we propose a Shared Feature Calibration (SFC) method for CAM generation. Specifically, we leverage the class prototypes that carry positive shared features and propose a Multi-Scaled Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the CAMs generated through classifier weights and class prototypes during training. The MSDW loss counterbalances over-activation and under-activation by calibrating the shared features in head-&#x2F;tail-class classifier weights. Experimental results show that our SFC significantly improves CAM boundaries and achieves new state-of-the-art performances. The project is available at <a href="https://github.com/Barrett-python/SFC">https://github.com/Barrett-python/SFC</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>图像级弱监督语义分割因其标注成本低而受到越来越多的关注。现有方法主要依靠类激活映射(Class Activation Mapping, CAM)获取伪标签，用于训练语义分割模型。在这项工作中，我们首次证明了训练数据中的长尾分布会导致通过分类器权重计算的CAM由于头类和尾类之间的共享特征而对头类过度激活而对尾类激活不足。这降低了伪标签的质量，并进一步影响最终的语义分割性能。为了解决这一问题，我们提出了一种用于CAM生成的共享特征校准(SFC)方法。具体来说，我们利用带有正共享特征的类原型，并提出了多尺度分布加权(MSDW)一致性损失，以缩小训练期间通过分类器权重和类原型生成的cam之间的差距。MSDW损失通过校准头&#x2F;尾类分类器权重中的共享特征来平衡过度激活和欠激活。实验结果表明，我们的SFC显著改善了CAM边界，实现了新的最先进的性能。该项目可在<a href="https://github.com/Barrett-python/SFC%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Barrett-python/SFC上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p><strong>图像级弱监督语义分割（WSSS）<strong>因标注成本低而备受关注，现有方法多依赖</strong>类激活映射</strong>（CAM）生成伪标签来训练语义分割模型。然而，研究发现WSSS的训练数据呈长尾分布，这使得头类和尾类之间的共享特征成分在头类分类器权重中倾向于为正，在尾类分类器权重中倾向于为负。 具体而言，头类权重接收的正梯度多于负梯度，尾类权重则相反。这导致包含共享特征的像素被头类分类器权重激活，而包含尾类特征的像素未被尾类权重激活，使得通过分类器权重计算的CAM对头部类过度激活，对尾部类激活不足，进而降低了伪标签的质量，影响了最终的WSSS性能。 目前，尚未有工作针对长尾分布训练数据导致的过激活和欠激活问题进行研究。因此，本文旨在分析该问题产生的原因，并提出共享特征校准（SFC）方法来解决这一问题，以提高CAM质量和WSSS性能。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：伪标签生成基于注意力映射，关键在于生成高质量的<strong>类激活映射</strong>（CAM）。已有方法采用启发式策略、自监督学习、对比学习等生成CAM，还使用<strong>CRF、IRN等方法对初始映射进行细化</strong>。此外，视觉-语言预训练成为解决下游视觉 - 语言任务（包括WSSS）的流行方法。</li><li><strong>分类中的共享特征</strong>：分类是语义分割的上游任务，现有方法多提取判别性部分特征进行分类，避免共享特征影响分类性能。而WSSS不能仅依赖判别性特征构建完整CAM，部分方法冻结预训练编码器的若干层以避免遗忘非判别性特征。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_13-57-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_13-57-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_13-57-44"></p><p>本文提出了一种名为<strong>共享特征校准</strong>（<strong>Shared Feature Calibration，SFC</strong>）的方法，用于解决弱监督语义分割（Weakly Supervised Semantic Segmentation，WSSS）中因训练数据<strong>长尾分布</strong>导致的类激活映射（Class Activation Mapping，CAM）过激活和欠激活问题。以下是该模型的详细介绍：</p><ol><li><strong>模型背景</strong>：图像级弱监督语义分割因标注成本低而受到广泛关注。现有方法主要依赖CAM获取伪标签来训练语义分割模型，但训练数据的长尾分布会使头类别的CAM过激活，尾类别的CAM欠激活，从而降低伪标签质量，影响最终分割性能。 </li><li><strong>模型结构</strong>    - <strong>图像库重采样（Image Bank Re-sampling，IBR）</strong>：维护一个图像库，存储每个前景类别的最新图像。在训练时，从图像库中均匀采样图像并与原始训练批次拼接，以增加尾类别样本的采样频率，有效校准尾类别分类器权重中的共享特征。   <ul><li><strong>多尺度分布加权一致性损失（Multi-Scaled Distribution-Weighted，MSDW）</strong>：提出两个分布加权一致性损失$L_{P_{DW}}$和$L_{W_{DW}}$，分别用于缩小原型CAM和分类器权重CAM之间的差距，以及缩小原始图像和下采样图像的分类器权重CAM之间的差距。通过计算缩放分布系数$DC_c$对一致性损失进行重新加权，使总需求较高的类别分配更高的一致性损失。</li></ul></li><li><strong>模型推理</strong>：最终的CAM通过将分类器权重CAM和原型CAM相加得到，即$(M_{final})<em>{\tilde{c}} &#x3D; (M</em>{W}(F, W, I))<em>{\tilde{c}} + (M</em>{P}(F, P))_{\tilde{c}}$，共同解决过激活和欠激活问题。 </li><li><strong>实验结果</strong>：在PASCAL VOC 2012和MS COCO 2014两个基准数据集上进行实验，结果表明SFC方法显著提高了CAM的边界质量，在弱监督语义分割任务中取得了新的最先进性能。 综上所述，SFC方法通过图像库重采样和多尺度分布加权一致性损失，有效解决了长尾分布下共享特征导致的CAM过激活和欠激活问题，提高了弱监督语义分割的性能。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：PASCAL VOC 2021、MS COCO 2014</p></blockquote><ul><li><strong>伪标签质量比较</strong>：评估伪标签生成过程中中间和最终结果的质量。比较分类模型生成的初始CAM、经过CRF和IRN后处理的CAM。实验结果表明，SFC生成的CAM明显优于以往方法，在PASCAL VOC数据集上，SFC的CAM比现有方法高2.6%；经过CRF处理后mIoU达到69.4%，再经过IRN处理后mIoU提高到73.7%，比AMN方法高1.5%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-01-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-01-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-01-54"></p><ul><li><strong>弱监督语义分割性能比较</strong>：将经过CRF和IRN后处理的伪掩码作为真实标签，以全监督方式训练语义分割模型。在PASCAL VOC 2012的验证集和测试集上，使用ImageNet预训练骨干网络，SFC方法的mIoU分别达到71.2%和72.5%，优于仅使用图像级标签或同时使用图像级标签和显著性图的其他弱监督语义分割方法。在MS COCO 2014验证集上，使用ResNet101骨干网络，SFC方法的mIoU达到46.8%，比AMN方法高2.1%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-02-00.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-02-00.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-02-00"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-03-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-03-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-03-21"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>SFC组件有效性验证</strong>：验证图像库重采样（IBR）和多尺度分布加权（MSDW）一致性损失（包括LP DW和LW DW）的有效性。结果表明，IBR可提高分类器权重CAM的mIoU，增加尾部类别的采样频率能提升LMSDW的有效性；LW DW可增强LP DW带来的性能提升，但单独使用LW DW无法校准下采样特征空间中的共享特征，性能会显著下降。</li><li><strong>DC系数有效性研究</strong>：研究式（6）中DC系数的有效性。结果显示，DC系数能有效调整每个类别的一致性损失权重，带来显著的性能提升。</li><li><strong>CAM组合性能比较</strong>：比较推理时单独使用MW、MP和组合使用Mfinal的性能。结果表明，组合使用Mfinal的性能最高，说明在SFC中用MP补充MW效果更好。</li><li><strong>不同类别集性能增益分析</strong>：分析有无DC系数时不同类别集的平均性能增益。结果显示，使用DC系数时，头部和尾部类别能获得更多的mIoU增益。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-04-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-04-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-04-01"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者得出以下结论：</p><ol><li>首次指出在长尾场景下，<strong>头类和尾类的共享特征</strong>会使头类的分类器权重生成的类激活映射（CAM）扩大，尾类的CAM缩小，导致伪标签质量下降，影响弱监督语义分割（WSSS）最终性能。</li><li>提出共享特征校准（SFC）方法，通过图像库重采样（IBR）和多尺度分布加权（MSDW）一致性损失，平衡不同分类器权重中的共享特征比例，避免共享特征导致的<strong>过激活和欠激活</strong>问题。</li><li>实验表明，SFC显著改善了CAM边界，在<strong>Pascal VOC 2012</strong>和<strong>MS COCO 2014</strong>数据集上仅使用图像级标签就取得了新的最优WSSS性能，为提高图像级弱监督语义分割中CAM的准确性提供了新视角，未来将探索其他可能的解决方案。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SFC </tag>
            
            <tag> semantic segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WeakCLIP Adapting CLIP for Weakly-Supervised Semantic Segmentation</title>
      <link href="/post/weakclip-adapting-clip-for-weakly-supervised-semantic/"/>
      <url>/post/weakclip-adapting-clip-for-weakly-supervised-semantic/</url>
      
        <content type="html"><![CDATA[<p>华中科技大学、西北工业大学</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Contrastive language and image pre-training (CLIP) achieves great success in various computer vision tasks and also presents<br>an opportune avenue for enhancing weakly-supervised image understanding with its large-scale pre-trained knowledge. As<br>an effective way to reduce the reliance on pixel-level human-annotated labels, weakly-supervised semantic segmentation<br>(WSSS) aims to refine the class activation map (CAM) and produce high-quality pseudo masks. Weakly-supervised semantic<br>segmentation (WSSS)aims to refine the class activationmap(CAM)as pseudo masks, but heavily relies on inductive biases like<br>hand-crafted priors and digital image processing methods. For the vision-language pre-trained model, i.e. CLIP, we propose a<br>novel text-to-pixel matching paradigm forWSSS.However, directly applying CLIP toWSSS is challenging due to three critical<br>problems: (1) the task gap between contrastive pre-training and WSSS CAM refinement, (2) lacking text-to-pixel modeling to<br>fully utilize the pre-trained knowledge, and (3) the insufficient details owning to the 1&#x2F;16 down-sampling resolution ofViT. Thus,<br>we proposeWeakCLIP to address the problems and leverage the pre-trained knowledge from CLIP toWSSS. Specifically, we<br>first address the task gap by proposing a pyramid adapter and learnable prompts to extract WSSS-specific representation. We<br>then design a co-attention matching module to model text-to-pixel relationships. Finally, the pyramid adapter and text-guided<br>decoder are introduced to gather multi-level information and integrate it with text guidance hierarchically.WeakCLIP provides<br>an effective and parameter-efficient way to transfer CLIP knowledge to refine CAM. Extensive experiments demonstrate that<br>WeakCLIP achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 74.0% mIoU on the val set of<br>PASCAL VOC 2012 and 46.1% mIoU on the val set of COCO 2014. The source code and model checkpoints are released<br>at <a href="https://github.com/hustvl/WeakCLIP">https://github.com/hustvl/WeakCLIP</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>对比语言和图像预训练(CLIP)在各种计算机视觉任务中取得了巨大的成功，并且利用其大规模的预训练知识为增强弱监督图像理解提供了一个很好的途径。弱监督语义分割(WSSS)是一种减少对像素级人工标注标签依赖的有效方法，其目的是细化类激活图(CAM)并生成高质量的伪掩膜。弱监督语义分割(WSSS)旨在将类激活图(CAM)细化为伪掩膜，但严重依赖于手工制作先验和数字图像处理方法等归纳偏差。对于视觉语言预训练模型，即CLIP，我们提出了一种新的文本到像素的wsss匹配范式。然而，直接应用CLIP toWSSS是具有挑战性的，因为存在三个关键问题:(1)对比预训练与WSSSCAM细化之间的任务差距;(2)缺乏文本到像素的建模以充分利用预训练的知识;(3)由于vit的下采样分辨率为1 16，细节不足。因此，我们提出了weakclip来解决问题，并利用CLIP toWSSS的预训练知识。具体来说，我们首先通过提出金字塔适配器和可学习的提示词符来提取特定于wss的表示来解决任务差距。然后，我们设计了一个共同关注匹配模块来模拟文本到像素的关系。最后，引入金字塔适配器和文本引导解码器，实现多级信息采集，并与文本引导分层集成。WeakCLIP提供了一种有效的、参数高效的方法来传递CLIP知识以改进CAM。大量的实验表明，WeakCLIP在标准基准测试上达到了最先进的WSSS性能，即在PASCAL VOC 2012的val集上达到了74.0%的mIoU，在COCO 2014的val集上达到了46.1%的mIoU。源代码和模型检查点在<a href="https://github.com/hustvl/WeakCLIP%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/hustvl/WeakCLIP上发布。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）任务，旨在解决当前方法在处理类激活图（CAM）种子时面临的问题，具体研究背景如下： </p><ul><li><strong>WSSS的重要性与挑战</strong>：语义分割中像素级标注耗时费力，限制了实际应用。WSSS利用弱监督信息生成伪像素级分割，可减轻标注负担，但仅使用图像级标签的WSSS是该领域最具挑战性的方向。</li><li><strong>现有CAM细化方法的局限性</strong>：现有方法多依赖手工先验和改进的数字图像处理算法来细化CAM，这些方法存在归纳偏差，限制了性能和鲁棒性。 </li><li><strong>CLIP的潜力与应用挑战</strong>：CLIP在计算机视觉任务中取得了巨大成功，为WSSS带来了新的机遇。然而，直接将CLIP应用于WSSS存在任务差距、缺乏文本到像素建模以及细节不足等问题。 基于以上背景，作者提出了WeakCLIP方法，旨在利用CLIP的预训练知识，通过文本到像素匹配范式解决WSSS中的关键问题，提高伪掩码的质量，从而推动WSSS的发展。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：为减轻像素级标注负担，出现多种基于不同弱监督信息（如<strong>边界框、涂鸦、点、图像级标签</strong>）的算法。其中，基于<strong>图像级标签</strong>的WSSS最具挑战性，常使用类激活图（CAM）定位目标，但原始CAM噪声大、易出错，已有多种方法对其进行优化。</li><li><strong>大规模预训练模型</strong>：大规模预训练模型在各领域广泛应用，如CLIP通过对比学习在大量图像-文本对上预训练，展现出强大的知识迁移能力。已有研究尝试将CLIP应用于WSSS，如CLIMS引入辅助损失，CLIP - ES利用文本提示和GradCAM提升CAM质量。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-44-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-44-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-09_08-44-06"></p><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>本文提出了一种名为<strong>WeakCLIP的弱监督语义分割（WSSS）方法</strong>，旨在利用预训练的CLIP模型知识来改进WSSS网络的类激活图（CAM）细化过程。以下是WeakCLIP模型的详细介绍： </p><ol><li><p><strong>文本到像素匹配范式</strong>：与以往基于CLIP的WSSS方法不同，WeakCLIP提出了<strong>文本到像素匹配</strong>范式，以在像素级别查询相似度。具体而言，输入图像通过CLIP预训练的ViT - B网络提取多层特征图，经过投影层后，定义文本到像素匹配操作，得到文本到像素匹配的嵌入。 </p></li><li><p><strong>WeakCLIP框架</strong> </p><ul><li><strong>可学习提示（Learnable Prompt）</strong>：受CoOp和CLIP - Adapter启发，提出可学习嵌入作为自适应提示。将类文本标记并嵌入为类文本嵌入，与随机初始化的可学习嵌入拼接，作为文本编码器的输入，最终投影得到文本嵌入。  </li><li><strong>金字塔适配器（Pyramid Adapter）</strong>：为解决CLIP视觉编码器专注于整体图像内容以及低分辨率问题，提出金字塔适配器。它独立于CLIP图像编码器，对不同分辨率的特征图进行处理，通过上采样和下采样操作，生成不同分辨率的特征，有效融合低级细节和高级表示。   </li><li><strong>协同注意力匹配模块（Co - attention Matching）</strong>：为充分利用CLIP预训练知识，提出协同注意力匹配模块，用于建模双向文本到像素匹配。该模块使用两个交叉注意力模块分别建模文本到像素和像素到文本的关系，并通过残差连接更新文本和图像嵌入，最后进行文本到图像匹配得到协同注意力匹配的嵌入。  </li><li><strong>文本引导解码器（Text - Guided Decoder）</strong>：为解决CLIP ViT - B的分辨率限制问题，引入文本引导解码器。将协同注意力匹配的嵌入插值到与适配器输出特征对应的大小，与适配器输出特征拼接后进行解码，得到分割预测。    </li><li><strong>WSSS损失（WSSS Losses）</strong>：采用DSRG中使用的WSSS损失，包括平衡种子损失和边界损失。平衡种子损失计算分割预测与CAM种子之间的加权交叉熵损失；边界损失先使用条件随机场（CRF）处理分割预测以细化对象边界，然后计算CRF细化结果与分割预测之间的Kullback - Leibler散度损失。</li></ul></li><li><p><strong>伪掩码生成和再训练</strong>：使用训练好的WeakCLIP网络生成高质量的伪掩码。当推理结果中的类别不在图像级标签中时，将其标记为未知标签。最后，使用生成的伪掩码进行全监督分割，采用DeepLabv1网络架构，并尝试使用更先进的基于ViT的分割方法进行再训练。 实验结果表明，WeakCLIP在PASCAL VOC 2012和COCO 2014数据集上取得了优于以往WSSS方法的结果，证明了该方法的有效性和高效性。</p></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-49-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-49-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-09_08-49-54"></p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>PASCAL VOC 2012、COCO 2014</strong></p><ul><li><strong>ASCAL VOC 2012</strong>：在CAM监督方面，WeakCLIP与MCTformer相同，但低于ViT - PCM；在伪掩码质量上，比基线MCTformer提高了8.1%，比AMN提高了5.0%。使用精炼后的伪掩码训练DeepLabV1网络，WeakCLIP在验证集和测试集上的mIoU分别达到74.0%和73.8%，优于其他仅使用图像级监督的方法，以及部分使用额外显著图监督或边界框监督的方法。使用基于ViT的再训练基准（Segmenter和SegFormer）可进一步提升分割结果，混合ViT再训练的WeakCLIP表现最佳。</li><li><strong>COCO 2014</strong>：WeakCLIP在验证集上的mIoU达到46.1%，比基线MCTformer提高了4.1%，优于其他仅使用图像级监督的方法。使用SegFormer和MiT - B2骨干进行再训练，WeakCLIP在COCO 2014验证集上取得最佳性能。</li></ul><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ol><li><ul><li><strong>组件改进</strong>：协同注意力匹配模块将验证集mIoU提高到67.4%；可学习提示将其提高到68.9%；金字塔适配器将性能提升到70.3%；文本引导解码器将验证集mIoU进一步提升到72.6%。</li><li><strong>可学习嵌入数量</strong>：可学习嵌入数量为8时性能最佳。</li><li><strong>可学习温度初始值</strong>：协同注意力匹配中可学习温度初始值为1e - 1时性能最佳。</li></ul></li></ol><h2 id="其他实验（Other-Experiments）-1st-place-medal"><a href="#其他实验（Other-Experiments）-1st-place-medal" class="headerlink" title="其他实验（Other Experiments）:1st_place_medal:"></a>其他实验（Other Experiments）:1st_place_medal:</h2><ol><li><strong>逐类语义分割结果</strong>：在PASCAL VOC 2012的验证集和测试集以及COCO 2014的验证集上，将WeakCLIP与基线MCTformer进行逐类分割结果比较，WeakCLIP在大多数类别中表现更优。</li><li><strong>可视化分析</strong><ul><li>比较MCTformer和WeakCLIP生成的伪掩码，WeakCLIP生成的语义信息更准确、精确，能识别出MCTformer遗漏或识别不准确的对象位置。</li><li>在PASCAL VOC 2012验证集上再训练后的分割结果可视化显示，WeakCLIP对室内和室外场景都能实现准确分割。</li></ul></li><li><strong>参数效率分析</strong>：与MCTformer相比，WeakCLIP仅训练12.4%的参数，训练帧率（FPS）快4.3倍，节省68.4%的GPU内存。</li><li><strong>不同CLIP骨干实验</strong>：使用不同CLIP骨干进行实验，结果表明WeakCLIP - ResNet101性能优于WeakCLIP - ResNet50，WeakCLIP - ViT - B表现最佳。</li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了名为<strong>WeakCLIP</strong>的新方案，旨在利用预训练CLIP模型的知识来增强弱监督语义分割（WSSS）网络的**类激活图（CAM）**细化过程。该框架采用了新的文本到像素匹配范式，有效解决了将CLIP集成到WSSS中存在的三个关键问题。在广泛使用的PASCAL VOC 2012和COCO 2014数据集上的实验结果表明，与以往的WSSS方法相比，WeakCLIP取得了显著改进。引入利用大规模视觉语言预训练的WeakCLIP范式，有望推动WSSS问题的解决。未来，作者计划探索更先进的大规模CLIP，以提升WSSS的像素级理解能力。 </p>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Weakly Supervised Semantic Segmentation </tag>
            
            <tag> CLIP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/all-pairs-consistency-learning-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/all-pairs-consistency-learning-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>澳大利亚国立大学、OpenNLPLab、上海人工智能实验室、厦门大学、OPPO研究院</p><p>::: tip</p><p>启发</p><p>:::</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>In this work, we propose a new <strong>transformer-based regularization</strong> to better localize objects for <strong>Weakly supervised semantic segmentation (WSSS)</strong>. In image-level WSSS, Class Activation Map (CAM) is adopted to generate object localization as pseudo segmentation labels. To address the partial activation issue of the CAMs, consistency regularization is employed to maintain activation intensity invariance across various image augmentations. However, such methods ignore pair-wise relations among regions within each CAM, which capture context and should also be invariant across image views. To this end, we propose a new<br>all-pairs consistency regularization (ACR). Given a pair of augmented views, our approach regularizes the activation intensities between a pair of augmented views, while also ensuring that the affinity across regions within each view remains consistent. We adopt vision transformers as the self-attention mechanism naturally embeds pair-wise affinity. This enables us to simply regularize the distance between the attention matrices of augmented image pairs. Additionally, we introduce a novel class-wise localization<br>method that leverages the gradients ofthe class token. Our method can be seamlessly integrated into existing WSSS methods using transformers without modifying the architectures. We evaluate our method on PASCAL VOC and MS COCO datasets. Our method produces noticeably better class localization maps (67.3% mIoU on PASCAL VOC train), resulting in superior WSSS performances.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>在这项工作中，我们提出了一种新的<strong>基于变换的正则化方法</strong>来更好地定位弱监督语义分割(WSSS)的对象。在图像级WSSS中，采用类激活图(Class Activation Map, CAM)生成目标定位作为伪分割标签。为了解决cam的部分激活问题，采用一致性正则化方法在不同的图像增强中保持激活强度的不变性。然而，这些方法忽略了每个CAM内区域之间的成对关系，这种关系捕获上下文，并且应该在图像视图之间保持不变。为此，我们提出了一种新的全对一致性正则化(ACR)。给定一对增强视图，我们的方法规范了一对增强视图之间的激活强度，同时还确保每个视图中跨区域的亲和性保持一致。我们采用视觉Transformer作为自注意力机制机制，自然嵌入成对的亲和力。这使我们能够简单地正则化增广图像对的注意矩阵之间的距离。此外，我们引入了一种新的类智能定位方法，利用类标记的梯度。我们的方法可以使用Transformer无缝集成到现有的WSSS方法中，而无需修改体系结构。我们在PASCAL VOC和MS COCO数据集上评估了我们的方法。我们的方法产生了明显更好的类定位图(在PASCAL VOC训练上有67.3%的mIoU)，从而获得了卓越的WSSS性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法的局限性，具体研究背景如下：</p><ul><li><strong>WSSS的意义与挑战</strong>：WSSS旨在利用图像级标签、点、涂鸦和边界框等弱标签，缓解像素级标注的繁琐和高成本问题。其中，图像级WSSS仅使用类别标签监督像素级预测，尤为具有挑战性。 </li><li><strong>现有方法的不足</strong>：现有图像级WSSS方法通常依赖基于卷积神经网络的类激活图（CAM）生成伪分割标签，但CAM存在激活不完整和不准确的问题，这是由于图像标签和像素级分割监督之间的差距导致的。</li><li><strong>一致性正则化的局限</strong>：现有工作使用增强不变一致性来改进CAM，考虑了区域激活一致性，但忽略了跨视图的成对一致性，即区域亲和性一致性。激活一致性只能发现新视图中的激活，无法解决未激活区域和背景噪声问题。</li><li><strong>本文的研究动机</strong>：鉴于亲和性是上下文编码的一种方式，且上下文对像素级预测至关重要，本文提出全对一致性正则化（ACR）方法，同时强制区域激活一致性和区域亲和性一致性，以提高WSSS的性能。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><p><strong>弱监督语义分割方法多样</strong>：采用图像级标签、涂鸦、点和边界框等弱标签，避免像素级标注的繁琐。图像级弱监督语义分割常依赖类激活图（CAM）生成伪分割标签，且有多种方法对其进行优化。</p></li><li><p><strong>一致性正则化受关注</strong>：不同类型的一致性被提出用于优化初始种子，如CAM一致性、特征一致性等。</p></li><li><p><strong>亲和性学习细化</strong>：成对亲和性常被用于优化初始种子，在CNN和Transformer时代都有相关研究。</p></li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-44-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-44-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-44-09"></p><p>本文提出了一种名为<strong>全对一致性正则化</strong>（<strong>All-pairs Consistency Regularization，ACR</strong>）的模型，用于弱监督语义分割（Weakly Supervised Semantic Segmentation，WSSS）任务。</p><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>在图像级弱监督语义分割中，<strong>类激活图</strong>（<strong>Class Activation Map，CAM</strong>）常被用于生成目标定位作为伪分割标签，但存在部分激活问题。现有方法采用一致性正则化来保持不同图像增强下的激活强度不变性，但忽略了每个CAM内区域之间的成对关系。因此，ACR模型旨在同时确保区域激活一致性和区域亲和性一致性，以更好地定位目标。</p><h3 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h3><ul><li><strong>基于视觉Transformer</strong>：选择视觉Transformer作为基础模型，因为其自注意力机制可以自然地编码区域之间的依赖关系，适合建模两种一致性，且无需引入额外模块。</li><li>注意力一致性正则化<ul><li><strong>区域激活一致性</strong>：通过比较原始图像和增强图像的类到块注意力（class-to-patch attention），计算两者之间的ℓ1损失，以鼓励网络生成对变换不变的目标定位。</li><li><strong>区域亲和性一致性</strong>：比较原始图像和增强图像的块到块注意力（patch-to-patch attention），计算两者之间的ℓ1损失，以鼓励图像区域之间的成对关系对变换不变。</li><li><strong>变换逆操作</strong>：为了解决图像增强后注意力矩阵空间顺序不一致的问题，引入变换逆操作，恢复注意力矩阵的原始空间顺序，以便直接计算两个注意力矩阵之间的距离。</li></ul></li><li>基于梯度的Transformer类定位图生成<ul><li><strong>梯度计算</strong>：通过反向传播分类分数，计算类到块注意力的类特定梯度，去除负值并重塑为h×w的图，得到类定位图。</li><li><strong>亲和性细化</strong>：利用学习到的块间亲和性对激活图进行细化，结合区域激活一致性和区域亲和性一致性，生成最终的类定位图。</li></ul></li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>PASCAL VOC、MS COCO</strong></p><h4 id="MS-COCO"><a href="#MS-COCO" class="headerlink" title="MS COCO"></a>MS COCO</h4><p>表1展示了在MS COCO上的分割结果。本文方法实现了45%的分割平均交并比（mIoU），明显优于现有方法。值得注意的是，该结果不依赖任何额外的显著性信息，但超过了所有先前的WSSS方法，包括使用显著性信息的方法。MS COCO是一个更大的数据集，有更多语义类别和包含多个对象的复杂图像。这一结果表明，显著性信息可能会阻碍WSSS方法在复杂场景中的扩展性，因此本文方法未纳入显著性信息。该结果证明了全对一致性正则化（ACR）能够在具有挑战性的场景中生成可靠的类别定位图。MS COCO的每类结果报告在补充材料中。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-50-42.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-50-42.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-50-42"></p><h4 id="PASCAL-VOC"><a href="#PASCAL-VOC" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h4><ul><li><strong>种子性能</strong>：表2报告了类别定位图的mIoU，包括有无亲和性细化的性能。结果显示，即使没有亲和性细化，ACR*仍优于大多数现有的非显著性方法（mIoU为59.4%）。本文的ACR显著改善了初始种子，证明了所提出的ACR的有效性。在没有显著性信息辅助的情况下，先前最佳方法[66]也采用变压器亲和性来细化种子，而ACR比其高出5.2%。图5展示了定性结果。此外，图6展示了在包含多个对象的复杂场景中的种子，ACR学习到精确的亲和性，有助于形成具有精确边界的完整对象形状。</li><li><strong>伪标签性能</strong>：表2的最后一列显示了伪分割标签的性能。遵循常见做法，采用PSA [2]将激活图（种子）处理为像素级伪分割标签。实验发现PSA容易受到误报样本（即过度激活）的影响。为避免过度激活，使用ACR*训练PSA网络，然后训练好的PSA网络将细化ACR种子（67.3%）为伪标签。结果表明，本文方法显著改善了伪标签。</li><li><strong>语义分割性能</strong>：表3展示了在PASCAL VOC上的语义分割结果。ACR在验证集和测试集上分别取得了71.2%和70.9%的有竞争力的结果，优于先前的非显著性方法。图7显示，使用本文伪标签训练的分割模型可以产生准确和完整的预测。PASCAL VOC的每类结果报告在补充材料中。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-51-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-51-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-51-40"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><p>本文提出在分类训练期间同时正则化区域激活和区域亲和性。表4对这两个正则化项进行了消融实验。首先观察到，即使在基线模型中，区域亲和性也能显著提高种子质量，这验证了视觉变压器的上下文编码能力。通过引入这两个正则化项，发现它们分别对性能有显著提升。同时使用两个正则化项取得了最优结果，与普通变压器基线（51.1%）相比，整体mIoU提高了15.8%，证明了ACR的有效性。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-53-12.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-53-12.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-53-12"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种名为<strong>全对一致性正则化（ACR）的训练框架</strong>，用于从变压器生成更好的类定位图。研究结果表明，ACR在分类训练中利用区域激活一致性和区域亲和一致性，通过变压器的自注意力机制同时规范这两种一致性。仅使用一个类令牌，ACR就能学习精确的对象定位和准确的成对亲和性，以提取对象范围。其类定位图显著优于先前方法，在PASCAL VOC和MS COCO数据集上取得了最先进的性能。此外，ACR可以无缝集成到视觉变压器网络中，无需额外修改，这有助于其他基于变压器的任务。因此，作者认为ACR是一种简单而有效的方法，能够为弱监督语义分割提供更好的初始种子。 </p>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Weakly Supervised Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Self-supervised vision transformers for semantic segmentation</title>
      <link href="/post/self-supervised-vision-transformers-for-semantic-segmentation/"/>
      <url>/post/self-supervised-vision-transformers-for-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Semantic segmentation is a fundamental task in computer vision and it is a building block of many other vision applications. Nevertheless, semantic segmentation annotations are extremely expensive to collect, so using pre-training to alleviate the need for a large number of labeled samples is appealing. Recently, <strong>self-supervised</strong> <strong>learning (SSL)</strong> has shown effectiveness in extracting strong representations and has been widely applied to a variety of downstream tasks. However, most works perform sub-optimally in semantic segmentation because they ignore the specific properties of segmentation: (i) the need of pixel level fine-grained understanding; (ii) with the assistance of global context understanding; (iii) both of the above achieve with the dense self-supervisory signal. Based on these key factors, we introduce a systematic self-supervised pre-training framework for semantic segmentation, which consists of a hierarchical encoder–decoder architecture MEVT for generating high-resolution features with global contextual information propagation and a self-supervised training strategy for learning fine-grained semantic features. In our study, our framework shows competitive performance compared with other main self-supervised pre-training methods for semantic segmentation on <strong>COCO-Stuff, ADE20K, PASCAL VOC, and Cityscapes</strong> datasets. e.g., MEVT achieves the advantage in linear probing by +1.3 mIoU on PASCAL VOC.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>语义分割是计算机视觉的基础任务，也是众多视觉应用的基础模块。但由于语义分割标注的获取成本极高，通过预训练减少对大量标注样本的依赖显得尤为重要。近年来，自监督学习（SSL）在特征提取方面展现出显著成效，已被广泛应用于各类下游任务。然而，现有方法在语义分割任务中效果欠佳，主要原因在于忽视了该任务的三个核心特性：(i) 需要像素级的细粒度理解；(ii) 需要结合全局上下文信息；(iii) 必须通过密集的自监督信号同时实现上述两个目标。基于这些关键要素，我们开发了系统的自监督预训练框架，包含以下创新：采用 MEVT 分层编码器-解码器架构生成具有全局上下文传播能力的高分辨率特征，以及专门设计的自监督训练策略用于学习细粒度语义特征。实验表明，在 COCO-Stuff、ADE20K、PASCAL VOC 和 Cityscapes 等数据集上，我们的框架相比其他主流自监督预训练方法展现出竞争优势。典型例证如：MEVT 在 PASCAL VOC 的线性探测任务中实现了 1.3 mIoU 的性能提升。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割是计算机视觉的基础任务，在自动驾驶、机器人操作等领域应用广泛，但标注成本高昂，多数语义分割数据集规模远小于分类数据集。因此，利用大规模无标签数据进行预训练以减少对大量标注样本的依赖成为潜在解决方案。 </p><p>近年来，<strong>自监督学习（SSL）及其在视觉Transformer</strong>上的应用在计算机视觉领域取得显著进展，能帮助网络学习通用视觉表示，降低对大规模标注数据的需求。然而，多数自监督学习方法在语义分割任务中表现欠佳，原因在于它们忽略了语义分割的特定属性：需要像素级的细粒度理解、借助全局上下文理解，且要通过密集的自监督信号实现上述两点。 基于这些问题，本文作者探索一种适用于语义分割的自监督预训练方法，提出了一个系统的自监督预训练框架，旨在生成具有全局上下文信息传播的高分辨率特征，并学习细粒度的语义特征，以提升语义分割任务的性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>自监督学习</strong>：提出多种预训练任务，如色彩化、图像修复等，对比学习在下游视觉任务表现良好，基于掩码图像建模的方法取得了不错的成果，部分研究还改进了训练目标和架构。</li><li><strong>密集预测预训练</strong>：利用自监督学习进行密集预测预训练，一些方法聚焦实例级&#x2F;原型对应，部分引入像素&#x2F;区域级自监督预训练方法。</li><li><strong>视觉Transformer</strong>：ViT将Transformer应用于图像识别，Swin Transformer引入卷积风格窗口计算，部分工作构建多分辨率特征图用于密集输出。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-00-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-00-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-14_21-00-59"></p><p>本文提出了一种用于<strong>语义分割的自监督预训练框架</strong>，该框架在网络架构和自监督目标方面都有创新，核心模型是多尺度编码器 - 解码器视觉变压器（Multi-scale Encoder–Decoder Vision Transformer, MEVT），以下是详细介绍： </p><ol><li><strong>模型架构</strong>    <ul><li><strong>多尺度解码器</strong>：为了获得高分辨率的细粒度特征，MEVT在预训练架构中引入了多尺度解码器，并对编码器和解码器进行联合预训练。解码器由全局注意力阶段（Stage 5）和局部注意力阶段（Stage 6）组成，通过“Patch Unmerging”层对特征图进行上采样，同时引入跳跃连接（skip-connections），促进训练过程中浅层的梯度传播。    </li><li><strong>混合注意力机制</strong>：为了融合全局和局部上下文信息，MEVT采用了一种简单而有效的混合注意力策略。在浅层（Stage 1、2和6）使用Swin Transformer的窗口注意力块来处理局部信息，在深层（Stage 3、4和5）使用全局自注意力（ViT块）来增强全局上下文信息的传播。</li></ul></li><li><strong>自监督预训练策略</strong>：MEVT使用图像级自蒸馏损失（来自DINO）对全局平均池化（GAP）特征进行预训练。将图像的两个增强视图分别输入到教师网络和学生网络中，通过最小化交叉熵损失将知识从教师网络蒸馏到学生网络。教师网络通过指数移动平均（EMA）更新。在Stage 4和Stage 6的输出处分别应用自蒸馏损失，并对两个损失项进行等权重加权，以确保编码器和解码器网络得到充分的预训练。</li><li><strong>模型优势</strong></li></ol><ul><li><strong>性能表现</strong>：在多个语义分割数据集（COCO - Stuff、ADE20K、PASCAL VOC和Cityscapes）上的实验结果表明，MEVT在各种设置（线性探测、微调、低样本学习）下均优于大多数现有方法。例如，在PASCAL VOC上，MEVT在线性探测中比其他方法提高了+1.3 mIoU。   </li><li><strong>特征学习</strong>：通过实验和消融研究，证明了MEVT能够学习到具有细粒度和全局上下文感知能力的视觉表示，适用于具有挑战性的语义分割任务。例如，在定性结果中，MEVT在复杂环境中识别小物体的能力优于其他基线方法。</li></ul><h2 id="实验（Compared-with-SOTA）-1st-place-medal"><a href="#实验（Compared-with-SOTA）-1st-place-medal" class="headerlink" title="实验（Compared with SOTA）:1st_place_medal:"></a>实验（Compared with SOTA）:1st_place_medal:</h2><p><strong>数据集</strong>：在ImageNet上进行300个epoch的预训练。</p><ul><li><strong>线性探测结果</strong>：在COCO - Stuff、ADE20K、Cityscapes和PASCAL等数据集上，MEVT在大多数数据集上优于所有基线方法。例如，在具有挑战性的Cityscapes数据集上，MEVT比基于Transformer的DINO方法高出8.4 mIoU，比采用Swin - T的MOBY方法高出2.6 mIoU。</li><li><strong>端到端微调结果</strong>：在ADE20K数据集上，使用线性头时，MEVT比之前最好的方法iBOT高出2.4 mIoU。</li><li><strong>低样本微调结果</strong>：在不同比例标记的ADE20K图像上，MEVT在各种监督水平下均优于现有方法，表明其在实际场景中能实现更高效的语义分割。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-14_21-04-37"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-14_21-04-43"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>预训练解码器</strong>：将解码器纳入预训练框架显著提高了线性探测mIoU（+4.9）和微调mIoU（+2.2）。</li><li><strong>解码器深度</strong>：默认使用两个块的解码器，从一个块增加到两个块可提高线性探测和微调性能，增加到三个块时性能下降。</li><li><strong>多尺度融合</strong>：MEVT在低分辨率阶段使用全局自注意力进行多尺度信息融合，比仅依赖窗口注意力的Swin - T + W.A.Dec.在线性探测和微调上分别高出2.9 mIoU和2.8 mIoU。</li><li><strong>跳跃连接</strong>：添加两个跳跃连接时性能最佳，线性探测mIoU从67.8提高到71.5。</li><li><strong>位置编码</strong>：成对相对位置偏差的效果优于其他位置偏差，线性探测mIoU提高了4.3。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种<strong>自监督预训练方法</strong>，用于促进下游<strong>语义分割任务</strong>，得出以下结论：</p><ol><li><strong>方法创新</strong>：该方法在神经网络架构和自监督目标方面均有创新，构建了包含多尺度编码器-解码器架构MEVT和自监督训练策略的框架。</li><li><strong>性能优越</strong>：此框架简单且强大，在<strong>COCO-Stuff、ADE20K、PASCAL VOC和Cityscapes</strong>四个常用数据集的多种语义分割和低样本评估指标上达到了最优性能。 </li><li><strong>应用前景</strong>：<strong>作者希望该简单框架能推动无标签或少量标签语义分割的广泛应用，减少对大量高质量标注数据的依赖。</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> 自监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> semantic segmentation </tag>
            
            <tag> ViT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation</title>
      <link href="/post/a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation/"/>
      <url>/post/a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="南京信息工程大学、青海师范大学、澳门大学、中国科学院-100"><a href="#南京信息工程大学、青海师范大学、澳门大学、中国科学院-100" class="headerlink" title="南京信息工程大学、青海师范大学、澳门大学、中国科学院  :100:"></a>南京信息工程大学、青海师范大学、澳门大学、中国科学院  :100:</h2><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a><strong>摘要：</strong></h2><blockquote><p><strong>Few-shot semantic segmentation (FSS)</strong> aims to generate a model for segmenting novel classes using a limited number of annotated samples. Previous FSS methods have shown sensitivity to background noise due to inherent bias, attention bias, and spatial-aware bias. In this study, we propose a <strong>Transformer-Based Adaptive Prototype Matching Network</strong> to establish robust matching relationships by improving the semantic and spatial perception of query features. The model includes three modules: <strong>target enhancement module (TEM)</strong>, <strong>dual constraint aggregation module (DCAM)</strong>, and <strong>dual classification module (DCM)</strong>. In particular, TEM mitigates inherent bias by exploring the relevance of multi-scale local context to enhance foreground features. Then, DCAM addresses attention bias through the dual semantic-aware attention mechanism to strengthen constraints. Finally, the DCM module decouples the segmentation task into semantic alignment and spatial alignment to alleviate spatial-aware bias. Extensive experiments on <strong>PASCAL-5i</strong> and <strong>COCO-20i</strong> confirm the effectiveness of our approach.</p></blockquote><h2 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a><strong>翻译：</strong></h2><blockquote><p>Few-shot语义分割（FSS）旨在通过少量的标注样本为新的类别生成一个分割模型。以往的FSS方法由于固有偏差、注意力偏差和空间感知偏差，往往对背景噪声过于敏感。在本研究中，我们提出了一种基于Transformer的自适应原型匹配网络，通过增强查询特征的语义和空间感知能力，建立更为稳定的匹配关系。该模型包含三个模块：目标增强模块（TEM）、双重约束聚合模块（DCAM）和双重分类模块（DCM）。其中，TEM通过探索多尺度局部上下文的相关性，增强前景特征，从而减轻固有的偏差。接着，DCAM通过双重语义感知注意力机制解决了注意力偏差问题，强化了约束效果。最后，DCM模块将分割任务拆解为语义对齐和空间对齐，帮助缓解空间感知偏差。我们在PASCAL-5i和COCO-20i数据集上进行了大量实验，验证了该方法的有效性。</p></blockquote><h2 id="研究背景："><a href="#研究背景：" class="headerlink" title="研究背景："></a><strong>研究背景：</strong></h2><p>近年来，由于深度学习在计算机视觉领域的快速发展，所以传统的语义分割取得了飞速进步。在这种情况下，少样本分割(few-shot segmentation, FSS)被提出用于模拟有限数据和多类别的真实世界场景。</p><p>FSS遵循元学习框架，执行过程分特征提取、匹配和分类三个阶段。现有FSS模型虽有成果，但受背景干扰，存在三方面问题：一是特征提取阶段，预训练骨干网络有固有偏差，易优先提取无关特征；二是特征匹配阶段，注意力机制在目标类别内差异大时，会导致注意力偏差；三是分类阶段，现有方法多依赖语义相关性，忽略空间信息，产生空间感知偏差。</p><p>基于上述问题，作者提出一种基于Transformer的自适应原型匹配网络，通过在模型执行的三个阶段进行策略性和高效交互，减轻FSS中的背景干扰，利用查询特征的语义和空间感知，增强模型的鲁棒性，以解决现有FSS模型存在的问题。</p><h2 id="研究现状："><a href="#研究现状：" class="headerlink" title="研究现状："></a><strong>研究现状：</strong></h2><ul><li><strong>Few - Shot Semantic Segmentation（FSS）</strong>：FSS旨在用<strong>少量标注样本</strong>为新类别生成分割模型，基于度量学习的FSS主要分为基于原型和基于像素匹配两类方法。<strong>基于原型的方法</strong>用原型代表目标类信息进行匹配预测；<strong>基于像素匹配</strong>的方法建立支持像素和查询像素的密集关联。</li><li><strong>Transformer应用</strong>：Transformer因能捕捉长距离相关性，在FSS中得到应用，如动态调整分类器权重、过滤无关像素、聚合多级别支持掩码等。</li></ul><h2 id="提出的模型："><a href="#提出的模型：" class="headerlink" title="提出的模型："></a><strong>提出的模型：</strong></h2><p>本文提出了一种基于Transformer的自适应原型匹配网络（Transformer - Based Adaptive Prototype Matching Network），用于<strong>少样本语义分割（Few - Shot Semantic Segmentation，FSS）<strong>任务，以解决现有FSS模型存在的</strong>固有偏差、注意力偏差和空间感知偏差</strong>导致的对背景噪声敏感的问题。该模型主要包含以下三个模块： </p><ol><li><blockquote><p><strong>目标增强模块（Target Enhancement Module，TEM）</strong>    <strong>设计目的</strong>：缓解骨干网络的<strong>固有偏差</strong>，增强前景特征。在特征提取阶段，以往工作依赖预训练骨干网络直接提取的特征，存在固有偏差，倾向于提取与当前任务无关的特征。    <strong>具体方法</strong>：引入基于卷积Transformer架构的多尺度局部感知调制Transformer进行多尺度特征提取，采用多尺度自适应局部注意力增强前景信息、减轻背景干扰；用可逆神经网络（INN）替代标准多层感知器（MLP），在前馈过程中保留更细粒度的特征。 </p></blockquote></li><li><blockquote><p><strong>双约束聚合模块（Dual Constraint Aggregation Module，DCAM）</strong>    <strong>设计目的</strong>：解决特征匹配阶段的<strong>注意力偏差</strong>问题。现有方法利用单层注意力机制建立支持集和查询集的关系，在目标类别存在显著类内差异时，这种关系不足以准确匹配，导致注意力偏差。    <strong>具体方法</strong>：由类内差异表示和双语义感知注意力机制两个关键部分组成。类内差异表示利用一组可学习向量建模支持集和查询集之间的差异；双语义感知注意力机制通过两层约束，先以支持原型为参考在查询特征中选择匹配置信度高的点，再以此为指导在整个查询特征图中寻找特征相似度高的点，生成鲁棒的支持类别原型。 </p></blockquote></li><li><blockquote><p><strong>双分类模块（Dual Classification Module，DCM）</strong>    <strong>设计目的</strong>：解决特征分类阶段的<strong>空间感知偏差</strong>问题。现有方法主要基于语义一致性进行预测，忽略了目标对象的空间一致性，导致难以准确定位目标类别。    <strong>具体方法</strong>：将分割任务解耦为语义对齐和空间对齐两个子任务。通过优化查询特征和类别原型生成基于语义相似度的掩码来识别目标类别；利用查询特征的内在引导，挖掘目标对象自身的空间一致性，得到基于空间分布概率的掩码用于精确的定位，最后将两个掩码相加得到最终的查询前景分割图。 实验结果表明，该模型在PASCAL - 5i和COCO - 20i两个基准数据集上取得了优于现有方法的性能，且参数数量较少。</p></blockquote></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-36-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-36-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-36-07"></p><h2 id="实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）"><a href="#实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）" class="headerlink" title="实验（compared with the state-of-the-art models and ablation experiments）"></a><strong>实验（compared with the state-of-the-art models and ablation experiments）</strong></h2><ul><li><h3 id="Comparison-with-the-State-of-the-Arts"><a href="#Comparison-with-the-State-of-the-Arts" class="headerlink" title="Comparison with the State-of-the-Arts"></a><strong>Comparison with the State-of-the-Arts</strong></h3></li></ul><p>数据集：PASCAL-5${^i}$，COCO-20${^i}$</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-39-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-39-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-39-17"></p><ul><li><h3 id="ablation-experiments"><a href="#ablation-experiments" class="headerlink" title="ablation experiments"></a><strong>ablation experiments</strong></h3></li></ul><ol><li><strong>组件分析</strong>：该方法包含目标增强模块（TEM）、双约束聚合模块（DCAM）和双分类模块（DCM）三个主要模块。与基线相比，单独使用TEM增强查询前景特征可使性能提升0.9%，单独使用DCAM增强类别原型的判别能力可提升2.2%，TEM和DCAM协同作用可提升2.6%，使用DCM实现语义对齐和空间对齐可额外提升1.3%。模型整体比基线提升了3.9%，表明引入的模块有效解决了固有偏差、注意力偏差和空间感知偏差问题，减少了背景干扰，实现了精确分割。 </li><li><strong>目标增强模块（TEM）</strong>：TEM旨在减轻骨干网络的固有偏差并增强查询前景区域。通过与其他方法在计算量和准确性方面进行对比实验，包括采用自对齐模块（SA）、卷积变压器架构（SAM）、多尺度自适应局部注意力（MSLA + MLP）以及用可逆神经网络（INN）代替多层感知器（MLP）作为前馈网络（MSLA + INN）。结果表明，该方法在降低计算复杂度的同时保持了较高的准确性，且前馈网络在略微增加计算成本的情况下保留了更多特征细节。 </li><li><strong>双约束聚合模块（DCAM）</strong>：对DCAM中的关键组件进行了全面分析，通过修改模型采用不同的注意力机制，如原始的普通注意力（VA）、掩码注意力（MA）、双语义感知注意力（DSAA）和类内差异表示（IDR）。结果显示，使用掩码注意力减轻背景噪声干扰对性能提升影响不大，因为支持集和查询集之间的相似度掩码在类内差异较大时准确性存在挑战。而双语义感知注意力机制通过可学习的方式减轻背景干扰，能应对类内差异的敏感性，类内差异表示在三种不同的注意力机制中都有益。</li><li><strong>双分类模块（DCM）</strong>：通过消融实验评估不同的DCM组件。仅使用基于语义相似度的掩码可使模型性能提升1.5%，证明了优化类别原型和查询特征的必要性；仅使用基于空间分布概率的分割图时，性能下降2.3%，这是因为仅依赖查询图像本身的前景分布会使模型偏向已知类别的区域，导致对未知类别的分割失败。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-19.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-19.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-41-19"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-41-25"></p><h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a><strong>结论：</strong></h2><blockquote><p>作者提出了一种<strong>基于Transformer的自适应原型匹配网络</strong>，以应对少样本语义分割（FSS）中<strong>固有偏差、注意力偏差和空间感知偏差</strong>导致的背景干扰问题。该网络包含目标增强模块（TEM）、双约束聚合模块（DCAM）和双分类模块（DCM）。TEM通过多尺度局部上下文相关性增强前景特征，解决固有偏差；DCAM利用双语义感知注意力机制加强约束，处理注意力偏差；DCM将分割任务解耦为语义对齐和空间对齐，缓解空间感知偏差。实验表明，该方法在PASCAL - 5i和COCO - 20i数据集上以最少的参数达到了最先进的性能，有效减少了背景干扰，实现了精确分割。 </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 少样本语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> Few-Shot Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation</title>
      <link href="/post/dsmf-net-dual-semantic-metric-learning-fusion-network-for-few-shot-aerial-image-semantic-segmentation/"/>
      <url>/post/dsmf-net-dual-semantic-metric-learning-fusion-network-for-few-shot-aerial-image-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Chinese Academy of Sciences、University of Chinese Academy of Sciences</strong></p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Semantic segmentation</strong> of aerial images is crucial yet resource-intensive. Inspired by human ability to learn rapidly, few-shot semantic segmentation offers a promising solution by utilizing limited labeled data for efficient model training and generalization. However, the intrinsic complexities of aerial images, compounded by scarce samples, often result in inadequate feature representation and semantic ambiguity, detracting from themodel’s performance. In this article, we propose to tackle these challenging problems via dual semantic metric learning and multisemantic features fusion<br>and introduce a novel few-shot segmentation Network (DSMF-Net). On the one hand, we consider the inherent semantic gap between the feature of graph and grid structures and metric learning of few-shot segmentation. To exploit multiscale global semantic context, we construct scale-aware graph prototypes from different stages of the feature layers based on graph convolutional networks (GCNs), while also incorporating prior-guided metric learning to further enhance context at the high-level convolution features. On the other hand, we design a pyramid-based fusion and condensa-<br>tion mechanism to adaptively merge and couple the multisemantic information from support and query images. The indication and fusion of different semantic features can effectively emphasize the representation and coupling abilities of the network. We have conducted extensive experiments over the challenging iSAID-5i andDLRSD benchmarks. The experiments have demonstrated our network’s effectiveness and efficiency, yielding on-par performance with the state-of-the-art methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>航空图像的语义分割</strong>是一个非常重要的问题。受人类快速学习能力的启发，少射语义分割通过利用有限的标记数据进行有效的模型训练和泛化，提供了一种很有前途的解决方案。然而，航空图像固有的复杂性，加上稀缺的样本，往往导致特征表示不足和语义模糊，从而降低了模型的性能。在本文中，我们提出通过双语义度量学习和多语义特征融合来解决这些具有挑战性的问题，并引入了一种新的少量样本学习分割网络(DSMFNet)。一方面，我们考虑了图和网格结构特征之间固有的语义差距和少量样本学习分割的度量学习。为了利用多尺度全局语义上下文，我们基于图卷积网络(GCNs)从特征层的不同阶段构建了尺度感知的图原型，同时还结合了先验引导的度量学习来进一步增强高级卷积特征的上下文。另一方面，我们设计了一种基于金字塔的融合与凝聚机制来自适应地融合和耦合来自支持和查询图像的多语义信息。不同语义特征的表示和融合可以有效地强调网络的表示和耦合能力。我们对具有挑战性的iSAID-5i和dlrsd基准进行了广泛的实验。实验证明了我们的网络的有效性和效率，产生了与最先进的方法相当的性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于航空影像少样本语义分割问题，研究背景如下：</p><ul><li><strong>语义分割需求与挑战</strong>：语义分割是计算机视觉基础技术，在城市管理、环境监测等领域应用广泛。但传统航空影像语义分割模型训练需大量标注数据，获取耗时耗力。</li><li><strong>少样本学习的潜力</strong>：少样本学习受人类学习能力启发，利用少量标注数据进行模型训练和泛化，为解决数据获取难题提供了思路。少样本语义分割作为其延伸，通过利用相关任务或领域的先验知识进行分割任务。</li><li><strong>航空影像少样本分割的困难</strong>：航空影像由机载或卫星传感器捕获，具有空间分辨率变化大、覆盖范围广的特点。不同语义对象外观差异大，同一类别对象在尺度和结构上也存在显著差异，导致特征表示不足和语义模糊，增加了少样本语义分割的难度。 </li><li><strong>现有方法的局限性</strong>：现有方法虽有一定进展，但传统卷积神经网络在捕捉航空影像的全局和可变结构关系方面效率较低，特征提取存在特征耦合和细节保留不足的问题，导致特征歧义。 基于以上背景，作者提出DSMF - Net网络，以解决航空影像少样本语义分割中的特征建模不佳和语义模糊问题。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>语义分割</strong>：FCN、U - Net等方法推动了语义分割发展，后续如DeepLabv3 +、PSPNet等通过引入新机制提升性能，但监督分割方法标注要求高。</li><li><strong>少样本语义分割</strong>：出现半监督、弱监督、无监督学习等方法，近期少样本学习受关注，如OSLSM、PL、PANet等方法不断涌现，部分还探索了知识迁移问题。</li><li><strong>航空影像少样本语义分割</strong>：不同方法被提出，如Wang等人的原型队列学习法、Yao等人的多原型框架、DMML - Net的深度特征金字塔比较网络等。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>文章提出了一种名为DSMF - Net（Dual Semantic Metric Learning Fusion Network）的新型少样本分割网络，用于解决航空影像少样本语义分割中特征建模不佳和语义模糊的问题。</p><ol><li><strong>图原型度量学习（Graph Prototypes Metric Learning）</strong><ul><li><strong>图卷积（Graph Convolution）</strong>：图卷积可有效表示图像中不同像素或区域之间的关系，捕捉长距离依赖和全局上下文信息，有助于减少特征耦合，使模型学习和区分图像内不同的结构和语义关系。</li><li><strong>尺度感知图原型（Scale - Aware Graph Prototypes）</strong>：静态网格卷积特征难以捕捉航空影像中的复杂关系，因此引入尺度感知图原型，以图投影的方式利用多尺度全局语义上下文。通过在预训练的ResNet - 50的三个中间阶段输出特征，利用GloRe单元进行全局推理，结合支持掩码加权和全局平均池化生成原型。</li><li><strong>原型度量学习（Prototype Metric Learning）</strong>：利用查询特征与原型之间的余弦距离进行度量学习，对查询特征应用相同的图投影操作，通过最小 - 最大归一化得到图原型概率图。</li></ul></li><li><strong>先验引导度量学习（Prior Guided Metric Learning）</strong>：图卷积特征金字塔为图结构数据的特征嵌入和全局信息捕捉提供了基础，但高级卷积特征中的语义信息也不能忽视。因此引入先验引导度量学习，生成高级查询和支持卷积特征之间的相似性度量，通过添加二进制支持掩码减轻背景影响，计算余弦距离并进行最小 - 最大归一化得到先验引导概率图。</li><li><strong>语义特征融合模块（SFF）</strong>：构建SFF模块解决特征耦合问题，增强模型对变化的鲁棒性。采用金字塔结构对不同尺度的特征进行上采样和下采样，通过1×1卷积合并特征生成中间尺度特征，最后插值和拼接生成新的融合特征，促进不同尺度特征的有效交互和集成。</li><li><strong>损失函数（Loss Function）</strong>：采用交叉熵损失作为主要损失函数$L_{main}$，并引入中间监督$L_{aux}$，总损失L是$L_{main}$和$L_{aux}$的加权和，其中λ设为1.0。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：iSAID-5i、DLRSD</p><ul><li><strong>SAID - 5i数据集</strong>：将提出的模型与其他流行方法进行比较，结果表明该模型明显优于当代少样本分割模型，随着骨干网络的增强，性能提升。在不同设置下，模型在各折数据上均有显著的mIoU提升，且通过配对t检验验证了模型性能提升的显著性。</li><li><strong>DLRSD数据集</strong>：在更具挑战性的DLRSD数据集上进行实验，结果显示该模型的mIoU得分高于其他方法，尤其在处理复杂场景和具有细微视觉差异的对象时表现出色。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-35-15"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-38.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-38.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-35-38"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li>消融实验：使用ResNet - 50骨干网络在1 - shot设置下进行广泛的消融实验，分析提出模块的有效性和不同设置的影响。<ul><li><strong>GPML模块</strong>：通过实验表明，基于图结构的GPML模块相比基于卷积结构的原型学习，能进一步提高模型性能，更好地捕捉航空图像中对象之间的复杂关系。</li><li><strong>SFF模块</strong>：实验验证了SFF模块在不同尺度下的特征融合策略的有效性，特别是引入图结构和合并操作后，模型的平均性能显著提升。</li><li><strong>效率评估</strong>：通过比较不同网络的参数数量和每秒帧数（FPS），证明了提出的模型在保持较低参数数量的同时，实现了较高的FPS，在少样本航空图像分割中具有高效性。</li></ul></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-36-29"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-39.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-39.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-36-39"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者聚焦于<strong>航拍影像少样本语义分割</strong>中特征建模不佳和语义模糊的问题，提出了名为<strong>DSMF-Net</strong>的少样本分割网络。该网络采用<strong>双度量学习和多语义信息融合</strong>，增强了模型的解析和表达能力。具体而言，通过尺度感知图原型以图投影方式挖掘多尺度全局语义上下文，集成先验引导度量学习增强高层语义上下文，设计基于金字塔的融合模块更好地提取和浓缩语义特征。在<strong>iSAID - 5i和DLRSD</strong>两个具有挑战性的基准数据集上的实验表明，该方法性能优越，能有效处理有限样本下的密集预测任务。不过，作者也指出，未来需在更大、更多样化的数据集上进一步评估，以全面了解其能力和局限性，后续研究将关注模型对不同航空影像类型的适应性、在大规模数据集上的泛化能力和计算复杂度。 </p>]]></content>
      
      
      <categories>
          
          <category> 少样本语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semantic Segmentation </tag>
            
            <tag> DSMF-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kill Two Birds with One Stone Domain Generalization for Semantic Segmentation via Network Pruning</title>
      <link href="/post/kill-two-birds-with-one-stone-domain-generalization-for-semantic-segmentation-via-network-pruning/"/>
      <url>/post/kill-two-birds-with-one-stone-domain-generalization-for-semantic-segmentation-via-network-pruning/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>浙江大学、内华达大学</strong></p></blockquote><p>::: tip</p><p>启发</p><p>:::</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Deep model</strong>s are notoriously known to perform poorly when encountering new domains with different statistics. To alleviate this issue, we present a new domain generalization method based on network pruning, dubbed NPDG. Our core idea is to prune the filters or attention heads that are more sensitive to domain shift while preserving those domain-invariant ones. To this end, we propose a new pruning policy tailored to improve generalization ability, which identifies the filter and head sensibility of domain shift by judging its activation variance among different domains (unary manner) and its correlation to other filter (binary manner). To better reveal those potentially sensitive filters and heads, we present a differentiable style perturbation scheme to imitate the domain variance dynamically. NPDG is trained on a single source domain and can be applied to both CNN- and Transformer-based backbones. To our knowledge, we are among the pioneers in tackling domain generalization in segmentation via network pruning. NPDG not only improves the generalization ability of a segmentation model but also decreases its computation cost. Extensive experiments demonstrate the state-of-the-art generalization performance of NPDG with a lighter-weight structure.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>众所周知，深度模型在遇到具有不同统计数据的新领域时表现不佳。为了解决这个问题，我们提出了一种新的基于<strong>网络剪枝</strong>的域泛化方法，称为NPDG。我们的核心思想是剪枝过滤器或注意头，更敏感的领域转移，同时保留那些领域不变的。为此，我们提出了一种新的剪枝策略来提高泛化能力，该策略通过判断不同域之间的激活方差(一元方式)和与其他滤波器的相关性(二值方式)来识别滤波器和域漂移的头部敏感性。为了更好地揭示那些潜在的敏感滤波器和头部，我们提出了一种可微风格的摄动方案来动态地模拟域方差。NPDG在单一源域上训练，可以应用于基于CNN和transformer的主干。据我们所知，我们是通过网络剪枝处理分割领域泛化的先驱之一。NPDG不仅提高了分割模型的泛化能力，而且降低了分割模型的计算量。大量的实验证明了具有较轻重量结构的NPDG具有最先进的泛化性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>‍深度学习方法在训练和测试数据独立同分布时，能在各种视觉任务中取得显著成功。然而在实际应用中，深度模型部署到统计特性不同的新环境时，性能会大幅下降。为缓解这一问题，<strong>领域泛化</strong>（<strong>DG</strong>）被提出，旨在增强深度神经网络对未见目标分布的泛化能力。 与领域自适应（DA）不同，DG在训练时无法获取目标域数据，更具挑战性。DG研究主要分为多源和单源两种设置，多源DG假设各源域存在共享因素，但多源样本获取和标注耗时费力，单源DG更具现实意义，因此成为研究热点。 现有单源DG方法多通过数据增强或风格迁移创建多个增强域来模拟未见域，但数据生成与下游任务独立，导致结果欠佳。还有方法尝试让模型学习域不变表示或解耦潜在表示，但存在网络结构或损失函数设计复杂，以及域无关特征占用存储空间和推理时间的问题。 基于此，本文提出一种基于网络剪枝的单源领域泛化方法NPDG，旨在解决上述问题，提高模型泛化能力并降低计算成本。 </p><p>‍</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><p>‍</p><ul><li><strong>领域适应与泛化</strong>：领域适应（DA）和领域泛化（DG）旨在让模型在未标记目标域表现良好。DA可获取目标数据，有分布对齐、合成样本等策略；DG无法获取目标数据，更具挑战性，分为多源和单源方法，单源DG因数据收集和标注成本高而受关注。</li><li><strong>网络剪枝</strong>：旨在减少网络复杂度，分为非结构化和结构化剪枝。多数现有方法用于图像分类，未考虑领域差距，部分跨领域剪枝方法聚焦粗粒度视觉任务。</li></ul><h2 id="提出的模型（NPDG）"><a href="#提出的模型（NPDG）" class="headerlink" title="提出的模型（NPDG）"></a>提出的模型（NPDG）</h2><p>‍</p><ol><li>可微风格扰动（Differentiable Style Perturbation, DSP）模块：<ul><li>受AdaIN启发，通过额外的领域变分自编码器（D - VAE）将风格统计信息编码为标准分布，动态生成具有高多样性的域外数据，以匹配模型的剪枝状态。</li><li>训练目标是最小化包含AdaIN损失、KL散度损失和重建损失的总损失。</li><li>在部署阶段，通过干扰采样向量ε生成任意新领域，且梯度可直接反向传播到ε，使整个生成过程可微。</li></ul></li><li>网络剪枝策略：<ul><li>为每个滤波器引入可学习的缩放因子γ，通过联合训练后缩放因子接近零的滤波器被认为是要被修剪的。</li><li>对于基于CNN的骨干网络，将γ重新用于批量归一化（BN）层；对于基于Transformer的模型，为每个注意力激活分配缩放因子。</li><li>训练目标包括任务损失和稀疏正则化项，通过修改稀疏正则化函数F(γ)来重新加权香草L1正则化，以抑制对域敏感的滤波器或注意力头。</li></ul></li><li>滤波器&#x2F;头敏感性度量：<ul><li><strong>一元滤波器&#x2F;头敏感性（Unary Filter&#x2F;Head Sensitivity）</strong>：测量第i个滤波器&#x2F;头在域转移下的激活方差，通过对共享相同内容但不同风格的小批量图像进行前向传播，计算激活图的方差并归一化得到wU i。</li><li><strong>二元滤波器&#x2F;头敏感性（Binary Filter&#x2F;Head Sensitivity）</strong>：考虑域转移下滤波器之间的二元关系，通过计算协方差矩阵并对其行求和得到wB i，以识别与同一层中其他滤波器高度相关的滤波器。</li><li>最终的滤波器敏感性w由一元和二元滤波器敏感性加权求和得到，即w &#x3D; λwU + (1 - λ)wB，其中λ是控制两者相对重要性的超参数。</li></ul></li></ol><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-31-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-31-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-31-14"></p><p>‍</p><p>‍</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p><strong>数据集：GTA5、SYNTHIA、Cityscapes、BDD100K、Mapillary</strong></p></blockquote><p>‍</p><ul><li><strong>与基于DG的方法比较</strong>：在合成到真实、真实到合成和跨真实的DG任务中，NPDG与多个先进的DG方法对比。仅使用DSP就能大幅提升基线方法的mIoU，在此基础上进行域敏感滤波器剪枝可进一步提高泛化能力。与SHADE方法相比，NPDG在多数情况下表现更优，且模型结构更轻、计算成本更低。在使用Transformer作为分割骨干时，NPDG在所有目标数据集上至少比基线模型提高4%，在7个DG任务中的3个达到了最优结果。</li><li><strong>鲁棒性</strong>：通过给出实验的标准差评估NPDG的鲁棒性。各剪枝迭代可能导致模型结构略有不同，使指标有轻微波动，但方差不大。较高的剪枝率会导致更大的偏差，与仅使用DSP的模型相比，域敏感滤波器剪枝带来了显著提升。</li><li><strong>效率</strong>：与现有DG方法相比，NPDG能用更轻量级的模型达到最优的分割精度，节省超过17 GFLOPs和35M参数。但进一步提高剪枝率（超过30%）会损害泛化性能，因为分割是细粒度任务，过多剪枝会降低语义边界的分割精度。</li></ul><p>‍</p><p>‍</p><p>‍<strong>与网络剪枝（NP）方法比较</strong>：选择在普通分类和分割任务中有效的NP方法，在ResNet - 101和VGG - 16上评估剪枝性能。由于这些方法在滤波器或权重剪枝时未考虑域偏移，在新域中的性能比未剪枝的基线模型下降。其中，SFP方法的GFLOPs和内存成本最低，而基于Network Slimming的方法（包括NPDG）在隐式训练过程中剪枝滤波器，大量剪枝滤波器位于浅层。</p><p>‍</p><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-35-11"></p><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-35-18"></p><p>‍</p><h2 id="实验（Ablation-Experiments）🥇"><a href="#实验（Ablation-Experiments）🥇" class="headerlink" title="实验（Ablation Experiments）🥇"></a>实验（Ablation Experiments）🥇</h2><p>‍对NPDG的核心组件（DSP模块、一元和二元滤波器敏感性）进行消融研究。所有组件都有助于提高基线模型的泛化性能，DSP生成的新变体域比随机采样策略效果更好。结合一元和二元滤波器敏感性可使mIoU达到最高，表明两者具有互补作用。</p><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-37-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-37-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-37-26"></p><h1 id="‍超参数研究"><a href="#‍超参数研究" class="headerlink" title="‍超参数研究"></a>‍超参数研究</h1><ul><li><strong>一元和二元权重的比例λ</strong>：通过网格搜索确定λ值，ResNet101和MiT - B5模型在λ &#x3D; 0.4时性能最优，表明一元和二元敏感性剪枝都有贡献，二元剪枝效果略更明显。</li><li><strong>剪枝率r</strong>：剪枝率是灵活参数，在分割任务中，过高的剪枝率会导致边缘模糊，影响分割性能。实验表明，最优剪枝率在20% - 40%之间，约30%时泛化效果最佳。可使用验证集找到剪枝率和mIoU的精确权衡，实际应用中，若没有验证集，使用约30%的剪枝率通常可行。</li><li><strong>剪枝阈值t</strong>：剪枝阈值不是非常敏感的超参数，在一定范围内取值均可。只要稀疏训练迭代次数足够，就能识别出满足要求的滤波器。一般设置t &#x3D; 0.1。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种基于<strong>网络剪枝</strong>的<strong>领域泛化</strong>方法NPDG，得出以下结论：</p><ol><li><strong>创新性</strong>：NPDG通过定制的剪枝策略，从一元和二元分析辨别滤波器对领域偏移的敏感性，同时引入可微风格扰动方案动态模拟领域变化，助力识别敏感滤波器，是利用网络剪枝解决领域泛化问题的先驱。 </li><li><strong>有效性</strong>：在CNN和Transformer架构上的大量实验表明，NPDG能以更轻量级的模型实现语义分割泛化的最优性能。 </li><li><strong>局限性与展望</strong>：当前NPDG主要考虑风格差异导致的领域偏移，难以识别所有因素，未来将全面解决领域偏移问题。此外，目前依赖经验值选择超参数，未来需开发测试时训练的方法确定超参数。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 领域泛化语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Domain Generalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stronger, Fewer, &amp; Superior Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation（DGSS）</title>
      <link href="/post/stronger-fewer-superior-harnessing-vision-foundation-models/"/>
      <url>/post/stronger-fewer-superior-harnessing-vision-foundation-models/</url>
      
        <content type="html"><![CDATA[<h2 id="中国科学技术大学，上海人工智能实验室"><a href="#中国科学技术大学，上海人工智能实验室" class="headerlink" title="中国科学技术大学，上海人工智能实验室"></a><strong>中国科学技术大学，上海人工智能实验室</strong></h2><p><a href="https://github.com/w1oves/Rein.git">https://github.com/w1oves/Rein.git</a></p><blockquote><p>摘要：In this paper, we first assess and harness various Vision Foundation Models (VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS). Driven by the motivation that Leveraging Stronger pre-trained models and Fewer trainable parameters for Superior generaliz- ability, we introduce a robust fine-tuning approach, namely “Rein”, to parameter-efficiently harness VFMs for DGSS. Built upon a set of trainable tokens, each linked to distinct instances, Rein precisely refines and forwards the feature maps from each layer to the next layer within the backbone. This process produces diverse refinements for different categories within a single image. With fewer trainable parameters, Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full parameter fine-tuning. Extensive experiments across various settings demonstrate that Rein significantly outperforms state-of-the-art methods. Remarkably, with just an extra 1% of trainable parameters within the frozen backbone, Rein achieves a mIoU of78.4% on the Cityscapes, without accessing any real urban-scene datasets. Code is available at <a href="https://github.com/w1oves/Rein.git">https://github.com/w1oves/Rein.git</a>.</p></blockquote><blockquote><p>翻译：在本文中，我们首先在领域泛化语义分割（DGSS）任务中，评估并应用了多种视觉基础模型（VFM）。我们提出的动机是：“通过利用更强大的预训练模型和更少的可训练参数，获得更好的泛化能力”。基于此，我们提出了一种高效的微调方法——“Rein”，该方法能够以参数高效的方式利用VFM来解决DGSS任务。Rein方法依赖于一组可训练的标记，每个标记与特定实例对应，能够精确地细化并将特征图从每一层传递到骨干网络的下一层。这样，Rein能够在单张图像中为不同的类别生成多样化的细化结果。通过减少可训练的参数，Rein在微调VFM时，效果出乎意料地优于完全参数微调。通过广泛的实验验证，Rein显著超越了现有的最先进方法。值得一提的是，仅在冻结的骨干网络中增加1%的可训练参数，Rein便在Cityscapes数据集上达到了78.4%的mIoU，而且无需使用任何真实的城市场景数据集。代码已发布，您可以通过<a href="https://github.com/w1oves/Rein.git%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/w1oves/Rein.git访问。</a></p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-26_21-13-37"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-26_21-13-59"></p><p>​                                                                                <strong>模型结构图</strong></p><p><strong>本文的研究背景：</strong> </p><ul><li><strong>传统DGSS方法的局限</strong>：以往DGSS方法着重提升模型在多未见领域的预测准确性，但多采用VGGNet、MobileNetV2和ResNet等经典骨干网络，且依赖复杂数据增强和领域不变特征提取策略，对更强的VFMs在DGSS中的效能探索不足。 </li><li><strong>VFMs的潜力与挑战</strong>：近年来，CLIP、MAE、SAM等大规模VFMs显著提升了计算机视觉任务的性能，其在不同未知场景下展现出强大泛化能力。然而，将VFMs用于DGSS任务存在挑战，常用数据集规模远小于ImageNet，对VFMs大量可训练参数进行微调会导致泛化能力受限，且现有的参数高效微调策略大多不适用于DGSS。-</li><li><strong>研究动机</strong>：基于利用更强预训练模型和更少可训练参数实现更优泛化能力的动机，作者评估并利用VFMs进行DGSS研究，提出“Rein”微调方法，以高效利用VFMs解决DGSS问题。</li></ul><p><strong>研究现状：</strong></p><ul><li><strong>领域广义语义分割（DGSS）</strong>：传统方法聚焦提升模型跨多未见领域的预测准确性，采用复杂数据增强和领域不变特征提取策略，多使用VGGNet、MobileNetV2等旧骨干网络。 </li><li><strong>视觉基础模型（VFMs）</strong>：如CLIP、MAE、SAM等在计算机视觉挑战中表现出色，具有显著的跨场景泛化能力，但在DGSS任务中的表现缺乏专门研究。 </li><li><strong>参数高效微调（PEFT）</strong>：在自然语言处理领域取得成功，部分方法开始应用于计算机视觉，但大多不是为DGSS设计，难以对单张图像中不同实例的特征进行细化。</li></ul><p><strong>研究思路：</strong></p><p>本文聚焦于在领域泛化语义分割（DGSS）中利用视觉基础模型（VFMs），研究思路清晰，具体如下：</p><ol><li><strong>提出问题</strong>：先前DGSS方法多采用传统骨干网络，而大规模VFMs虽在计算机视觉挑战中表现出色，但在DGSS中的性能及利用方式尚不明确。因此，作者提出评估VFMs在DGSS中的性能以及如何有效利用VFMs的问题。</li><li><strong>构建框架</strong>：以利用更强预训练模型和更少可训练参数实现更优泛化能力为动机，作者引入<strong>Rein</strong>微调方法，在骨干网络层间嵌入该机制，以有效利用VFMs的强大能力。</li><li><strong>选择方法</strong>：选择CLIP、MAE、SAM、EVA02和DINOv2等五种不同训练策略和数据集的VFMs进行评估。设置“Full”和“Freeze”两个基本基线，并提出“Rein”方法。采用AdamW优化器，设置特定学习率、迭代次数、批量大小等进行训练。</li><li><strong>分析数据</strong>：在多个数据集和三种泛化设置下进行实验，对比Rein与现有DGSS和参数高效微调（PEFT）方法的性能。通过消融实验分析Rein各组件的有效性、令牌长度和秩对模型性能的影响，以及训练速度、GPU内存使用和模型存储要求。</li><li><strong>得出结论</strong>：实验表明，冻结的VFMs性能优于先前DGSS方法，Rein以更少可训练参数显著增强VFMs的泛化能力，大幅超越现有方法。证明了VFMs在DGSS领域的巨大潜力以及Rein方法的有效性。</li></ol><p><strong>本文的创新点：</strong> </p><ol><li><strong>评估并利用视觉基础模型（VFMs）</strong>：首次在**领域泛化语义分割（DGSS）**中评估多种VFMs，证实其强大泛化能力，为该领域建立重要基准。 </li><li><strong>提出“Rein”微调方法</strong>：通过可学习令牌对特征图进行实例级细化，以较少可训练参数有效利用VFMs，显著提升泛化性，超越现有方法。</li><li><strong>设计优化策略</strong>：采用层共享MLP权重和低秩token序列，减少参数冗余，提高训练效率。</li></ol><blockquote><p>写作启发：<strong>领域泛化语义分割（DGSS）</strong>、<strong>视觉基础模型（VFMs）</strong>、<strong>参数高效微调（PEFT</strong>）</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 领域泛化语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Vision Foundation Models </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LGAD Local and Global Attention Distillation for Efficient Semantic Segmentation</title>
      <link href="/post/lgad-local-and-global-attention-distillation-for-efficient-semantic-segmentation/"/>
      <url>/post/lgad-local-and-global-attention-distillation-for-efficient-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Shaoxing University、Central South University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Efficient <strong>semantic segmentation</strong> is essential for a wide array of computer vision applications, and knowledge distillation has emerged as a promising methodology for model compression and efficiency. However, we observed that an excess of positive pixels can dilute attention weights, hindering the student model’s learning process. To tackle this significant challenge, we introduce the Local and Global Attention Distillation (LGAD) framework, a pioneering block-based technique that distills both local and global attention. The LGAD framework segments feature maps and output probabilities into well-defined local and global blocks, effectively mitigating the dilution of attention weights. By doing so, it enhances the distinction between positive and negative pixels, particularly amplifying the focus on salient regions within each local and global block. We have conducted comprehensive experiments on three benchmark datasets, Cityscapes, CamVid, and Pascal VOC 2012. The experiment results demonstrate the effectiveness of our proposed LGAD and confirm its superiority over several state-of-the-art distillation methods for semantic segmentation.</p><p>::: tip</p><p><strong>正像素：通常表示目标、高亮度、激活区域或有效数据。</strong></p><p><strong>负像素：通常表示背景、低亮度、抑制区域或噪声&#x2F;无效数据</strong></p><p>:::</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>高效的语义分割对于广泛的计算机视觉应用至关重要，知识蒸馏已经成为一种很有前途的模型压缩和效率方法。然而，我们观察到过多的正像素会稀释注意力权重，阻碍学生模型的学习过程。为了应对这一重大挑战，我们引入了局部和全局注意力蒸馏(LGAD)框架，这是一种开创性的基于块的技术，可以提取局部和全局注意力。LGAD框架将特征图和输出概率分割为定义良好的局部和全局块，有效地减轻了注意力权重的稀释。通过这样做，它增强了正像素和负像素之间的区别，特别是放大了对每个局部和全局块内显著区域的关注。我们在cityscape、CamVid和Pascal VOC 2012三个基准数据集上进行了全面的实验。实验结果证明了我们所提出的语义分割方法的有效性，并证实了它比几种最先进的语义分割方法的优越性。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割在<strong>自动驾驶、机器人导航</strong>等众多实际应用中至关重要，深度卷积神经网络（DCNNs）成为主流方法，如PSPNet、DeepLab系列等。然而，这些模型存在存储和计算开销大的问题，限制了其在现实场景中的部署，因此开发紧凑且高效的分割模型成为研究热点。 知识蒸馏是一种有前景的模型压缩技术，可通过将大而复杂模型（教师模型）的知识转移到小模型（学生模型）来提升学生模型性能。已有研究者将知识蒸馏引入高效语义分割并提出多种框架，但现有知识蒸馏方法主要集中于全局交互的蒸馏。 研究发现过多正像素会稀释注意力权重，导致正、负像素注意力值差距小，阻碍学生模型识别和学习特征。如图1所示，正像素注意力值约为0.0004，背景像素近于零。为解决这一挑战，作者提出了**Local and Global Attention Distillation（LGAD）**框架，旨在通过将特征图和输出概率划分为局部和全局块，增强正、负像素注意力值差距，提升学生模型的语义分割性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>语义分割模型</strong>：<strong>深度卷积神经网络（DCNNs）<strong>成为主流方法，如PSPNet、DeepLab系列等取得了不错的性能，但存在存储和计算开销大的问题。为解决此问题，出现了一些</strong>轻量级框架</strong>，如ENet、SegNet等，还有基于<strong>Vision Transformer</strong>的语义分割框架。</li><li><strong>知识蒸馏</strong>：作为模型压缩的有效技术，被广泛应用于语义分割。现有方法主要集中在对齐中间特征图和输出概率，如SKDS、IFVD、IDD等，但大多聚焦于全局交互的蒸馏。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-44-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-44-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-44-32"></p><p>本文提出了用于高效**语义分割的局部和全局注意力蒸馏（Local and Global Attention Distillation，LGAD）**框架。</p><p><strong>模型组件</strong></p><ul><li><strong>局部注意力特征蒸馏（Local Attention Feature Distillation）</strong>：聚焦于蒸馏教师模型中间特征图中的有价值知识，让学生模型关注教师模型表示中的关键区域。</li><li><strong>局部注意力输出蒸馏（Local Attention Output Distillation）</strong>：旨在蒸馏分割输出中的有价值知识，使学生模型获得教师模型关注特定感兴趣区域并进行精确预测的能力。</li><li><strong>全局注意力特征蒸馏（Global Attention Feature Distillation）</strong>：与局部注意力特征蒸馏类似，但侧重于全局层面的特征蒸馏。</li><li><strong>全局注意力输出蒸馏（Global Attention Output Distillation）</strong>：与局部注意力输出蒸馏类似，但侧重于全局层面的输出蒸馏</li></ul><p><strong>损失函数</strong>：总损失函数为$L &#x3D; L_{seg} + \lambda_1 \cdot (L_{lafd} + L_{gafd}) + \lambda_2 \cdot (L_{laod} + L_{gaod})$，其中$L_{seg}$是语义分割的交叉熵损失，$\lambda_1 &#x3D; 30$和$\lambda_2 &#x3D; 3$是两个超参数，用于平衡LGAD框架中不同组件的影响。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：Cityscapes、Pascal VOC 2012、CamVid</p><p>评估指标：mIoU、模型参数、浮点运算次数（FLOPs）</p><ul><li><strong>Cityscapes</strong>：在验证集上，对于PSPNet - R18学生模型，LGAD将mIoU提升了7.12%，优于SKDS、IFVD、CWD和APD等方法；对于PSPNet - B0学生模型，LGAD使mIoU提升了7.39%，也超过了对比方法。</li><li><strong>CamVid</strong>：在测试集上，LGAD能提升两个学生模型的性能。对于PSPNet - R18，提升了2.9%，超过SKDS、IFVD和CWD；对于PSPNet - B0，提升了3.5%，同样优于对比方法。</li><li><strong>Pascal VOC 2012</strong>：在验证集上，LGAD大幅提升了两个学生模型的性能。对于PSPNet - R18，mIoU提升了5.39%；对于PSPNet - B0，提升了2.98%，均超过了对比方法。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-49.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-49.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-48-49"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-48-58"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-03.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-03.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-49-03"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>不同损失项的有效性</strong>：在Cityscapes验证集上，以PSPNet - R18为学生模型进行实验。结果表明，局部和全局注意力输出蒸馏损失（Llaod和Lgaod）能独立提升学生模型性能，且二者结合效果更佳；局部和全局注意力特征蒸馏损失（Llafd和Lgafd）同时应用时也能进一步提升性能；当四个蒸馏损失项都应用时，提升达到7.12%，验证了LGAD的有效性和整合局部与全局注意力蒸馏的重要性。</li><li><strong>窗口大小的影响</strong>：研究了不同局部块数量（P×P）和全局块数量（G×G）对蒸馏的影响，设置P &#x3D; G &#x3D; {1, 2, 4, 8}。结果显示，窗口数量为2×2时带来最高的mIoU值76.22%，随着窗口数量增加，学生模型的mIoU得分略有下降，这表明窗口数量过大时，学生模型可能难以有效学习。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-49-40"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者针对<strong>知识蒸馏</strong>在语义分割中存在的正像素过多稀释注意力权重、阻碍学生模型学习的问题，提出了<strong>Local and Global Attention Distillation（LGAD）框架</strong>。该框架将特征图和输出概率划分为块，设计局部和全局注意力蒸馏方法，增强了学生模型识别和学习判别特征的能力。 通过在Cityscapes、CamVid和Pascal VOC 2012三个基准数据集上的大量实验，验证了LGAD框架的有效性，且其性能优于多个现有最先进的知识蒸馏方法。作者认为该方法在语义分割领域引入了有意义的进展，为开发轻量级且准确的密集预测模型提供了有价值的见解。 </p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 注意力蒸馏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Class Tokens Infusion for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/class-tokens-infusion-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/class-tokens-infusion-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Weakly Supervised Semantic Segmentation (WSSS)</strong> relies on Class Activation Maps (CAMs) to extract spatial information from image-level labels. With the success of Vision Transformer (ViT), the migration of ViT is actively conducted in WSSS. This work proposes a novel WSSS framework with Class Token Infusion (CTI). By infusing the class tokens from images, we guide class tokens to possess class-specific distinct characteristics and global-local consistency. For this, we devise two kinds of token infusion: 1) Intra-image Class Token Infusion (I-CTI) and 2)Cross-image Class Token Infusion (C-CTI). In I-CTI, we infuse the class tokens from the same but differently augmented images and thus make CAMs consistent among var-<br>ious deformations (i.e. view, color). In C-CTI, by infusing the class tokens from the other images and imposing the resulting CAMs to be similar, it learns class-specific distinct characteristics. Besides the CTI, we bring the background (BG) concept into ViT with the BG token to reduce the false positive activation ofCAMs. We demonstrate the effectiveness ofour method on PASCAL VOC 2012 and MS COCO 2014 datasets, achieving state-of-the-art results in weakly supervised semantic segmentation. The code is available at <a href="https://github.com/yoon307/CTI">https://github.com/yoon307/CTI</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>弱监督语义分割(WSSS)依靠类激活图(CAMs)从图像级标签中提取空间信息。随着视觉transformer(ViT)的成功，视觉transformer的迁移在WSSS中得到了积极的开展。本文提出了一种基于类标记注入(CTI)的WSSS框架。通过注入来自图像的类标记，我们引导类标记具有特定于类的独特特征和全局-局部一致性。为此，我们设计了两种标记注入:1)图像内类标记注入(I-CTI)和2)跨图像类标记注入(C-CTI)。在I-CTI中，我们从相同但不同的增强图像中注入类标记，从而使cam在各种变形(即视图，颜色)之间保持一致。在C-CTI中，通过注入来自其他图像的类标记并强制生成的cam相似，它学习特定于类的独特特征。除了CTI之外，我们还通过BG标记将背景(BG)概念引入ViT，以减少cam的误报激活。我们在PASCAL VOC 2012和MS COCO 2014数据集上证明了我们的方法的有效性，在弱监督语义分割中取得了最先进的结果。代码可在<a href="https://github.com/yoon307/CTI%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/yoon307/CTI上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法存在的问题，具体研究背景如下： </p><ul><li><strong>WSSS的兴起</strong>：全监督语义分割虽在多领域表现出色，但标注成本高、耗时长。为减轻标注负担，WSSS应运而生，其利用图像级标签、涂鸦和边界框等弱监督信息进行研究，其中仅利用图像级分类标签的设置最具实用性和挑战性。</li><li><strong>现有方法的局限性</strong>：传统WSSS研究多依赖卷积神经网络（CNNs）生成类激活图（CAMs），但CNN的感受野有限，导致CAMs存在稀疏性问题，仅关注物体的判别区域。Vision Transformer（ViT）虽能缓解该问题，但原始ViT使用单类令牌进行分类，定位图缺乏类别特异性，且基于多类令牌的ViT仍存在类令牌特征表示相关性高、CAMs过度扩展导致假阳性区域增加等问题。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：利用图像级标签、涂鸦、边界框等弱监督信息进行研究，其中基于图像级分类标签的研究最具挑战性。主要通过类激活图（CAMs）定位目标，为提高CAMs精度，出现了对抗擦除、局部 - 全局一致性等方法，也有不少工作对CAMs进行后处理以获取可靠标签。</li><li><strong>基于视觉Transformer（ViT）的WSSS</strong>：ViT凭借自注意力机制能捕捉长距离依赖，缓解了CAMs稀疏性问题。一些工作采用多类令牌或直接用补丁令牌训练分类器来提取特定类别的激活图。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>本文提出了一种基于视觉Transformer（ViT）的弱监督语义分割（WSSS）框架，该框架引入了类令牌注入（Class Token Infusion，CTI）和背景令牌（Background Token，BGT）两种方法，以解决传统多类令牌WSSS方法的局限性，生成更精确的类激活图（Class Activation Maps，CAMs）。</p><h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p>使用三个配对图像进行训练，将每个图像分割成 $N \times N$ 个补丁并嵌入为补丁令牌 $T_{patch}$，同时使用 $C$ 个前景类令牌 $T_{cls - fg}$ 和 1 个背景类令牌 $T_{cls - bg}$ 组成输入类令牌 $T_{cls}$。将类令牌和补丁令牌连接形成输入令牌 $T_{input}$，添加位置嵌入后输入到 $L$ 个Transformer块中。从最后一个Transformer块的补丁令牌输出 $T_{L_{patch}}$ 中获取CAMs $M$，并通过池化类令牌输出 $T_{L_{cls}}$ 得到类预测 $y_{pred}$，使用多标签软边缘损失计算分类损失 $L_{cls}$ 和补丁级分类损失 $L_{cls - patch}$。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-29_21-01-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-29_21-01-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-29_21-01-48"></p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><ol><li><strong>数据集</strong>：选用PASCAL VOC 2012和MS - COCO 2014这两个广泛使用的基准数据集。前者包含20个前景对象类和1个背景类，有10582、1449和1456张图像分别用于训练集、验证集和测试集；后者更具挑战性，有82k训练集和40k验证集，包含80个前景对象类和1个背景类。</li><li><strong>评估指标</strong>：采用平均交并比（mIoU）评估语义分割性能，在验证集上评估语义分割模型，在训练集上评估类激活图（CAMs）性能，PASCAL VOC 2012测试集结果通过在线官方服务器评估。</li></ol></blockquote><ol><li><strong>PASCAL VOC数据集</strong>：在训练集上，所提方法在CAMs（种子）和伪像素级真值（掩码）方面性能均优于其他方法，相比第二好的结果，种子性能提升1.8%p，掩码性能提升0.9%p。在语义分割性能上，基于高质量标签训练的模型在验证集和测试集上均大幅超越现有技术，且语义分割模型性能优于伪标签，比第二好的模型在验证集上有超过1.7%p的提升。</li><li><strong>MS COCO数据集</strong>：在该数据集上训练和评估模型，虽数据集类别多、场景复杂，但所提方法取得45.4%的mIoU，显示出良好的泛化能力，有效减少了ViT - based WSSS方法在该数据集上因错误激活和类间激活重叠导致的性能差距。</li></ol><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ol><li>组件分析<ul><li>引入背景类令牌（BGT）和背景CAMs概念到ViT，相比基线有2.9%p的提升。</li><li>提出的图像内类令牌注入（I - CTI）额外带来1.1%p的性能提升。</li><li>结合图像间类令牌注入（C - CTI），性能提升至69.5%。</li></ul></li><li><strong>背景类令牌的重要性</strong>：训练无BGT但有BG CAM的基线模型，结果比基线下降4.1%p，凸显BGT在训练BG CAM中的重要性。</li><li><strong>类令牌注入的作用</strong>：通过t - SNE可视化不同层的类令牌，表明所提方法的类令牌在各层特征空间区分度好，生成的CAMs更具独特性，不侵犯其他类区域，而基线的类令牌区分度差，CAMs存在错误激活区域。</li><li><strong>注入索引的影响</strong>：改变注入索引L1从2到10（总层数L为12），mIoU性能在L1设为3时最高，虽不同索引有轻微性能差异，但均高于无CTI的情况，因此将注入索引设为3。</li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者旨在增强ViT中类别令牌的特定类别表示能力，以实现图像中物体的清晰定位，得出以下结论：</p><ol><li><strong>提出CTI方法</strong>：提出两种类别令牌注入（CTI）方法，即图像内类别令牌注入（I - CTI）和跨图像类别令牌注入（C - CTI）。I - CTI使激活图具有全局 - 局部一致性，C - CTI让类别令牌和激活图具备跨图像的一致特定类别知识。 </li><li><strong>引入背景令牌</strong>：将背景令牌（BGT）引入ViT，有效解决了激活图过度扩展问题，减少了错误激活。</li><li><strong>实验验证有效性</strong>：在PASCAL VOC 2012和MS COCO 2014数据集上的大量实验结果，支持了所提方法的有效性和泛化性，该方法在两个数据集上均达到了最先进水平。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scribble-Supervised Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation</title>
      <link href="/post/use-universal-segment-embeddings-for-open-vocabulary-image-segmentation/"/>
      <url>/post/use-universal-segment-embeddings-for-open-vocabulary-image-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Bosch Research North America、Bosch Center for Artificial Intelligence (BCAI)</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>The <strong>open-vocabulary image segmentation</strong> task involves partitioning images into semantically meaningful segments and classifying them with flexible text-defined categories. The recent vision-based foundation models such as the Segment Anything Model (SAM) have shown superior performance in generating class-agnostic image segments. The main challenge in open-vocabulary image segmentation now lies in accurately classifying these segments into text-defined categories. In this paper, we introduce the Universal Segment Embedding (USE) framework to address this challenge. This framework is comprised of two key components: 1) a <strong>data pipeline</strong> designed to efficiently curate a large amount of segment-text pairs at various granularities, and 2) a <strong>universal segment embedding model</strong> that enables precise segment classification into a vast range oftext defined categories. The USE model can not only help open-vocabulary image segmentation but also facilitate otherdownstream tasks (e.g., querying and ranking). Through comprehensive experimental studies on semantic segmen-<br>tation and part segmentation benchmarks, we demonstrate that the USE framework outperforms state-of-the-art open-<br>vocabulary segmentation methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>开放词汇图像分割任务包括将图像划分为语义上有意义的片段，并使用灵活的文本定义类别对其进行分类。近年来，基于视觉的基础模型(如SAM)在生成与类别无关的图像片段方面表现出了优异的性能。目前，开放词汇表图像分割的主要挑战在于将这些片段准确地分类到文本定义的类别中。在本文中，我们引入了通用段嵌入(USE)框架来解决这一挑战。该框架由两个关键组件组成:1)一个数据解决方案，旨在有效地管理各种粒度的大量片段-文本对;2)一个通用的片段嵌入模型，能够将精确的片段分类到大量文本定义的类别中。USE模型不仅可以帮助开放词汇表图像分割，还可以促进其他下游任务(例如查询和排序)。通过对语义切分和零件切分基准的综合实验研究，我们证明了USE框架优于最先进的开放词汇切分方法。</p><p>::: tip</p><p>启发</p><p>:::</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>开放词汇图像分割的目的是将图像分割成语义上有意义的片段，并用文本定义的任意类对其进行分类。最近的基础模型在将图像的像素分为有意义的片段上效果显著，例如SAM，然而，现有的开放词汇图像分割方法面临着挑战：<strong>端到端的方法</strong>不能将基础模型生成的图像段作为输入或提示来分配类标签；<strong>两阶段的方法</strong>由于人类标签的限制，它们在分类不同粒度的片段方面仍然受到限制。基于上述的存在的问题，本文的作者提出了<strong>通用片段嵌入框架（Universal Segment Embedding，USE）</strong>，该框架由两个关键的组件：数据方案和通用片段嵌入模型。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li>多模态表示学习：</li></ul><p>从大规模图像-文本数据中学习(例如CLIP)在将视觉概念与文本描述联系起来方面显示出了很好的结果，然而，对于文本数据的多模态表示学习，目前的研究还很少。</p><ul><li>开放词汇图像分割：</li></ul><p>在<strong>自动驾驶</strong>等现实世界视觉任务需求日益增长的驱动下，开放词汇图像分割的重要性正在迅速增长，现有的方法可以分为两类:端到端方法和两阶段方法，但是上述两种方法都存在不足，不能满足本文的任务需求。</p><ul><li>改进图像-文本数据集：</li></ul><p>现有的工作可以分为两类:数据过滤和数据改进。数据过滤旨在通过过滤噪声的图像-文本对来提高模型训练的效率和鲁棒性，而数据改进则侧重于改善图像和文本数据的对齐。</p><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><strong>USE Data Pipeline：</strong></p><ol><li><strong>多粒度图像字幕生成</strong>：利用多模态大语言模型（MLLM），通过特定提示生成图像中对象及其属性的详细描述，以获取更丰富的语义信息。</li><li><strong>从字幕中进行指代表达定位</strong>：从字幕中提取名词短语并扩展为指代表达，使用开放词汇定位模型（如Grounding DINO）获取与这些表达对应的边界框，生成框 - 文本对。</li><li><strong>使用框提示生成掩码</strong>：将边界框转换为掩码，使用图像分割模型SAM生成掩码，并进行后处理，最后通过基于掩码的非极大值抑制（NMS）合并段 - 文本对。</li></ol><p><strong>USE Model：</strong></p><ol><li><strong>图像编码器</strong>：利用预训练的视觉变换器（ViTs）提取图像块嵌入，通过多级别特征合并，结合CLIP和DINOv2的信息，同时获取全局图像特征。在训练过程中，CLIP和DINOv2保持冻结，仅线性层归一化（LLN）模块和块尺度参数可训练。</li><li><strong>段嵌入头</strong>：根据输入段从图像块嵌入中提取段嵌入，并将其映射到视觉 - 语言联合空间。通过计算段在每个图像块内的面积并归一化，得到段在每个块内的权重，进而计算加权平均嵌入，最后通过线性层映射为段嵌入。</li><li><strong>训练与损失</strong>：使用段 - 文本对比损失来训练模型，在训练时，每个段随机采样一个文本描述来计算文本嵌入。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>训练集：COCO、Visual Genome (VG)       测试集：ADE20K、Pascal Context</p><blockquote><p>评估方法：使用类无关掩码，通过提示SAM生成掩码，经过滤和合并后，用模型获取掩码嵌入，计算与目标类文本嵌入的相似度，转换为概率，聚合像素上所有片段的概率进行类别预测。</p><p>对比结果：与最先进的开放词汇语义分割方法在<strong>ADE20K和Pascal Context数据集</strong>上对比，以平均交并比（mIoU）评估性能。结果表明，USE方法在所有数据集上大幅优于最先进的两阶段方法，与端到端方法相比平均性能最佳，在COCO和VG图像上训练时性能进一步提升。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-31-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-31-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-26_14-31-25"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>预训练骨干网络选择</strong>：在ADE20K数据集和开放词汇语义分割任务上研究，结果表明结合CLIP和DINOv2可获得性能提升。</li><li><strong>图像编码器架构设计</strong>：研究cls令牌对性能的影响，结果显示包含cls令牌可提高mIoU。</li><li><strong>定性比较</strong>：对比从真实标注和MLLM增强标注中提取的对象，MLLM增强标注能捕获更细粒度的对象和部件。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-33-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-33-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-26_14-33-30"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出用于<strong>开放词汇图像分割的USE框架</strong>，通过实验研究得出以下结论：</p><blockquote><ol><li><strong>方法有效性</strong>：该框架结合精心设计的数据管道和轻量级嵌入模型，能在无人工标注下以零样本方式有效对图像片段进行分类。 </li><li><strong>性能优越性</strong>：在语义分割和部件分割任务的实验中，USE框架在<strong>ADE20K、Pascal Context</strong>和<strong>PartImageNet</strong>等数据集上，大幅超越现有最先进的两阶段方法，平均性能也优于端到端方法。 </li><li><strong>研究意义</strong>：此工作为构建开放词汇图像分割的基础模型和基于片段的表征学习提供了参考，有望推动相关领域的研究发展。</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 开放词汇图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Universal Segment Embeddings </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation</title>
      <link href="/post/llmformer-large-languagemodel-for-open-vocabulary-semantic/"/>
      <url>/post/llmformer-large-languagemodel-for-open-vocabulary-semantic/</url>
      
        <content type="html"><![CDATA[<p>Hunan University、Monash University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Open-vocabulary (OV) semantic segmentation</strong> has attracted increasing attention in recent years, which aims to recognize<br>objects in an open class set for real-world applications. While prior OV semantic segmentation approaches have relied on<br>additional semantic knowledge derived from vision-language (VL) pre-training, such as the popular CLIP model, this paper<br>introduces a novel paradigm by harnessing the unprecedented capabilities of large language models (LLMs). Inspired by<br>recent breakthroughs in LLMs that provide a richer knowledge base compared to traditional vision-language pre-training, our proposed methodology capitalizes on the vast knowledge embedded within LLMs for OV semantic segmentation. Particularly, we partition LLM knowledge into object, attribute, and relation priors, and propose three novel attention modules-semantic, scaled visual, and relation attentions, to utilize the LLM priors. Extensive experiments are conducted on common benchmarks including ADE20K (847 classes) and Pascal Context (459 classes). The results show that our model outperforms previous state-of-the-art (SoTA) methods by up to 7.2% absolute. Moreover, unlike previous VL-pre-training-based works, our method can even predict OV segmentation results without target candidate classes.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>开放词汇语义分割近年来受到越来越多的关注，其目的是为了在现实应用中识别开放类集中的对象。虽然之前的开放词汇语义分割方法依赖于来自视觉语言(VL)预训练的额外语义知识，例如流行的CLIP模型，但本文通过利用大型语言模型(大语言模型)前所未有的能力引入了一种新的范式。与传统的视觉语言预训练相比，大语言模型提供了更丰富的知识库，受其最新突破的启发，我们提出的方法利用大语言模型中嵌入的大量知识进行开放词汇语义分割。特别地，我们将大语言模型知识划分为对象先验、属性先验和关系先验，并提出了语义关注、尺度视觉关注和关系关注三个新的关注模块来利用大语言模型先验。在包括ADE20K(847个类)和Pascal Context(459个类)在内的常见基准测试上进行了广泛的实验。结果表明，我们的模型比以前的最先进的(SoTA)方法高出7.2%。此外，与之前基于vl预训练的工作不同，我们的方法甚至可以在没有目标候选类的情况下预测开放词汇分割结果。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>基于先验的语义分割方法可以识别固定集的目标种类，但不能够很好的处理真实世界中各种新的目标，由此引出开放词汇语义分割。开放词汇语义分割分为两种：一阶段和两阶段，尽管取得了不错的效果，但是大多数还是利用预训练的视觉语言模型提取embeding，这种方式仅提供有限的语义信息。随着大语言模型的发展，由于其提供了对场景的综合理解能力，本文作者尝试采用LLM的知识解决开放词汇语义分割中的挑战，即利用大语言模型描述中的目标名字、目标属性和目标关系。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_15-58-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_15-58-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_15-58-54"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>固定集语义分割</strong>：早期采用CNN架构，后引入多尺度组合、全局上下文建模和transformer等方法，但难以利用大预训练模型知识，且只能识别固定集对象。</li><li><strong>开放词汇语义分割</strong>：分为单阶段和两阶段方法。两阶段方法依赖训练良好的掩码生成器，计算成本高；单阶段方法虽有改进，但大多仅从视觉 - 语言预训练模型提取知识，语义信息有限。</li><li><strong>大语言模型</strong>：在许多领域取得成功，具备综合复杂推理能力，部分模型可理解视觉内容，但主要用于通用表示和预测</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-00-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-00-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-00-15"></p><ol><li><p>整体架构</p><p>LLMFormer由三个主要部分组成：</p><ul><li><strong>图像特征提取（Image Feature Extraction）</strong>：使用多模态大语言模型（MLLM）中的视觉编码器、适配器（ViT Adapter）和多尺度可变形注意力（MSDA）模块，以捕获多尺度的图像特征。</li><li><strong>LLM先验提取（LLM Prior Extraction）</strong>：通过向LLM输入问题（如“描述图像”），获取图像的全面描述，并利用语言解析工具从中提取对象、属性和关系先验知识。</li><li><strong>LLM先验引导的分割（LLM-Prior-Guided Segmentation）</strong>：引入了三种新型注意力模块，分别是语义注意力（Semantic Attention）、缩放视觉注意力（Scaled Visual Attention）和关系注意力（Relation Attention），以利用LLM的先验知识进行开放词汇语义分割。</li></ul></li><li><p>关键组件</p></li></ol><ul><li><strong>语义注意力（Semantic Attention）</strong>：将对象和属性先验知识嵌入到掩码嵌入中，以增强开放词汇对象的发现、掩码预测和分类能力。该模块通过多头交叉注意力机制，捕捉对象先验与掩码之间的对应关系。</li><li><strong>缩放视觉注意力（Scaled Visual Attention）</strong>：基于属性先验知识，为每个掩码选择合适的视觉特征图，以更好地分割不同大小的对象。具体来说，该模块根据属性先验生成掩码的属性嵌入，并通过多层感知机（MLP）预测尺度选择分数，从而选择合适尺度的特征图。</li><li><strong>关系注意力（Relation Attention）</strong>：利用LLM的关系先验知识，学习掩码之间的关系。该模块通过生成对象关系图，并将其映射到掩码级别，然后通过多头自注意力机制将关系先验知识编码到掩码嵌入中。</li></ul><p>::: tip</p><p>重要</p><p>:::</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：训练：COCO-Stuff，测试：ADE20K、Pascal Context、Pascal VOC</p></blockquote><ul><li><strong>Pascal Context数据集</strong>：与FC - CLIP相比，在PC - 459和PC - 59上分别提高了7.2%和5.8%；与SAN相比，分别提高了8.3%和4.0%。</li><li><strong>ADE20K数据集</strong>：在A - 847和A - 150上均达到了最先进的性能，超过FC - CLIP分别为1.7%和4.4%，超过使用VIT - L骨干的SAN分别为2.8%和5.2%。</li><li><strong>Pascal VOC 2012数据集</strong>：取得了最佳的mIoU，显著超过之前的SOTA方法FC - CLIP 1.4%。在Open IoU指标下也有显著提升，证明了模型的开放词汇能力。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-04-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-04-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-04-53"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-00.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-00.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-05-00"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-05-17"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>主要组件的影响</strong>：语义注意力模块能显著提升性能，基于大语言模型（LLM）属性的尺度选择和关系注意力可进一步提高性能，完整模型在A - 847和A - 150上取得最佳性能。</li><li><strong>语义注意力</strong>：使用对象先验能持续提高开放词汇分割性能，属性先验可进一步增强结果，交叉注意力在整合先验的方法中表现最优。</li><li><strong>缩放视觉注意力</strong>：多尺度方法优于单尺度方法，基于属性先验的尺度选择方法优于其他非选择方法。</li><li><strong>关系注意力</strong>：完整模型通过整合LLM关系先验到自注意力图中，在A - 847和A - 150上分别比仅使用原始注意力图的模型提高了1.7%和2.3%。</li><li><strong>不同注意力方法</strong>：模型的注意力方法显著优于掩码注意力和TSG注意力，因为它们能利用LLM先验来改进开放词汇语义分割。</li><li><strong>成本比较</strong>：与Zegformer和OV - Seg相比，模型的可训练参数更少，同时精度更高。使用LLAVA - Phi2 - 2.7B可进一步降低计算开销，性能仅有轻微下降。</li><li><strong>不同提示</strong>：详细提示（描述所有对象、属性和关系）能进一步提高分割性能，但实验中主要使用简单提示。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><blockquote><p>作者提出了LLMFormer这一利用大语言模型（LLM）知识进行开放词汇语义分割（OV）的新方法，并得出以下结论： </p></blockquote><ol><li><strong>方法有效性</strong>：提出语义、缩放视觉和关系三种注意力模块，利用LLM的对象、属性和关系先验知识进行分割，大量实验证明LLMFormer及各注意力模块有效。 </li><li><strong>性能优势</strong>：在ADE20K、Pascal Context和Pascal VOC等数据集上显著优于现有方法，绝对提升最高达7.2%，还能在无预定义候选类别的情况下预测OV分割结果，更适用于实际应用。</li><li><strong>未来方向</strong>：当前工作虽提升了OV识别能力，但LLM参数多致速度慢，且存在细粒度分类和边界分割问题，未来将研究效率、细粒度分类和分割问题。</li></ol><blockquote><p>不足及展望：使用LLM带来了巨大的参数量，导致速度减小；本文当前的工作关注于提升开放词汇的分类能力，其他的细粒度分类和边界框分割问题没有涉及，这也是未来工作的研究方法。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 开放词汇语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMFormer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation</title>
      <link href="/post/corrmatch-label-propagation-via-correlation-matching-for-semi-supervised-semantic-segmentation/"/>
      <url>/post/corrmatch-label-propagation-via-correlation-matching-for-semi-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Nankai University、NKIARI, Shenzhen Futian、SICE, UESTC</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>This paper presents a simple but performant semi-supervised semantic segmentation approach, called CorrMatch. Previous approaches mostly employ complicated training strategies to leverage unlabeled data but overlook the role ofcorrelation maps in modeling the relationships between pairs oflocations. We observe that the correlation maps not only enable clustering pixels ofthe same category easily but also contain good shape information, which previous works have omitted. Motivated by these, we aim to improve the use efficiency of unlabeled data by designing two novel label propagation strategies. First, we propose to conduct pixel propagation by modeling the pairwise similarities of pixels to spread the high-confidence pixels and dig out more. Then, we perform region propagation to enhance the pseudo labels with accurate class-agnostic masks extracted from the correlation maps. CorrMatch achieves great performance on popular segmentation benchmarks. Taking the DeepLabV3+ with ResNet-101 backbone as our segmentation model, we receive a 76%+ mIoU score on the Pascal VOC 2012 dataset with only 92 annotated images. Code is available at <a href="https://github.com/BBBBchan/CorrMatch">https://github.com/BBBBchan/CorrMatch</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>本文提出了一种简单但高性能的半监督语义分割方法 CorrMatch。现有方法大多采用复杂的训练策略来利用未标注数据，但忽视了关联图在建模像素位置关系中的重要作用。我们发现，关联图不仅能够轻松实现同类像素的聚类，还包含了被以往研究忽略的优质形状信息。基于这些观察，我们设计了两种创新的标签传播策略来提升未标注数据的使用效率。首先，我们提出<strong>像素传播策略</strong>，通过建模像素对的相似性关系来扩展高置信度像素区域，并挖掘更多潜在的高置信度像素。其次，我们开发了<strong>区域传播策略</strong>，通过从关联图中提取精确的类别无关掩码来增强伪标签质量。CorrMatch 在主流分割基准测试中表现优异：当使用 ResNet-101 为主干的 DeepLabV3+ 模型时，在仅含 92 张标注图像的 Pascal VOC 2012 数据集上实现了 76%+ 的 mIoU。代码已开源：<a href="https://github.com/BBBBchan/CorrMatch%E3%80%82">https://github.com/BBBBchan/CorrMatch。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦半监督语义分割领域，旨在解决深度学习方法对大规模像素级标注数据集的依赖问题，具体研究背景如下： </p><ul><li><strong>标注成本高</strong>：基于深度学习的语义分割方法通常需要大量像素级标注图像，但准确标注分割数据集成本高、耗时长，限制了其应用。 </li><li><strong>半监督学习受关注</strong>：为减少对大规模准确标注数据的需求，研究者提出弱监督、半监督和无监督分割方法。其中，半监督语义分割仅需少量标注数据和大量未标注数据进行训练，更接近现实场景，受到广泛关注。 </li><li><strong>现有方法存在不足</strong>：现有半监督语义分割方法多采用复杂训练策略，如Mean Teacher架构或自训练策略，需要额外网络或训练阶段，增加了训练复杂度。此外，常用的固定阈值筛选伪标签方法难以有效利用未标注数据。</li><li><strong>Correlation map的潜力</strong>：像素间的相关性可反映成对相似性，相关性地图不仅能轻松聚类同一类别的像素，还包含良好的形状信息，但以往研究忽略了其在建模位置对关系中的作用。 基于以上背景，作者提出了CorrMatch方法，通过设计两种新颖的标签传播策略，提高未标注数据的使用效率。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>方法多样</strong>：在半监督语义分割领域，已有多种方法被提出，如采用Mean Teacher架构的方法（U2PL、PS - MT等）、基于自训练策略的方法（ST++、SimpleBase等），以及近期的UniMatch等单阶段框架。</li><li><strong>成果显著</strong>：这些方法在一些公开数据集（如Pascal VOC 2012、Cityscapes）上取得了一定成果，推动了半监督语义分割技术的发展。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-03-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-03-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-03-57"></p><ol><li><strong>模型框架</strong>：CorrMatch是一个单阶段框架，基于具有弱到强一致性正则化的简单框架构建。对于有标签图像，使用标准的交叉熵损失；对于无标签图像，主要通过强制预测一致性来利用，同时考虑弱增强和强增强图像在高置信区域的对数似然一致性。</li><li>标签传播策略<ul><li><strong>像素传播</strong>：通过计算相关图并将其传播到预测中，增强模型对像素对之间相似性的整体感知，从而提高无标签数据的利用率。具体步骤为：首先通过网络编码器后的线性层提取特征，计算特征向量对之间的相关性得到相关图；然后将相关图传播到模型的对数似然输出中，得到另一种预测表示；最后计算该预测表示与高置信伪标签之间的相关损失作为监督。</li><li><strong>区域传播</strong>：利用相关图中隐含的形状信息来增强伪标签。具体做法是将相关图的每一行进行归一化并转换为二值图，当二值图与高置信区域有较大重叠时，计算高置信形状内每个唯一类别的数量，找到最显著的类别，并将该类别传播到增强的伪标签和扩展的高置信区域中。为了提高效率，采用随机采样的方法。</li></ul></li><li>其他策略<ul><li><strong>动态阈值</strong>：使用与训练过程相关的动态阈值策略，避免固定阈值过严或过松对模型收敛的不利影响。通过指数移动平均（EMA）根据对数似然输出迭代更新阈值。</li><li><strong>损失函数</strong>：整体目标函数是监督损失和无监督损失的组合。监督损失是基本监督损失和监督相关损失的组合；无监督损失包括无监督硬损失、软损失和相关损失。</li></ul></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：Pascal VOC 2012 、Cityscapes</p><ul><li><strong>经典Pascal VOC 2012</strong>：在不同分割比例下与其他最先进方法比较，CorrMatch在全分割比例下mIoU达到81.8%，且在各分割比例上均优于现有方法，如比UniMatch在各分割比例上分别高出1.2%、1.3%、0.6%、0.7%和0.6%。</li><li><strong>aug Pascal VOC 2012</strong>：在不同训练尺寸和分割比例下进行实验，结果显示CorrMatch始终优于现有最佳方法。例如，在321×321训练尺寸下，比监督基线在1&#x2F;16、1&#x2F;8和1&#x2F;4分割比例上分别提高12.0%、7.4%和5.5%，比UniMatch在各分割比例上分别高出1.1%、0.8%和1.1%。</li><li><strong>Cityscapes</strong>：采用滑动窗口评估和在线难例挖掘（OHEM）损失技术，CorrMatch在所有分割比例下均优于其他方法，比UniMatch在1&#x2F;16、1&#x2F;8、1&#x2F;4和1&#x2F;2分割比例上分别高出0.7%、0.6%、0.2%和0.9%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-08-09"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-08-15"></p><p>: : : important</p><p>重要！！！</p><p>: : :</p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>组件有效性</strong>：验证了CorrMatch不同组件的有效性，包括硬无监督损失、软损失、标签传播等。完整的CorrMatch在92和1464分割比例下mIoU分别达到76.4%和81.8%，比基线分别提高2.8%和1.8%。动态阈值策略与标签传播策略配合良好。</li><li><strong>标签传播策略的影响</strong>：像素传播策略带来了一定的性能提升，区域传播策略进一步提高了性能。例如，像素传播策略在92、366和1464分割比例上分别提高1.4%、0.4%和0.8%，区域传播策略在此基础上分别再提高0.6%、0.5%和0.5%。</li><li><strong>特征提取位置</strong>：默认从骨干网络提取特征，实验表明使用骨干网络特征的性能始终优于其他位置。</li><li><strong>不同采样策略</strong>：比较了随机采样和均匀采样方法，随机采样效果更好，其中随机采样128个样本时性能最佳。</li><li><strong>不同初始值</strong>：基于EMA的阈值更新策略对不同初始值不敏感，在训练早期所有阈值都会快速趋近相似值。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-09-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-09-06"></p><h1 id="其他实验"><a href="#其他实验" class="headerlink" title="其他实验"></a>其他实验</h1><ul><li><strong>统计分析</strong>：统计挖掘比例和有效伪标签比例，结果表明使用标签传播策略后，这两个比例显著高于未使用时，说明未标记数据的利用率得到有效提高。</li><li><strong>定性分析</strong>：可视化结果显示，使用标签传播策略后，高置信区域的像素数量和完整性明显优于未使用时，能够有效扩展高置信区域并填充正确类别。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-10-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-10-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-10-04"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种名为<strong>CorrMatch的半监督语义分割方法</strong>，通过实验分析得出以下结论： </p><ul><li><strong>策略有效</strong>：重新考虑了相关图的使用，设计了<strong>像素传播和区域传播</strong>两种标签传播策略，能利用相关图中的相似性和形状信息，显著扩大高置信度区域，有效提升伪标签的整体质量。 </li><li><strong>效率提升</strong>：这些策略使模型能更高效地利用未标记数据，解决了传统方法中阈值选择困难、未标记数据利用不充分等问题。 </li><li><strong>性能优越</strong>：在Pascal VOC 2012和Cityscapes等数据集上，CorrMatch始终优于其他现有方法，取得了新的最先进性能，且推理过程无额外计算负担。</li></ul><blockquote><p>我们提出了CorrMatch，它可以利用标签传播和相关匹配来发现更准确的高置信度区域，用于半监督语义分割。CorrMatch的主要贡献是重新考虑相关映射的使用，并设计了两种标签传播策略来丰富伪标签。利用这些策略，CorrMatch显著扩展了高置信度区域，从而可以更有效地利用未标记的数据。实验证明了我们的cormatch算法优于其他方法。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 半监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semi-Supervised Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation</title>
      <link href="/post/cc4s-encouraging-certainty-and-consistency-in-scribble-supervised-semantic-segmentation/"/>
      <url>/post/cc4s-encouraging-certainty-and-consistency-in-scribble-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="Peking-University、Shandong-Universit"><a href="#Peking-University、Shandong-Universit" class="headerlink" title="Peking University、Shandong Universit"></a>Peking University、Shandong Universit</h2><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Deep learning-based</strong> solutions have achieved impressive performance in semantic segmentation but often require large<br>amounts of training data with fine-grained annotations. To alleviate such requisition, a variety of weakly supervised annotation<br>strategies have been proposed, among which scribble supervision is emerging as a popular one due to its user-friendly annotation way. However, the sparsity and diversity of scribble annotations make it nontrivial to train a network to produce deterministic and consistent predictions directly. To address these issues, in this paper we propose holistic solutions involving the design of network structure, loss and training procedure, named <strong>CC4S</strong> to improve Certainty and Consistency for Scribble-Supervised Semantic Segmentation. Specifically, to reduce uncertainty, CC4S embeds a random walkmodule into the network structure to make neural representations uniformly distributed within similar semantic regions, which works together with a soft entropy loss function to force the network to produce deterministic predictions. To encourage consistency, CC4S adopts self-supervision training and imposes the consistency loss on the eigenspace of the probability transition matrix in the random walk module (we named neural eigenspace). Such self-supervision inherits the category-level discriminability from the neural eigenspace and meanwhile helps the network focus on producing consistent predictions for the salient parts and neglect semantically heterogeneous backgrounds. Finally, to further improve the performance, CC4S uses the network predictions<br>as pseudo-labels and retrains the network with an extra color constraint regularizer. From comprehensive experiments, CC4S<br>achieves comparable performance to those from fully supervised methods and shows promising robustness under extreme supervision cases.</p><p>代码： <a href="https://github.com/panzhiyi/CC4S">https://github.com/panzhiyi/CC4S</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>基于深度学习的方法在语义分割中取得了令人印象深刻的性能，但通常需要大量带有细粒度标注的训练数据。为了减少这种需求，研究者提出了多种弱监督标注策略，其中<strong>涂鸦监督</strong>因其用户友好的标注方式而逐渐流行。然而，涂鸦标注的稀疏性和多样性使得直接训练网络生成确定且一致的预测具有挑战性。为解决这些问题，本文提出了包含网络结构设计、损失函数和训练流程的完整解决方案——CC4S（提升涂鸦监督语义分割确定性与一致性的方法）。具体而言，为降低不确定性，CC4S在网络架构中嵌入<strong>随机游走模块</strong>，使神经表征在相似语义区域内均匀分布。该模块与软熵损失函数共同作用，迫使网络生成确定性预测结果。为增强一致性，CC4S采用自监督训练策略，在随机游走模块的概率转移矩阵特征空间（称为神经特征空间）中施加一致性损失。这种自监督机制既继承了神经特征空间的类别级判别能力，又能促使网络专注于对显著区域生成一致预测，同时忽略语义异构的背景区域。为进一步提升性能，CC4S将网络预测结果作为伪标签，通过引入额外的色彩约束正则化项对网络进行重训练。综合实验表明，CC4S取得了与全监督方法相媲美的性能，在极端监督条件下也展现出良好的鲁棒性。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于涂鸦监督语义分割领域，旨在解决该领域存在的问题，其研究背景主要如下：</p><ul><li><strong>数据标注难题</strong>：基于深度学习的语义分割方法虽表现出色，但需大量细粒度标注的训练数据。以Cityscapes为例，手动生成像素级语义分割标注平均耗时3 - 5分钟，收集大规模标注数据集并非易事。</li><li><strong>弱监督方法兴起</strong>：为缓解数据标注压力，多种弱监督标注策略应运而生，如<strong>图像级监督、边界框监督、点监督和涂鸦监督</strong>等。其中，涂鸦监督因<strong>标注方式友好</strong>且能提供有效监督信息，受到越来越多关注。</li><li><strong>涂鸦监督现存问题</strong>：尽管涂鸦监督语义分割取得了一定进展，但仍存在预测结果不确定和不一致的问题。标注稀疏会导致预测结果不确定，而标注的多样性会使网络难以学习到稳定一致的分割模式，从而产生不一致的预测结果。 基于以上背景，作者提出了CC4S方法，以提高涂鸦监督语义分割的确定性和一致性，减少标注稀疏和多样性带来的影响。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-12-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-12-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-12-13"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><blockquote><p>弱监督语义分割：分为四种，图像级监督、边界框监督、点监督和涂鸦监督。图像级监督仅为整个图像提供类别标签，缺乏定位信息；边界框监督仍然缺乏可靠和有效的措施来产生高质量的物体掩膜；点监督通过在每个图像对象内标记带有类别信息的点来完成注释；涂鸦监督是一种用户友好的弱监督形式。</p></blockquote><blockquote><p>涂鸦监督语义分割：现有方法包括利用辅助任务信息、图割算法传播标注、在损失函数引入分割正则化等，但仍存在预测不确定和不一致问题。</p></blockquote><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>本文提出了一种名为<strong>CC4S（Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation）<strong>的模型，旨在解决涂鸦监督语义分割任务中预测结果的</strong>不确定性和不一致性</strong>问题</p><p>核心网络包含两个模块：</p><ul><li><strong>ResNet骨干网络</strong>：用于提取图像的特征。</li><li><strong>相似度测量模块（SMM）</strong>：计算每两个神经元之间的转移概率，形成转移矩阵。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-24-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-24-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-24-44"></p><h4 id="减少神经表示的不确定性"><a href="#减少神经表示的不确定性" class="headerlink" title="减少神经表示的不确定性"></a>减少神经表示的不确定性</h4><h4 id="神经特征空间的自监督学习"><a href="#神经特征空间的自监督学习" class="headerlink" title="神经特征空间的自监督学习"></a>神经特征空间的自监督学习</h4><h4 id="带有颜色约束的伪标签再训练"><a href="#带有颜色约束的伪标签再训练" class="headerlink" title="带有颜色约束的伪标签再训练"></a>带有颜色约束的伪标签再训练</h4><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：Pascal VOC 2012 and Pascal Context</p><p>比较的方法：Scribblesup, RAWKS, NCL, GraphNet，KCL, BPG, URSS, PSI, SPML, A2GNN, DBFNet, PCE , CCL , TEL and CDL</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-30-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-30-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-30-25"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-29-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-29-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-29-58"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者通过研究得出以下结论：</p><ol><li>仅使用涂鸦注释进行语义分割会导致预测结果<strong>不确定和不一致</strong>。为此，提出了两种策略，即减少神经表示的不确定性以产生可靠结果，以及在神经特征空间进行自监督以保证输出的一致性。</li><li>结合<strong>伪标签再训练</strong>，该方法达到了最先进的性能，甚至可与全标签监督方法相媲美，且整个过程无需额外信息或注释准备要求。</li><li>大量的消融实验和中间可视化验证了所提解决方案的有效性。</li><li>该方法在涂鸦<strong>随机丢弃或按比例缩小</strong>的困难情况下也能表现良好，具有较强的鲁棒性。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 涂鸦监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scribble-Supervised Semantic Segmentation </tag>
            
            <tag> CC4S </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings</title>
      <link href="/post/scaling-upmulti-domain-semantic-segmentation-with-sentence/"/>
      <url>/post/scaling-upmulti-domain-semantic-segmentation-with-sentence/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>The <strong>state-of-the-art semantic segmentation methods</strong> have achieved impressive performance on predefined close-set individual datasets, but their generalization to zero-shot domains and unseen categories is limited. Labeling a large-scale dataset is challenging and expensive, Training a robust semantic segmentation model on multi-domains has drawn much attention. However, inconsistent taxonomies hinder the naive merging of current publicly available annotations. <strong>To address this, we propose a simple solution to scale up the multi-domain semantic segmentation dataset with less human effort</strong>. We replace each class label with a sentence embedding, which is a vector-valued embedding of a sentence describing the class. This approach enables the merging of multiple datasets from different domains, each with varying class labels and semantics. We merged publicly available noisy and weak annotations with the most finely annotated data, over 2 million images, which enables training a model that achieves performance equal to that of state-of-the-art supervised methods on 7 benchmark datasets, despite not using any images therefrom. Instead of manually tuning a consistent label space, we utilized a vector-valued embedding of short paragraphs to describe the classes. By fine-tuning the model on standard semantic segmentation datasets, we also achieve a significant improvement over the state-of-the-art supervised segmentation on NYUD-V2 (Silberman et al., in: European conference on computer vision, Springer, pp 746–760, 2012) and PASCAL-context (Everingham et al. in Int J Comput Visi 111(1):98–136, 2015) at 60% and 65% mIoU, respectively. Our method can segment unseen labels based on the closeness of language embeddings, showing strong generalization to unseen image domains and labels. Additionally, it enables impressive performance improvements in some adaptation applications, such as depth estimation and instance segmentation. Code is available at <a href="https://github.com/YvanYin/SSIW">https://github.com/YvanYin/SSIW</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>当前最先进的语义分割方法在预设的封闭数据集上表现出色，但其在零样本（zero-shot）领域和未见过类别上的泛化能力仍然有限。由于大规模数据标注既困难又昂贵，开发跨领域通用的鲁棒语义分割模型成为研究热点。然而，现有公开数据集的不同分类标准阻碍了它们的直接融合。为此，我们提出了一种高效扩展多领域语义分割数据集的方法：用文本嵌入（text embedding）替代传统类别标签，这种向量化的语义表示可以融合不同领域、不同标签体系的数据。通过整合包含 200 万张图像的精细标注数据与公开的带噪声弱标注数据，我们训练的模型在 7 个主流测试集上达到了监督学习的顶尖水平，尽管完全没有使用这些测试集的训练图像。与人工统一标签体系不同，我们通过语言模型生成短文本描述来表征类别语义。在标准数据集微调后，模型在 NYUD-V2 (Silberman et al., 2012) 和 PASCAL-context (Everingham et al., 2015) 上分别取得 60% 和 65% 的平均交并比（mIoU），显著超越现有监督方法。该方法通过计算语义相似度实现未见过标签的分割，展现出优异的跨领域泛化能力，**同时在深度估计、实例分割等下游任务中带来显著性能提升。**代码已开源：<a href="https://github.com/YvanYin/SSIW">https://github.com/YvanYin/SSIW</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割是计算机视觉的基础任务，在自动驾驶、农业机器人和医学等领域应用广泛。当前语义分割方法虽在预定义封闭集数据集上表现出色，但存在明显局限：</p><ol><li><strong>泛化能力不足</strong>：这些方法假设测试集中的所有类别都在训练时出现，然而现实场景并非如此，且模型受限于训练数据集的图像领域，难以泛化到新领域和标签。</li><li><strong>数据集合并难题</strong>：训练多领域语义分割模型是提升模型鲁棒性和泛化能力的自然途径，但直接合并不同领域的数据集会导致标签分类体系冲突，手动统一标签集和重新标注的方法不仅费力，在开放集场景下也存在局限性。</li><li><strong>现有零样本方法的缺陷</strong>：现有解决开放集问题的方法多在小数据集上实验，限制了其在现实场景中的应用潜力。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>语义分割</strong>：深度学习方法在特定高质量数据集上取得显著成果，但泛化能力受限。如FCN开启全卷积方法，后续ResNet、Transformer等推动性能提升。</li><li><strong>零样本语义分割</strong>：分为判别式和生成式方法，前者如Xian等将像素特征转换到语义词嵌入空间，后者如ZS3Net用生成模型生成像素特征。</li><li><strong>跨领域密集预测</strong>：有方法合并分割数据集提升性能和泛化能力，如Ros合并六个驾驶数据集，Lambert提出统一分类法合并多领域数据集。</li><li><strong>零样本学习标签编码</strong>：许多方法为类别标签生成语义嵌入，如Bucher用Word2Vec编码标签，Lseg用语言嵌入监督类别标签。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><ol><li><ul><li><p><strong>创建语言嵌入</strong>：从Wikipedia收集每个类别的简短描述，使用CLIP - ViT语言模型将这些描述编码为向量值的句子嵌入。这种方式能保留标签之间的语义关系，相比单字标签嵌入，更能反映类别间的语义相似性，有助于零样本标签的分割。</p></li><li><p><strong>混合数据的异构约束</strong>：为解决合并数据集中标注质量不平衡的问题，提出了异构损失函数。</p></li></ul></li></ol><ul><li><strong>高质量标注数据集</strong>：对所有样本施加像素级损失。<ul><li><strong>粗标注数据集（如OpenImages）</strong>：通过自适应阈值，对高置信度样本施加损失，忽略噪声较大的部分。</li></ul></li><li><strong>弱标注数据集（如Objects365）</strong>：采用蒸馏方法，利用CLIP分类模型的知识，对分割模型进行监督。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_09-38-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_09-38-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_09-38-37"></p><h2 id="实验（Compared-with-SOTA-and-ablation-experiment）"><a href="#实验（Compared-with-SOTA-and-ablation-experiment）" class="headerlink" title="实验（Compared with SOTA and ablation experiment）"></a>实验（Compared with SOTA and ablation experiment）</h2><h3 id="1-数据集与实现细节"><a href="#1-数据集与实现细节" class="headerlink" title="1. 数据集与实现细节"></a>1. 数据集与实现细节</h3><ul><li><strong>训练数据</strong>：合并了7个高质量语义分割数据集（<strong>ADE20K、Mapillary、COCO Panoptic、IDD、BDD100K、Cityscapes、SUNRGBD</strong>），并从OpenImagesV6和Objects365中采样部分数据用于训练。这些数据集涵盖了不同的标注风格和图像领域，总训练图像约<strong>200万张</strong>。</li><li>测试数据<ul><li><strong>语义分割测试</strong>：在8个零样本数据集（CamVid、KITTI、Pascal VOC、Pascal Context、ScanNet、WildDash1、WildDash2、YoutubeVIS）上进行测试，并在NYUv2和Pascal Context上微调模型以评估性能。</li><li><strong>下游应用测试</strong>：在零样本数据集上创建伪语义标签，用于提升实例分割和单目深度估计的性能。实例分割在COCO数据集上进行测试，深度估计在NYUDv2、KITTI、DIODE、ScanNet和Sintel等数据集上进行评估。</li></ul></li><li>评估指标<ul><li><strong>语义分割</strong>：使用平均交并比（mIoU）进行评估。</li><li><strong>实例分割</strong>：使用平均精度（AP）进行评估。</li><li><strong>深度估计</strong>：采用绝对相对误差（AbsRel）和满足特定条件的像素百分比（δτ）进行评估。</li></ul></li><li><strong>多尺度评估</strong>：在评估语义分割性能时，将测试图像调整为多个尺度（0.5 - 1.75，步长为0.25）输入模型，然后平均得分作为最终预测结果。</li><li><strong>实现细节</strong>：使用HRNet - W48和Segformer两种网络架构进行实验。训练时，采用不同的优化器和学习率衰减策略，并对图像进行数据增强处理。推理时，将图像短边调整为三种分辨率（480&#x2F;720&#x2F;1080），并根据需要采用多尺度或单尺度测试。</li></ul><h3 id="2-实验内容"><a href="#2-实验内容" class="headerlink" title="2. 实验内容"></a>2. 实验内容</h3><ul><li><p><strong>语义分割评估</strong></p><p>   在15个数据集上进行评估，以验证模型的鲁棒性和有效性。</p><ul><li><strong>鲁棒性评估</strong>：与现有最先进的方法在6个零样本数据集上进行比较，结果表明该方法在CamViD、ScanNet和WildDash1上达到了最先进的性能，并且在混合数据集上训练的模型比在单个数据集上训练的HRNet更具鲁棒性。</li><li><strong>Wilddash2评估</strong>：在Wilddash2基准测试中，该方法取得了最先进的性能。</li><li><strong>异质损失聚合效果评估</strong>：提出异质损失来监督合并的数据集，实验结果表明，在聚合OpenImages和Objects365时，使用异质损失可以持续提高所有零样本数据集的性能。</li><li><strong>未见标签泛化能力评估</strong>：通过在YoutubeVIS数据集上采样具有5个零样本标签的约2100张图像进行实验，结果表明该方法比Mseg和JoEm更具鲁棒性，并且使用句子编码可以获得更好的性能。</li><li><strong>小数据集微调评估</strong>：在NYUv2和Pascal Context上微调模型，与其他预训练权重相比，该方法的预训练权重可以显著提升性能，超过现有最先进的方法。</li><li><strong>蒸馏效果评估</strong>：使用CLIP对模型进行知识蒸馏，实验结果表明，即使只在裁剪的边界框区域进行粗略的知识蒸馏，性能仍然可以得到显著提升。</li><li><strong>语言嵌入合并训练数据效果评估</strong>：比较了两种合并训练数据标签的方法，结果表明，使用句子嵌入来表示标签可以更好地解决标签冲突问题，从而获得更好的性能。</li><li><strong>与Lseg比较</strong>：与Lseg方法在不同粒度的标签集上进行比较，结果表明，在细化标签集上，该方法的分割结果更优。</li></ul></li><li><p><strong>下游应用提升评估</strong></p><ul><li><strong>单目深度估计</strong>：在多个零样本深度数据集上创建伪语义标签，将其转换为像素级语言嵌入并输入到深度预测网络中。实验结果表明，与基线方法LeReS相比，添加创建的嵌入可以持续提高所有数据集的性能。</li><li><strong>实例分割</strong>：在采样的Objects365上创建伪实例掩码，用于训练CondInst模型。实验结果表明，使用仅25%的COCO数据，该方法可以达到与基线方法相当的性能，并且在使用完整的COCO数据进行微调时，性能比基线方法高约4% AP。</li></ul></li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出一种语义分割方法，能在多个零样本跨域数据集上取得良好性能。具体结论如下：</p><ol><li><strong>数据融合</strong>：从维基百科收集标签简短描述，编码为向量嵌入替代标签，可轻松合并多数据集，得到强大鲁棒的分割模型。</li><li><strong>损失函数</strong>：提出异质损失，利用噪声和弱标注数据集。 </li><li><strong>性能表现</strong>：在7个跨域数据集上，性能优于或与当前最先进方法相当，模型能分割零样本标签。</li><li><strong>下游应用</strong>：该模型显著提升单目深度估计和实例分割等下游应用的性能。不过，模型性能可能受语言模型表示限制，对训练语言空间外的类别泛化能力不足，但增加数据类别和改进语言模型有望解决。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sentence Embeddings </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scribble Hides Class Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label</title>
      <link href="/post/scribbl-hides-class-promoting-scribble-based-weakly-supervised-semantic-segmentation-with-its-class-label/"/>
      <url>/post/scribbl-hides-class-promoting-scribble-based-weakly-supervised-semantic-segmentation-with-its-class-label/</url>
      
        <content type="html"><![CDATA[<p>Peking University, Beijing, China</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Scribble-based weakly-supervised semantic segmentation</strong> using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we pro-pose a class-driven scribble promotion network, which uti-<br>lizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction,which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label’s boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness<br>of our method. The code is available at <a href="https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network">https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p> **基于涂鸦的弱监督语义分割（Weakly-supervised Semantic Segmentation, WSSS）**通过使用稀疏涂鸦监督正逐渐受到关注，相较于需要完整标注的替代方案，这种方法能显著降低标注成本。现有方法主要通过基于局部特征线索的像素扩散机制，将已标注像素的特征传播至未标注区域来生成伪标签。然而，这种扩散过程未能有效利用全局语义信息和类别特异性特征线索，而这些要素对实现高质量的语义分割至关重要。本研究提出了一种类驱动的涂鸦增强网络，该网络通过协同利用涂鸦标注和基于图像级类别及全局语义引导的伪标签实现监督。考虑到直接使用伪标签可能误导分割模型，我们特别设计了定位校正模块，用于在特征空间中修正前景目标表示。为了深度融合两种监督方式的优势，我们还开发了距离熵损失函数，通过涂鸦标注与伪标签边界的可靠区域确定机制，动态调整各像素点的置信权重以降低不确定性。在ScribbleSup数据集上采用不同质量涂鸦标注的实验表明，我们的方法在性能表现和鲁棒性方面均优于现有所有方法。相关代码已开源：<a href="https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network%E3%80%82">https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割领域已经取得很大的进步，但是也面临着一些挑战：手动的处理大量的数据集费时费力，且不能在现实世界中进行语义分割。基于涂鸦的WSSS的内在挑战在于稀疏标签提供的部分监督，现存的方法有三种：正则化损失、一致性学习和标签扩散。基于正则化损失的方法设计了特定的损失函数来提高模型的稳定性，基于一致性学习的方法旨在捕获不变特征，从而通过一致性损失来提高细粒度分割性能。基于标签扩散的方法通过将标记的像素扩散到未标记的像素来生成像素级伪标签，但以上的方法都存在不足，利用涂鸦标签的方法也存在不足。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>图像级弱监督语义分割</strong>：早期深度学习图像分类成果推动特征可视化工作，如引入类激活图（CAM）技术，后续有多种方法基于此生成语义伪标签以训练分割网络，还出现利用像素相关性、自注意力机制等的方法。</li><li><strong>涂鸦级弱监督语义分割</strong>：早期采用传统交互式分割方法，近年分为正则化损失、一致性学习和标签扩散三类方法。部分新方法尝试自适应生成伪标签。</li><li><strong>其他弱监督语义分割</strong>：点级和边界框级标注也是常见方式，但在训练监督和人工成本间难以平衡。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_14-48-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_14-48-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_14-48-25"></p><p>利用从<strong>稀疏涂鸦</strong>中提取的图像级类别标签为图像监督分割提供全局线索，生成全局考虑的伪标签，同时引入定位校正模块（Localization Rectification Module，LoRM）和距离熵损失（Distance Entropy Loss，DEL）来结合两种监督的优势。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：PASCAL VOC2012 and SBD</p></blockquote><h3 id="1-ScribbleSup数据集上的比较"><a href="#1-ScribbleSup数据集上的比较" class="headerlink" title="1. ScribbleSup数据集上的比较"></a>1. ScribbleSup数据集上的比较</h3><ul><li><strong>模型配置</strong>：部署resnet101作为骨干网络，deeplabV3+作为分割器，超参数设置为(λs &#x3D; e2, λc &#x3D; e7)以生成最佳结果。</li><li><strong>公平性处理</strong>：对于先前工作RAWKS和NCL采用的CRF后处理，因其耗时较长，在比较中进行了考虑。对于近期工作TEL和AGMM，为确保公平性，使用标准涂鸦重新实现它们。</li><li><strong>实验结果</strong>：该方法优于所有先前方法，比TEL高0.6%，比AGMM高1.6%。测试结果从PASCAL VOC2012网站获取。可视化比较显示，近期方法未能捕捉到正确的全局语义。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-01-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-01-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-01-53"></p><h3 id="2-涂鸦收缩和丢弃实验"><a href="#2-涂鸦收缩和丢弃实验" class="headerlink" title="2. 涂鸦收缩和丢弃实验"></a>2. 涂鸦收缩和丢弃实验</h3><p>由于基于涂鸦的注释具有灵活性，用户注释的涂鸦长度可能不同，有时会丢弃一些对象。因此，评估模型在不同收缩或丢弃比率下的鲁棒性很重要。实验结果表明，随着丢弃或收缩比率的增加，模型性能下降。当涂鸦收缩到点（收缩比率 &#x3D; 1）时，AGMM和TEL的性能下降约10%，而该方法的性能仅下降不到1%，显示出其鲁棒性。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-02-32"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><h3 id="3-组件消融实验"><a href="#3-组件消融实验" class="headerlink" title="3. 组件消融实验"></a>3. 组件消融实验</h3><ul><li><strong>模型配置</strong>：采用resnet50作为骨干网络，deeplabV2作为分割器，使用ScribbleSup数据集进行训练和验证。</li><li><strong>超参数调整</strong>：通过网格搜索找到距离熵损失所有组件的最佳超参数组合，即λs &#x3D; 1, λc &#x3D; 6。</li><li><strong>实验结果</strong>：单独使用涂鸦或伪标签作为基本监督产生的结果不理想（约67%），而同时使用两者产生了更好的结果（72.13%），表明涂鸦和伪标签提供了互补的监督。仅添加Ldc会使模型性能下降到与仅使用Lsegc几乎相同的水平，这是由于模型对伪标签中的噪声标签过拟合，而LoRM可以解决这个问题，将模型性能从67.33%提高到73.64%。与基线相比，所有组件都能提高性能，同时使用所有组件可获得最佳性能。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-02-52"></p><h3 id="4-伪标签消融实验"><a href="#4-伪标签消融实验" class="headerlink" title="4. 伪标签消融实验"></a>4. 伪标签消融实验</h3><p>使用deeplabV3+作为分割器，对不同伪标签进行实验，以评估其影响。结果表明，随着伪标签基础准确率的提高，该方法的性能也随之提高，这表明该方法直接受益于图像级弱监督语义分割方法，是一个有前途的发展方向。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-03-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-03-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-03-15"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于**基于涂鸦的弱监督语义分割（scribble-based WSSS）**问题的类驱动涂鸦提升网络（<strong>CDSP</strong>），并得出以下结论： </p><blockquote><ol><li><p>引入定位校正模块（LoRM）解决模型对噪声标签过拟合问题，通过参考其他前景位置的特征表示，校正被误导的前景特征。 </p></li><li><p>采用距离熵损失（DEL）增强网络鲁棒性，根据涂鸦和伪标签边界确定可靠区域，为预测分配不同置信度。 </p></li><li><p>实验结果表明，该方法优于现有方法，在不同质量涂鸦的实验中表现出卓越的鲁棒性，达到了当前最优性能，证明了利用图像级类别信息生成全局伪标签用于基于涂鸦的弱监督语义分割的有效性。</p></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 涂鸦监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scribble </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Generalized Medical Image Segmentation from Decoupled Feature Queries</title>
      <link href="/post/learning-generalized-medical-image-segmentation-from-decoupled-feature-queries/"/>
      <url>/post/learning-generalized-medical-image-segmentation-from-decoupled-feature-queries/</url>
      
        <content type="html"><![CDATA[<p>Jarvis Research Center、Wuhan University、Guangxi Medical University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Domain generalized medical image segmentation requires models to learn from multiple source domains and generalize well to arbitrary unseen target domain. Such a task is both technically challenging and clinically practical, due to the domain shift problem (i.e., images are collected from different hospitals and scanners). Existing methods focused on either learning shape-invariant representation or reaching consensus among the source domains. An ideal generalized representation is supposed to show similar pattern responses within the same channel for cross-domain images. However, to deal with the significant distribution discrepancy, the network tends to capture similar patterns by mul-tiple channels, while different cross-domain patterns are also allowed to rest in the same channel. To address this issue, we propose to leverage channel-wise decoupled deep features as queries. With the aid of cross-attention mechanism, the long-range dependency between deep and shallow features can be fully mined via self-attention and then guides the learning of<br>generalized representation. Besides, a relaxed deep whitening transformation is proposed to learn channel-wise decoupled features in a feasible way. The proposed decoupled feature query (DFQ) scheme can be seamlessly integrate into the Transformer segmentation model in an end-to-end manner. Extensive experiments show its state-of-the-art performance, notably outperforming the runner-up by 1.31% and 1.98% with DSC metric on generalized fundus and prostate benchmarks, respectively. Source code is available at <a href="https://github.com/BiQiWHU/DFQ">https://github.com/BiQiWHU/DFQ</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>域泛化医学图像分割</strong>任务要求模型能够从多个源域学习，并有效泛化到任意未知目标域。这种任务不仅在技术上具有挑战性，同时也具有重要的临床应用价值，其核心难点在于域偏移问题 (即医学图像采集自不同医院和扫描设备) 。现有方法主要聚焦于学习形状不变性表征或实现源域间的特征共识。理想的泛化表征应确保跨域图像在相同特征通道上呈现相似的模式响应。然而，面对显著的分布差异，网络往往通过多个通道捕捉相似模式，而不同跨域模式又可能共存于同一通道。针对这一矛盾，我们提出基于通道解耦深度特征的查询机制。通过交叉注意力机制，深度特征与浅层特征间的长程依赖关系可经由自注意力充分挖掘，从而指导泛化表征的学习。此外，我们开发了一种自适应深度白化变换，以更灵活的方式实现通道解耦特征学习。所提出的解耦特征查询 (DFQ) 框架能够以端到端方式无缝集成于Transformer分割模型。大量实验验证了该方案的先进性，在眼底和前列腺的泛化性基准测试中，其Dice相似系数 (DSC) 指标分别以1.31%和1.98%的优势显著超越次优方法。项目代码已开源：<a href="https://github.com/BiQiWHU/DFQ%E3%80%82">https://github.com/BiQiWHU/DFQ。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><ol><li><strong>数据分布差异</strong>：多数现有医学图像分割方法假定训练和测试样本遵循相同统计分布，但实际中，医学图像来自不同医院、由不同水平的标注者标注，存在显著的领域偏移问题，导致模型泛化能力要求高。 </li><li><strong>现有方法不足</strong>：过去医学图像分割的领域适应研究需目标域样本参与训练，只能泛化到训练中见过的目标域。现有领域泛化医学图像分割方法主要分为学习形状不变特征和明确学习多源域间的域间偏移两类，但难以应对不同成像条件下任意未见领域的特征分布变化。 </li><li><strong>特征问题</strong>：领域偏移使深度学习模型同一通道中不同领域的医学图像激活模式差异大，浅层特征的特征不对齐问题明显，网络为捕捉各领域模式会在多通道学习相似模式，导致特征冗余，影响模型对未见领域的泛化能力。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>医学图像分割技术发展</strong>：深度学习技术推动医学图像分割发展，早期U - Net及其变体占主导，后DeepLab及改进模型成为趋势，近期Vision Transformer因强大特征表示能力受关注，且弱监督、半监督和多标注场景下的分割研究也有开展。</li><li><strong>领域泛化研究</strong>：计算机视觉和机器学习领域对领域泛化广泛研究，计算机视觉中领域泛化分割多聚焦驾驶场景，医学图像分割领域泛化旨在从源域学习泛化到任意未见目标域的语义表示，现有方法分学习形状不变特征和学习域间偏移两类</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-06-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-06-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-06-07"></p><ol><li>减少通道冗余<ul><li><strong>问题</strong>：为应对不同领域间的分布差异，深度神经网络倾向于在多个通道中提取相似模式，导致特征冗余。</li><li><strong>解决方案</strong>：提出松弛深度白化变换（RDWT）。传统的深度白化变换（DWT）在学习解相关表示时存在问题，可能无法有效消除通道相关性。而RDWT通过在计算协方差矩阵之前对特征进行归一化，只关注通道之间的相关性，更有效地减少了特征冗余。</li></ul></li><li>从解耦特征查询中学习<ul><li><strong>问题</strong>：通道解耦特征虽增强了深度神经网络在跨领域场景中的表示能力，但松弛白化变换损失无法保证不同领域的医学图像在同一通道上显示相似的特征响应。</li><li><strong>解决方案</strong>：利用自注意力机制中固有的长距离依赖关系。在解码高层特征时，查询由深层特征生成，键和值基于浅层特征。深层特征查询对不同领域浅层表示的一致性施加了隐式约束。</li></ul></li><li>解码泛化表示<ul><li><strong>方法</strong>：通过一个由权重$W_1$和偏置$b_1$参数化的线性层对学习到的泛化表示进行特征融合，然后将结果输入到语义分割头进行最终预测。</li><li><strong>损失函数</strong>：总损失函数$L$是标准的二元交叉熵损失和Dice损失（记为$L_{seg}$）与每个特征的$L_{i_{RDWT}}$的组合，即$L &#x3D; L_{seg} + λ · \sum_{i&#x3D;1}^{4}L_{i_{RDWT}}$，其中$λ$设置为$1 × 10^{-4}$。</li></ul></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-04-46.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-04-46.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-04-46"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-05-05"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-05-13"></p><ol><li>与现有最优方法对比<ul><li><strong>前列腺分割基准</strong>：论文提出的方法显著优于现有最优方法。与次优方法相比，在第一、二、四、五和六个领域的ASD指标分别提高了0.14%、0.18%、0.49%、0.38%和0.26%；DSC指标在六个领域中的五个领域超过了所有现有方法，最高提升了2.08%。</li><li><strong>眼底图像分割基准</strong>：该方法同样显著优于现有最优方法。与次优的RAM - DSIR方法相比，平均DSC提高了1.63%，ASD改善了0.80%。</li></ul></li><li>对DFQ的理解<ul><li><strong>减少通道冗余</strong>：通过计算并可视化特征查询的协方差矩阵，发现所提出的DFQ方案在消除非对角元素方面表现最佳。</li><li><strong>跨领域特征对齐</strong>：通过t - SNE可视化特征空间，表明DFQ使来自不同领域的样本更均匀地混合，有助于最小化领域差距。</li></ul></li><li><strong>可视化分割结果</strong>：与现有方法相比，所提出的方法显示出更精确和合理的预测。</li></ol><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>各组件实验</strong>：DFQ框架由分割骨干网络、特征查询和松弛深度白化变换（RDWT）三个关键组件组成。实验表明，使用特征查询使DSC提高了0.94%，ASD提高了0.88%；RDWT进一步使DSC提高了1.16%，ASD提高了0.68%。</li><li><strong>各尺度实验</strong>：研究了解耦查询和风格不变的键与值的影响。结果显示，使用风格解耦的键和值（F1）以及风格解耦的查询（F2、F3、F4）都对分割结果有积极贡献，其中风格解耦查询的贡献更大。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li><p><strong>方法创新</strong>：为解决跨领域医学图像的特征不对齐问题，提出了放松的深度白化变换（RDWT），增强了通道表示能力并减少通道冗余；创新性地使用解耦的深度特征作为查询，引导整个框架学习不同领域相似的通道特征模式。</p></li><li><p><strong>性能表现</strong>：大量实验表明，该方法在前列腺和眼底图像分割基准测试中显著优于现有最先进的方法，在多个指标上取得了最佳性能，如在眼底和前列腺基准测试中，DSC指标分别至少高出1.31%和1.98%，展现出了卓越的领域泛化能力。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> Decoupled Feature Queries </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/progressive-feature-self-reinforcement-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/progressive-feature-self-reinforcement-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p><strong>Zhejiang Lab、Xidian University、Zhejiang University、University of Manchester</strong></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Compared to conventional semantic segmentation with pixel-level supervision, <strong>weakly supervised semantic segmentation(WSSS)</strong> with image-level labels poses the challenge that it commonly focuses on the most discriminative regions, resulting in a disparity between weakly and fully supervision scenarios. A typical manifestation is the diminished precision on object boundaries, leading to deteriorated accuracy of WSSS. To alleviate this issue, we propose to adaptively partition the image content into certain regions (e.g., confident<br>foreground and background) and uncertain regions (e.g., object boundaries and misclassified categories) for separate processing. For uncertain cues, we propose an adaptive masking strategy and seek to recover the local information with self-distilled knowledge.We further assume that confident regions should be robust enough to preserve the global semantics, and introduce a complementary self-distillation method that constrains semantic consistency between confident regions and an augmented view with the same class labels. Extensive experiments conducted on PASCAL VOC 2012 and MS COCO 2014 demonstrate that our proposed single-stage approach for WSSS not only outperforms state-of-the-art counterparts but also surpasses multi-stage methods that trade complexity foraccuracy.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>相较于需要像素级标注的传统语义分割方法，仅使用图像级标签的<strong>弱监督语义分割 (Weakly Supervised Semantic Segmentation, WSSS)</strong> 面临一个关键挑战：这类方法通常会过度关注最具区分度的区域，导致弱监督与全监督方法间存在显著性能差异。这种现象的典型表现是物体边界区域的识别精度下降，从而影响 WSSS 的整体准确性。为解决这一问题，我们提出将图像内容自适应划分为确定区域（如高置信度的前景和背景）与不确定区域（如物体边界和易混淆类别）进行差异化处理。针对不确定区域，我们设计了一种自适应掩码策略，通过<strong>自蒸馏知识 (self-distilled knowledge)</strong> 来恢复局部特征信息。同时我们提出，确定区域应当具备足够的鲁棒性以保持全局语义特征，为此开发了互补式自蒸馏方法，通过约束高置信区域与经过数据增强的同类别图像视图之间的语义一致性来强化模型。在 PASCAL VOC 2012 和 MS COCO 2014 数据集上的大量实验表明，我们提出的单阶段 WSSS 方法不仅超越了当前最先进的同类方案，其性能表现甚至优于那些通过增加模型复杂度来提升准确率的多阶段方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法存在的问题，提升分割性能。研究背景如下：</p><ul><li><strong>WSSS的优势与挑战</strong>：与传统的像素级监督语义分割相比，WSSS使用如边界框、涂鸦、点和图像级标签等“弱”标签，可降低标注成本。其中，图像级标签最为经济，但难以利用。 -</li><li><strong>现有方法的局限性</strong>：基于图像级标签的WSSS常用方法是先训练图像分类网络，生成类激活图（CAMs）作为种子区域，再将其细化为伪分割标签来监督分割网络。然而，CAMs本质上存在缺陷，它主要关注对象最具判别性的区域，导致前景对象与背景的边界区域以及多语义不同对象内的误分类区域存在高度不确定性，影响分割精度。 </li><li><strong>本文的研究目标</strong>：为解决上述问题，本文提出一种渐进式特征自我强化方法，通过自适应划分图像内容为确定区域和不确定区域并分别处理，以明确不确定区域的视觉语义，提高WSSS性能。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割方法</strong>：多阶段方法先通过分类模型生成类激活图（CAMs）作为伪标签，再训练分割模型评估性能，部分采用视觉变换器提升长程建模能力；单阶段方法将分类、伪标签细化和分割联合训练，但性能常不如多阶段方法。</li><li><strong>自蒸馏方法</strong>：将自监督学习与知识蒸馏结合，动态构建教师网络，简化训练过程并取得不错效果。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-23-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-23-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_10-23-43"></p><p>采用<strong>编码器 - 解码器</strong>架构实现图像级监督的语义分割。</p><ul><li><strong>编码器</strong>：使用在 ImageNet 上预训练的 ViT - B 视觉变压器，由图像级类标签监督。采用补丁令牌对比（PTC）进行亲和性学习，以约束最后一层补丁令牌之间的亲和性，防止过度平滑。</li><li><strong>解码器</strong>：借鉴 DeepLab 中的轻量级卷积解码器，由类激活映射（CAMs）生成的伪分割标签监督。</li><li><strong>聚合模块</strong>：将patch token汇总为一个类token。</li><li><strong>投影器</strong>：由 3 层感知器和权重归一化的全连接层组成，将所有令牌转换到合适的特征空间进行特征学习。</li><li><strong>自蒸馏机制</strong>：通过学生和教师管道实现自蒸馏，以改进模型训练。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：PASCAL VOC 2012、MS COCO 2014</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_10-26-30"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_10-26-43"></p><ul><li><strong>PASCAL VOC 2012</strong>：提出的特征自强化（FSR）方法在验证集和测试集上的mIoU分别达到75.7%和75.0%，显著优于其他单阶段方法，甚至超过了一些复杂的多阶段方法，如比BECO分别高2.0%和1.5%。与使用图像级标签和现成显著性图的多阶段方法相比，也取得了更优的性能。</li><li><strong>MS COCO 2014</strong>：在验证集上mIoU达到45.5%，优于之前的单阶段解决方案，略高于多阶段的MCTformer + 0.2%，进一步证明了该方法的优越性。</li></ul><h2 id="实验（Alabtion-Experiments）"><a href="#实验（Alabtion-Experiments）" class="headerlink" title="实验（Alabtion Experiments）"></a>实验（Alabtion Experiments）</h2><ul><li><p><strong>不确定特征选择分析</strong>：比较了基于边缘和基于CAM的两种严格选择不确定特征的方法，基于CAM的选择略优于基于边缘的选择，且当基于CAM的选择不那么严格时，性能进一步提升，经验上掩码比率r &#x3D; 0.4效果最佳。不确定特征掩码在大多数情况下比随机特征掩码性能更高，表明强化不确定特征对语义澄清很重要。</p></li><li><p><strong>特征自强化分析</strong></p><p>展示了FSR在不确定区域（unc.FSR）和确定区域（cer.FSR）的消融结果。unc.FSR在伪标签和预测标签上都有显著提升，证明了强化不确定特征的有效性。结合unc.FSR和cer.FSR可以进一步提高伪标签和预测标签的质量，表明强化确定特征与unc.FSR互补，增强了全局理解。</p><ul><li><strong>unc.FSR分析</strong>：通过分析注意力机制，计算每个注意力头在Transformer层上的平均注意力熵。应用unc.FSR时，深层（如第7 - 11层）的熵更高且更集中，表明unc.FSR通过提高深层的上下文程度有利于语义分割。</li><li><strong>cer.FSR分析</strong>：将确定特征的注意力聚合（MCA）与全局平均池化（GAP）和全局最大池化（GMP）两种传统方法进行比较。GAP性能优于GMP，MCA大幅优于GAP，表明注意力加权机制优于平均加权。可视化的类到补丁注意力图显示类令牌可以自适应地学习关注目标区域。</li></ul></li><li><p><strong>数据增强</strong>：与其他数据增强方法的比较结果表明，数据增强对性能的影响有限。例如，加入高斯模糊或日光化处理时，性能在预期范围内波动；使用强大的AutoAugment时，结果略有下降，因为强增强可能会干扰分割目标。</p></li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种<strong>基于语义不确定性引导的弱监督语义分割方法</strong>，并将其集成到单阶段框架中，在PASCAL VOC 2012和MS COCO 2014基准上验证了有效性。具体结论如下： 1. <strong>方法设计有效</strong>：设计了基于激活的掩码策略，利用自蒸馏知识恢复局部信息，并引入自蒸馏方法增强语义一致性，能有效估计边界。 2. <strong>性能表现优异</strong>：在两个基准测试中，所提方法显著优于其他单阶段方法，甚至超过了一些复杂的多阶段方法，证明了基于Transformer的单阶段训练的有效性。 3. <strong>消融实验验证</strong>：消融实验表明，增强不确定特征和确定特征对语义分割都很重要，两者结合可进一步提高伪标签和预测标签的质量。 </p><blockquote><p>启发：弱监督语义分割、自蒸馏</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Weakly Supervised Semantic Segmentation </tag>
            
            <tag> Feature Self-Reinforcement </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch教程</title>
      <link href="/post/fcn/20250312-pytorch-jiao-cheng/"/>
      <url>/post/fcn/20250312-pytorch-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-02-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-02-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_17-02-44"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-10-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-10-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_17-10-25"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-12_14-39-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-12_14-39-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-12_14-39-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_15-05-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_15-05-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_15-05-58"></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation</title>
      <link href="/post/relevant-intrinsic-feature-enhancement-network-for-few-shot-semantic-segmentation/"/>
      <url>/post/relevant-intrinsic-feature-enhancement-network-for-few-shot-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>University of Chinese Academy of Sciences、Chinese Academy of Sciences、Alibaba group</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><p>For few-shot semantic segmentation, the primary task is to extract class-specific intrinsic information from limited labeled data. However, the semantic ambiguity and inter-class similarity of previous methods limit the accuracy of pixel-level foreground-background classification. To alleviate these issues, we propose the Relevant Intrinsic Feature Enhancement Network (RiFeNet). To improve the semantic consistency of foreground instances, we propose an unlabeled branch as an efficient data utilization method, which teaches the model how to extract intrinsic features robust to intra-class differences. Notably, during testing, the proposed unlabeled branch is excluded without extra unlabeled data and computation. Furthermore, we extend the inter-class variability between foreground and background by proposing a novel multi-level prototype generation and interaction module. The different-grained complementarity between global and local prototypes allows for better distinction between similar categories. The qualitative and quantitative performance of RiFeNet surpasses the state-of-the-art methods on PASCAL−5i and COCO benchmarks.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a><strong>翻译</strong></h2><p>对于<strong>少样本语义分割任务</strong>，核心挑战在于如何从有限标注数据中提取类别本质特征。传统方法常受限于语义模糊性和类间相似性，导致前景-背景的像素级分类精度不足。为此，我们提出相关本质特征增强网络 (Relevant Intrinsic Feature Enhancement Network, RiFeNet)。该网络创新性地引入无标注分支训练策略，通过指导模型提取对类内差异具有鲁棒性的本质特征，显著提升了前景实例的语义一致性。值得一提的是，该无标注分支在测试阶段可完全移除，无需额外无标注数据支持且不增加计算负担。</p><p>为增强类间区分度，我们设计了多层次原型生成与交互模块。该模块通过建立全局原型（表征整体类别特征）与局部原型（捕捉细节特征）之间的多粒度互补关系，有效提升相似类别的可区分性。实验表明，RiFeNet 在 PASCAL-5i 和 COCO 基准测试中，无论是定性可视化结果还是定量评估指标，均超越了当前最先进的语义分割方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><p><strong>语义分割</strong>是计算机视觉领域的基础且关键任务，在医疗图像理解、工业缺陷检测等众多视觉任务中应用广泛。随着卷积神经网络和基于Transformer方法的发展，全监督语义分割取得显著成功，但获取像素级标注需大量人力和成本。因此，少样本语义分割范式受到关注，该范式让模型利用少量标注数据学习分割，再迁移到查询输入进行测试。</p><p> 然而，以往少样本语义分割方法存在语义模糊和类间相似性问题，影响分割效果。对于前景对象，同一类不同实例存在语义模糊，类内差异会导致查询图像出现语义错误；在区分前景和背景方面，类间相似性使像素级二分类困难，不同类但纹理相似的对象同时出现时，前景和背景的局部特征易混淆。 为解决这些问题，本文提出相关内在特征增强网络（RiFeNet），旨在提高少样本任务中前景分割性能，增强前景语义一致性，扩大前景和背景的类间差异。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><strong>语义分割</strong>：自全卷积网络（FCN）将语义分割转化为像素级分类后，编码器 - 解码器架构被广泛应用，近期研究聚焦于多尺度特征融合、注意力模块插入和上下文先验等。受视觉变压器启发，相关方法在语义分割任务中表现良好，但难以应对稀疏训练数据。</li><li><strong>少样本分割</strong>：主流方法分为原型提取和空间相关两类。空间相关方法虽保留空间结构，但计算复杂度高、参数多；原型学习方法以较低计算成本取得不错效果，如SG - ONE、PFENet等。</li><li><strong>少样本分割中无标签数据利用</strong>：少数研究探索了无标签数据的利用，如PPNet和Soopil的方法，但都需额外无标签数据，与原始少样本任务设置不符</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><p>本文提出了<strong>相关内在特征增强网络（Relevant Intrinsic Feature Enhancement Network，RiFeNet）</strong>，用于解决少样本语义分割任务中存在的语义模糊和类间相似性问题，以下是该模型的详细介绍： </p><ol><li><strong>整体架构</strong>：RiFeNet由三个共享主干网络的分支组成，在传统的支持 - 查询框架基础上增加了一个无标签分支，帮助模型学习保证语义一致性。其前向传播过程包含三个主要模块：   <ul><li><strong>多级原型生成模块</strong>：从支持特征中提取全局原型，从查询分支中提取局部原型，为更好的类间区分提供多粒度证据。    </li><li><strong>多级原型交互模块</strong>：构建不同粒度原型之间的交互，增强特征挖掘能力，以进行准确的识别。   </li><li><ul><li><strong>特征激活模块</strong>：使用n层Transformer编码器进行特征激活，激活包含目标类对象的像素并停用其他像素，提供最终的分割结果。</li></ul></li></ul></li><li><strong>无标签数据特征增强</strong>：引入辅助无标签分支作为有效的数据利用方法，通过对训练样本的子集进行重采样作为无标签数据，并应用相同的分割损失，教导模型避免学习有标签输入的特定样本偏差。无标签分支与查询分支共享参数，使用相互生成的伪标签进行训练。 </li><li><strong>多级原型处理</strong></li></ol><ul><li><strong>全局支持原型生成</strong>：通过对全局特征进行掩码平均池化，从支持特征中提取全局原型，以捕获高维类别级别的类别信息。    </li><li><strong>局部查询原型生成</strong>：从查询分支中额外提取局部原型，为二进制分类提供细粒度信息。使用先验掩码和局部平均池化，并通过1×1卷积和通道注意力机制进行细化。</li><li>-<strong>多级原型交互</strong>：将生成的全局和局部原型扩展到特征图的大小，然后与查询特征和先验掩码连接，经过1×1卷积和激活操作，得到增强的查询特征。</li></ul><ol start="4"><li><strong>特征激活</strong>：使用Transformer编码器对增强的查询特征进行自注意力和交叉注意力处理，输出经过调整大小后传递给分类头，得到最终的逐像素分割结果。</li><li><strong>损失函数</strong>：RiFeNet的损失函数是主损失和自监督损失的加权和。主损失使用DICE损失计算查询输入的预测结果与真实标签之间的差异，自监督损失用于无标签分支的训练，权重β经验性地设置为0.5。 实验结果表明，RiFeNet在PASCAL - 5i和COCO数据集上的表现优于现有方法，证明了其在少样本语义分割任务中的有效性。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a><strong>实验（Compared with SOTA）</strong></h2><p>数据集：PASCAL-5${^i}$ 、COCO</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_09-38-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_09-38-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_09-38-26"></p><ul><li><p><strong>PASCAL - 5i数据集</strong>：RiFeNet在大多数实验场景下优于最佳方法。在单样本设置下比CyCTR高约3.5%，五样本设置下高约2%。与现有最先进的DCAMA相比，使用ResNet50骨干时，单样本设置下高出2.5%；使用ResNet101时，高出2.7%。单样本设置下增益更大，原因是随着有标签图像增加，无标签数据与有标签图像的比例从2降至0.4，无标签分支的积极影响减小。</p></li><li><p><strong>COCO数据集</strong>：在该数据集的复杂场景下，RiFeNet在单样本设置的几乎所有分割中仍比当前最佳的DCAMA高出0.8%。定性结果也证明了RiFeNet的有效性，在支持集和查询集中前景对象姿态、外观和拍摄角度差异较大的情况下，RiFeNet在保持前景语义一致性方面有显著改进，在处理前景与背景相似性问题上表现更好。</p></li></ul><h2 id="实验（Ablation-Experiments）"><a href="#实验（Ablation-Experiments）" class="headerlink" title="实验（Ablation Experiments）"></a><strong>实验（Ablation Experiments）</strong></h2><ul><li><strong>关键组件有效性</strong>：在单样本设置下，使用无标签分支或多级别原型交互均可使性能提升约2%，两者结合时，RiFeNet在基线基础上提高3.1%。</li><li><strong>多级别原型设计选择</strong>：实验证明了为无标签分支添加引导等操作的合理性和可靠性。</li><li><strong>无标签分支设计选择</strong>：无引导查询原型的无标签分支性能比基线更差，增加基线的训练迭代次数对性能影响不大，证明该方法的有效性源于学习到的判别性和语义特征，而非数据的多次采样。</li><li><strong>不同超参数</strong>：在单样本元训练过程中，无标签图像数量设置为2时效果最佳。初始时，随着无标签图像数量增加，模型分割效果提升；数量继续增加，准确率反而下降，原因是无标签增强效果过强会使特征挖掘注意力转向无标签分支，干扰查询预测，导致特征模糊。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h2><p>在少样本分割任务中，传统方法存在<strong>语义模糊和类间相似性问题</strong>。为此，作者提出了<strong>相关内在特征增强网络（RiFeNet）</strong>。该网络引入无标签分支，在不增加额外数据的情况下，约束前景语义一致性，提高了前景的类内泛化能力。同时，提出多级原型生成与交互模块，进一步增强了背景和前景的区分度。 实验表明，RiFeNet在PASCAL - 5i和COCO基准测试中超越了现有技术水平，定性结果也证明了其有效性。消融实验显示，无标签增强和多级原型策略共同作用时，RiFeNet性能提升显著。综上，RiFeNet是一种有效的少样本语义分割模型。 </p>]]></content>
      
      
      <categories>
          
          <category> 少样本语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Feature Enhancement Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation</title>
      <link href="/post/scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation/"/>
      <url>/post/scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation/</url>
      
        <content type="html"><![CDATA[<p><strong>Hohai University, Nanjing, China</strong></p><p><strong>RMIT University, Melbourne, Australia</strong></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><p><strong>Scribble-supervised semantic segmentation</strong> presents a cost-effective training method that utilizes annotations generated through scribbling. It is valued in attaining high performance while minimizing annotation costs, which has made it highly regarded among researchers. Scribble supervision propagates information from labeled pixels to the surrounding unlabeled pixels, enabling semantic segmentation for the entire image. However, existing methods often ignore the features of classified pixels during feature<br>propagation. To address these limitations, this paper proposes a prototype-based feature augmentation method that leverages feature prototypes to augment scribble supervision. Experimental results demonstrate that our approach achieves state-of-the-art performance on the PASCAL VOC 2012 dataset in scribble-supervised semantic segmentation tasks. The code is available at<br><a href="https://github.com/TranquilChan/PFA">https://github.com/TranquilChan/PFA</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a><strong>翻译</strong></h2><p>**涂鸦监督语义分割（Scribble-supervised semantic segmentation）**提出了一种经济高效的训练方法，通过使用涂鸦生成的标注进行模型训练。该方法因能在显著降低标注成本的同时实现高性能表现，因此备受研究人员推崇。其核心原理是通过将已标注像素的信息传递至相邻未标注区域，从而完成整幅图像的语义分割。然而，我们发现现有方法在特征传递过程中普遍忽视已分类像素的特征特性。针对这一局限性，本文提出基于原型（prototype）的特征增强方法，通过挖掘特征原型（feature prototypes）的统计特性来强化涂鸦监督效果。实验表明，我们的方法在 PASCAL VOC 2012 数据集的涂鸦监督语义分割任务中达到了当前最佳水平，相关代码已开源：<a href="https://github.com/TranbilChan/PFA%E3%80%82">https://github.com/TranbilChan/PFA。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><p><strong>标注成本问题</strong>：深度学习技术推动了深度神经网络在图像分割的发展，但是，对于标注像素级别的样本需要大量的人力和财力，并且其标注过程也非常繁琐。因此，研究者越来越关注利用涂鸦标签进行监督学习的方法。<strong>涂鸦标签属于弱监督学习</strong>，相比像素级标注，能显著减少标注工作量、提高效率，且比点、边界框和图像级标签提供更多关键语义信息。</p><p><strong>现有方法的局限性</strong>：现有涂鸦监督语义分割方法主要依赖<strong>正则化损失、一致性损失、伪建议、辅助任务和标签扩散</strong>等，但这些方法存在一定缺陷。例如，正则化方法常忽略利用高层语义信息，一致性损失未在类别层面提供直接监督，伪标签方法耗时，辅助任务会引入额外数据和预测误差，标签扩散主要依赖局部信息，且许多方法忽略了正确分类像素特征在指导边界区域像素分类中的作用。 基于以上背景，作者提出基于原型的特征增强方法，以解决现有方法的不足，提高涂鸦监督语义分割的性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><p><strong>标注方式</strong>：图像语义分割任务训练通常需大量高质量标注样本，像素级标注耗时耗力，因此弱监督学习方法受关注，如使用涂鸦、点、边界框和图像级标签等。其中，涂鸦监督能提供更多关键语义信息，表现更优。</p></li><li><p><strong>现有方法</strong>：现有涂鸦监督语义分割方法主要依赖正则化损失、一致性损失、伪建议、辅助任务和标签扩散等，但这些方法存在一定局限性。</p></li><li><p><strong>原型方法</strong>：特征原型在计算机视觉任务中用于增强模型识别能力，部分方法在弱监督语义分割中探索了原型的使用，但未充分发挥其特征增强和引导作用。</p></li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-29-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-29-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_19-29-04"></p><ul><li><strong>特征提取</strong>：使用基于Mix Transformer的编码器（Segformer中的MiT-B1）提取初始特征图。</li><li><strong>初始预测</strong>：将特征图输入解码器生成语义分割预测图，通过部分交叉熵损失（partial cross-entropy loss），利用涂鸦标签进行监督，细化预测结果。</li><li><strong>原型提取与更新</strong>：从初始预测图的高置信区域中提取对应特征向量，通过加权平均形成局部原型。在训练迭代中，局部原型动态更新全局原型。</li><li><strong>特征增强</strong>：使用局部和全局原型通过原型特征增强器对初始特征进行增强。</li><li><strong>一致性监督</strong>：将增强后的特征图再次通过解码器生成增强预测图，使用一致性损失（consistency loss）对初始预测图和增强预测图进行约束。</li></ul><h2 id="实验过程（Compared-with-SOTA）"><a href="#实验过程（Compared-with-SOTA）" class="headerlink" title="实验过程（Compared with SOTA）"></a><strong>实验过程（Compared with SOTA）</strong></h2><p>数据集：<strong>PASCAL-Scribble</strong></p><ul><li>选择<strong>MiT-B1</strong>作为骨干网络，与现有方法在<strong>PASCAL VOC 2012</strong>验证集上进行比较。</li><li>与当前最先进的方法TEL相比，尽管MiT-B1骨干网络在全监督数据集上的性能稍弱，但该方法的mIoU仍提高了0.6%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-33-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-33-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_19-33-24"></p><h2 id="实验过程（Ablation-Experiments）"><a href="#实验过程（Ablation-Experiments）" class="headerlink" title="实验过程（Ablation Experiments）"></a><strong>实验过程（Ablation Experiments）</strong></h2><ul><li><strong>各组件有效性</strong>：以仅使用部分交叉熵损失作为基线，对<strong>局部原型增强</strong>和<strong>全局原型增强</strong>方法进行消融实验。结果表明，同时使用两种原型增强时性能最佳，mIoU比基线提高了10.4%。</li><li><strong>原型设置</strong>：实验发现，当每个类别的全局原型数量增加到约5时，mIoU的增加趋于饱和；在原型提取时，k百分比为8%时方法性能较好。</li><li><strong>骨干网络影响</strong>：研究了不同骨干网络对方法的影响，发现基于Transformer的骨干网络在效率和性能上限方面表现更优。使用MiT - B5时，mIoU达到81.5%，显著超过现有方法。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h2><p>作者提出了一种基于原型的特征增强方法用于涂鸦监督语义分割，得出以下结论：</p><ul><li><p>从<strong>涂鸦监督</strong>初始结果的置信部分提取原型，利用这些原型增强初始特征，并根据涂鸦监督的具体情况采用不同原型策略，能以正确分类像素的原型引导错误分类像素的分类，提升预测性能。</p></li><li><p>实验结果表明，该方法在PASCAL VOC 2012数据集上达到了最先进的性能，相比当前最优方法TEL，使用稍弱的骨干网络MiT-B1仍使mIoU提高了0.6%。</p></li><li><p><strong>未来计划将此方法应用于其他任务，以挖掘其巨大潜力和应用价值 （下一个创新点）</strong></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 涂鸦监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semantic Segmentation </tag>
            
            <tag> Feature Augmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation</title>
      <link href="/post/cross-domain-few-shot-semantic-segmentation-via-doubly-matching-transformation/"/>
      <url>/post/cross-domain-few-shot-semantic-segmentation-via-doubly-matching-transformation/</url>
      
        <content type="html"><![CDATA[<p>Nanjing University of Aeronautics and Astronautics 、State Key Laboratory of Integrated Services Networks, Xidian University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><blockquote><p>Cross-Domain Few-shot Semantic Segmentation (CD-FSS) aims to train generalized models that can segment classes from different domains with a few labeled images. Previous works have proven the effectiveness of feature transformation in ad-<br>dressing CD-FSS. However, they completely rely on support images for feature transformation, and repeatedly utilizing a few support images for each class may easily lead to overfitting and overlooking intra-class appearance differences. In this paper,<br>we propose a Doubly Matching Transformation-based Network (DMTNet) to solve the above issue. Instead of completely relying on support images, we propose Self-Matching Transformation (SMT) to construct query-specific transformation matri-<br>ces based on query images themselves to transform domain-specific query features into domain-agnostic ones. Calculating query-specific transformation matrices can prevent overfitting, especially for the meta-testing stage where only one or several images are used as support images to segment hundreds or thousands of images. After obtaining domain-agnostic features, we exploit a Dual Hypercorrelation Construction (DHC) module to explore the hypercorrelations between the query im-<br>age with the foreground and background of the support image, based on which foreground and background prediction maps are generated and supervised, respectively, to enhance the segmentation result. In addition, we propose a Test-time Self-Finetuning (TSF) strategy to more accurately self-tune the query prediction in unseen domains. Extensive experiments on four popular datasets show that DMTNet achieves superior performance over state-of-the-art approaches. Code is available at<br><a href="https://github.com/ChenJiayi68/DMTNet">https://github.com/ChenJiayi68/DMTNet</a>.</p></blockquote><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a><strong>翻译</strong></h2><blockquote><p>**跨域少样本语义分割（CD-FSS）**旨在训练能够以少量标注图像对不同领域进行分割的通用模型。以往的研究已经证明了特征转换在解决CD-FSS问题中的有效性。然而，这些方法完全依赖于支持图像进行特征转换，而重复利用少量支持图像来处理每个类别，容易导致过拟合，并忽视类内外观的差异。为了解决上述问题，本文提出了一种基于双重匹配转换的网络（DMTNet）。我们并不完全依赖支持图像，而是提出了自匹配转换（SMT），通过查询图像自身构建特定的转换矩阵，将领域特定的查询特征转换为领域无关的特征。计算查询特定的转换矩阵有助于防止过拟合，特别是在元测试阶段，此时仅使用一张或几张图像作为支持图像来对数百或数千张图像进行分割。在获得领域无关特征后，我们利用双重超相关构建（DHC）模块，探索查询图像与支持图像前景和背景之间的超相关性，基于此生成前景和背景的预测图并进行监督，从而增强分割结果。此外，我们还提出了一种测试时自我微调（TSF）策略，以更准确地在未知领域自我调整查询预测。在四个流行数据集上的大量实验表明，DMTNet在性能上优于现有的最先进方法。相关代码可在 <a href="https://github.com/ChenJiayi68/DMTNet">https://github.com/ChenJiayi68/DMTNet</a> 获取。</p></blockquote><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><blockquote><p><strong>语义分割</strong>近年来依赖大规模标注数据集取得快速发展，但实际场景中收集大量训练数据耗时且成本高。少样本语义分割（FSS）应运而生，旨在用少量标注支持图像实现查询图像的准确分割，常采用元学习，但在实际应用中，源数据集和目标数据集存在较大领域差距，导致FSS模型对未见领域的泛化能力较差。 为解决FSS模型在跨领域场景下性能显著下降的问题，跨领域少样本语义分割（CD - FSS）被提出。现有主要的CD - FSS方法PATNet通过将特定领域特征转换为领域无关特征来消除领域差距，但在元测试阶段仅基于少量支持图像的转换矩阵为大量查询图像生成领域无关特征，易导致过拟合。此外，多数现有CD - FSS方法在分割过程中只关注前景目标区域，忽略背景区域。 基于上述问题，本文提出一种基于<strong>双重匹配变换的网络（DMTNet）</strong>，以解决特征变换过度依赖支持图像、类内外观差异以及信息利用不充分等关键问题。 </p></blockquote><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><strong>少样本语义分割（FSS）</strong>：现有方法分为基于度量和基于关系两类。前者将支持图像表示为类原型，通过非参数测量工具分割查询图像；后者构建支持 - 查询对的密集对应关系。但在源域和目标域差距大时，性能会下降。</li><li><strong>跨域语义分割</strong>：分为域<strong>自适应语义分割（DASS）<strong>和</strong>域泛化语义分割（DGSS）</strong>。DASS通过联合使用源域和目标域数据训练模型；DGSS通过归一化和白化（NW）、域随机化（DR）等方法缩小域差距。</li><li><strong>跨域少样本语义分割（CD - FSS）</strong>：近期提出了一些方法，如PixDA、RTD、PATNet等，旨在解决少样本和域差距问题。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-05-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-05-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-05-44"></p><p><strong>本文提出了一种基于双重匹配变换的网络（Doubly Matching Transformation-based Network，DMTNet）用于跨域少样本语义分割（Cross-Domain Few-shot Semantic Segmentation，CD-FSS）</strong></p><ol><li><p><strong>自匹配变换模块（Self-Matching Transformation，SMT）</strong></p><ul><li><p><strong>相似性自匹配</strong>：通过查询特征与支持图像的前景和背景原型之间的基于相似性的自匹配，为查询图像生成粗略的分割掩码。将支持特征划分为多个局部特征，通过测量支持局部原型与查询全局特征之间的相似性，生成更细粒度的预测查询掩码，并使用二元交叉熵（BCE）损失函数进行监督。</p></li><li><p><strong>自适应特征变换</strong>：为支持和查询特征分别构建专门的变换矩阵，确保在自适应变换过程中前景对象的不变性。通过求解线性方程得到变换矩阵，同时提出整合支持图像的广义逆来优化查询图像的广义逆。</p></li></ul></li><li><p><strong>双超相关构建模块（Dual Hypercorrelation Construction，DHC）</strong>：探索查询特征与支持图像的前景和背景特征在无域特征空间中的密集相关性。分别基于支持前景特征和背景特征与查询特征构建4D相关张量，将得到的密集相关图输入到4D卷积金字塔编码器和2D卷积金字塔解码器中，生成预测的查询前景掩码和背景掩码，并使用BCE损失函数进行训练监督。</p></li><li><p><strong>测试时自微调策略（Test-time Self-Finetuning，TSF）</strong>：在元测试阶段，通过尝试预测支持图像的真实掩码来微调网络，使模型学习目标域的风格信息，从而为查询图像生成更准确的掩码。只微调编码器的少数参数，避免对支持图像过拟合。</p></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a><strong>实验（Compared with SOTA）</strong></h2><blockquote><p>数据集：PASCAL VOC 2012、ISIC2018、Chest X-ray、 Deepglobe、 and FSS-1000</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-10-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-10-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-10-33"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-14-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-14-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-14-22"></p><p>与迁移学习、少样本语义分割和跨域少样本语义分割的几种先进方法进行比较，<strong>DMTNet在四个数据集的平均结果上表现优异</strong>，在1 - shot设置下达到**59.74%<strong>的平均IoU，在5 - shot设置下达到</strong>66.01%**的平均IoU。与最先进的PATNet相比，在1 - shot和5 - shot设置下分别提高了3.68%和4.02%。</p><h2 id="实验（Ablation-Study）"><a href="#实验（Ablation-Study）" class="headerlink" title="实验（Ablation Study）"></a><strong>实验（Ablation Study）</strong></h2><p>验证了SMT、DHC和TSF三个关键模块的有效性，使用所有三个模块时模型性能最佳，移除任何一个模块都会导致平均性能下降。同时，通过实验确定了TSF策略中微调编码器层能取得最佳性能。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-12-18"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-35.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-35.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-12-35"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h2><p><strong>作者提出用于跨域少样本语义分割的DMTNet，并得出以下结论：</strong></p><ol><li><strong>DMTNet利用SMT模块基于自身原型为支持和查询图像计算变换矩阵，将特定领域特征自适应转换为通用特征，避免过度依赖支持图像导致过拟合。</strong> </li><li><strong>DHC模块在通用特征空间中探索查询图像与支持图像前景和背景的双重超相关性，生成并监督前景和背景预测掩码，提升分割效果。</strong> </li><li><strong>在元测试阶段，TSF策略微调少量参数，使模型学习目标域风格信息，进一步提高分割性能。</strong></li><li><strong>大量实验表明，DMTNet在四个具有不同领域差距的数据集上有效，达到了当前最优性能。</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> 少样本语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Few-Shot Semantic Segmentation </tag>
            
            <tag> Transformation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prompt-and-Transfer Dynamic Class-Aware Enhancement for Few-Shot Segmentation</title>
      <link href="/post/prompt-and-transfer-dynamic-class-aware-enhancement-for-few-shot-segmentation/"/>
      <url>/post/prompt-and-transfer-dynamic-class-aware-enhancement-for-few-shot-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="中科院"><a href="#中科院" class="headerlink" title="中科院"></a>中科院</h2><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a><strong>摘要：</strong></h2><blockquote><p>For more efficient generalization to unseen domains(classes), most Few-shot Segmentation (FSS) would directly exploit pretrained encoders and only fine-tune the decoder, especially in the current era of large models. However, such fixed feature<br>encoders tend to be class-agnostic, inevitably activating objects that are irrelevant to the target class. In contrast, humans can<br>effortlessly focus on specific objects in the line of sight. This paper mimics the visual perception pattern ofhumanbeings and proposes a novel and powerful prompt-driven scheme, called “Prompt and Transfer” (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task. Three key points are elaborated to enhance the prompting: 1) Cross-modal linguistic information is introduced to initialize prompts for each task. 2) Semantic Prompt Transfer (SPT) that precisely transfers the class-specific semantics within the images to prompts. 3) Part Mask Generator(PMG)thatworks in conjunction withSPT to adaptively generate different but complementary part prompts for different individuals. Surprisingly, PAT achieves competitive performance on 4 different tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new state-of-the-arts on 11 benchmarks.</p></blockquote><h2 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a><strong>翻译：</strong></h2><blockquote><p>为了更高效地在未知领域（类别）中进行泛化，大多数少样本分割（FSS）方法通常会直接使用预训练的编码器，并仅微调解码器，尤其是在当前大模型时代。然而，这种固定的编码器通常是类别无关的，往往会错误地激活与目标类别无关的物体。与此不同，人类可以轻松聚焦于视线中的特定物体。本文模仿人类的视觉感知方式，提出了一种新颖且高效的基于提示的方案——“提示与迁移”（PAT）。该方案构建了一种动态的类别感知提示机制，能够调整编码器专注于当前任务中的目标类别（感兴趣的物体）。为了增强提示效果，本文重点介绍了三项关键技术：1）引入跨模态的语言信息来初始化每个任务的提示。2）语义提示迁移（SPT），通过精确地将图像中的类别特定语义迁移到提示中，提升模型的识别能力。3）部分掩码生成器（PMG），与SPT协同工作，为不同个体生成不同但互补的部分提示。令人惊讶的是，PAT在四个任务中表现优异，包括标准FSS、跨域FSS（如计算机视觉、医学、遥感领域）、弱标签FSS和零-shot分割，并在11个基准测试中设立了新的技术标准。</p></blockquote><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><blockquote><p>本文聚焦于**少样本分割（Few-shot Segmentation，FSS）**领域，其研究背景主要源于当前FSS方法存在的局限性以及人类视觉感知模式带来的启示： </p><ol><li><strong>数据驱动方法的局限</strong>：深度学习在计算机视觉任务中取得显著进展，但这些数据驱动的技术在标注数据不足时表现不佳，半监督学习也难以很好地泛化到未见类别。因此，少样本学习（FSL）应运而生，旨在利用少量标注样本快速泛化到未见领域。 </li><li><strong>现有FSS方法的问题</strong>：为了更有效地泛化到未见类别，大多数FSS方法直接使用预训练编码器，仅微调解码器。然而，这种固定参数的特征编码器往往对类别不敏感，会激活与目标类别无关的对象，增加后续解码器分割新类别的负担，且这一问题未得到实质性解决。 </li><li><strong>人类视觉感知的启示</strong>：人类能够以独特的视觉感知模式选择性地关注视线中的关键对象。受此启发，作者认为理想的FSS特征编码器应具有类别感知能力，能够针对不同任务激活相应的类别对象。因此，本文提出了一种基于提示学习的“Prompt and Transfer”（PAT）方法，以动态驱动编码器关注特定对象，实现类别感知增强。</li></ol></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-30-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-30-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-30-17"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><strong>少样本学习（FSL）</strong>：多数方法遵循元学习范式，可分为优化和度量两类，部分方法引入文本信息用于分类。</li><li><strong>少样本分割（FSS）</strong>：主要有原型匹配、特征融合和像素匹配三种方法，多数采用预训练编码器并微调解码器。部分研究开始探索适用于FSS的特征编码器。</li><li><strong>提示学习</strong>：源于自然语言处理，计算机视觉领域尝试引入可学习参数激活语义知识，部分工作探索了与少样本学习的结合</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><ol><li>提出“Prompt and Transfer”（PAT）动态类别感知提示范式，模仿人类视觉感知模式，动态驱动编码器关注特定对象，解决固定编码器类别无关问题。 </li><li>构建三个关键增强点：引入跨模态语言信息初始化提示；设计语义提示转移（SPT）精确转移语义；构建部分掩码生成器（PMG）挖掘细粒度语义提示。 </li><li>在多个任务和基准上取得新的最优性能，且可扩展到跨领域、弱标签和零样本分割等场景。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-34-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-34-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-34-33"></p><h2 id="实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）"><a href="#实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）" class="headerlink" title="实验（compared with the state-of-the-art models and ablation experiments）"></a><strong>实验（compared with the state-of-the-art models and ablation experiments）</strong></h2><ol><li><strong>Comparison with the State-of-the-Arts</strong></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-36-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-36-33"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-50.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-50.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-36-50"></p><ol start="2"><li><h2 id="ablation-experiments"><a href="#ablation-experiments" class="headerlink" title="ablation experiments"></a><strong>ablation experiments</strong></h2></li></ol><blockquote><ol><li><strong>组件分析</strong>：</li></ol><p>   ​         <strong>引入前景提示（FG）</strong>：与基线相比，在PASCAL - 5i和COCO - 20i数据集上分别实现了0.96%和1.61%的平均交并比（mIoU）提升，证明了动态类别感知提示范式对少样本分割（FSS）的有效性。    </p><p>   ​            <strong>结合语义提示转移（SPT）</strong>：在PASCAL - 5i和COCO - 20i数据集上分别实现了2.27%和3.44%的mIoU提升，表明从支持和查询图像中提取特定类别的线索能显著增强类别感知能力，使编码器更精准地聚焦于目标对象。    </p><p>   ​          <strong>结合部分掩码生成器（PMG）</strong>：在PASCAL - 5i和COCO - 20i数据集上分别实现了3.38%和4.78%的mIoU提升，说明挖掘细粒度的部分语义能进一步发挥提示的作用，实现更精确的分割。    </p><p>   ​        <strong>引入背景提示（BG）并结合SPT</strong>：在PASCAL - 5i和COCO - 20i数据集上分别实现了4.36%和5.91%的mIoU提升，体现了背景语义对分割的重要性。 </p><ol start="2"><li><p><strong>提示初始化消融实验</strong>：    - <strong>随机初始化提示</strong>：在没有SPT和PMG持续增强其类别感知能力的情况下，使用随机初始化的提示来调整编码器不会带来性能提升。    - <strong>引入额外类别语义</strong>：将额外的类别语义作为初始提示可以调整编码器，使其初步定位目标，提高分割精度。    - <strong>语言信息的优势</strong>：语言信息比支持平均令牌更具类别代表性，使用CLIP提取的语言信息作为初始提示具有最优的分割结果。 </p></li><li><p><strong>提示增强消融实验</strong>：    - <strong>部分掩码生成器（PMG）</strong>：        - <strong>定性评估</strong>：可视化结果显示，PMG可以将目标对象清晰地划分为不同的互补部分区域，证明其能够自适应地生成不同的部分掩码。        - <strong>部分掩码数量影响</strong>：当部分掩码数量（Np）从1增加到8时，mIoU精度随之增加；继续增加数量，mIoU精度反而下降，过多的部分掩码可能导致目标对象无法清晰划分，产生冗余和噪声。    - <strong>语义提示转移（SPT）</strong>：        - <strong>支持和查询语义的重要性</strong>：仅将支持图像或查询图像的目标语义转移到提示中会导致不同程度的性能下降，说明两者对于查询图像的分割都至关重要。        - <strong>高斯抑制的作用</strong>：SPT中的高斯抑制通过调整注意力分布，使特定区域的全局语义更好地聚合到提示中，从而提高分割精度。    - <strong>提示增强次数</strong>：在变压器编码器的最后L个块中进行提示增强，更多的语义迁移次数通常能带来更高的分割精度，但综合效率和性能考虑，选择3次（L &#x3D; 3）较为合适。</p></li><li><p><strong>骨干网络设置消融实验</strong>：    - <strong>不同骨干网络的性能</strong>：使用DeiT - B&#x2F;16骨干网络在所有设置下具有最佳的分割精度，DeiT - S&#x2F;16次之，较小的ViT - S&#x2F;16或DeiT - T&#x2F;16虽然分割性能较差，但推理速度具有竞争力。    - <strong>块数量的影响</strong>：不同骨干网络中，块的数量并非越多越好，例如两种ViT变体在10个块时效果最佳，三种DeiT变体在11个块时效果最佳。 </p></li><li><p><strong>使用一致编码器的比较</strong>：在使用一致的特征编码器设置下，PAT取得了最佳的分割性能；盲目使用变压器提取特征可能无法带来预期的性能提升；引入额外的语言信息有助于生成更强大的提示，以生成类别感知特征。</p></li><li><p><strong>PAT与其他FSS方法的结合</strong>：使用更复杂的解码器不一定能优于简单的相似度计算；配备PAT提出的动态类别感知编码器后，其他FSS方法的性能有显著提升，表明该编码器能灵活地为不同的新类别生成类别感知特征，并且与基于解码器的方法具有良好的兼容性。</p></li></ol></blockquote><h2 id="研究结论"><a href="#研究结论" class="headerlink" title="研究结论"></a><strong>研究结论</strong></h2><blockquote><p>作者在摒弃以往冻结编码器以泛化到未见类别的小样本分割（FSS）做法后，提出了一种新颖的动态类别感知提示范式（PAT）。该范式模仿人类视觉感知模式，用于调整编码器以聚焦不同FSS任务中的特定对象。实验结果表明，PAT在三个流行的FSS基准测试中创造了新的最优性能。令人惊喜的是，当将其扩展到<strong>跨领域、弱标签甚至零样本</strong>等更现实的场景时，也取得了令人满意的结果。作者希望这项工作能为小样本场景下的编码器设计提供新视角，并激发未来相关研究聚焦于此。 </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 少样本语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Prompt-and-Transfer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prompting Multi-Modal Image Segmentation with Semantic Grouping</title>
      <link href="/post/prompting-multi-modal-image-segmentation-with-semantic-grouping/"/>
      <url>/post/prompting-multi-modal-image-segmentation-with-semantic-grouping/</url>
      
        <content type="html"><![CDATA[<h2 id="University-of-Chinese-Academy-of-Sciences"><a href="#University-of-Chinese-Academy-of-Sciences" class="headerlink" title="University of Chinese Academy of Sciences"></a>University of Chinese Academy of Sciences</h2><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a><strong>摘要：</strong></h2><p>Multi-modal image segmentation is one of the core issues in computer vision. The main challenge lies in integrating common information between modalities while retaining specific patterns for each modality. Existing methods typically perform full fine-tuning on RGB-based pre-trained parameters to inherit the powerful representation of the foundation model. Although effective, such paradigm is not optimal due to weak transferability and scarce downstream data. Inspired by the recent success of prompt learning in language models, we propose the Grouping Prompt Tuning Framework(GoPT), which introduces explicit semantic grouping to learn modal-related prompts, adapting the frozen pre-trained foundation model to various downstream multi-modal segmentation tasks. Specifically, a class-aware uni-modal prompter is designed to balance intra- and inter-modal semantic propaga-<br>tion by grouping modality-specific class tokens, thereby improving the adaptability of spatial information. Furthermore,<br>an alignment-induced cross-modal prompter is introduced to aggregate class-aware representations and share prompt parameters among different modalities to assist in modeling common statistics. Extensive experiments show the superiority of our GoPT, which achieves SOTA performance on various downstream multi-modal image segmentation tasks by training only &lt; 1% model parameters.</p><h2 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a><strong>翻译：</strong></h2><p>多模态图像分割技术是计算机视觉领域的关键挑战。这项技术的核心难点在于如何有效融合不同模态（如图像、红外等）的共性特征，同时保留各模态独有的特征模式。当前主流方法主要通过对基于可见光（RGB）预训练模型进行全局微调，以继承基础模型的强大特征提取能力。但这种方法存在明显局限：一方面模型可迁移性较弱，另一方面下游任务标注数据往往匮乏。受大语言模型中提示学习方法取得突破的启发，我们研发了分组提示调优框架（GoPT），通过语义分组机制学习模态专属提示，使冻结的预训练模型能灵活适配多种多模态分割任务。该框架包含两大创新模块：首先是<strong>类感知单模态提示器</strong>，通过聚类同类模态特征，在保留模态内独特空间信息的同时，促进跨模态语义对齐；其次是<strong>对齐引导的跨模态提示器</strong>，通过共享提示参数聚合不同模态的类特征表示，有效捕捉多模态数据的共有统计规律。实验数据显示，GoPT 仅需微调模型不足 1% 的参数，就在多个多模态图像分割基准任务中刷新了最高性能记录，展现出显著优势。</p><h2 id="研究背景："><a href="#研究背景：" class="headerlink" title="研究背景："></a><strong>研究背景：</strong></h2><ol><li><strong>多模态融合的重要性</strong>：语义分割旨在为场景中每个像素分配语义类别，随着传感器技术发展，多模态融合用于分割成为图像解释核心问题。深度学习推动下，深度多模态融合展现出比单模态分割更显著的优势。 </li><li><strong>现存方法的挑战</strong>：现有多模态分割方法主要分为基于对齐和基于聚合的融合，但面临诸多挑战。一方面，不同成像机制的模态存在异质差距，基于对齐的融合因信息交换弱，常提供无效的跨模态融合；另一方面，不同模态有效信息不同，基于聚合的融合易忽略模态内传播，导致模态间知识共享和模态内信息处理失衡。 </li><li><strong>全微调方法的局限</strong>：多模态方法常采用基于RGB的预训练分割器，全微调虽有效，但效率低、参数存储负担大，且因样本标注有限，无法充分利用预训练模型知识获得通用表示。</li></ol><h2 id="研究现状："><a href="#研究现状：" class="headerlink" title="研究现状："></a><strong>研究现状：</strong></h2><ul><li><strong>多模态图像分割主流方法</strong>：以深度多模态融合为主，旨在利用多数据源增强细粒度细节和像素级语义，主要分为基于对齐和基于聚合的融合方法。前者通过条件损失对齐子网络嵌入，后者运用特定算子组合多模态子网络。</li><li><strong>模型训练方式</strong>：因缺乏大规模多模态训练集，现有方法通常先加载基于RGB的预训练模型参数，再在特定下游任务数据集上微调。</li><li><strong>视觉提示学习应用</strong>：提示调优作为新范式，在自然语言处理中表现出色，近期也开始应用于视觉任务。</li></ul><h2 id="提出的模型："><a href="#提出的模型：" class="headerlink" title="提出的模型："></a><strong>提出的模型：</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_14-56-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_14-56-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-06_14-56-04"></p><p>本文提出了<strong>分组提示调优框架（Grouping Prompt Tuning Framework，GoPT）</strong>，用于<strong>多模态图像分割</strong>任务，以下是该模型的详细介绍：</p><ol><li><strong>核心思想</strong>：引入显式语义分组机制到提示学习中，通过冻结预训练的基础模型，仅微调少量视觉提示参数，使模型适应各种下游多模态分割任务，以解决现有方法在整合模态间信息和保留各模态特定模式方面的挑战。 </li><li><strong>整体架构</strong>    <ul><li><strong>输入处理</strong>：将RGB图像和辅助模态图像输入到补丁嵌入层，生成对应的RGB标记和辅助模态标记。    </li><li><strong>提示生成</strong>：把标记送入分组提示器，生成特定于模态的提示。    </li><li><strong>特征融合</strong>：将学习到的提示作为残差添加到原始RGB流中，再输入到基础模型的下一层。</li></ul></li><li><strong>主要组件</strong>    <ul><li><strong>类感知单模态提示器（Class-Aware Uni-Modal Prompter，CUP）</strong>：通过引入特定于模态的类标记，对辅助模态的视觉概念进行分层渐进分组，平衡模态内和模态间的语义传播，提高空间信息的适应性。    </li><li><strong>对齐诱导跨模态提示器（Alignment-Induced Cross-Modal Prompter，ACP）</strong>：根据显式分组的语义相似性，聚合辅助模态的类感知表示，将其他数据源的关键模式整合到RGB流中，生成新的跨模态对齐诱导提示，辅助建模模态公共统计信息。</li></ul></li><li><strong>优化策略</strong>：使用基于RGB的预训练基础模型的参数初始化多模态分割模型，在提示调优过程中，仅更新分组提示器和分割头的梯度值，以少量提示参数促进模型快速收敛，并有效继承预训练基础模型的先验知识。 </li><li><strong>实验验证</strong>：在多个下游多模态图像分割任务（RGB - D、RGB - T、RGB - SAR分割）上进行了广泛实验，结果表明GoPT仅训练不到1%的模型参数，就能在各项指标上取得优于现有方法的性能，展现出高效性和优越性。</li></ol><h2 id="实验过程（与SOTA方法的对比）："><a href="#实验过程（与SOTA方法的对比）：" class="headerlink" title="实验过程（与SOTA方法的对比）："></a><strong>实验过程（与SOTA方法的对比）：</strong></h2><p>数据集：NYUDv2、SUN RGB-D、MFNet、PST900、WHU-OS</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-02-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-02-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-06_15-02-29"></p><h2 id="实验过程（消融实验）："><a href="#实验过程（消融实验）：" class="headerlink" title="实验过程（消融实验）："></a><strong>实验过程（消融实验）：</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-05-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-05-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-06_15-05-05"></p><p>通过消融实验，证明了GoPT已经达到了SOTA的标准</p><ul><li>Effectiveness of Prompter Structure  （<strong>table 4，row 7 and row 8</strong>）</li><li>Impact of Multi-Modal Information   （<strong>table 4</strong>）</li><li>Number of Grouping Prompter          <strong>（Figure 6）</strong></li><li>Hard vs. Soft Assignment                <strong>（Figure 7）</strong></li></ul><p><strong>结论：</strong></p><p>作者提出了用于<strong>多模态图像分割</strong>的参数高效视觉调优框架GoPT，通过在提示学习中引入显式语义分组，使冻结的预训练基础模型适应各种下游多模态分割任务。具体而言，设计了类感知单模态提示器（CUP），通过对特定模态的类令牌进行分组，平衡了模态内和模态间的语义传播；引入了对齐诱导的跨模态提示器（ACP），聚合类感知表示并辅助建模公共统计信息。大量下游任务实验表明，GoPT在准确性和效率上达到了最佳平衡，仅训练不到1%的模型参数，就在多个下游多模态图像分割任务中取得了SOTA性能，证明了该框架的优越性和泛化性。 </p>]]></content>
      
      
      <categories>
          
          <category> 多模态图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semantic Grouping </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Disentangle then Parse Night-time Semantic Segmentation with Illumination Disentanglement</title>
      <link href="/post/disentangle-then-parse-night-time-semantic-segmentation-with-illumination-disentanglement/"/>
      <url>/post/disentangle-then-parse-night-time-semantic-segmentation-with-illumination-disentanglement/</url>
      
        <content type="html"><![CDATA[<p>​University of Science and Technology of China               Shanghai AI Laboratory</p><p><strong>摘要：</strong></p><p>Most prior semantic segmentation methods have been developed for day-time scenes, while typically underperforming in night-time scenes due to insufficient and complicated lighting conditions. In this work, we tackle this challenge by proposing a novel night-time semantic segmentation paradigm, i.e., disentangle then parse (DTP). DTP explicitly disentangles night-time images into light-invariant reflectance and light-specific illumination components and then recognizes semantics based on their adaptive fusion. Concretely, the proposed DTP comprises two key components: 1) Instead of processing lighting-entangled features as in prior works, our Semantic-Oriented Disentanglement (SOD) framework enables the extraction of re-<br>flectance component without being impeded by lighting, allowing the network to consistently recognize the semantics under cover of varying and complicated lighting conditions. 2) Based on the observation that the illumination component can serve as a cue for some semantically confused regions, we further introduce an Illumination-Aware Parser (IAParser) to explicitly learn the correlation between semantics and lighting, and aggregate the illumination features to yield more precise predictions. Extensive experiments on the night-time segmentation task with various settings demonstrate that DTP significantly outperforms state-of-the-art methods. Furthermore, with negligible additional parameters, DTP can be directly used to benefit existing day-time methods for night-time segmentation. Code and dataset are available at <a href="https://github.com/w1oves/DTP.git">https://github.com/w1oves/DTP.git</a>.</p><p><strong>翻译：</strong></p><p>大多数现有的语义分割方法都是为白天场景设计的，因此在夜间场景中往往表现不佳，主要是因为夜间的光照条件复杂且不足。为了解决这个问题，本文提出了一种全新的夜间语义分割方法——“先解耦再解析”（DTP）。DTP方法首先将夜间图像分解为不受光照影响的<strong>反射成分</strong>和与光照相关的<strong>光照成分</strong>，然后通过自适应融合这两部分信息来进行语义识别。具体来说，DTP包含两个关键技术：1）与以往方法将光照信息与特征混合的做法不同，我们提出的“语义导向解耦”（SOD）框架能够在不受光照影响的情况下提取反射成分，这样可以帮助网络在复杂多变的光照条件下持续准确地识别语义。2）通过观察到光照成分可以作为一些语义模糊区域的线索，我们引入了“光照感知解析器”（IAParser），该模块能够学习语义与光照之间的关系，并聚合光照特征，从而提高预测的精度。在各种夜间分割任务的实验中，DTP显著超越了现有的先进方法。而且，DTP几乎不增加额外的参数，能够直接用于现有的白天分割方法，提升其在夜间的分割效果。相关代码和数据集可以在<a href="https://github.com/w1oves/DTP.git%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/w1oves/DTP.git下载。</a></p><p><strong>研究背景：</strong></p><blockquote><p>大多现存的语义分割方法都是基于白天场景开发的，这些场景光照充足且均匀。然而在实际应用中，视觉系统近一半时间需在光照不足且复杂的夜间环境下工作，现有白天方法在夜间性能会下降。此前采用无监督域适应技术将白天知识迁移到夜间，但因夜间缺乏对应标签，分割性能提升有限。虽有NightCity等大规模夜间数据集及相关方法提出，一定程度上改善了夜间场景表现，但它们通常基于光照纠缠的表示进行场景解析，不适合夜间复杂的光照条件。</p></blockquote><blockquote><p>夜间场景光照强度低且人工光源复杂，导致物体外观随光照变化，使光不变反射率和光特定光照之间的纠缠加剧，难以提取用于语义分割的判别特征。</p></blockquote><p><strong>研究现状：</strong></p><ol><li><strong>语义分割</strong>：基于全卷积网络（FCN）结合编码器 - 解码器架构的方法成为主流，如DeepLab系列引入空洞空间金字塔池化（ASPP），还有基于自注意力机制和Transformer的网络被应用，但大多聚焦白天场景。</li><li><strong>夜间语义分割</strong>：早期因缺乏大规模标注数据集，采用无监督域适应技术将白天知识迁移到夜间；近期有基于NightCity数据集的方法，如EGNet、NightLab等，但这些方法未明确估计光照对语义的影响，而是学习内容和光照的纠缠表示。</li><li><strong>深度表示解耦</strong>：此前探索了多种图像表示解纠缠方法，如在GAN框架中学习跨域不变表示。</li></ol><p><strong>创新点：</strong></p><ol><li>提出“先分离再解析”（DTP）的夜间语义分割范式，可提升现有日间方法在夜间的性能。</li><li>设计语义导向解纠缠框架（SOD），借助语义约束分离图像，使网络在不同光照下提取一致特征。 </li><li>引入光照感知解析器（IAParser），利用光照组件作为线索，实现更精准预测。 </li><li>细化NightCity数据集，提出NightCity-fine，为夜间分割提供更可靠基准。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-03_15-54-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-03_15-54-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-03_15-54-06"></p><p>实验：</p><ol><li><strong>数据集</strong></li></ol><ul><li><p><strong>NightCity - fine</strong>：原始的NightCity是最大的夜间语义分割数据集，但存在标注错误。作者提出了NightCity - fine，对训练集和验证集中不合理的标注进行了仔细修改，共修正了2554个标签图。    </p></li><li><p><strong>Cityscapes</strong>：这是一个自动驾驶数据集，在白天场景下从50个不同城市采集，包含2975张训练图像和500张验证图像，具有19个语义类别，图像分辨率为2048x1024。</p></li><li><p><strong>BDD100K</strong>：使用其子集BDD100K - night进行补充实验，该子集包含314张夜间训练图像和31张验证图像，其互补数据集为BDD100K - day。 </p></li><li><p><strong>与SOTA方法的对比实验</strong></p></li></ul><p>使用的数据集：<strong>NightCity, NightCity-fine,  and BDD100K-night</strong></p><ul><li><strong>消融实验</strong></li></ul><p><strong>结论：</strong></p><p>作者提出了一种新颖的夜间语义分割范式——Disentangle then Parse（DTP），并得出以下结论： </p><ol><li><strong>方法优势</strong>：提出语义导向的解纠缠（SOD）框架，使分割不受复杂光照干扰；引入光照感知解析器（IAParser），利用光照中的语义线索实现更精确预测。DTP可作为即插即用范式，以极少额外参数助力现有方法提升性能。</li><li><strong>数据集贡献</strong>：对最大的夜间分割数据集NightCity进行细化，提出NightCity - fine，用于更有效的训练和验证评估。</li><li><strong>性能验证</strong>：大量实验表明，DTP显著优于现有技术，与NightCity - fine一起为夜间分割提供了更优的基准。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 夜间语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Illumination Disentanglement </tag>
            
            <tag> Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SED A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation</title>
      <link href="/post/sed-a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/"/>
      <url>/post/sed-a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="天津大学，重庆大学等"><a href="#天津大学，重庆大学等" class="headerlink" title="天津大学，重庆大学等"></a><strong>天津大学，重庆大学等</strong></h2><blockquote><p>摘要：Open-vocabulary semantic segmentation strives to distinguish pixels into different semantic groups from an open<br>set of categories. Most existing methods explore utilizing pre-trained vision-language models, in which the key is to<br>adapt the image-level model for pixel-level segmentation task. In this paper, we propose a simple encoder-decoder,<br>named SED, for open-vocabulary semantic segmentation, which comprises a hierarchical encoder-based cost map generation and a gradual fusion decoder with category early rejection. The hierarchical encoder-based cost map generation employs hierarchical backbone, instead ofplain transformer, to predict pixel-level image-text cost map.<br>Compared to plain transformer, hierarchical backbone better captures local spatial information and has linear computational complexity with respect to input size. Our gradual fusion decoder employs a top-down structure to com-<br>bine cost map and the feature maps of different backbone levels for segmentation. To accelerate inference speed, we<br>introduce a category early rejection scheme in the decoder that rejects many no-existing categories at the early layer<br>ofdecoder, resulting in at most 4.7 times acceleration without accuracy degradation. Experiments are performed on<br>multiple open-vocabulary semantic segmentation datasets, which demonstrates the efficacy ofour SED method. When<br>using ConvNeXt-B, our SED method achieves mIoU score of 31.6% on ADE20K with 150 categories at 82 millisecond (ms) per image on a single A6000. Our source code is available at <a href="https://github.com/xb534/SED">https://github.com/xb534/SED</a></p></blockquote><blockquote><p>翻译：开放词汇语义分割旨在将像素划分到一个开放类别集中的不同语义组。大多数现有的方法尝试使用预训练的视觉-语言模型，关键是将图像级的模型适应为像素级的分割任务。本文提出了一种简单的编码器-解码器架构，称为SED，用于开放词汇语义分割。SED包含两部分：基于层次编码器的成本图生成和逐渐融合的解码器，并且具有类别早期拒绝的功能。基于层次编码器的成本图生成使用层次化的骨干网络，而不是传统的Transformer，来预测像素级的图像-文本成本图。与普通的Transformer相比，层次化骨干网络能够更好地捕捉局部空间信息，并且计算复杂度与输入的大小成线性关系。我们的逐渐融合解码器采用自上而下的结构，将成本图与不同层级骨干网络的特征图进行融合，完成分割任务。为了提高推理速度，我们在解码器中引入了类别早期拒绝方案，通过在解码器的早期层次中剔除不存在的类别，从而在不牺牲精度的情况下，实现最多4.7倍的加速。我们在多个开放词汇语义分割数据集上进行了实验，验证了SED方法的有效性。当使用ConvNeXt-B时，SED方法在ADE20K数据集上，针对150个类别的mIoU得分为31.6%，每张图像的推理时间为82毫秒（ms），运行于单个A6000显卡。我们的源代码可以通过<a href="https://github.com/xb534/SED%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/xb534/SED下载。</a></p></blockquote><p><strong>研究背景：</strong> <strong>传统的方法只能分割训练集的种类，不能识别出来在训练集中没有的未知场景</strong>，同时两阶段和单阶段的方法都存在不足。两阶段的框架存在不足：计算效率低，没有充分利用上下文信息；单阶段的框架存在不足：对于低分辨率的输入，主干网络对空间信息变得不敏感，即使加入额外的网络来提供空间信息，也会增加计算资源，分割种类的增加也会增加计算资源。</p><p><strong>研究现状：</strong></p><ul><li><strong>语义分割方法</strong>：传统语义分割方法主要有基于FCN和基于Transformer的方法。前者通过融合深浅层特征、利用空间金字塔网络或注意力模块等提取上下文信息；后者将Transformer用作骨干网络或分割解码器。</li><li><strong>视觉-语言模型</strong>：早期基于预训练的视觉和语言模型开发，后如CLIP从大规模图像-文本对数据中学习视觉特征，ALIGN从噪声图像-文本数据集中学习，在零样本任务上表现出色。</li><li><strong>开放词汇语义分割</strong>：早期通过学习特征映射对齐视觉和文本特征，CLIP成功后，出现基于两阶段和单阶段框架的方法。两阶段先生成掩码提案再分类，单阶段直接扩展视觉-语言模型进行分割。</li></ul><p><strong>研究方法：</strong></p><ul><li><p><strong>Hierarchical Encoder-based Cost Map（基于分层编码器的代价图）</strong></p></li><li><p><strong>Gradual Fusion Decoder（渐进式融合编码器）</strong></p></li><li><p><strong>Category Early Rejection（类别早期拒接）</strong></p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-53-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-53-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_08-53-28"></p><p><strong>实验设定（对比实验+消融实验）</strong></p><blockquote><p>训练集：COCO-Stuff 的训练集，包含大约 118k 密集标注的 171 类目标。<br>测试集：跨数据集测试。<br>ADE20K，包含 20K training 和 2K validation &#x3D;&gt; A-150 和 A-847。<br>PASCAL VOC，包含 1.5k training 和 1.5k validation &#x3D;&gt; PAS-20。<br>PASCAL-Context 来自原始的 PASCAL VOC 数据集 &#x3D;&gt; PC-59 和 PC-459。</p></blockquote><blockquote><p>模型设定：<br>基于 ConvNeXt-B&#x2F;L 视觉编码器形式的预训练 CLIP。<br>类别模板数量 P PP 同 CAT-Seg 一致，均为 80。<br>文本编码器冻结，只训练图像编码器和解码器。</p></blockquote><blockquote><p>GPU: 4xA6000<br>图像编码器学习率多乘以一个 0.01 倍的因子。<br>共 80k 次迭代。<br>训练时剪裁图像 768 × 768 大小，测试时直接放缩图像到 768 × 768大小。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-08.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-08.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_08-59-08"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_08-59-43"></p><p><strong>实验结果：本文的SED方法都表现出较好的效果，缩短了推理时间</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_09-00-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_09-00-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_09-00-24"></p><p><strong>结论与不足</strong></p><p>作者提出了用于<strong>开放词汇语义分割</strong>的SED方法，得出以下结论：</p><ol><li><strong>方法构成</strong>：SED由基于分层编码器的代价图生成和带有类别早期拒绝的渐进式融合解码器组成。先利用分层编码器生成像素级图像 - 文本代价图，再基于此和分层编码器的不同特征图，用渐进式融合解码器生成高分辨率特征图进行分割。</li><li><strong>速度提升</strong>：在解码器中引入类别早期拒绝方案，能提前拒绝不存在的类别，有效提升推理速度。</li><li><strong>效果验证</strong>：在多个数据集（ADE20K、PASCAL VOC、PASCA-Context）上的实验表明，SED在准确性和速度方面均有效。不过，模型在识别近义词类别时存在困难，未来将探索设计类别注意力策略或使用大规模细粒度数据集来解决该挑战。</li></ol><p><strong>不足：</strong></p><p><strong>类别识别局限</strong>：模型有时难以区分近义词类别，在对语义相近的类别进行分类和分割时存在困难，影响了对复杂场景的理解和处理能力</p>]]></content>
      
      
      <categories>
          
          <category> 开放词汇语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SED </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>High Quality Segmentation for Ultra High-resolution Images</title>
      <link href="/post/high-quality-segmentation-for-ultra-high-resolution-images/"/>
      <url>/post/high-quality-segmentation-for-ultra-high-resolution-images/</url>
      
        <content type="html"><![CDATA[<h1 id="香港中文大学-Adobe-等"><a href="#香港中文大学-Adobe-等" class="headerlink" title="香港中文大学  Adobe 等"></a>香港中文大学  Adobe 等</h1><p>**摘要：**To segment 4K or 6K ultra high-resolution images needs extra computation consideration in image segmentation. Common strategies, such as down-sampling, patch crop- ping, and cascade model, cannot address well the balance issue between accuracy and computation cost. Motivated by the fact that humans distinguish among objects continu- ously from coarse to precise levels, we propose the Contin- uous Refinement Model (CRM) for the ultra high-resolution segmentation refinement task. CRM continuously aligns the feature map with the refinement target and aggregates fea- tures to reconstruct these image details. Besides, our CRM shows its significant generalization ability to fill the resolu- tion gap between low-resolution training images and ultra high-resolution testing ones. We present quantitative per- formance evaluation and visualization to show that our pro- posed method is fast and effective on image segmentation refinement. Code is available at <a href="https://github.com/dvlab-research/Entity/tree/main/CRM">https://github.com/dvlab-research/Entity/tree/main/CRM</a>.</p><p><strong>翻译:</strong> 对4K或6K超高分辨率图像进行分割时，需要额外的计算资源。常见的策略，如下采样、图像裁剪和级联模型，往往难以很好地平衡准确性和计算成本。基于人类从粗略到精细地逐步区分物体的方式，我们提出了“连续细化模型”（CRM）来进行超高分辨率图像的分割细化任务。CRM通过不断地对齐特征图和细化目标，逐步聚合特征，重建图像细节。更重要的是，CRM展现出了很强的泛化能力，能够有效弥补低分辨率训练图像与超高分辨率测试图像之间的分辨率差距。我们通过定量的性能评估和可视化结果，展示了该方法在图像分割细化上的高效性和快速性。相关代码可以在<a href="https://github.com/dvlab-research/Entity/tree/main/CRM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dvlab-research/Entity/tree/main/CRM找到。</a></p><p><strong>研究背景</strong></p><p>随着相机和显示设备的快速发展，图像分辨率越来越高，4K和6K分辨率变得常见，这在人像照片后期处理、工业缺陷检测、医学诊断等领域带来了新机遇。然而，超高分辨率图像也给经典图像分割方法带来了挑战：</p><ol><li><strong>计算成本高</strong>：大量的输入像素在计算上代价高昂，且对GPU内存需求大。 </li><li><strong>细节重建难</strong>：大多数现有方法通过插值对最终预测进行4到8倍上采样，无法在输出掩码上构建细粒度细节。 以往的分割细化方法，如针对1K - 2K分辨率图像的方法，存在图形模型依赖低级别颜色边界、基于传播的方法面临计算和内存限制、大模型易过拟合而浅细化网络细化能力有限等问题。而处理超高分辨率细化的级联解码器方法，虽能取得较好性能，但在推理时需要下采样和裁剪补丁，增加了成本、丢失了细节并破坏了全局上下文。 因此，为解决超高分辨率图像分割中精度与计算成本的平衡问题，作者提出了连续细化模型（CRM），以实现高效、精确的图像分割细化。</li></ol><p><strong>研究现状</strong></p><ol><li><strong>语义分割</strong>：FCN 引入深度卷积网络，PSPNet、DeepLab 系列等方法不断发展，输出步长多设为 4×或 8×，但直接插值预测结果存在边缘锯齿和细节缺失问题。</li><li><strong>分割细化</strong>：针对 1K 分辨率图像的细化技术能提升分割质量，但存在图形模型依赖低层次颜色边界、传播方法有计算和内存限制、模型易过拟合或细化能力有限等问题。对于 4K - 6K 超高清图像，级联解码器方法能取得较好效果，但结构复杂。</li><li><strong>隐式函数表示</strong>：在神经网络中用于表示对象或场景，如 NeRF、PixelNerf 等，其多视图一致性和光滑性有利于分割。</li></ol><p><strong>研究方法</strong></p><ol><li>提出<strong>连续细化模型（CRM）</strong>，引入<strong>隐函数</strong>，利用连续位置信息和特征对齐，有效降低计算成本，重建更多细节。 </li><li>CRM采用多<strong>分辨率推理策略</strong>，适用于<strong>低分辨率训练和超高分辨率测试</strong>，总推理时间不到CascadePSP的一半。 </li><li>实验表明，CRM在超高分辨率图像上取得最佳分割结果，还能提升现有全景分割模型性能，无需微调。</li></ol><p><strong>实验步骤</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-02_14-24-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-02_14-24-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-02_14-24-58"></p><ol><li><strong>数据集和对比方法选择</strong>    - <strong>训练数据集</strong>：遵循CascadePSP的设置，将MSRA - 10K、DUT - OMRON、EC - SSD和FSS - 1000合并为训练数据集，包含36,572张具有超过1000个语义类别的图像。    - <strong>测试数据集</strong>：使用CascadePSP提出的高分辨率图像分割数据集BIG（图像分辨率范围2K - 6K）进行超高清图像评估；还在重新标注的PASCAL VOC 2012数据集上进行评估；为证明模型的通用性，将CRM作为全景分割和实体分割的扩展进行评估。    - <strong>对比方法</strong>：选择CascadePSP作为超高清图像的主要对比方法；选择MGMatting作为掩码引导抠图方法，Segfix作为高分辨率分割细化方法；选择PanopticFCN和Entity Segmentor作为全景和实体分割的基准方法。 </li><li><strong>实现细节</strong>    - <strong>模型实现</strong>：使用PyTorch实现模型，使用去掉conv5 x的ResNet - 50作为编码器Eθ。    - <strong>训练设置</strong>：使用Adam优化器，学习率为2.25×10⁻⁴，在22,500和37,500步时将学习率降至十分之一，总步数45,000步。训练输入是从原始图像及其对应的扰动掩码中裁剪的224×224的图像块，扰动掩码是在真实掩码上随机扰动得到，随机IoU阈值在0.8 - 1.0之间。    - <strong>评估设置</strong>：在实验中从连续范围中选择4个缩放比例进行细化，CRM的总推理时间仍不到CascadePSP的一半。</li><li>定量结果评估**    - <strong>对比实验</strong>：在BIG数据集上对比CRM、CascadePSP、Segfix和MGMatting的性能，包括交并比（IoU）、平均边界准确率（mBA）、全景质量（PQ）和平均精度（AP）等指标。结果表明CRM性能更好，在高分辨率图像上运行速度更快，且FLOPs和参数更少。    - <strong>扩展实验</strong>：在全景分割和实体分割实验中，将CRM添加到PanopticFCN和EntitySeg后，它们的分割性能得到增强；在重新标注的Pascal VOC 2012数据集上，CRM比Segfix表现更好，与CascadePSP的IoU相当，但更注重细节</li><li><strong>定性结果展示</strong>    - <strong>可视化对比</strong>：展示CascadePSP、Segfix和CRM的细化结果对比，CRM的细化结果包含更多细节，仅使用语义分割标注训练就能生成类似抠图的结果，并且能更好地重建粗掩码中缺失的部分。    - <strong>应用示例</strong>：展示将CRM应用于全景分割的可视化结果，掩码细节和整体分割效果都有显著改善。</li><li><strong>消融实验</strong>    - <strong>CRM和推理分辨率</strong>：分析CRM和隐式函数对不同分辨率下性能的影响，CRM在低分辨率下能细化出较好的通用掩码，随着分辨率增加，能生成更多细节，mBA提高。    - <strong>CAM和隐式函数</strong>：验证CAM和隐式函数都是CRM不可或缺的部分，二者协同作用能提升性能。    - <strong>推理连续性的影响</strong>：性能随着采样的缩放比例数量增加而提升，更多的缩放比例意味着推理分辨率的连续性更好，有助于提高性能直至收敛。</li></ol><p><strong>结论：</strong></p><p>作者提出了用于超高清图像分割细化的连续细化模型（CRM），并得出以下结论：</p><ol><li><strong>性能优势</strong>：CRM 能连续对齐特征图与细化目标，有助于聚合特征以重建高分辨率掩码上的细节，在性能和速度方面表现出色，实验表明其在超高清图像上的分割效果最佳，还能提升现有全景分割模型的性能。</li><li><strong>泛化能力</strong>：CRM 具有显著的泛化潜力，可处理低分辨率训练和超高清测试之间的分辨率差距，即使从低分辨率细化到高分辨率，总推理时间也不到 CascadePSP 的一半。 </li><li><strong>未来展望</strong>：目前采用“低分辨率训练和超高清测试”的配置，使用超高清图像进行训练和测试仍耗资源，解决该问题将是未来工作方向。</li></ol><p><strong>不足：</strong></p><ol><li><strong>训练资源限制</strong>：目前采用“低分辨率训练和超高分辨率测试”的配置，使用超高分辨率图像进行训练和测试仍面临资源消耗大的问题，这限制了模型在实际应用中对超高分辨率数据的处理能力。</li><li><strong>未来工作待明确</strong>：如使用预训练或低分辨率训练测试。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 超高分辨率图像语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ultra High-resolution Images </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Segment Anything</title>
      <link href="/post/segament-anything/"/>
      <url>/post/segament-anything/</url>
      
        <content type="html"><![CDATA[<h2 id="Meta-AI"><a href="#Meta-AI" class="headerlink" title="Meta AI"></a><strong>Meta AI</strong></h2><p><a href="https://segment-anything.com/">https://segment-anything.com</a></p><blockquote><p>摘要：We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion<br>masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can<br>transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that<br>its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We<br>are releasing the Segment Anything Model (SAM) and cor-responding dataset (SA-1B) of 1B masks and 11M images<br>at segment-anything.com to foster research into foundation models for computer vision</p></blockquote><blockquote><p>翻译：我们推出了Segment Anything (SA)项目：一个全新的图像分割任务、模型和数据集。通过使用我们高效的模型进行数据收集，我们成功构建了迄今为止最大的图像分割数据集，包含超过10亿个分割掩码和1100万张符合许可且尊重隐私的图像。这个模型被特别设计并训练为能够接受简单提示，因此它可以零样本迁移到不同的图像类型和任务。我们在多个任务中测试了该模型，发现它的零样本表现非常优秀——通常与之前的完全监督方法相当，甚至在某些情况下表现更好。我们将在segment-anything.com网站发布Segment Anything Model (SAM)和对应的数据集(SA-1B)，该数据集包括10亿个掩码和1100万张图像，旨在推动计算机视觉领域的基础模型研究。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-28-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-28-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-28-44"></p><p><strong>task, model, dataset, data engine, experiments, responsible AI, release</strong></p><ul><li><p>What <strong>task</strong> will enable zero-shot generalization?</p></li><li><p>What is the corresponding <strong>model</strong> architecture?</p></li><li><p>What <strong>data</strong> can power this task and model?</p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-37-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-37-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-37-11"></p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-39-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-39-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-39-37"  /><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-40-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-40-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-40-29"></p><blockquote><p>**研究背景：**本文聚焦于构建图像分割基础模型，其研究背景主要源于自然语言处理（NLP）和计算机视觉领域的发展现状与需求。</p><p>在NLP中，基于大规模网络数据集预训练的大语言模型展现出强大的零样本和少样本泛化能力，通过提示工程可适应多种任务和数据分布，且性能随模型规模、数据集大小和训练计算量的增加而提升。</p><p>计算机视觉领域虽也对基础模型有所探索，如CLIP和ALIGN利用对比学习训练文本和图像编码器实现零样本泛化，但计算机视觉问题广泛，许多问题缺乏充足的训练数据。</p><p>在图像分割方面，尚无网络规模的数据源，且现有方法难以实现强大的泛化能力。因此，本文旨在开发一个可提示的模型，并在广泛的数据集上进行预训练，以解决新数据分布下的一系列下游分割问题。具体通过定义可提示的分割任务、设计相应的模型架构（SAM）以及构建数据引擎收集大规模数据集（SA - 1B）来实现这一目标，从而推动图像分割进入基础模型时代。</p></blockquote><p><strong>研究现状：</strong></p><ul><li><strong>基础模型发展</strong>：大语言模型在自然语言处理（NLP）领域展现出强大的零样本和少样本泛化能力，通过提示工程可适应多种任务和数据分布。计算机视觉领域也在探索基础模型，如CLIP和ALIGN利用对比学习训练文本和图像编码器，实现零样本泛化。</li><li><strong>图像分割任务</strong>：图像分割领域存在多种任务，如交互式分割、边缘检测、实例分割等，但缺乏大规模、多样化的分割数据集，且现有模型在泛化能力和处理模糊提示方面存在不足。</li></ul><p><strong>创新点：</strong></p><ol><li><strong>任务创新</strong>：提出可提示分割任务，能作为预训练目标，通过提示工程实现零样本迁移到下游分割任务。<strong>（Task）</strong></li><li><strong>模型创新</strong>：设计Segment Anything Model（SAM），由图像编码器、提示编码器和掩码解码器组成，支持灵活提示、实时计算且能处理歧义。<strong>（Model）</strong></li><li><strong>数据创新</strong>：构建数据引擎收集SA - 1B数据集，含超10亿掩码，数量和质量远超现有数据集，为模型训练提供强大支撑。<strong>（Data）</strong></li></ol><p><strong>最后提出本文的不足：</strong></p><ol><li><strong>细节处理欠佳</strong>：SAM可能会遗漏图像中的精细结构，有时会生成小的、不相连的虚假组件，且生成的边界不如一些计算密集型的“放大”方法清晰。</li><li><strong>特定场景表现弱</strong>：在提供多个提示点时，专门的交互式分割方法通常会优于SAM，因为SAM更侧重于通用性和广泛适用性，而非高IoU的交互式分割。</li><li><strong>文本到掩码任务待完善</strong>：文本到掩码任务的探索还不够成熟，不够稳健，需要更多的努力来改进。</li><li><strong>特定提示设计困难</strong>：目前尚不清楚如何设计简单的提示来实现语义和全景分割。</li><li><strong>特定领域表现不佳</strong>：在特定领域，一些专门的工具可能会比SAM表现更好。</li></ol><blockquote><p>写作启发：<strong>Promptable Segment（提示词分割</strong>），<strong>Foundation models（基础模型）</strong>，<strong>数据集的创新</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> mata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模板</title>
      <link href="/post/mo-ban/"/>
      <url>/post/mo-ban/</url>
      
        <content type="html"><![CDATA[<p>作者单位</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>精读模板</title>
      <link href="/post/jing-du-mo-ban/"/>
      <url>/post/jing-du-mo-ban/</url>
      
        <content type="html"><![CDATA[<p>作者单位</p><table><thead><tr><th>标题</th><th>XXXXXXXXXXXXXXXXXXXXX</th></tr></thead><tbody><tr><td>翻译</td><td>XXXXXXXXXXXXXXXXXXXXXXX</td></tr><tr><td>主题</td><td>XXXXXXXX</td></tr><tr><td>方法</td><td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</td></tr></tbody></table><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><blockquote><p>第一句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第二句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第三句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第四句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第五句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第六句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第七句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第八句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第九句：</p></blockquote><blockquote><p>翻译：</p></blockquote><h1 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h1><p><strong>研究背景</strong></p><blockquote><p>第一句：随着数字媒体产业的快速发展，由各种资源捕捉到的海量视频数据集正以爆炸式的速度增长。  </p><p>第二句：大多数监控视频只包含有限的重要事件。</p></blockquote><p><strong>提出视频摘要的概念</strong></p><blockquote><p>第三句：用户将视频中的重要事件浓缩转发  </p><p>第四句：提出了视频摘要的概念</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FCN</title>
      <link href="/post/fcn/20250224-fcn/"/>
      <url>/post/fcn/20250224-fcn/</url>
      
        <content type="html"><![CDATA[<h1 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h1><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-51-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-51-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-51-25"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-53-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-53-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-53-54"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-55-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-55-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-55-17"></p><h2 id="FCN基本原理"><a href="#FCN基本原理" class="headerlink" title="FCN基本原理"></a>FCN基本原理</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-03-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-03-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-03-28"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-07-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-07-54"></p><h2 id="FCN细节"><a href="#FCN细节" class="headerlink" title="FCN细节"></a>FCN细节</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-13-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-13-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-13-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-14-27.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-14-27.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-14-27"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-15-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-15-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-15-24"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-19-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-19-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-19-15"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-20-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-20-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-20-21"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-21-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-21-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-21-01"></p><h2 id="FCN结果"><a href="#FCN结果" class="headerlink" title="FCN结果"></a>FCN结果</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-22-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-22-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-22-21"></p><h1 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h1><h2 id="SegNet的基本原理"><a href="#SegNet的基本原理" class="headerlink" title="SegNet的基本原理"></a>SegNet的基本原理</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-56-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-56-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_08-56-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-57-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-57-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_08-57-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-00-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-00-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-00-18"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-01-08.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-01-08.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-01-08"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-02-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-02-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-02-54"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-04-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-04-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-04-52"></p><h1 id="UNet"><a href="#UNet" class="headerlink" title="UNet"></a>UNet</h1><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-10-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-10-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-10-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-11-20.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-11-20.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-11-20"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-12-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-12-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-12-11"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-16-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-16-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-16-06"></p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FCN模型讲解</title>
      <link href="/post/fcn/20250224-fcn-mo-xing-jiang-jie/"/>
      <url>/post/fcn/20250224-fcn-mo-xing-jiang-jie/</url>
      
        <content type="html"><![CDATA[<figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">class FCN_VGG16(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">    FCN 的 backbone，由 VGG16 修改而来，舍弃最后的全连接层</span></span><br><span class="line"><span class="string">    以池化层为区分，一个池化层到上一个池化层之间的部分认为一个卷积块。</span></span><br><span class="line"><span class="string">    &#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(FCN_VGG16, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            # 第一个卷积块: 输入通道数：3，输出通道数：64，卷积核大小：3<span class="number">*3</span>，步长：1，填充：1</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=3, <span class="attribute">out_channels</span>=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=64, <span class="attribute">out_channels</span>=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第二个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=64, <span class="attribute">out_channels</span>=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=128, <span class="attribute">out_channels</span>=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第三个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=128, <span class="attribute">out_channels</span>=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=256, <span class="attribute">out_channels</span>=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=256, <span class="attribute">out_channels</span>=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第四个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=256, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512,out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第五个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # 每一层在 features 中的范围，&#123;0，1，2，3，4&#125; 为第一个卷积块，&#123;5，6，7，8，9&#125; 为第二个卷积块<span class="built_in">..</span>.</span><br><span class="line">        self.range = ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31))</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = &#123;&#125;</span><br><span class="line">        # 每一块的输出</span><br><span class="line">        <span class="keyword">for</span> idx, (start, end) <span class="keyword">in</span> enumerate(self.range):</span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> range(start, end):</span><br><span class="line">                input = self.features[layer](input)</span><br><span class="line">            output[<span class="string">&quot;x%d&quot;</span> % (idx + 1)] = input</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def <span class="built_in">test_vgg</span>():</span><br><span class="line">    # Backbone 的测试函数</span><br><span class="line">    input_x = torch.<span class="built_in">randn</span>((<span class="number">1</span>,<span class="number">3</span>,<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line">    vgg = <span class="built_in">FCN_VGG16</span>()</span><br><span class="line">    output_y = <span class="built_in">vgg</span>(input_x)</span><br><span class="line"></span><br><span class="line">    for key in output_y:</span><br><span class="line">        <span class="built_in">print</span>(output_y[key].<span class="built_in">size</span>())</span><br><span class="line"></span><br><span class="line"><span class="built_in">test_vgg</span>()</span><br></pre></td></tr></table></figure><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_10-19-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_10-19-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_10-19-29"></p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语义分割基础</title>
      <link href="/post/fcn/20250224-tu-xiang-fen-ge-ji-chu/"/>
      <url>/post/fcn/20250224-tu-xiang-fen-ge-ji-chu/</url>
      
        <content type="html"><![CDATA[<h1 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h1><h2 id="1-1-什么是图像分割"><a href="#1-1-什么是图像分割" class="headerlink" title="1.1 什么是图像分割"></a>1.1 什么是图像分割</h2><p>预测目标的轮廓</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-24-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-24-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-24-33"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-25-10.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-25-10.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-25-10"></p><h2 id="1-2-图像分割的应用场景"><a href="#1-2-图像分割的应用场景" class="headerlink" title="1.2 图像分割的应用场景"></a>1.2 图像分割的应用场景</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-26-34.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-26-34.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-26-34"></p><p>医学图像分割：医学影像，CT照片等</p><h2 id="1-3-图像分割的前景和背景"><a href="#1-3-图像分割的前景和背景" class="headerlink" title="1.3 图像分割的前景和背景"></a>1.3 图像分割的前景和背景</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-28-51.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-28-51.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-28-51"></p><blockquote><p>things和stuff</p></blockquote><h2 id="1-4-图像分割的三个层次"><a href="#1-4-图像分割的三个层次" class="headerlink" title="1.4 图像分割的三个层次"></a>1.4 图像分割的三个层次</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-31-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-31-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-31-28"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-33-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-33-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-33-52"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-35-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-35-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-35-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-37-03.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-37-03.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-37-03"></p><h1 id="2-经典数据集"><a href="#2-经典数据集" class="headerlink" title="2.经典数据集"></a>2.经典数据集</h1><h2 id="2-1-PASCAL数据集"><a href="#2-1-PASCAL数据集" class="headerlink" title="2.1 PASCAL数据集"></a>2.1 PASCAL数据集</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-43-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-43-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-43-44"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-44-22"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-44-48"></p><h2 id="2-1Cityscape-用于自动驾驶场景"><a href="#2-1Cityscape-用于自动驾驶场景" class="headerlink" title="2.1Cityscape(用于自动驾驶场景)"></a>2.1Cityscape(用于自动驾驶场景)</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-47-03.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-47-03.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-47-03"></p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-48-10.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-48-10.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-48-10" style="zoom:150%;" /><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-49-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-49-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-49-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-50-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-50-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-50-40"></p><h2 id="2-3-COCO数据集"><a href="#2-3-COCO数据集" class="headerlink" title="2.3 COCO数据集"></a>2.3 COCO数据集</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-53-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-53-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-53-15"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-54-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-54-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-54-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-55-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-55-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-55-37"></p><h1 id="3-评估指标和优化目标"><a href="#3-评估指标和优化目标" class="headerlink" title="3. 评估指标和优化目标"></a>3. 评估指标和优化目标</h1><h2 id="3-1-语义分割评估指标"><a href="#3-1-语义分割评估指标" class="headerlink" title="3.1 语义分割评估指标"></a>3.1 语义分割评估指标</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-57-42.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-57-42.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-57-42"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-59-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-59-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-59-26"></p><h2 id="3-2-语义分割常用优化目标"><a href="#3-2-语义分割常用优化目标" class="headerlink" title="3.2 语义分割常用优化目标"></a>3.2 语义分割常用优化目标</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-04-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-04-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-04-23"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-06-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-06-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-06-05"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-07-51.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-07-51.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-07-51"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-09-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-09-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-09-59"></p><h1 id="4-上采样"><a href="#4-上采样" class="headerlink" title="4. 上采样"></a>4. 上采样</h1><h2 id="4-1-图像分割网络的两个模块"><a href="#4-1-图像分割网络的两个模块" class="headerlink" title="4.1 图像分割网络的两个模块"></a>4.1 图像分割网络的两个模块</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-13-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-13-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-13-52"></p><h2 id="4-2-上采样实现方法–插值法"><a href="#4-2-上采样实现方法–插值法" class="headerlink" title="4.2 上采样实现方法–插值法"></a>4.2 上采样实现方法–插值法</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-16-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-16-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-16-32"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-18-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-18-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-18-13"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-26-45.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-26-45.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-26-45"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-27-49.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-27-49.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-27-49"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-31-39.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-31-39.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-31-39"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-36-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-36-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-36-24"></p><h2 id="4-3-典型的图像分割网络"><a href="#4-3-典型的图像分割网络" class="headerlink" title="4.3 典型的图像分割网络"></a>4.3 典型的图像分割网络</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-39-19.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-39-19.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-39-19"></p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语义分割上采样</title>
      <link href="/post/fcn/20250224-yu-yi-fen-ge-shang-cai-yang/"/>
      <url>/post/fcn/20250224-yu-yi-fen-ge-shang-cai-yang/</url>
      
        <content type="html"><![CDATA[<h2 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h2><blockquote><p>可以将较小的 feature 映射回一个较大的 feature map，这样的操作称为上采样，常用的上采样包括转置卷积，反池化，插值等操作。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-54-56.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-54-56.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-54-56"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-55-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-55-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-55-44"></p><p><strong>小特征图 -&gt; 大特征图</strong></p><p>可学习的上采样：转置卷积</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_15-01-27.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_15-01-27.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_15-01-27"></p><p>转置卷积，也叫反卷积，它并不是正向卷积的完全逆过程，用一句话来解释：</p><blockquote><p>反卷积是一种特殊的正向卷积，先按照一定的比例通过补 0 来扩大输入图像的尺寸，再进行普通的卷积。</p></blockquote><blockquote><p>卷积核大小：kernelsize</p><p>卷积步长：stride</p><p>特征图填充宽度：padding</p></blockquote><p>对于普通的 kernelsize&#x3D;(3,3),strides&#x3D;(2,2) 的卷积，其过程如下图所示</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides.gif" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides.gif" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="padding_strides"></p><p>kernelsize&#x3D;(3,3),strides&#x3D;(1,1) 情况下的反卷积则体现为：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides_transposed.gif" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides_transposed.gif" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="padding_strides_transposed"></p><p>我们可以发现，对于反卷积而言，补 0 主要是通过输入边缘的 padding 和输入内部插 0 实现。</p><p>具体的说，padding 的层数为 $kernelsize−stride$，而对于输入的内部插 0，其插入的 0 的数量为$stride−1$。</p><p><strong>接口与参数说明</strong></p><p><strong>torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride&#x3D;1, padding&#x3D;0, output_padding&#x3D;0, groups&#x3D;1, bias&#x3D;True, dilation&#x3D;1)</strong></p><ol><li><strong>in_channels</strong>(int) – 输入信号的通道数</li><li><strong>out_channels</strong> (int) – 卷积产生的通道数</li><li><strong>kerner_size</strong> (int or tuple) - 卷积核的大小</li><li><strong>stride</strong> (int or tuple,optional) - 卷积步长，即要将输入扩大的倍数。</li><li><strong>padding</strong> (int or tuple, optional) - 输入的每一条边补充0的层数，高宽都增加2*padding</li><li><strong>output_padding</strong> (int or tuple, optional) - 输出边补充0的层数，高宽都增加padding</li><li><strong>groups</strong> (int, optional) – 从输入通道到输出通道的阻塞连接数</li><li><strong>bias</strong> (bool, optional) - 如果bias&#x3D;True，添加偏置</li><li><strong>dilation</strong> (int or tuple, optional) – 卷积核元素之间的间距</li></ol><p>输出尺寸的计算公式为:<br>$$<br>H_{out}&#x3D;(H_{in}-1)<em>stride[0]-2</em>padding[0]+kernelsize[0]+outputpadding[0]\W_{out}&#x3D;(W_{in}-1)<em>stride[1]-2</em>padding[1]+kernelsize[1]+outputpadding[1]<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语义分割概述</title>
      <link href="/post/fcn/20250224-yu-yi-fen-ge-gai-shu/"/>
      <url>/post/fcn/20250224-yu-yi-fen-ge-gai-shu/</url>
      
        <content type="html"><![CDATA[<h2 id="1-什么是语义分割"><a href="#1-什么是语义分割" class="headerlink" title="1.什么是语义分割"></a>1.什么是语义分割</h2><blockquote><p>所谓的分割，就是从像素层面上对图像进行描述，即某一个像素属于哪一类物体</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-03-50.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-03-50.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-03-50"></p><p>分类：<strong>图片级</strong>，比如区分一张图片是猫还是狗</p><p>检测：<strong>区域级</strong>，比如检测一个区域是猫还是狗</p><p>分割：<strong>像素级</strong>，比如区分一个像素是猫还是狗</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-09-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-09-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-09-11"></p><p><strong>语义分割</strong>：也称为全像素语义分割，它是将每个像素分类为属于对象类的过程。</p><p><strong>实例分割</strong>：是语义分割或全像素语义分割的子类型，它将每个像素分类为属于对象类以及该类的实体 ID。<br>简单的说，语义分割只需要对像素进行分类就行了，而实例分割不仅要对像素进行分类，同一类的不同物体也要进行分割。</p><p><strong>全景分割</strong>： 是语义分割和实例分割的结合，即要对所有目标都检测出来，又要区分出同个类别中的不同实例。</p><h2 id="2-模型的输入和输出"><a href="#2-模型的输入和输出" class="headerlink" title="2.模型的输入和输出"></a>2.模型的输入和输出</h2><blockquote><p>我们先要对模型有一个大的了解。模型的输入是什么？很显然，是一张张的图片；那么输出是什么呢？</p></blockquote><p>我们假设模型的分类数量为 <code>n</code>，输入的图像大小为$W\times H$的 <code>RGB</code> 三通道的图像，那么实际上模型的输出为$W\times D \times n$，这要我们就可以把每一个像素的预测看成是一个分类任务，回顾一下之前的全连接网络的分类模型，对于 <code>n</code> 分类模型，输出节点数为 <code>n</code>。</p><p>所以，在语义分割而言，可以看成是$W\times H$个分类任务，每个像素的分类类别均为 <code>n</code>，所以得到的输出 <code>shape</code> 为$W\times D \times n$。事实上，输出也可能会比实际输入更大或者更小一些，因为网络的设计未必那么完美，这时候需要对边缘进行裁切或者补零</p><h2 id="3-常见的分割模型"><a href="#3-常见的分割模型" class="headerlink" title="3.常见的分割模型"></a>3.常见的分割模型</h2><p>语义分割模型：FCN，RetinaNet，RefineNet，Deeplab 等等</p><p>实例分割模型：Mask RCNN，DeepMask等等</p><h2 id="4-语义分割的思路"><a href="#4-语义分割的思路" class="headerlink" title="4.语义分割的思路"></a>4.语义分割的思路</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-23-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-23-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-23-53"></p><p>滑动窗口的思路可以概括如下：</p><ol><li>将图像分割成一个个小的区域；</li><li>遍历一些小区域；</li><li>对这些小区域做分类。</li></ol><p>但是这个思想一个很重要的问题就是效率低，重叠区域的特征反复计算</p><h2 id="5-评价指标"><a href="#5-评价指标" class="headerlink" title="5.评价指标"></a>5.评价指标</h2><p>一般来说，分割的指标主要有两个：像素分类精度（<code>accuracy</code>）和区域的交并比（<code>IoU</code>），下面给出具体的计算公式。</p><p>说明：</p><ul><li><p>$n_{ij}$表示像素点属于分类$i$被预测为分类$j$的像素点数量，</p></li><li><p>$t_i&#x3D;\sum_{j}n_{ij}$表示标签中所有分类为$i$的像素点数量</p></li><li><p>总共有$n_{cl}$个不同的分类。</p></li><li><p>像素分类准确率 (<code>pixel accurarcy</code>)</p></li></ul><p>$$<br>pacc&#x3D;\frac{\sum_{i}n_{ii}}{\sum_{i}t_{i}}<br>$$</p><p>该指标表示所有像素中分类正确的比例</p><ul><li><p>平均准确率 (<code>mean accuracy</code>)：<br>$$<br>macc&#x3D;\frac1{n_{c1}}\sum_{i}\frac{n_{ii}}{t_{i}}<br>$$<br>计算每一类分类正确的像素点数和该类的所有像素点数的比例然后求平均</p></li><li><p>平均交并比 (<code>mean IoU</code>)：<br>$$<br>mIoU&#x3D;\frac{1}{n_{cl}}\sum_{i}\frac{n_{ii}}{(t_{i}+\sum_{j}n_{ji}-n_{ii})}<br>$$<br>其中 IoU 的概念与目标检测中的 IoU 概念一致，表示两个区域的交并比，只不过相比较目标检测的矩形框，在分割任务中，区域为不规则的物体边界。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习环境配置2——windows下的torch=1.2.0环境配置</title>
      <link href="/post/windows-xia-de-torch-1.2.0-huan-jing-pei-zhi/"/>
      <url>/post/windows-xia-de-torch-1.2.0-huan-jing-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Anaconda安装"><a href="#一、Anaconda安装" class="headerlink" title="一、Anaconda安装"></a>一、Anaconda安装</h3><p><strong>Anaconda的安装主要是为了方便环境管理，可以同时在一个电脑上安装多种环境，不同环境放置不同框架：pytorch、tensorflow、keras可以在不同的环境下安装，只需要使用conda create –n创建新环境即可。</strong></p><h4 id="1、Anaconda的下载"><a href="#1、Anaconda的下载" class="headerlink" title="1、Anaconda的下载"></a>1、Anaconda的下载</h4><p>同学们可以选择安装新版Anaconda和旧版的Anaconda，安装步骤没有什么区别。</p><p><strong>旧版本anaconda的下载：</strong><br><strong>新版本的Anaconda没有VSCODE，如果大家为了安装VSCODE方便可以直接安装旧版的Anaconda，百度网盘连接如下。也可以装新版然后分开装VSCODE。</strong><br>链接: <a href="https://pan.baidu.com/s/12tW0Oad_Tqn7jNs8RNkvFA">https://pan.baidu.com/s/12tW0Oad_Tqn7jNs8RNkvFA</a> 提取码: i83n</p><p><strong>新版本anaconda的下载：</strong><br>如果想要安装最新的Anaconda，首先登录Anaconda的官网：<a href="https://www.anaconda.com/distribution/">https://www.anaconda.com/distribution/</a>。直接下载对应安装包就可以。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ccda457f3e2c14fa490e5dee510e15ff.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ccda457f3e2c14fa490e5dee510e15ff.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/79729ea1f6363089a7b848e2bbb41119.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/79729ea1f6363089a7b848e2bbb41119.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>一般是下载64位的，下载完成后打开。</p><h4 id="2、Anaconda的安装"><a href="#2、Anaconda的安装" class="headerlink" title="2、Anaconda的安装"></a>2、Anaconda的安装</h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b04e1b9b3c820f4212c77e872f721ff0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b04e1b9b3c820f4212c77e872f721ff0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>选择安装的位置，可以不安装在C盘。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cf41baaf1550d7d707c56da7997bf467.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cf41baaf1550d7d707c56da7997bf467.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>我选择了Add Anaconda to my PATH environment variable，这样会自动将anaconda装到系统的环境变量中，配置会更加方便一些。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/34a7c27d1eb9256186f88e6e610ffbd5.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/34a7c27d1eb9256186f88e6e610ffbd5.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>等待安装完之后，Anaconda的安装就结束了。</p><h3 id="二、Cudnn和CUDA的下载和安装"><a href="#二、Cudnn和CUDA的下载和安装" class="headerlink" title="二、Cudnn和CUDA的下载和安装"></a>二、Cudnn和CUDA的下载和安装</h3><p><strong>我这里使用的是torch&#x3D;1.2.0，官方推荐的Cuda版本是10.0，因此会用到cuda10.0，与cuda10.0对应的cudnn是7.4.1。</strong></p><h4 id="1、Cudnn和CUDA的下载"><a href="#1、Cudnn和CUDA的下载" class="headerlink" title="1、Cudnn和CUDA的下载"></a>1、Cudnn和CUDA的下载</h4><p><strong>网盘下载：</strong><br>链接: <a href="https://pan.baidu.com/s/1znYSRDtLNFLufAuItOeoyQ">https://pan.baidu.com/s/1znYSRDtLNFLufAuItOeoyQ</a><br>提取码: 8ggr</p><p><strong>官网下载：</strong><br>cuda10.0官网的地址是：<br><a href="https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal">cuda10.0官网地址</a><br>cudnn官网的地址是：需要大家进去后寻找7.4.1.5。<br><a href="https://developer.nvidia.com/cudnn">cudnn官网地址</a></p><p>下载完之后得到这两个文件。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/829e732c6e6228e02d96c3b7bd115d9b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/829e732c6e6228e02d96c3b7bd115d9b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8d273ca827020e1e079a78743bd000c5.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8d273ca827020e1e079a78743bd000c5.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h4 id="2、Cudnn和CUDA的安装"><a href="#2、Cudnn和CUDA的安装" class="headerlink" title="2、Cudnn和CUDA的安装"></a>2、Cudnn和CUDA的安装</h4><p>下载好之后可以打开exe文件进行安装。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c1fa30103f2316fc350436a8815d54e0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c1fa30103f2316fc350436a8815d54e0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>这里选择自定义。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bf16ce7629339969e0830a1630bd182.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bf16ce7629339969e0830a1630bd182.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="不"><br>然后直接点下一步就行了。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/d833eb6e1fa90ca9f621eb1072fe25aa.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/d833eb6e1fa90ca9f621eb1072fe25aa.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>安装完后在C盘这个位置可以找到根目录。<br>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0<br>然后大家把Cudnn的内容进行解压。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0ae6fdd762c2435ef118a642b341d4ba.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0ae6fdd762c2435ef118a642b341d4ba.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>把这里面的内容直接复制到C盘的根目录下就可以了。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a88d013177374fcfecfec1e3865e3c5e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a88d013177374fcfecfec1e3865e3c5e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h3 id="三、配置torch环境"><a href="#三、配置torch环境" class="headerlink" title="三、配置torch环境"></a>三、配置torch环境</h3><h4 id="1、pytorch环境的创建与激活"><a href="#1、pytorch环境的创建与激活" class="headerlink" title="1、pytorch环境的创建与激活"></a>1、pytorch环境的创建与激活</h4><p>Win+R启动cmd，在命令提示符内输入以下命令：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create –n pytorch python=3.6</span><br></pre></td></tr></table></figure><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate pytorch</span><br></pre></td></tr></table></figure><p>这里一共存在两条指令：<br>前面一条指令用于创建一个名为pytorch的环境，该环境的python版本为3.6。<br>后面一条指令用于激活一个名为pytorch的环境。</p><h4 id="2、pytorch库的安装"><a href="#2、pytorch库的安装" class="headerlink" title="2、pytorch库的安装"></a>2、pytorch库的安装</h4><p>由于我们所有的操作都要在对应环境中进行，所以在进行库的安装前需要先激活环境。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate pytorch </span><br></pre></td></tr></table></figure><p>此时cmd窗口的样子为：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e67dbf5cfc4f4125fedbffcb3bd85b77.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e67dbf5cfc4f4125fedbffcb3bd85b77.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h5 id="a、官方推荐安装方法（推荐）"><a href="#a、官方推荐安装方法（推荐）" class="headerlink" title="a、官方推荐安装方法（推荐）"></a>a、官方推荐安装方法（推荐）</h5><p>打开pytorch的官方安装方法：<br><a href="https://pytorch.org/get-started/previous-versions/">https://pytorch.org/get-started/previous-versions/</a><br>官网推荐的安装代码如下，我使用的是Cuda10的版本，不太懂为什么要写3个&#x3D;才能正确定位，两个&#x3D;会定位到cuda92的whl：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># CUDA 10.0</span><br><span class="line">pip install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure><p>这是pytorch官方提供的指令，用于安装torch和torchvision。</p><h5 id="b、先下载whl后安装"><a href="#b、先下载whl后安装" class="headerlink" title="b、先下载whl后安装"></a>b、先下载whl后安装</h5><p>需要注意的是，直接这样安装似乎特别慢，因此我们可以进入如下网址:<br><a href="https://download.pytorch.org/whl/torch_stable.html">https://download.pytorch.org/whl/torch_stable.html</a><br>找到自己需要的轮子下载。下载的时候使用迅雷下载就行了，速度还是比较快的！<br><img src="https://i-blog.csdnimg.cn/blog_migrate/08b8a756b9d7d214ce81f10bb5b73758.png#pic_center" class="lazyload placeholder" data-srcset="https://i-blog.csdnimg.cn/blog_migrate/08b8a756b9d7d214ce81f10bb5b73758.png#pic_center" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"  /><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/737ffc86979d1e7ceda0d98b5ddcef41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/737ffc86979d1e7ceda0d98b5ddcef41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>下载完成后找到安装路径：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1eae8e7fae0ea98cc5559e6287059451.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1eae8e7fae0ea98cc5559e6287059451.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>在cmd定位过来后利用文件全名进行安装就行了！<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/32c32f513e9be037243f885cd6f4ef11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/32c32f513e9be037243f885cd6f4ef11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>这里我也传一个百度网盘的版本。<br>链接: <a href="https://pan.baidu.com/s/14-QVk7Kb_CVwaVZxVPIgtw">https://pan.baidu.com/s/14-QVk7Kb_CVwaVZxVPIgtw</a><br>提取码: rg2e<br><strong>全部安装完成之后重启电脑。</strong></p><h4 id="3、其它依赖库的安装"><a href="#3、其它依赖库的安装" class="headerlink" title="3、其它依赖库的安装"></a>3、其它依赖库的安装</h4><p>但如果想要跑深度学习模型，还有一些其它的依赖库需要安装。具体如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scipy==1.2.1</span><br><span class="line">numpy==1.17.0</span><br><span class="line">matplotlib==3.1.2</span><br><span class="line">opencv_python==4.1.2.30</span><br><span class="line">torch==1.2.0</span><br><span class="line">torchvision==0.4.0</span><br><span class="line">tqdm==4.60.0</span><br><span class="line">Pillow==8.2.0</span><br><span class="line">h5py==2.10.0</span><br></pre></td></tr></table></figure><p>如果想要更便捷的安装可以在桌面或者其它地方创建一个requirements.txt文件，复制上述内容到txt文件中。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/7f76e6ad79f6bed1e2f4676b627354d3.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/7f76e6ad79f6bed1e2f4676b627354d3.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>使用如下指令安装即可。<strong>下述指令中，requirements.txt前方的路径是我将文件放在桌面的路径，各位同学根据自己的电脑修改。</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -r C:\Users\33232\Desktop\requirements.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="4、安装较慢请注意换源"><a href="#4、安装较慢请注意换源" class="headerlink" title="4、安装较慢请注意换源"></a>4、安装较慢请注意换源</h4><p>需要注意的是，如果在pip中下载安装比较慢可以换个源，可以到用户文件夹下，创建一个pip文件夹，然后在pip文件夹里创建一个txt文件。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/28006284902c6a57318e718daccee1a8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/28006284902c6a57318e718daccee1a8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>修改txt文件的内容，并且把后缀改成ini</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = http://pypi.mirrors.ustc.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">use-mirrors =true</span><br><span class="line">mirrors =http://pypi.mirrors.ustc.edu.cn/simple/</span><br><span class="line">trusted-host =pypi.mirrors.ustc.edu.cn</span><br></pre></td></tr></table></figure><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/783a72953baad1fd9de83303701cbaf8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/783a72953baad1fd9de83303701cbaf8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8bd41332dee625e1c6e182608acb9a29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8bd41332dee625e1c6e182608acb9a29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><strong>全部安装完成之后重启电脑。</strong></p><h3 id="四、安装VSCODE"><a href="#四、安装VSCODE" class="headerlink" title="四、安装VSCODE"></a>四、安装<a href="https://so.csdn.net/so/search?q=VSCODE&spm=1001.2101.3001.7020">VSCODE</a></h3><p><strong>我个人喜欢VSCODE，所以就安装它啦。其它的编辑软件也可以，个人喜好罢了。</strong></p><h4 id="1、下载安装包安装（推荐）"><a href="#1、下载安装包安装（推荐）" class="headerlink" title="1、下载安装包安装（推荐）"></a>1、下载安装包安装（推荐）</h4><p><strong>最新版本的Anaconda没有VSCODE因此可以直接百度VSCODE进行安装。</strong></p><h5 id="a、VSCODE的下载"><a href="#a、VSCODE的下载" class="headerlink" title="a、VSCODE的下载"></a>a、VSCODE的下载</h5><p>直接加载VSCODE的官网<a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a>，点击Download for Windows即可下载。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f69abbceb271a9dc5d12142e76df4ebc.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f69abbceb271a9dc5d12142e76df4ebc.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h5 id="b、VSCODE的安装"><a href="#b、VSCODE的安装" class="headerlink" title="b、VSCODE的安装"></a>b、VSCODE的安装</h5><p>首先同意协议，点一下步。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a80d57bcc63ffc394fb5b59aed099347.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a80d57bcc63ffc394fb5b59aed099347.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>其他里面的几个勾要打起来，因为这样就可以右键文件夹用VSCODE打开，非常方便。下一步。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4156c82700455d0ab65ea5bb8f68eeb3.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4156c82700455d0ab65ea5bb8f68eeb3.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>继续下一步安装即可。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6523e522b685fc8512cacaedfb1934d8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6523e522b685fc8512cacaedfb1934d8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p><strong>安装完成后在左下角更改自己的环境就行了。</strong><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4a6e8c3ce2dec68836338fdcc57a0dc1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4a6e8c3ce2dec68836338fdcc57a0dc1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h4 id="2、anaconda上安装"><a href="#2、anaconda上安装" class="headerlink" title="2、anaconda上安装"></a>2、anaconda上安装</h4><p>打开anaconda，切换环境。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/361dd496e006335bd418e8b03e91354e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/361dd496e006335bd418e8b03e91354e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>安装VSCODE，安装完就可以launch一下了，之后就可以把VScode固定到任务栏上，方便打开。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f781c276062dded3ab0d4c9f37aef3bf.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f781c276062dded3ab0d4c9f37aef3bf.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> CUDA </tag>
            
            <tag> cudnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vscode配置latex环境</title>
      <link href="/post/latex/visual-studio-code/"/>
      <url>/post/latex/visual-studio-code/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>LaTeX</strong> 作为一种强大的排版系统，对于理工科，特别是公式比较多的数学专业（秃头专业），其重要性自不必多说，不在本文探讨范围之内。</p><p>而选择一个比较好的编译器是很重要的，至少对笔者而言是如此。笔者前期使用的是<strong>TeXstudio</strong>进行文档的编译的，但是其编译速度比较慢，并且页面不是很美观。最让人头疼的是，当公式比较长的时候，使用的括号就比较多，但<strong>Texstudio</strong>的代码高亮功能实在是…（它对于括号根本就没有高亮，头秃）</p><p>而<strong>Visual Studio Code</strong>呢？话不多说，直接上图！<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5b90636b6875472ff796fe988e98826e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5b90636b6875472ff796fe988e98826e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="展示图"></p><p>可以看到，它不仅能够对代码高亮，不同级别括号用不同颜色标注了，颜值也很高。且 vscode 最突出的特点就是其强大的插件功能，每个使用者都能够根据自己的需求和想法下载相应的插件，从而将之配置为高度个性化的编辑器。可以这么说，每个使用者的 vscode 都不一样，为其专属定制编辑器。</p><p>笔者配置了好久，找了很多资料，很多博主也只是贴上了配置代码，没有详细的介绍说明。为了让更多人能够有一个比较清晰的了解，以此可以随时对自己的配置代码进行更改，故笔者写下了此文。希望能够对大家有所帮助。</p><p>注 1： 本文使用图片均为笔者自身编辑器截图或笔者朋友的编辑器截图（经过对方同意），且所有引用在文中或文末注明了来源，其余均为原创内容（代码不算哈哈哈）。</p><p>注 2： 若您的 vscode 页面和笔者所用图片中展示的页面有略微不同，均为笔者所安装的其余插件以及其余设置所致，并不影响本文中所说的所有配置，您无需担心，只需按照图片中所指向图标进行配置即可。</p><p>注 3： 文末有完整的个人配置代码（有的地方需要更改路径，有具体说明）。</p><h2 id="1-TeX-Live-下载与安装"><a href="#1-TeX-Live-下载与安装" class="headerlink" title="1 TeX Live 下载与安装"></a>1 TeX Live 下载与安装</h2><p>笔者选用的 Tex 系统是 TeX Live ，如果您想了解 TeX Live 和 MiKTeX 的区别，可以查看此篇文章：<a href="https://www.cnblogs.com/liuliang1999/p/12656706.html">https://www.cnblogs.com/liuliang1999/p/12656706.html</a></p><p>接下来是 TeX Live 的<strong>下载与安装说明</strong>：</p><p>① 通过网址 ：<a href="http://tug.org/texlive/acquire-iso.html">http://tug.org/texlive/acquire-iso.html</a> 进入 ISO 下载页面，点击图示红框圈画位置进入随机的镜像网站。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b238c6ecae7799a007459ab0cacddfdc.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b238c6ecae7799a007459ab0cacddfdc.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="下载网址"><br>② 可以看到的是，笔者进入了清华大学镜像网站，点击红框圈画链接进行 TeX Live 下载。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4cea74b28cf59bd59bd28cb03bdcb6e4.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4cea74b28cf59bd59bd28cb03bdcb6e4.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="下载"></p><p>③ 如果<strong>下载速度过慢</strong>，可以返回前一页面，进行重新点击，随机进入另一镜像网站进行下载尝试，直到下载速度在您的可接受范围内即可。或者在前一页面，点击 <strong>“mirror list”</strong> 进入镜像列表</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2a9a2b2fa4447587774d4f2f4eb1c9fb.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2a9a2b2fa4447587774d4f2f4eb1c9fb.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="mirror list"></p><p>然后手动选择某一镜像网站进行下载：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/45f4adf32411091223ef279f96fda7ed.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/45f4adf32411091223ef279f96fda7ed.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="镜像列表选择"></p><p>④ 找到下载好的压缩包，右键，在打开方式中选择**“Windows 资源管理器”**打开</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/db2bbda5fb583717e095af7edf5465a6.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/db2bbda5fb583717e095af7edf5465a6.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="资源管理器打开"></p><p>⑤ 找到 <strong>“install-tl-windows”</strong> 文件，为了后面不必要的麻烦，右键<strong>以管理员身份运行</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b666c4f9426f2fdf0a4a69bc86413ada.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b666c4f9426f2fdf0a4a69bc86413ada.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="以管理员身份运行"></p><p>⑥ 会先出现下图，无需理会，等会儿会消失</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/94aa279e1d0f6a59c34d95f4e85e0666.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/94aa279e1d0f6a59c34d95f4e85e0666.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="出现狮子"></p><p>⑦ **基本更改：**出现下图后，需要进行路径的更改；由于 TeX Live 自带的 TeXworks 不怎么好用，并且此文主要将 vscode 作为 LaTeX 的编辑器，故而取消 <strong>安装 TeXworks 前端</strong>的选项，再点击安装</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5382a34df2373e3c3febc4172ad4af1d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5382a34df2373e3c3febc4172ad4af1d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="修改路径"></p><p>⑧ <strong>个性化安装：</strong> 如果您需要个性化程度高的话，那么可以点击上图左下角的 <strong>Advancde</strong> ，根据您的需要进行相应的更改，但<strong>建议</strong>在不明白各个选项的作用时，不要对其进行修改，以免后期使用产生奇怪的问题。要注意的是，<strong>Adjust searchpath</strong> 这个选项一定要选中，将之添加到环境变量，否则后期手动添加比较麻烦；<br>而对于我们大部分人来说，只需要更改下图框选出的部分即可，也就是上图所完成的功能，再点击<strong>安装</strong>即可</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/64e11c5a3bfce6dae1620833aa528df6.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/64e11c5a3bfce6dae1620833aa528df6.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="advanced"></p><p>⑨ **进行安装：**接着就会出现下图，具体的安装指标已在下图标明，可根据其数字来判断安装所需时间。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/22a9567ff88b850c6de3365fd1bd570e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/22a9567ff88b850c6de3365fd1bd570e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="安装ing"></p><p>当上面标示的时间安装完之后，会出现一些配置文件的安装运行写入，进行等待即可，几分钟左右：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4bd5a634521b8a18328976fa6884d688.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4bd5a634521b8a18328976fa6884d688.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="安装后续"></p><p>当出现下图所示弹窗时，说明安装完毕，点击关闭即可。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/dd26e23808c26c27614803e4fbca04ca.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/dd26e23808c26c27614803e4fbca04ca.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="欢迎进入"></p><p>⑩ <strong>检查安装是否正常：</strong> 按win + R 打开<strong>运行</strong>，输入<code>cmd</code>，打开命令行窗口；然后输入命令<code>xelatex -v</code> ，如下图</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4042a6e9ec2aa56e6b1442fa1d4c96a0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4042a6e9ec2aa56e6b1442fa1d4c96a0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="安装检查"></p><p>如上图所示，若输出了一些版本信息，则安装正常。</p><h2 id="2-vscode下载与安装"><a href="#2-vscode下载与安装" class="headerlink" title="2 vscode下载与安装"></a>2 vscode下载与安装</h2><p>官网下载： <a href="https://code.visualstudio.com/">Click here to download Visual Studio Code</a>.</p><p>点进去之后就可以进行下载了。具体安装过程与常见的软件安装过程一致，这里就不作赘述。笔者只对几个要点进行提及：</p><p>① 记得<strong>修改安装路径</strong></p><p>② 根据个人想法可以选择是否在开始菜单文件夹创建 vscode 的快捷方式</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b52103381640e671ae6421fd329a600e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b52103381640e671ae6421fd329a600e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p>③ 一定要选上”添加到PATH”这个选项，能省很多麻烦。其余如图所示，自行选择。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c721d489a01c6fde012b9777eaae64ea.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c721d489a01c6fde012b9777eaae64ea.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p>安装好之后，打开 vscode，应如下图页面所示：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/56e8910ef21f541e6a699bb00f0a4f57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/56e8910ef21f541e6a699bb00f0a4f57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="启动页面"></p><h2 id="3-中文语言环境配置"><a href="#3-中文语言环境配置" class="headerlink" title="3 中文语言环境配置"></a>3 中文语言环境配置</h2><p>vscode的中文环境需要下载插件来进行支持。如下图所示：</p><p>① 点击拓展图标，打开拓展；</p><p>② 输入”Chinese”，选择第一个Chinese (Simplified) Language Pack for Visual Studio Code插件；</p><p>③ 点击”install”进行安装，等待安装完成；</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e74e7680283789d9bec131c5f69234d7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e74e7680283789d9bec131c5f69234d7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="下载中文插件"></p><p>④ 点击页面右下角跳出窗口中的”Restart now”，进行 vscode 重启。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/02a0f5f7d9a509bab220dfc4fd2b26b9.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/02a0f5f7d9a509bab220dfc4fd2b26b9.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="vscode重启"></p><p>⑤ 完成中文环境配置，显示如下：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57e75ca8310629f3fc4e2daf27cc8610.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57e75ca8310629f3fc4e2daf27cc8610.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="中文环境启动页面"></p><h2 id="4-LaTeX的支持插件-LaTeX-Workshop安装"><a href="#4-LaTeX的支持插件-LaTeX-Workshop安装" class="headerlink" title="4 LaTeX的支持插件 LaTeX Workshop安装"></a>4 LaTeX的支持插件 LaTeX Workshop安装</h2><p>① 点击拓展图标，打开拓展；</p><p>② 输入”latex workshop”，选择第一个LaTeX Workshop插件；</p><p>③ 点击”install”进行安装，等待安装完成；</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/254d1f24e8a90b349a77958b5c192d4d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/254d1f24e8a90b349a77958b5c192d4d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="latex workshop安装"></p><p>④ 若在安装完该插件之后在 vscode 页面右下角跳出如下弹窗，无需在意，只是提醒该插件已经更新到了8.11.1版本。若您想要了解新版本增加的功能，可以点击”Change log”进行查看；若不想了解，点击 “Disable this message” 即可。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c96cdbe7e01da3ff326da9df306918d1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c96cdbe7e01da3ff326da9df306918d1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="弹窗"></p><h2 id="5-打开LaTeX环境设置页面"><a href="#5-打开LaTeX环境设置页面" class="headerlink" title="5 打开LaTeX环境设置页面"></a>5 打开LaTeX环境设置页面</h2><p>① 点击设置图标</p><p>② 点击设置</p><p>③ 转到 UI 设置页面</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2ee6e759bdd67de85e8380d332db1a1a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2ee6e759bdd67de85e8380d332db1a1a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="打开设置"></p><p>④ 点击下图 1 处打开 json 文件，进入代码设置页面</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b8d742a52940e90d31777f7efd9943f1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b8d742a52940e90d31777f7efd9943f1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="json设置"></p><p>注 4 ： UI 设置页面和代码设置页面均为设置页面，其功能是一样的。不同的是，UI 设置页面交互能力较强，但一些设置需要去寻找，比较麻烦；而代码设置页面虽然相对 UI 而言不那么直观，但却可以对自己想要的功能直接进行代码编写，且代码设置可以直接克隆别人的代码到自己的编辑器中，从而直接完成相应设置，比较便捷。</p><p>注 5 ： 可以直接按Ctrl + ，进入设置页面。</p><h2 id="6-LaTeX环境的代码配置"><a href="#6-LaTeX环境的代码配置" class="headerlink" title="6 LaTeX环境的代码配置"></a>6 LaTeX环境的代码配置</h2><h3 id="6-1-LaTeX配置代码展示"><a href="#6-1-LaTeX配置代码展示" class="headerlink" title="6.1 LaTeX配置代码展示"></a>6.1 LaTeX配置代码展示</h3><p>先给出效果图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43a5acbf6be4a170f4a9dbdbbcff2209.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43a5acbf6be4a170f4a9dbdbbcff2209.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="LaTeX代码配置"></p><p>LaTeX 配置代码如下（不包含外部 pdf 查看器设置）：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;latex-workshop.latex.autoBuild.run&quot;: &quot;never&quot;,</span><br><span class="line">    &quot;latex-workshop.showContextMenu&quot;: true,</span><br><span class="line">    &quot;latex-workshop.intellisense.package.enabled&quot;: true,</span><br><span class="line">    &quot;latex-workshop.message.error.show&quot;: false,</span><br><span class="line">    &quot;latex-workshop.message.warning.show&quot;: false,</span><br><span class="line">    &quot;latex-workshop.latex.tools&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;-pdf&quot;,</span><br><span class="line">                &quot;-outdir=%OUTDIR%&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.latex.recipes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;XeLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;PDFLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;BibTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;bibtex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;LaTeXmk&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;latexmk&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex -&gt; bibtex -&gt; xelatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.latex.clean.fileTypes&quot;: [</span><br><span class="line">        &quot;*.aux&quot;,</span><br><span class="line">        &quot;*.bbl&quot;,</span><br><span class="line">        &quot;*.blg&quot;,</span><br><span class="line">        &quot;*.idx&quot;,</span><br><span class="line">        &quot;*.ind&quot;,</span><br><span class="line">        &quot;*.lof&quot;,</span><br><span class="line">        &quot;*.lot&quot;,</span><br><span class="line">        &quot;*.out&quot;,</span><br><span class="line">        &quot;*.toc&quot;,</span><br><span class="line">        &quot;*.acn&quot;,</span><br><span class="line">        &quot;*.acr&quot;,</span><br><span class="line">        &quot;*.alg&quot;,</span><br><span class="line">        &quot;*.glg&quot;,</span><br><span class="line">        &quot;*.glo&quot;,</span><br><span class="line">        &quot;*.gls&quot;,</span><br><span class="line">        &quot;*.ist&quot;,</span><br><span class="line">        &quot;*.fls&quot;,</span><br><span class="line">        &quot;*.log&quot;,</span><br><span class="line">        &quot;*.fdb_latexmk&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.latex.autoClean.run&quot;: &quot;onFailed&quot;,</span><br><span class="line">    &quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;,</span><br><span class="line">    &quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注 6 ： 若您不想要配置外部查看器以及了解内部查看和外部查看之间切换操作，可以直接复制上述代码至 json 文件中，即可完成 LaTeX 的配置，从而可以对 LaTeX 代码进行编译。</p><p>注 7 ： 根据 json 文件编写规则，每个代码语句（除了代码块儿最后一句）都需要加上英文状态下的<code>,</code>，否则就会报错；而每个代码块儿的最后一句是不需要加上<code>,</code>的。从上文整个代码块儿可以看出此规则。</p><p>如果您日后需要在上述代码之后再添加其他代码，请记得在最后一句</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;</span><br></pre></td></tr></table></figure><p>后添加上<code>,</code>，即变为</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;,</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>其中的<code>......</code>为您添加的其余代码。</p><p><strong>切记！</strong></p><h3 id="6-2-LaTeX配置代码解读"><a href="#6-2-LaTeX配置代码解读" class="headerlink" title="6.2 LaTeX配置代码解读"></a>6.2 LaTeX配置代码解读</h3><p>如果您对此不感兴趣，可以跳过该小节。下面进行代码<strong>注释解读</strong>：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.autoBuild.run&quot;: &quot;never&quot;</span><br></pre></td></tr></table></figure><p>设置何时使用默认的(第一个)编译链自动构建 LaTeX 项目，即什么时候自动进行代码的编译。有三个选项：</p><ol><li><strong>onFileChange</strong>：在检测任何依赖项中的文件更改(甚至被其他应用程序修改)时构建项目，即当检测到代码被更改时就自动编译tex文件；</li><li><strong>onSave</strong> : 当代码被保存时自动编译文件；</li><li><strong>never</strong>: 从不自动编译，即需编写者手动编译文档</li></ol><p>此项笔者设置为<strong>never</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.showContextMenu&quot;: true</span><br></pre></td></tr></table></figure><p>启用上下文LaTeX菜单。此菜单默认状态下停用，即变量设置为<strong>false</strong>，因为它可以通过新的 LaTeX 标记使用（新的 LaTeX 标记能够编译文档，将在下文提及）。只需将此变量设置为<strong>true</strong>即可恢复菜单。即此命令设置是否将编译文档的选项出现在鼠标右键的菜单中。</p><p>下图展示两者区别，左边为设置<strong>false</strong>情况，右边为设置<strong>true</strong>情况。可以看到的是，设置为<strong>true</strong>时，菜单中多了两个选项，其中多出来的第一个选项为进行tex文件的编译，而第二个选项为进行正向同步，即从代码定位到编译出来的 pdf 文件相应位置，下文会进行提及。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/80cef33dcac33ac0e86c82c101461f3b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/80cef33dcac33ac0e86c82c101461f3b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="无"><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bfedcbe76bb0f0ee3a27db3f6e4d538.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bfedcbe76bb0f0ee3a27db3f6e4d538.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="有"><br>笔者觉得菜单多了此选项较方便，故此项笔者设置为<strong>true</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.intellisense.package.enabled&quot;: true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>设置为<strong>true</strong>，则该拓展能够从使用的宏包中自动提取命令和环境，从而补全正在编写的代码。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.message.error.show&quot;  : false,</span><br><span class="line">&quot;latex-workshop.message.warning.show&quot;: false</span><br></pre></td></tr></table></figure><p>这两个命令是设置当文档编译错误时是否弹出显示出错和警告的弹窗。因为这些错误和警告信息能够从终端中获取，且弹窗弹出比较烦人，故而笔者设置均设置为<strong>false</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.tools&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;-pdf&quot;,</span><br><span class="line">                &quot;-outdir=%OUTDIR%&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>这些代码是定义在下文 recipes 编译链中被使用的编译命令，此处为默认配置，不需要进行更改。其中的<code>name</code>为这些命令的标签，用作下文 recipes 的引用；而<code>command</code>为在该拓展中的编译方式。</p><p>可以更改的代码为，将编译方式: pdflatex 、 xelatex 和 latexmk 中的<code>%DOCFILE</code>更改为<code>%DOC</code>。<code>%DOCFILE</code>表明编译器访问没有扩展名的根文件名，而<code>%DOC</code>表明编译器访问的是没有扩展名的根文件完整路径。这就意味着，使用<code>%DOCFILE</code>可以将文件所在路径设置为中文，但笔者不建议这么做，因为毕竟涉及到代码，当其余编译器引用时该 tex 文件仍需要根文件完整路径，且需要为英文路径。笔者此处设置为<code>%DOCFILE</code>仅是因为之前使用 TeXstudio，导致路径已经是中文了。</p><p>更多详情可以访问 github 中 LaTeX-Workshop 的 Wiki: <a href="https://github.com/James-Yu/LaTeX-Workshop/wiki/Compile#placeholders">Click here for more details about this.</a></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.recipes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;XeLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;PDFLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;BibTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;bibtex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;LaTeXmk&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;latexmk&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex -&gt; bibtex -&gt; xelatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>此串代码是对编译链进行定义，其中<code>name</code>是标签，也就是出现在工具栏中的链名称；<code>tool</code>是<code>name</code>标签所对应的编译顺序，其内部编译命令来自上文<code>latex-workshop.latex.recipes</code>中内容。</p><p>定义完成后，能够在 vscode 编译器中能够看到的编译顺序，具体看下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/bfcdea57459bf5f1687f3a4c548e868a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/bfcdea57459bf5f1687f3a4c548e868a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="编译链"></p><p>可以看到的是，在编译链中定义的命令出现在了vscode右侧的工具栏中。</p><p>注 8 ： <strong>PDFLaTeX</strong> 编译模式与 <strong>XeLaTeX</strong> 区别如下：</p><blockquote><ol><li><p>PDFLaTeX 使用的是TeX的标准字体，所以生成PDF时，会将所有的非 TeX 标准字体进行替换，其生成的 PDF 文件默认嵌入所有字体；而使用 XeLaTeX 编译，如果说论文中有很多图片或者其他元素没有嵌入字体的话，生成的 PDF<br>文件也会有些字体没有嵌入。</p></li><li><p>XeLaTeX 对应的 XeTeX 对字体的支持更好，允许用户使用操作系统字体来代替 TeX 的标准字体，而且对非拉丁字体的支持更好。</p></li><li><p>PDFLaTeX 进行编译的速度比 XeLaTeX 速度快。</p></li></ol></blockquote><p>注 9 ： 编译链的存在是为了更方便编译，因为如果涉及到**.bib**文件，就需要进行多次不同命令的转换编译，比较麻烦，而编译链就解决了这个问题。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.clean.fileTypes&quot;: [</span><br><span class="line">        //&quot;*.aux&quot;,</span><br><span class="line">       // &quot;*.bbl&quot;,</span><br><span class="line">        &quot;*.blg&quot;,</span><br><span class="line">        &quot;*.idx&quot;,</span><br><span class="line">        &quot;*.ind&quot;,</span><br><span class="line">        &quot;*.lof&quot;,</span><br><span class="line">        &quot;*.lot&quot;,</span><br><span class="line">        &quot;*.out&quot;,</span><br><span class="line">        &quot;*.toc&quot;,</span><br><span class="line">        &quot;*.acn&quot;,</span><br><span class="line">        &quot;*.acr&quot;,</span><br><span class="line">        &quot;*.alg&quot;,</span><br><span class="line">        &quot;*.glg&quot;,</span><br><span class="line">        &quot;*.glo&quot;,</span><br><span class="line">        &quot;*.gls&quot;,</span><br><span class="line">        &quot;*.ist&quot;,</span><br><span class="line">        &quot;*.fls&quot;,</span><br><span class="line">        &quot;*.log&quot;,</span><br><span class="line">        &quot;*.fdb_latexmk&quot;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>这串命令则是设置编译完成后要清除掉的辅助文件类型，若无特殊需求，无需进行更改。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.autoClean.run&quot;: &quot;onFailed&quot;</span><br></pre></td></tr></table></figure><p>这条命令是设置什么时候对上文设置的辅助文件进行清除。其变量有：</p><ol><li><strong>onBuilt</strong> : 无论是否编译成功，都选择清除辅助文件；</li><li><strong>onFailed</strong> : 当编译失败时，清除辅助文件；</li><li><strong>never</strong> : 无论何时，都不清除辅助文件。</li></ol><p>由于 tex 文档编译有时需要用到辅助文件，比如编译目录和编译参考文献时，如果使用<code>onBuilt</code>命令，则会导致编译不出完整结果甚至编译失败；</p><p>而有时候将 tex 文件修改后进行编译时，可能会导致 pdf 文件没有正常更新的情况，这个时候可能就是由于辅助文件没有进行及时更新的缘故，需要清除辅助文件了，而<code>never</code>命令做不到这一点；</p><p>故而笔者使用了<code>onFailed</code>，同时解决了上述两个问题。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;</span><br></pre></td></tr></table></figure><p>该命令的作用为设置 vscode 编译 tex 文档时的默认编译链。有两个变量：</p><ol><li><strong>first</strong> : 使用<code>latex-workshop.latex.recipes</code>中的第一条编译链，故而您可以根据自己的需要更改编译链顺序；</li><li><strong>lastUsed</strong> : 使用最近一次编译所用的编译链。</li></ol><p>笔者选择使用<strong>lastUsed</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;</span><br></pre></td></tr></table></figure><p>用于反向同步（即从编译出的 pdf 文件指定位置跳转到 tex 文件中相应代码所在位置）的内部查看器的快捷键绑定。变量有两种：</p><ol><li><strong>ctrl-click</strong> ： 为默认选项，使用Ctrl&#x2F;cmd+鼠标左键单击</li><li><strong>double-click</strong> : 使用鼠标左键双击</li></ol><p>此处笔者使用的为<strong>double-click</strong>。</p><h2 id="7-tex文件编译"><a href="#7-tex文件编译" class="headerlink" title="7 tex文件编译"></a>7 tex文件编译</h2><h3 id="7-1-tex测试文件下载"><a href="#7-1-tex测试文件下载" class="headerlink" title="7.1 tex测试文件下载"></a>7.1 tex测试文件下载</h3><p>为了测试 vscode 功能是否比较完整，笔者编写了一份简单的 tex 文件，以此测试其是否支持中英文，能否编译目录，能否插入图片，能否进行引用，能否编译参考文献（编译bixtex文件）等功能。</p><p>测试所用的 tex 文件可以从 github 下载：<a href="https://github.com/Ali-loner/Ali-loner.github.io">Click here to download the LaTeX testfile for vscode</a></p><p><strong>下载步骤</strong>如图：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6aca9b730b7c4ee2946a88fbe6ac40ad.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6aca9b730b7c4ee2946a88fbe6ac40ad.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="测试文件下载"><br>将之下载后，进行解压。</p><p>注 10 ： 若因网络原因无法连接到github导致无法下载，可以使用自己的tex文件进行测试，或者复制以下代码进行文档的简单编译测试，但其只能测试一部分功能：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[a4paper]&#123;article&#125;</span><br><span class="line">\usepackage[margin=1in]&#123;geometry&#125; % 设置边距，符合Word设定</span><br><span class="line">\usepackage&#123;ctex&#125;</span><br><span class="line">\usepackage&#123;lipsum&#125;</span><br><span class="line">\title&#123;\heiti\zihao&#123;2&#125; This is a test for vscode&#125;</span><br><span class="line">\author&#123;\songti Ali-loner&#125;</span><br><span class="line">\date&#123;2020.08.02&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">\maketitle</span><br><span class="line">\begin&#123;abstract&#125;</span><br><span class="line">\lipsum[2]</span><br><span class="line">\end&#123;abstract&#125;</span><br><span class="line">\tableofcontents</span><br><span class="line">\section&#123;This is a section&#125;</span><br><span class="line">Hello world! 你好，世界 ！</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><h3 id="7-2-tex-测试文件编译"><a href="#7-2-tex-测试文件编译" class="headerlink" title="7.2 tex 测试文件编译"></a>7.2 tex 测试文件编译</h3><p><strong>① 打开测试文件所在文件夹</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/927b15521ea5f9c20d3ba6c426e098ba.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/927b15521ea5f9c20d3ba6c426e098ba.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="打开文件夹"></p><p><strong>② 点击选中 tex 文件</strong>，进行文件内容查看</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6a28c240c45d77c6972a7e2fb6cd62a8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6a28c240c45d77c6972a7e2fb6cd62a8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="tex文件查看"></p><p><strong>③ 开始编译文件。</strong> 由于进行测试的文件中涉及参考文献的引用（<strong>.bib</strong>的编译），故而选择<code>xelatex -&gt; bibtex -&gt; xelatex*2</code>编译链。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/306b76bae450c39ddd5a7cea4074c84a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/306b76bae450c39ddd5a7cea4074c84a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="开始编译"></p><p>注 11 ： 为了更方便进行编译，可对其设置快捷键，设置快捷键步骤如下：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/24c26cd24b87c3f1e59568b8b8e8bb00.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/24c26cd24b87c3f1e59568b8b8e8bb00.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="快捷键绑定"></p><p>笔者将快捷键设置为Ctrl+Alt+R。</p><p><strong>选中tex文件的代码页面</strong>（若未选中，则无法进行编译），然后按下该快捷键，在编辑器页面上端进行编译链选择，如下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cbaed92ca564f9dbbb4204e0d9c2fee7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cbaed92ca564f9dbbb4204e0d9c2fee7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="快捷键编译"></p><p><strong>④ 编译成功</strong></p><p>当发现页面下方出现 <strong>√</strong> 符号时，说明编译成功，相反，如果出现 <strong>×</strong> 符号，说明编译失败，就要找失败原因了。</p><p><strong>a.</strong> 左侧工具栏</p><p>当编译成功后，选中 tex 文件中任意的代码，以此来选中 tex 文件，然后进行图示操作。其中侧边栏所展现的就是上文提及的新的 LaTeX 标记。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/04f02e79054c25953a761251a72e8682.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/04f02e79054c25953a761251a72e8682.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="pdf查看"></p><p><strong>b.</strong> 快捷键</p><p>选中 tex 文件中任意的代码，然后按Ctrl+Alt+V，出现编译好的 pdf 页面。该快捷键为默认设置。若您想要更改，可以根据上文进行配置。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/518090c31940a4a61b55f75a31f7c923.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/518090c31940a4a61b55f75a31f7c923.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="编译成功"></p><p>注意到，现在编译的结果为内部查看器查看。</p><p><strong>⑤ 正向同步测试</strong>，即从代码定位到 pdf 页面相应位置。有以下三种方法：</p><p><strong>a.</strong> 使用侧边工具栏</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/20bc13379b5e1d69eecb53c98175e896.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/20bc13379b5e1d69eecb53c98175e896.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="正向同步1"></p><p><strong>b.</strong> 使用右键菜单</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ac759bac5a92854811260f5ea5f56f0f.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ac759bac5a92854811260f5ea5f56f0f.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="正向同步2"><br><strong>c.</strong> 使用快捷键</p><p>选中需要跳转的代码所在行，按Ctrl+Alt+J，右侧就会跳转到相应行。这里的快捷键为默认设置，可自行通过上文方式设置为您想要的快捷键。</p><p><strong>⑥ 反向同步测试</strong>,即从 pdf 页面定位到代码相应位置</p><p>在编译生成的 pdf 上，选中想要跳转行，鼠标左键双击或ctrl+鼠标左键单击，跳转到对应代码。此处快捷键的选择为上文设置，若使用笔者的代码，则为鼠标左键双击。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bc30efe92862befcd35000da3b3eb93.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bc30efe92862befcd35000da3b3eb93.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="反向同步"></p><h2 id="8-SumatraPDF-安装设置（可选）"><a href="#8-SumatraPDF-安装设置（可选）" class="headerlink" title="8 SumatraPDF 安装设置（可选）"></a>8 SumatraPDF 安装设置（可选）</h2><p>您可自行选择是否需要设置此部分内容。</p><p>有的时候，由于想要看到 pdf 文件的完整展现效果，使用内置查看器已无法满足需求，这时可以使用外部查看器进行查看。</p><p>外部查看器的优势是能够看到 pdf 文件在查看器中的目录，可以实时进行跳转；且根据笔者使用来看，外部查看器展示出来的 pdf 默认会放大一些，使得字体变大，要更加让人舒服一些。</p><p>笔者选择 <strong>SumatraPDF</strong> 作为外部查看器，该软件的优点在于在具有 pdf 阅读功能的同时很轻量，安装包不到 10MB 大小，且支持双向同步功能。通过调整其与 vscode 的窗口位置，能够在拥有这些优势的同时，达到与内置 pdf 查看具有相同的效果。</p><h3 id="8-1-SumatraPDF下载与安装"><a href="#8-1-SumatraPDF下载与安装" class="headerlink" title="8.1 SumatraPDF下载与安装"></a>8.1 SumatraPDF下载与安装</h3><p>官网下载：<a href="https://www.sumatrapdfreader.org/download-free-pdf-viewer.html">Click here to download SumatraPDF</a></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/46ef16263fbbb2396ceb7029c1963a32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/46ef16263fbbb2396ceb7029c1963a32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="sumatraPDF"></p><p>其安装很简单，与通用软件安装过程一致，记得更改安装路径并记住，下文配置需要使用其路径。</p><h3 id="8-2-使用SumatraPDF查看的代码配置"><a href="#8-2-使用SumatraPDF查看的代码配置" class="headerlink" title="8.2 使用SumatraPDF查看的代码配置"></a>8.2 使用SumatraPDF查看的代码配置</h3><h4 id="8-2-1-代码展示"><a href="#8-2-1-代码展示" class="headerlink" title="8.2.1 代码展示"></a>8.2.1 代码展示</h4><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;,</span><br><span class="line">    &quot;latex-workshop.view.pdf.ref.viewer&quot;:&quot;auto&quot;,</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.args&quot;: [</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.args&quot;: [</span><br><span class="line">        &quot;-forward-search&quot;,</span><br><span class="line">        &quot;%TEX%&quot;,</span><br><span class="line">        &quot;%LINE%&quot;,</span><br><span class="line">        &quot;-reuse-instance&quot;,</span><br><span class="line">        &quot;-inverse-search&quot;,</span><br><span class="line">        &quot;code \&quot;F:/Microsoft VS Code/resources/app/out/cli.js\&quot; -r -g \&quot;%f:%l\&quot;&quot;, // 注意修改路径</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此代码仅为展示所用，让您进行查看，为下文解读之用。如需写入到 json 文件内，可直接完整复制文末笔者的个人配置到自己的编译器内。</p><h4 id="8-2-2-代码解读"><a href="#8-2-2-代码解读" class="headerlink" title="8.2.2 代码解读"></a>8.2.2 代码解读</h4><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;</span><br></pre></td></tr></table></figure><p>设置默认的pdf查看器，有三种变量参数：</p><ol><li><strong>tab</strong> : 使用 vscode 内置 pdf 查看器；</li><li><strong>browser</strong> : 使用电脑默认浏览器进行 pdf 查看；</li><li><strong>external</strong> : 使用外部 pdf 查看器查看。</li></ol><p>此处选择 <strong>external</strong> 参数，使用外部查看器。</p><p>注 12 ： 此参数为下文进行pdf内部查看和外部查看进行切换的关键参数。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.ref.viewer&quot;:&quot;auto&quot;</span><br></pre></td></tr></table></figure><p>设置PDF查看器用于在 <strong>\ref</strong> 命令上的[View on PDF]链接，此命令作用于 <strong>\ref</strong> 引用查看。有三个参数变量：</p><ol><li><strong>auto</strong> : 由编辑器根据情况自动设置；</li><li><strong>tabOrBrowser</strong> : 使用vscode内置pdf查看器或使用电脑默认浏览器进行pdf查看；</li><li><strong>external</strong> : 使用外部pdf查看器查看。</li></ol><p>此处设置为<strong>auto</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.viewer.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;// 注意修改路径</span><br></pre></td></tr></table></figure><p>使用外部查看器时要执行的命令，设置外部查看器启动文件<strong>SumatraPDF.exe</strong>文件所在位置，此处需要您根据自身情况进行路径更改，正常情况下只需更改磁盘盘符即可。</p><p><strong>请注意</strong>中间为 “ &#x2F; “ 而不是” \ “ ，不然会报错。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.viewer.args&quot;: [</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>此代码是设置使用外部查看器时，<code>latex-workshop.view.pdf.external.view .command</code>的参数。<code>%PDF%</code>是用于生成PDF文件的绝对路径的占位符。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.synctex.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot; // 注意修改路径</span><br></pre></td></tr></table></figure><p>此命令是将生成的辅助文件 <strong>.synctex.gz</strong> 转发到外部查看器时要执行的命令,设置其位置参数，您注意更改路径，此路径为 <strong>SumatraPDF.exe</strong> 文件路径。与上文相同。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.synctex.args&quot;: [</span><br><span class="line">        &quot;-forward-search&quot;,</span><br><span class="line">        &quot;%TEX%&quot;,</span><br><span class="line">        &quot;%LINE%&quot;,</span><br><span class="line">        &quot;-reuse-instance&quot;,</span><br><span class="line">        &quot;-inverse-search&quot;,</span><br><span class="line">        &quot;code \&quot;F:/Microsoft VS Code/resources/app/out/cli.js\&quot; -r -g \&quot;%f:%l\&quot;&quot;// 注意修改路径</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>设置当 <strong>.synctex.gz</strong> 文件同步到外部查看器时<code>latex-workshop.view.pdf.external.synctex</code>的参数。<code>%LINE%</code>是行号，<code>%PDF%</code>是生成PDF文件的绝对路径的占位符，<code>%TEX%</code>是当触发syncTeX被触发时，扩展名为 <strong>.tex</strong> 的 LaTeX 文件路径。</p><p>上面代码串中记得进行 <strong>Microsoft VS Code</strong> 路径修改，修改如下图:</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43de3a7d084a4b2b2eae6a259ba24f33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43de3a7d084a4b2b2eae6a259ba24f33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="路径修改"></p><h2 id="9-SumatraPDF-的使用"><a href="#9-SumatraPDF-的使用" class="headerlink" title="9 SumatraPDF 的使用"></a>9 SumatraPDF 的使用</h2><p>将完整代码复制到自己的 json 文件内后，即可使用 SumatraPDF作为自己的 pdf 外部查看器了。以下为具体操作：</p><p>① 点击编辑页面任意位置来选中 tex 文件；<br>② 按Ctrl+Alt+V，打开编译出的 pdf 文件；<br>③ 出现如下图页面。可以看到的是，原本内嵌输出的 pdf 变为了在 SumatraPDF 上查看，且侧面带有书签：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e5003691bd206012a74ecddf6c7bfa82.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e5003691bd206012a74ecddf6c7bfa82.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="sumatrapdf查看"></p><p>④ 为了出现和内嵌输出具有相同的效果，可以将 vscode 和 SumatraPDF 进行分屏，且根据需要关闭标签，如下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57974febf6f2c77c5b9bb9e22235887d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57974febf6f2c77c5b9bb9e22235887d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="分屏查看"></p><p>⑤ 且同样支持双向同步（正向同步和反向同步），其操作步骤与内嵌输出 pdf 时操作步骤相同，此处就不再赘述。查看效果图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6118b9cb1ea3e22d80a5a04edd5fb6a7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6118b9cb1ea3e22d80a5a04edd5fb6a7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="双向同步"></p><h2 id="10-pdf-内部查看与外部查看的切换"><a href="#10-pdf-内部查看与外部查看的切换" class="headerlink" title="10 pdf 内部查看与外部查看的切换"></a>10 pdf 内部查看与外部查看的切换</h2><p>以下展示由外部查看转为内部查看的操作，由内转外操作相同。</p><p>共有两种操作方式：<strong>UI界面设置</strong> 或 <strong>Json界面设置</strong> 。具体见下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8fde188378a7f0d0d15e09941768c364.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8fde188378a7f0d0d15e09941768c364.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="内外切换"></p><p>您可根据个人适应选择相应的方法。</p><h2 id="11-个人完整配置"><a href="#11-个人完整配置" class="headerlink" title="11 个人完整配置"></a>11 个人完整配置</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> //------------------------------LaTeX 配置----------------------------------</span><br><span class="line">    // 设置是否自动编译</span><br><span class="line">    &quot;latex-workshop.latex.autoBuild.run&quot;:&quot;never&quot;,</span><br><span class="line">    //右键菜单</span><br><span class="line">    &quot;latex-workshop.showContextMenu&quot;:true,</span><br><span class="line">    //从使用的包中自动补全命令和环境</span><br><span class="line">    &quot;latex-workshop.intellisense.package.enabled&quot;: true,</span><br><span class="line">    //编译出错时设置是否弹出气泡设置</span><br><span class="line">    &quot;latex-workshop.message.error.show&quot;: false,</span><br><span class="line">    &quot;latex-workshop.message.warning.show&quot;: false,</span><br><span class="line">    // 编译工具和命令</span><br><span class="line">    &quot;latex-workshop.latex.tools&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;-pdf&quot;,</span><br><span class="line">                &quot;-outdir=%OUTDIR%&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    // 用于配置编译链</span><br><span class="line">    &quot;latex-workshop.latex.recipes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;XeLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;PDFLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;BibTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;bibtex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;LaTeXmk&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;latexmk&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex -&gt; bibtex -&gt; xelatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    //文件清理。此属性必须是字符串数组</span><br><span class="line">    &quot;latex-workshop.latex.clean.fileTypes&quot;: [</span><br><span class="line">        &quot;*.aux&quot;,</span><br><span class="line">        &quot;*.bbl&quot;,</span><br><span class="line">        &quot;*.blg&quot;,</span><br><span class="line">        &quot;*.idx&quot;,</span><br><span class="line">        &quot;*.ind&quot;,</span><br><span class="line">        &quot;*.lof&quot;,</span><br><span class="line">        &quot;*.lot&quot;,</span><br><span class="line">        &quot;*.out&quot;,</span><br><span class="line">        &quot;*.toc&quot;,</span><br><span class="line">        &quot;*.acn&quot;,</span><br><span class="line">        &quot;*.acr&quot;,</span><br><span class="line">        &quot;*.alg&quot;,</span><br><span class="line">        &quot;*.glg&quot;,</span><br><span class="line">        &quot;*.glo&quot;,</span><br><span class="line">        &quot;*.gls&quot;,</span><br><span class="line">        &quot;*.ist&quot;,</span><br><span class="line">        &quot;*.fls&quot;,</span><br><span class="line">        &quot;*.log&quot;,</span><br><span class="line">        &quot;*.fdb_latexmk&quot;</span><br><span class="line">    ],</span><br><span class="line">    //设置为onFaild 在构建失败后清除辅助文件</span><br><span class="line">    &quot;latex-workshop.latex.autoClean.run&quot;: &quot;onFailed&quot;,</span><br><span class="line">    // 使用上次的recipe编译组合</span><br><span class="line">    &quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;,</span><br><span class="line">    // 用于反向同步的内部查看器的键绑定。ctrl/cmd +点击(默认)或双击</span><br><span class="line">    &quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //使用 SumatraPDF 预览编译好的PDF文件</span><br><span class="line">    // 设置VScode内部查看生成的pdf文件</span><br><span class="line">    &quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;,</span><br><span class="line">    // PDF查看器用于在\ref上的[View on PDF]链接</span><br><span class="line">    &quot;latex-workshop.view.pdf.ref.viewer&quot;:&quot;auto&quot;,</span><br><span class="line">    // 使用外部查看器时要执行的命令。此功能不受官方支持。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    // 使用外部查看器时，latex-workshop.view.pdf.external.view .command的参数。此功能不受官方支持。%PDF%是用于生成PDF文件的绝对路径的占位符。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.args&quot;: [</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ],</span><br><span class="line">    // 将synctex转发到外部查看器时要执行的命令。此功能不受官方支持。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    // latex-workshop.view.pdf.external.synctex的参数。当同步到外部查看器时。%LINE%是行号，%PDF%是生成PDF文件的绝对路径的占位符，%TEX%是触发syncTeX的扩展名为.tex的LaTeX文件路径。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.args&quot;: [</span><br><span class="line">        &quot;-forward-search&quot;,</span><br><span class="line">        &quot;%TEX%&quot;,</span><br><span class="line">        &quot;%LINE%&quot;,</span><br><span class="line">        &quot;-reuse-instance&quot;,</span><br><span class="line">        &quot;-inverse-search&quot;,</span><br><span class="line">        &quot;code \&quot;F:/Microsoft VS Code/resources/app/out/cli.js\&quot; -r -g \&quot;%f:%l\&quot;&quot;, // 注意修改路径</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>写在最后</strong> ： 笔者也只是一个初学者，文中如果出现错误的地方，欢迎您在评论区批评指正，笔者会虚心接受这些产生错误的地方，争取以后学得更扎实再编写这些文字。</p><p>另：若您感觉此文写得勉强还行，希望您能够不吝点赞，给笔者一点小小的激励，以此来进行更多更好的文字编写。非常感谢！！！</p><p>注：转载自<a href="https://zhuanlan.zhihu.com/p/166523064">https://zhuanlan.zhihu.com/p/166523064</a></p>]]></content>
      
      
      <categories>
          
          <category> latex </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vscode </tag>
            
            <tag> latex </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
