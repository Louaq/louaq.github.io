<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>BraTs2023数据集处理及python读取.nii文件</title>
      <link href="/post/dataset/brats2023-men-shu-ju-ji-jie-shao/"/>
      <url>/post/dataset/brats2023-men-shu-ju-ji-jie-shao/</url>
      
        <content type="html"><![CDATA[<h2 id="数据集信息"><a href="#数据集信息" class="headerlink" title="数据集信息"></a>数据集信息</h2><p><strong>BraTS2023-MEN</strong>(Brain Tumor Segmentation 2023 Meningioma Challenge) 是 BraTS2023 五个分割子任务中之一，与 BraTS 常规分割脑胶质瘤不同，该子任务目标是从多模态 MR 图像 (mpMRI) 中分割<strong>脑膜瘤</strong>。该数据集在 23 年 5 月份放出合计 6 个中心的 1650 例数据，其中有标注的训练集 1000 例，每例提供四种序列 MR 的输入图像（t1w, t1c, t2w, t2f）以及脑膜瘤的分割结果，标注内容主要包括非增强肿瘤核心（NETC）、周围非增强的FLAIR高信号（SNFH）和增强型肿瘤（ET）。验证集提供图像但没有标注，可以在官网提交验证，而测试集数据不公开。</p><p><strong>脑膜瘤</strong>是成人最常见的原发性颅内肿瘤，大多数脑膜瘤（约80%）是世界卫生组织（WHO）1级良性肿瘤，通常可以通过观察、手术切除和&#x2F;或放射治疗来良好控制。然而，高级别的脑膜瘤（WHO 2级和3级）与显著更高的发病率和死亡率相关且容易复发。和 BraTS23 其它分割任务一样，数据所有标签和数据都经过了预处理，这包括与统一的解剖模板对齐、调整到相同的分辨率（1 mm³）并进行了颅骨剥离。</p><h2 id="数据集元信息"><a href="#数据集元信息" class="headerlink" title="数据集元信息"></a>数据集元信息</h2><p>数据集所有图像的 spacing 和 size 都已经被预处理到一致。</p><p>二维切片个数：620,000（基于 1000 例训练集统计 155000 × 4）</p><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p><img src="https://pic2.zhimg.com/v2-94086c2af9de4a1a4a3246aa225a8e2f_r.jpg" class="lazyload placeholder" data-srcset="https://pic2.zhimg.com/v2-94086c2af9de4a1a4a3246aa225a8e2f_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><h2 id="文件结构"><a href="#文件结构" class="headerlink" title="文件结构"></a>文件结构</h2><p>官方文件结构如下，包含两个主要目录：ASNR-MICCAI-BraTS2023-MEN-Challenge-TrainingData 和 ASNR-MICCAI-BraTS2023-MEN-Challenge-ValidationData，分别代表训练数据和验证数据。</p><h2 id="作者及机构"><a href="#作者及机构" class="headerlink" title="作者及机构"></a>作者及机构</h2><p>Evan Calabrese (放射医学部，神经放射学分部，杜克大学医学中心，美国)</p><p>Dominic Labella (放射治疗学部，杜克大学医学中心，美国)</p><p>转载自知乎：<a href="https://zhuanlan.zhihu.com/p/662644135">https://zhuanlan.zhihu.com/p/662644135</a></p>]]></content>
      
      
      <categories>
          
          <category> BraTs2023数据集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BraTs2023数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BraTs2024数据集处理及python读取.nii文件</title>
      <link href="/post/dataset/brats2024-shu-ju-ji-jie-shao/"/>
      <url>/post/dataset/brats2024-shu-ju-ji-jie-shao/</url>
      
        <content type="html"><![CDATA[<p><strong>一、BraTS-GLI2024介绍</strong></p><p>国际脑肿瘤分割(BraTS)挑战。自2012年以来，BraTS一直专注于为成人脑胶质瘤的描述生成基准环境和数据集。今年挑战的重点仍然是产生一个共同的基准环境，但与2023年类似，该数据集将大幅扩展到约4,500例，以解决额外的i)人口(例如撒哈拉以南非洲患者)，ii)肿瘤(例如脑膜瘤)，iii)临床问题(例如缺失数据)，以及iv)技术考虑(例如增强)。具体而言，BraTS 2024的重点是确定当前最先进的算法，用于(任务1)使用新数据集处理治疗后胶质瘤，(任务2)服务不足的撒哈拉以南非洲脑胶质瘤患者群体，(任务3)使用新数据集处理放疗脑膜瘤，(任务4)脑转移，(任务5)儿科脑肿瘤患者，(任务7和8)全局和局部缺失数据，(任务9)有用的增强技术，以及(任务10)使用新数据集处理病理学。值得注意的是，所有数据都是常规的临床获得的，脑肿瘤患者的多位点多参数磁共振成像(mpMRI)扫描。所有数据集的真实参考注释由神经放射学专家为训练、验证和测试数据集中的每个主题创建和批准，以定量评估参与算法的性能。   神经胶质瘤是成人中最常见的恶性原发性脑肿瘤，其中弥漫性神经胶质瘤最为常见。弥漫性神经胶质瘤的特点是其在中枢神经系统内的浸润性生长模式，由于其生物行为、预后和对治疗的反应多变，给治疗和监测带来了巨大挑战。弥漫性神经胶质瘤的治疗涉及针对肿瘤特征和患者健康状况的多模式方法，包括手术、放射治疗和全身治疗。MRI 仍然是弥漫性神经胶质瘤治疗后成像的黄金标准。它提供了有关肿瘤大小、位置和形态随时间变化的重要信息。弥漫性神经胶质瘤的治疗后成像是患者管理的基本组成部分，它决定了治疗的变化并与临床结果相关。脑肿瘤分割 (BraTS) 挑战赛旨在确定当前最先进的脑中枢神经系统肿瘤分割算法。训练和盲验证数据可用于构建和评估分割算法。盲法验证数据预测的评估指标将立即返回给参与者，并附上不断更新的排行榜。2024 年 BraTS 治疗后胶质瘤子挑战赛的目的是开发一种自动多室脑肿瘤分割算法，用于治疗后 MRI 上的高级别和低级别弥漫性胶质瘤。从该挑战赛中开发的数据和算法可用于创建客观评估残留肿瘤体积的工具，以进行治疗计划和结果预测。</p><p><strong>二、BraTS-GLI2024任务</strong></p><p>肿瘤亚区域分割。注释包括增强组织（ET - 标签 3）、周围非增强 FLAIR 高信号（SNFH）- 标签 2）、非增强肿瘤核心（NETC - 标签 1）和切除腔（RC - 标签 4）。</p><p><img src="https://pic2.zhimg.com/v2-3cdaabd26c5dea4a928777dae57a19a1_r.jpg" class="lazyload placeholder" data-srcset="https://pic2.zhimg.com/v2-3cdaabd26c5dea4a928777dae57a19a1_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><strong>三、BraTS-GLI2024数据集</strong></p><p>今年的 BraTS 挑战赛使用多机构常规治疗后临床获取的胶质瘤多参数 MRI (mpMRI) 扫描作为训练、验证和测试数据。数据来自七个不同的学术医疗中心，杜克大学，加州大学旧金山分校，密苏里大学哥伦比亚分校，加州大学圣地亚哥分校，海德堡大学医院，密歇根大学，印第安纳大学。专家神经放射学家为训练、验证和测试数据集中的每个受试者创建并批准肿瘤子区域的地面实况注释，以定量评估预测的肿瘤分割。所有 BraTS mpMRI 扫描均可作为 NIfTI 文件 (.nii.gz) 获得，并描述 a) 原始 (T1) 和 b) 对比后 T1 加权 (T1Gd)、c) T2 加权 (T2) 和 d) T2 液体衰减反转恢复 (FLAIR) 体积，并使用来自多个数据提供机构的不同临床方案和各种扫描仪获取。真实数据是在预处理后创建的，包括与同一解剖模板的联合配准、以相同分辨率 (1 mm3) 进行插值以及颅骨剥离。所有成像数据集均由一到四名评分员按照相同的注释协议手动注释，其注释均由经验丰富的神经放射学家批准。注释包括增强组织（ET - 标签 3）、周围非增强 FLAIR 高信号（SNFH）- 标签 2）、非增强肿瘤核心（NETC - 标签 1）和切除腔（RC - 标签 4）。数据下载：</p><p><strong><a href="https://www.synapse.org/Synapse:syn59059776">https://www.synapse.org/Synapse:syn59059776</a></strong></p><p>考虑评估的子区域包括：ET 描述活动性肿瘤区域以及增强的结节区域。NETC 表示肿瘤内的坏死和囊肿。SNFH 包括水肿、浸润性肿瘤和治疗后变化。RC 包括近期和慢性切除腔，通常包含液体、血液、空气和&#x2F;或蛋白质物质。肿瘤核心 (ET 加 NETC) 描述通常在手术过程中切除的内容。整个肿瘤 (ET 加 SNFH 加 NETC) 定义肿瘤的整个范围，包括肿瘤核心、浸润性肿瘤、肿瘤周围水肿和治疗相关变化。评价指标：病变骰子相似系数 (DSC)，它测量预测分割和地面真实分割之间的体素分割重叠，忽略真正的负体素。病变95% Hausdorff 距离 (HD95)，它测量预测分割和地面真实分割中心之间的距离。</p><p><strong>四、技术路线</strong></p><p>1、根据固定阈值和形态学最大连通域分析得到大脑ROI区域，然后提取原始图像和标注图像的ROI。2、分析步骤1的ROI图像信息，得到图像平均大小是136x175x142，因此将图像缩放到固定大小160x160x160。3、图像预处理，对步骤2的原始图像进行像素值（1，99）截断，然后采用均值为0，方差为1的方式进行归一化处理。然后将数据分成训练集和验证集，对训练集做5倍数据增强处理。4、搭建VNet3d网络，使用AdamW优化器，学习率是0.001，batchsize是4，epoch是300，损失函数采用多分类的dice和交叉熵。5、训练结果和验证结果</p><p><img src="https://pic3.zhimg.com/v2-df11b9a607030b10063b3776ea5c5f06_r.jpg" class="lazyload placeholder" data-srcset="https://pic3.zhimg.com/v2-df11b9a607030b10063b3776ea5c5f06_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pic1.zhimg.com/v2-5f06201f34e9ccd1d6a568f69066c444_r.jpg" class="lazyload placeholder" data-srcset="https://pic1.zhimg.com/v2-5f06201f34e9ccd1d6a568f69066c444_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p>6、验证集分割结果   左图是金标准结果，右图是预测分割结果。</p><p><img src="https://picx.zhimg.com/v2-772a08fb349aafe011903a23fa9a72b5_r.jpg" class="lazyload placeholder" data-srcset="https://picx.zhimg.com/v2-772a08fb349aafe011903a23fa9a72b5_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pic3.zhimg.com/v2-22f9323403582a110505bf641348f048_r.jpg" class="lazyload placeholder" data-srcset="https://pic3.zhimg.com/v2-22f9323403582a110505bf641348f048_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pic2.zhimg.com/v2-9c2a1ae27705510554e52a95e142cfa7_r.jpg" class="lazyload placeholder" data-srcset="https://pic2.zhimg.com/v2-9c2a1ae27705510554e52a95e142cfa7_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pica.zhimg.com/v2-58ebcbc0db271f4a1dae082b7a5dcd92_r.jpg" class="lazyload placeholder" data-srcset="https://pica.zhimg.com/v2-58ebcbc0db271f4a1dae082b7a5dcd92_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://picx.zhimg.com/v2-3e62926bfd194368a03cc3296711b455_r.jpg" class="lazyload placeholder" data-srcset="https://picx.zhimg.com/v2-3e62926bfd194368a03cc3296711b455_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://picx.zhimg.com/v2-27b119d7007bcd7d171201f3c93e4143_r.jpg" class="lazyload placeholder" data-srcset="https://picx.zhimg.com/v2-27b119d7007bcd7d171201f3c93e4143_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pic1.zhimg.com/v2-90d8240ba53aa5c24a8ef05b532af2d8_r.jpg" class="lazyload placeholder" data-srcset="https://pic1.zhimg.com/v2-90d8240ba53aa5c24a8ef05b532af2d8_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p>7、测试集分割结果</p><p><img src="https://pic4.zhimg.com/v2-e062d0eef5992c61950b7f44edca0119_r.jpg" class="lazyload placeholder" data-srcset="https://pic4.zhimg.com/v2-e062d0eef5992c61950b7f44edca0119_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pic1.zhimg.com/v2-34787a1bddd8ca065211f4605627c122_r.jpg" class="lazyload placeholder" data-srcset="https://pic1.zhimg.com/v2-34787a1bddd8ca065211f4605627c122_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pica.zhimg.com/v2-6be28a74936153edf7c6688714d82a7a_r.jpg" class="lazyload placeholder" data-srcset="https://pica.zhimg.com/v2-6be28a74936153edf7c6688714d82a7a_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pic1.zhimg.com/v2-0f4eefdf7f6a420aba946c24ca0f2b2c_r.jpg" class="lazyload placeholder" data-srcset="https://pic1.zhimg.com/v2-0f4eefdf7f6a420aba946c24ca0f2b2c_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pic1.zhimg.com/v2-3b20ad70c62a32e07790444e55e77ed4_r.jpg" class="lazyload placeholder" data-srcset="https://pic1.zhimg.com/v2-3b20ad70c62a32e07790444e55e77ed4_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://picx.zhimg.com/v2-eaa010190a14e4605d023980f5a20141_r.jpg" class="lazyload placeholder" data-srcset="https://picx.zhimg.com/v2-eaa010190a14e4605d023980f5a20141_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pic1.zhimg.com/v2-a25ab522c925af4455759d847b70e7f0_r.jpg" class="lazyload placeholder" data-srcset="https://pic1.zhimg.com/v2-a25ab522c925af4455759d847b70e7f0_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pic1.zhimg.com/v2-10858af719a3a72e615bd01577a21d2c_r.jpg" class="lazyload placeholder" data-srcset="https://pic1.zhimg.com/v2-10858af719a3a72e615bd01577a21d2c_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pica.zhimg.com/v2-540c7563c132616191b403d34e21f48c_r.jpg" class="lazyload placeholder" data-srcset="https://pica.zhimg.com/v2-540c7563c132616191b403d34e21f48c_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p><img src="https://pic2.zhimg.com/v2-8a152bddab7691f608059404063bbccf_r.jpg" class="lazyload placeholder" data-srcset="https://pic2.zhimg.com/v2-8a152bddab7691f608059404063bbccf_r.jpg" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="图片描述"></p><p>转载自知乎：<a href="https://zhuanlan.zhihu.com/p/979775484">https://zhuanlan.zhihu.com/p/979775484</a></p>]]></content>
      
      
      <categories>
          
          <category> BraTs2024数据集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BraTs2024数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BraTs2019数据集处理及python读取.nii文件</title>
      <link href="/post/dataset/brats2019-shu-ju-ji-chu-li-ji-python-du-qu.nii-wen-jian/"/>
      <url>/post/dataset/brats2019-shu-ju-ji-chu-li-ji-python-du-qu.nii-wen-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>BraTS 是MICCAI脑肿瘤分割比赛的数据集，BraTs 2018中的训练集( training set) 有285个病例<br>每个病例有四个模态(t1、t2、flair、t1ce)，需要分割三个部分：whole tumor(WT), enhance tumor(ET), and tumor core(TC)，相当于三个label。<br>每例病例中包含4种模态的MRI序列和1个seg文件,所有序列尺寸全部为(240, 240, 155)，如下图：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6c1124180c7b582959148d148a04f54c.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6c1124180c7b582959148d148a04f54c.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p>MR图像属性信息：（240x240x155）一个MR序列有155张图片，每张图片的大小为240x240</p><h2 id="HGG、LGG"><a href="#HGG、LGG" class="headerlink" title="HGG、LGG"></a>HGG、LGG</h2><p>BraTs训练集( training set) 又划分为HGG和LGG,他们分别表示:<br><strong>HGG :高级别胶质瘤（WHO3～4级）</strong> 为低分化胶质瘤；这类肿瘤为恶性肿瘤，患者预后较差。<br><strong>LGG :低级别胶质瘤（WHO1～2级）</strong> 为分化良好的胶质瘤；虽然这类肿瘤在生物上并不属于良性肿瘤，但是患者的预后相对较好。</p><blockquote><p><strong>Domain shift 域转移</strong><br>即试图将学习的模型应用于与训练数据(源域)分布不同的测试数据(目标域)上，其性能会下降。由于医学图像多模态，数据域偏移的情况更加自然和严重。如下图所示，不同的医学图像（核磁共振（MR)成像和计算机断层扫描（CT））可以看到两幅图像的心脏区域在视觉上明显不同。毫无疑问的是，网络在MR图像上进行训练后不能用于CT图像上的测试。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/361793728e77ba4f81e9fa2a4b26c188.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/361793728e77ba4f81e9fa2a4b26c188.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p></blockquote><h2 id="多模态"><a href="#多模态" class="headerlink" title="多模态"></a>多模态</h2><p>多模态通俗地讲就是一个东西可以有多种不同的形态表现，个人理解四个模态就相当于channels。BraTs数据集中有t1、t2、t1ce、flair四个模态，而在医学界，把像 t1、t2、t1ce、flair这样的称为序列,一个病例的可以有多个序列,每个序列由许多切片组成, 此外,获得每种序列的方式不同,例如t1、t2是由于测量电磁波的物理量不同而产生的两种不同的序列,再例如t1ce序列要在做MR之前往血液打造影剂.</p><h2 id="nii格式数据"><a href="#nii格式数据" class="headerlink" title="nii格式数据"></a>nii格式数据</h2><p>标准NIFTI图像的扩展名是.nii，也包含了头文件及图像资料，NIFTI格式也可使用独立的图像文件（.img）和头文件（.hdr）。<br><strong>nii图像为三维图像，进行切片后分别表示<br>矢量面Median sagittal section (人体从前向后切开)<br>冠状面coronal section (人体沿左向右切开)<br>水平面Transverse section</strong><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1c71bfc511b80e8dbecddb1063ae4cc7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1c71bfc511b80e8dbecddb1063ae4cc7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h2 id="读取nii文件方法"><a href="#读取nii文件方法" class="headerlink" title="读取nii文件方法"></a>读取nii文件方法</h2><h3 id="NII文件可以直接用软件ITK-SNAP-打开"><a href="#NII文件可以直接用软件ITK-SNAP-打开" class="headerlink" title="NII文件可以直接用软件ITK-SNAP 打开"></a>NII文件可以直接用软件<strong>ITK-SNAP</strong> 打开</h3><h3 id="2-利用代码进行解析"><a href="#2-利用代码进行解析" class="headerlink" title="2.利用代码进行解析"></a>2.利用代码进行解析</h3><p><strong>2.1 安装torchio</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install torchio</span><br><span class="line">AI写代码python运行1</span><br></pre></td></tr></table></figure><p>读取文件并显示图片(以下仅读取t1)：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torchio as tio</span><br><span class="line">t1_path = &#x27;BraTS19_2013_2_1_t1.nii.gz&#x27;</span><br><span class="line">t1_img = tio.ScalarImage(t1_path)</span><br><span class="line">t1_img.plot()</span><br><span class="line">AI写代码python运行1234</span><br></pre></td></tr></table></figure><p>t1显示如下：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/3565edad611614d765c5f18680583840.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/3565edad611614d765c5f18680583840.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p><strong>2.2 安装nibabel：</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install nibabel</span><br><span class="line">AI写代码python运行1</span><br></pre></td></tr></table></figure><p>读取文件并显示图片(以下仅读取t1)：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import nibabel as nib</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">img = nib.load(&#x27;BraTS19_2013_2_1_flair.nii.gz&#x27;)</span><br><span class="line">print(img.shape)        # shape(240, 240, 155)</span><br><span class="line">print(img.header[&#x27;db_name&#x27;]) </span><br><span class="line">width, height, queue = img.dataobj.shape    # 由文件本身维度确定，可能是3维，也可能是4维 </span><br><span class="line">#print(&quot;width&quot;,width)  # 240</span><br><span class="line">#print(&quot;height&quot;,height) # 240</span><br><span class="line">#print(&quot;queue&quot;,queue)   # 155</span><br><span class="line">nib.viewers.OrthoSlicer3D(img.dataobj).show()</span><br><span class="line"></span><br><span class="line">num = 1</span><br><span class="line">for i in range(0,queue,10):</span><br><span class="line"> </span><br><span class="line">    img_arr = img.dataobj[:,:,i]</span><br><span class="line">    plt.subplot(5,4,num)</span><br><span class="line">    plt.imshow(img_arr,cmap=&#x27;gray&#x27;)</span><br><span class="line">    num +=1</span><br><span class="line"> </span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">AI写代码python运行12345678910111213141516171819202122</span><br></pre></td></tr></table></figure><p>显示图片如下：<em>（如果是黑色，鼠标拖动一下图片或者动一下滚轮，可能是坐标在原点）</em><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/383a27fe5b998e62881b627bcc20ed64.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/383a27fe5b998e62881b627bcc20ed64.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ae90cda8e929faa17e89aed401044ca8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ae90cda8e929faa17e89aed401044ca8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p>两种方式好像第二种会显示的的是3D的，更多态化一点，光是看一下数据长什么样子的话，两种方式都能用</p><h4 id="T1加权、T2加权"><a href="#T1加权、T2加权" class="headerlink" title="T1加权、T2加权"></a>T1加权、T2加权</h4><p>MRI扫描方式可以简单的划分为常规扫描和功能扫描两大类。常规扫描主要反映解剖形态；功能扫描则以不同方式反映人体新陈代谢、血液流动等功能信息。<br><strong>T1加权（T1 Weighted）和T2加权（T2 Weighted）是最常用，也是最基础的常规扫描。几乎所有的临床MRI检查都会包含T1加权和T2加权扫描。这里的“加权”，就是突出的意思。</strong></p><p><strong>T1看结构</strong></p><p>T1图像的整体感官跟“临床图像”的“习惯配色风格”非常接近,你看白质是白的,灰质是灰的,脑脊液是黑的.所以T1图像就可以看出各种断层解剖图.于是“T1看解剖结构”的说法就这么来了.</p><p><strong>T2看病变</strong></p><p>T2信号跟水含量有关(而Flair是结合水)很多病灶的T2信号要强于周围的正常组织.而很多病变都伴随组织水肿.从下图中可以看到,非常白的是水肿,然后比较白的那块影影约约的就是病变(红色)的地方了.<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/76f9af39dca5f18ef8a0cd485dd9a11d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/76f9af39dca5f18ef8a0cd485dd9a11d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p>如下图所示，左：T1加权图像 右：T2加权图像<br>T1加权突出显示解剖结构，T2加权则能够突出显示病灶<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c060f70ebac1fd8d70fab3cea10a690b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c060f70ebac1fd8d70fab3cea10a690b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h4 id="T1CE"><a href="#T1CE" class="headerlink" title="T1CE"></a>T1CE</h4><p>t1ce序列要在做MR之前往血液打造影剂(颜料),<strong>亮的地方血供丰富</strong>,强化明显说明血流很丰富,什么地方需要血流很快呢?<strong>肿瘤嘛~它们不断分裂需要大量的营养.</strong><br>下图所示, <strong>蓝色区域是增强瘤(enhancing tumor),它环绕的里面绿色那些是坏疽(ju)(necrotic components),坏疽就是细胞坏死然后液化.所以这些坏疽还被被称为非增强瘤(non-enhancing tumor).</strong><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/7e6911952c9ee327e5eb42e35dde0493.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/7e6911952c9ee327e5eb42e35dde0493.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h4 id="Flair"><a href="#Flair" class="headerlink" title="Flair"></a>Flair</h4><p>FLAIR序列是核磁共振(MR)的一种常用的序列，全称是液体衰减反转回复序列，也称为水抑制成像技术. 通俗地说,它是压水像。在该序列上，脑脊液呈现低信号(暗一些)，实质性病灶和含有结合水的病灶显示为明显的高信号(亮一些)<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/803d8fd6791236b331a58ff404df5820.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/803d8fd6791236b331a58ff404df5820.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h4 id="分割部分WT、ET、TC"><a href="#分割部分WT、ET、TC" class="headerlink" title="分割部分WT、ET、TC"></a>分割部分WT、ET、TC</h4><p>下图是Brats的数据集的一个序列的病例[颜色标签和上面那些图无关],我通过ITK-SNAP导入flair序列以及对应的分割标签, 需要分割有三个部分,分别是WT、ET、TC.<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/fbd1e73394ba9ccd7edf12beb79e0660.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/fbd1e73394ba9ccd7edf12beb79e0660.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><strong>数据集规定, 图中绿色为浮肿区域(ED,peritumoral edema) (标签2)、黄色为增强肿瘤区域(ET,enhancing tumor)(标签4)、红色为坏疽(NET,non-enhancing tumor)(标签1)、背景(标签0)</strong></p><p>然后这些标签合并为3个嵌套的子区域:<br><strong>whole tumor (WT) ——–包含所有labels（红+黄+绿）<br>tumor core (TC) ——— 红色+黄色label<br>enhancing tumor (ET) ——-黄色label</strong></p><p><strong>即 WT &#x3D; ED + ET + NET<br>    TC &#x3D; ET+NET<br>    ET</strong></p><p>转载自CSDN：<a href="https://blog.csdn.net/qq_42740834/article/details/124473611">https://blog.csdn.net/qq_42740834/article/details/124473611</a></p>]]></content>
      
      
      <categories>
          
          <category> BraTs数据集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BraTs数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态医学图像数据集</title>
      <link href="/post/duo-mo-tai-yi-xue-tu-xiang-shu-ju-ji/"/>
      <url>/post/duo-mo-tai-yi-xue-tu-xiang-shu-ju-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="BraTS2019-ET、TC、WT"><a href="#BraTS2019-ET、TC、WT" class="headerlink" title="BraTS2019 (ET、TC、WT)"></a>BraTS2019 (ET、TC、WT)</h2><p>来自The Brain Tumor Segmentation Challenge 2019的BraTS2019数据集包含335个标记的MRI图像。每个病例都有四种模式:<strong>T1、T1Gd、T2和FLAIR</strong>。数据集的标注区域包括三个肿瘤亚区:<strong>水肿(ED)、增强肿瘤(ET)和非增强肿瘤(NET)</strong>。相应的分割目标roi为<strong>增强肿瘤区域(ET)</strong>、<strong>肿瘤核心区域(TC &#x3D; ET+NET)<strong>和</strong>整个肿瘤区域(WT &#x3D; ED + ET+NET)</strong>。</p><h2 id="BraTS2020-NCR、ED、WT"><a href="#BraTS2020-NCR、ED、WT" class="headerlink" title="BraTS2020 (NCR、ED、WT)"></a>BraTS2020 (NCR、ED、WT)</h2><p>BraTS2020数据集包含<strong>369</strong>例多模态MRI扫描数据，其中训练集包含293例带有ground truth标注的数据，验证集包含125例数据。每个病例都包含四种MRI模态：<strong>T1、T1Gd、T2和FLAIR</strong>。所有图像都已经过预处理，包括颅骨剥离、重采样到1mm³分辨率、配准到同一解剖模板等步骤。标注包括三个肿瘤区域：<strong>坏死和囊性成分（NCR）、水肿区域（ED）和增强肿瘤区域（ET）</strong>，最终评估指标包括整个肿瘤、肿瘤核心和增强肿瘤的Dice相似性系数和Hausdorff距离。</p><h2 id="BraTS2021-NCR、ED、WT"><a href="#BraTS2021-NCR、ED、WT" class="headerlink" title="BraTS2021 (NCR、ED、WT)"></a>BraTS2021 (NCR、ED、WT)</h2><p>BraTS2021数据集在2020版本基础上进行了扩充，训练集增加到1251例数据，验证集包含219例，测试集包含570例。除了延续传统的分割任务外，BraTS2021还新增了总生存期预测任务，要求参赛者不仅要完成肿瘤分割，还要基于影像特征预测患者的生存时间。数据集同样包含四种MRI模态，并且在数据质量控制方面更加严格，所有数据都经过了多位专家的交叉验证和质量检查。</p><h2 id="BraTS2022-NCR、ED、WT"><a href="#BraTS2022-NCR、ED、WT" class="headerlink" title="BraTS2022 (NCR、ED、WT)"></a>BraTS2022 (NCR、ED、WT)</h2><p>BraTS2022数据集进一步扩大了数据规模，训练集包含1251例数据，验证集包含219例，测试集大幅增加到570例。除了保持传统的成人胶质瘤分割任务外，BraTS2022首次引入了儿童脑肿瘤分割任务（BraTS-PED），专门针对儿科患者的脑肿瘤进行分割，这填补了儿童脑肿瘤图像分析的空白。同时，数据集在非洲地区的数据收集方面也有所加强（BraTS-Africa），旨在提高算法在不同人群和设备条件下的泛化能力。在技术规范上，BraTS2022继续保持高标准的数据预处理和标注质量，所有图像都统一到240×240×155的体素空间，像素间距为1mm³。</p><h2 id="BraTS2023-ET、TC、WT"><a href="#BraTS2023-ET、TC、WT" class="headerlink" title="BraTS2023 (ET、TC、WT)"></a>BraTS2023 (ET、TC、WT)</h2><p>来自The Brain Tumor Segmentation Challenge 2023的BraTS2023数据集包含1251个标记的MRI图像。每个病例都有四种模式:<strong>T1、T1Gd、T2和FLAIR</strong>。数据集的标注区域包括三个肿瘤亚区:<strong>水肿(ED)、增强肿瘤(ET)和非增强肿瘤(NET)</strong>。相应的分割目标roi为<strong>增强肿瘤区域(ET)</strong>、<strong>肿瘤核心区域(TC &#x3D; ET+NET)<strong>和</strong>整个肿瘤区域(WT &#x3D; ED + ET+NET)</strong>。</p><h2 id="BraTS2024-ET、TC、WT、NETC、SNFH、RC"><a href="#BraTS2024-ET、TC、WT、NETC、SNFH、RC" class="headerlink" title="BraTS2024(ET、TC、WT、NETC、SNFH、RC)"></a>BraTS2024(ET、TC、WT、NETC、SNFH、RC)</h2><p>BraTS2024挑战侧重于使用治疗后MRI(<strong>包括T1, T1Gd, T2和FLAIR</strong>)对高级别和低级别弥漫性胶质瘤进行治疗后自动多室脑肿瘤分割方法。在这个挑战中考虑评估的子区域是<strong>NETC</strong>(非增强肿瘤核心，肿瘤内坏死和囊肿区域)，<strong>SNFH</strong>(周围非增强FLAIR高强度，包括水肿，浸润性肿瘤和治疗后改变区域)，<strong>RC</strong>(切除腔，包括近期和慢性切除腔的区域，通常含有液体、血液、空气和&#x2F;或蛋白质物质)、<strong>ET、TC (TC &#x3D; ET + NETC)和WT (WT &#x3D; ET + SNFH + NETC)</strong>。</p><h2 id="Prostate158-PZ、TZ"><a href="#Prostate158-PZ、TZ" class="headerlink" title="Prostate158(PZ、TZ)"></a>Prostate158(PZ、TZ)</h2><p>prostat158数据集包括158张带有专家标签的前列腺MRI。所有病例均包括T2加权(T2)和弥散加权(DWI)图像以及表观弥散系数(ADC)图。相应的目标ROI为前列腺周围区(PZ)和过渡区(TZ)。</p><h2 id="CHAOS-liver-right-kidney-left-kidney-spleen"><a href="#CHAOS-liver-right-kidney-left-kidney-spleen" class="headerlink" title="CHAOS(liver, right kidney, left kidney, spleen)"></a>CHAOS(liver, right kidney, left kidney, spleen)</h2><p>来自联合(CT-MR)健康腹部器官分割挑战任务5的CHAOS数据集旨在从使用两种不同序列(T1-DUAL和T2-SPIR)获得的20个MRI数据集中分割腹部器官(肝脏，右肾，左肾，脾脏)。</p><h2 id="哈佛全脑图谱数据集-Glioma"><a href="#哈佛全脑图谱数据集-Glioma" class="headerlink" title="哈佛全脑图谱数据集(Glioma)"></a>哈佛全脑图谱数据集(Glioma)</h2><p><strong>MRI、CT、PET、SPECT</strong></p><h2 id="前庭神经梢瘤数据集"><a href="#前庭神经梢瘤数据集" class="headerlink" title="前庭神经梢瘤数据集"></a>前庭神经梢瘤数据集</h2><p>数据集包含了不同阶段和大小的前庭神经鞘瘤病例，从微小的肿瘤到较大的占位性病变都有涵盖。每个病例都包含多种MRI序列，主要包括T1加权、T1增强、T2加权和构造干扰稳态序列（CISS）等。T1增强序列对于显示肿瘤的强化特征最为重要，因为前庭神经鞘瘤通常表现为明显的均匀强化。CISS序列由于其高分辨率和优异的软组织对比，特别适用于显示内听道和桥小脑角区域的精细解剖结构。</p><h2 id="MyoPS-2020-Dataset"><a href="#MyoPS-2020-Dataset" class="headerlink" title="MyoPS 2020 Dataset"></a>MyoPS 2020 Dataset</h2><p>心肌病理分割</p>]]></content>
      
      
      <categories>
          
          <category> 数据集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码复现记录nnUNet</title>
      <link href="/post/dai-ma-fu-xian-ji-lu-nnunet/"/>
      <url>/post/dai-ma-fu-xian-ji-lu-nnunet/</url>
      
        <content type="html"><![CDATA[<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>BraTS2021 脑肿瘤分割</p><h2 id="所需指令"><a href="#所需指令" class="headerlink" title="所需指令"></a>所需指令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nnUNetv2_convert_MSD_dataset -i D:\nnunet\nnUNet_raw\Task44_example</span><br><span class="line"></span><br><span class="line">nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity</span><br><span class="line"></span><br><span class="line">nnUNetv2_train 2 3d_fullres all -<span class="built_in">tr</span> nnUNetTrainer_250epochs</span><br><span class="line"></span><br><span class="line">nnUNetv2_predict -i D:/nnunet/nnUNet_raw/Dataset044_example /imagesTs -o output -d 44 -c 2d -f all</span><br></pre></td></tr></table></figure><h2 id="方法1：通过命令行参数（部分参数）"><a href="#方法1：通过命令行参数（部分参数）" class="headerlink" title="方法1：通过命令行参数（部分参数）"></a>方法1：通过命令行参数（部分参数）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本训练命令</span></span><br><span class="line">nnUNetv2_train DATASET_ID CONFIG FOLD [其他参数]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一些可用的命令行参数：</span></span><br><span class="line">nnUNetv2_train 001 2d <span class="number">0</span> --npz  <span class="comment"># 使用npz格式保存</span></span><br><span class="line">nnUNetv2_train 001 2d <span class="number">0</span> --c    <span class="comment"># 继续之前的训练</span></span><br><span class="line">nnUNetv2_train 001 2d <span class="number">0</span> --val  <span class="comment"># 同时进行验证</span></span><br></pre></td></tr></table></figure><h2 id="方法2：修改配置文件中的默认参数"><a href="#方法2：修改配置文件中的默认参数" class="headerlink" title="方法2：修改配置文件中的默认参数"></a>方法2：修改配置文件中的默认参数</h2><p>找到并编辑nnUNetTrainer类文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /root/autodl-tmp/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py</span><br></pre></td></tr></table></figure><p>在<code>__init__</code>方法中修改默认参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, plans: <span class="built_in">dict</span>, configuration: <span class="built_in">str</span>, fold: <span class="built_in">int</span>, dataset_json: <span class="built_in">dict</span>, unpack_dataset: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">             device: torch.device = torch.device(<span class="params"><span class="string">&#x27;cuda&#x27;</span></span>)</span>):</span><br><span class="line">    <span class="comment"># 修改学习率</span></span><br><span class="line">    <span class="variable language_">self</span>.initial_lr = <span class="number">1e-2</span>  <span class="comment"># 默认是1e-2，可以修改为其他值</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 修改批次大小会在后面的方法中设置</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 修改训练轮数</span></span><br><span class="line">    <span class="variable language_">self</span>.num_epochs = <span class="number">1000</span>  <span class="comment"># 默认1000个epoch</span></span><br></pre></td></tr></table></figure><h2 id="方法3：创建自定义Trainer类（推荐）"><a href="#方法3：创建自定义Trainer类（推荐）" class="headerlink" title="方法3：创建自定义Trainer类（推荐）"></a>方法3：创建自定义Trainer类（推荐）</h2><p>创建一个新的trainer类继承自nnUNetTrainer：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建自定义trainer文件</span></span><br><span class="line">vim /root/autodl-tmp/nnUNet/nnunetv2/training/nnUNetTrainer/MyCustomTrainer.py</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nnunetv2.training.nnUNetTrainer.nnUNetTrainer <span class="keyword">import</span> nnUNetTrainer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyCustomTrainer</span>(<span class="title class_ inherited__">nnUNetTrainer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, plans: <span class="built_in">dict</span>, configuration: <span class="built_in">str</span>, fold: <span class="built_in">int</span>, dataset_json: <span class="built_in">dict</span>, unpack_dataset: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 device: torch.device = torch.device(<span class="params"><span class="string">&#x27;cuda&#x27;</span></span>)</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(plans, configuration, fold, dataset_json, unpack_dataset, device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 自定义学习率</span></span><br><span class="line">        <span class="variable language_">self</span>.initial_lr = <span class="number">5e-3</span>  <span class="comment"># 修改初始学习率</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 自定义训练轮数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_epochs = <span class="number">500</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 自定义权重衰减</span></span><br><span class="line">        <span class="variable language_">self</span>.weight_decay = <span class="number">3e-5</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 自定义优化器参数</span></span><br><span class="line">        optimizer = torch.optim.SGD(</span><br><span class="line">            <span class="variable language_">self</span>.network.parameters(),</span><br><span class="line">            lr=<span class="variable language_">self</span>.initial_lr,</span><br><span class="line">            weight_decay=<span class="variable language_">self</span>.weight_decay,</span><br><span class="line">            momentum=<span class="number">0.99</span>,</span><br><span class="line">            nesterov=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> optimizer</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_rotation_dummyDA_mirroring_and_inital_patch_size</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 修改数据增强参数</span></span><br><span class="line">        rotation_for_DA, do_dummy_2d_data_aug, initial_patch_size, mirror_axes = \</span><br><span class="line">            <span class="built_in">super</span>().configure_rotation_dummyDA_mirroring_and_inital_patch_size()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 可以在这里修改patch size等参数</span></span><br><span class="line">        <span class="keyword">return</span> rotation_for_DA, do_dummy_2d_data_aug, initial_patch_size, mirror_axes</span><br></pre></td></tr></table></figure><h2 id="方法4：修改batch-size"><a href="#方法4：修改batch-size" class="headerlink" title="方法4：修改batch size"></a>方法4：修改batch size</h2><p>batch size通常由硬件内存自动确定，但可以通过修改plans文件或代码强制设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在自定义trainer中重写方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 强制设置batch size</span></span><br><span class="line">    <span class="variable language_">self</span>.batch_size = <span class="number">4</span>  <span class="comment"># 设置你想要的batch size</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 重新计算相关参数</span></span><br><span class="line">    optimizer = torch.optim.SGD(</span><br><span class="line">        <span class="variable language_">self</span>.network.parameters(),</span><br><span class="line">        lr=<span class="variable language_">self</span>.initial_lr,</span><br><span class="line">        weight_decay=<span class="variable language_">self</span>.weight_decay,</span><br><span class="line">        momentum=<span class="number">0.99</span>,</span><br><span class="line">        nesterov=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> optimizer</span><br></pre></td></tr></table></figure><h2 id="方法5：使用自定义trainer进行训练"><a href="#方法5：使用自定义trainer进行训练" class="headerlink" title="方法5：使用自定义trainer进行训练"></a>方法5：使用自定义trainer进行训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用自定义trainer训练</span></span><br><span class="line">nnUNetv2_train DATASET_ID CONFIG FOLD -tr MyCustomTrainer</span><br></pre></td></tr></table></figure><h2 id="常用参数修改示例"><a href="#常用参数修改示例" class="headerlink" title="常用参数修改示例"></a>常用参数修改示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyCustomTrainer</span>(<span class="title class_ inherited__">nnUNetTrainer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 学习率相关</span></span><br><span class="line">        <span class="variable language_">self</span>.initial_lr = <span class="number">1e-3</span>           <span class="comment"># 初始学习率</span></span><br><span class="line">        <span class="variable language_">self</span>.lr_scheduler_eps = <span class="number">1e-8</span>     <span class="comment"># 学习率调度器参数</span></span><br><span class="line">        <span class="variable language_">self</span>.lr_scheduler_patience = <span class="number">30</span>  <span class="comment"># 学习率调度器耐心值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 训练相关</span></span><br><span class="line">        <span class="variable language_">self</span>.num_epochs = <span class="number">500</span>            <span class="comment"># 训练轮数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_iterations_per_epoch = <span class="number">250</span>  <span class="comment"># 每个epoch的迭代次数</span></span><br><span class="line">        <span class="variable language_">self</span>.save_every = <span class="number">25</span>             <span class="comment"># 每多少个epoch保存一次</span></span><br><span class="line">        <span class="variable language_">self</span>.validate_every = <span class="number">5</span>          <span class="comment"># 每多少个epoch验证一次</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 优化器相关</span></span><br><span class="line">        <span class="variable language_">self</span>.weight_decay = <span class="number">3e-5</span>         <span class="comment"># 权重衰减</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 损失函数相关参数</span></span><br><span class="line">        <span class="variable language_">self</span>.deep_supervision_scales = [<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.25</span>, <span class="number">0.125</span>]  <span class="comment"># 深度监督的尺度</span></span><br></pre></td></tr></table></figure><h2 id="使用环境变量"><a href="#使用环境变量" class="headerlink" title="使用环境变量"></a>使用环境变量</h2><p>某些参数也可以通过环境变量设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export nnUNet_n_proc_DA=<span class="number">8</span>  <span class="comment"># 数据增强进程数</span></span><br><span class="line">nnUNetv2_train 001 2d <span class="number">0</span></span><br></pre></td></tr></table></figure><p>推荐使用方法3创建自定义trainer类，这样可以保持原始代码不变，同时实现参数的灵活修改。</p>]]></content>
      
      
      <categories>
          
          <category> 代码复现记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码复现记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码复现记录BSAFusion</title>
      <link href="/post/dai-ma-fu-xian-ji-lu-bsafusion/"/>
      <url>/post/dai-ma-fu-xian-ji-lu-bsafusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p>癌症影像档案馆下载脚本(TICA)</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TCIADownloader</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.session = requests.Session()</span><br><span class="line">        <span class="variable language_">self</span>.session.headers.update(&#123;</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36&#x27;</span></span><br><span class="line">        &#125;)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">read_manifest</span>(<span class="params">self, manifest_file</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;读取manifest文件&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(manifest_file, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            lines = f.readlines()</span><br><span class="line">        </span><br><span class="line">        config = &#123;&#125;</span><br><span class="line">        series_list = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;=&#x27;</span> <span class="keyword">in</span> line <span class="keyword">and</span> <span class="keyword">not</span> line.startswith(<span class="string">&#x27;1.3.6.1.4.1.14519&#x27;</span>):</span><br><span class="line">                key, value = line.split(<span class="string">&#x27;=&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">                config[key] = value</span><br><span class="line">            <span class="keyword">elif</span> line.startswith(<span class="string">&#x27;1.3.6.1.4.1.14519&#x27;</span>):</span><br><span class="line">                series_list.append(line)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> config, series_list</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">try_method_1_nbia_servlet</span>(<span class="params">self, series_uid, download_dir</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;方法1: 使用NBIA servlet&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;    方法1: NBIA Servlet&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        url = <span class="string">&quot;https://nbia.cancerimagingarchive.net/nbia-download/servlet/DownloadServlet&quot;</span></span><br><span class="line">        </span><br><span class="line">        param_combinations = [</span><br><span class="line">            &#123;<span class="string">&#x27;annotation&#x27;</span>: <span class="string">&#x27;true&#x27;</span>, <span class="string">&#x27;series&#x27;</span>: series_uid&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;includeAnnotation&#x27;</span>: <span class="string">&#x27;true&#x27;</span>, <span class="string">&#x27;series&#x27;</span>: series_uid&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;seriesInstanceUID&#x27;</span>: series_uid&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;SeriesInstanceUID&#x27;</span>: series_uid, <span class="string">&#x27;format&#x27;</span>: <span class="string">&#x27;zip&#x27;</span>&#125;,</span><br><span class="line">        ]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, params <span class="keyword">in</span> <span class="built_in">enumerate</span>(param_combinations):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;      尝试参数组合 <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(param_combinations)&#125;</span>&quot;</span>)</span><br><span class="line">                </span><br><span class="line">                response = <span class="variable language_">self</span>.session.get(url, params=params, stream=<span class="literal">True</span>, timeout=<span class="number">120</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                    filename = <span class="string">f&quot;<span class="subst">&#123;series_uid&#125;</span>_method1.zip&quot;</span></span><br><span class="line">                    filepath = os.path.join(download_dir, filename)</span><br><span class="line">                    </span><br><span class="line">                    total_size = <span class="built_in">int</span>(response.headers.get(<span class="string">&#x27;content-length&#x27;</span>, <span class="number">0</span>))</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">with</span> <span class="built_in">open</span>(filepath, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                        downloaded = <span class="number">0</span></span><br><span class="line">                        <span class="keyword">for</span> chunk <span class="keyword">in</span> response.iter_content(chunk_size=<span class="number">8192</span>):</span><br><span class="line">                            <span class="keyword">if</span> chunk:</span><br><span class="line">                                f.write(chunk)</span><br><span class="line">                                downloaded += <span class="built_in">len</span>(chunk)</span><br><span class="line">                                </span><br><span class="line">                                <span class="keyword">if</span> total_size &gt; <span class="number">0</span>:</span><br><span class="line">                                    progress = (downloaded / total_size) * <span class="number">100</span></span><br><span class="line">                                    <span class="built_in">print</span>(<span class="string">f&quot;\r      下载进度: <span class="subst">&#123;progress:<span class="number">.1</span>f&#125;</span>%&quot;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">                    </span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;\r      下载完成: <span class="subst">&#123;downloaded&#125;</span> bytes&quot;</span>)</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> downloaded &gt; <span class="number">1000</span>:  <span class="comment"># 至少1KB</span></span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span>, filepath</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        os.remove(filepath)</span><br><span class="line">                        </span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;      错误: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span>, <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">try_method_2_tcia_api</span>(<span class="params">self, series_uid, download_dir</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;方法2: 使用TCIA REST API&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;    方法2: TCIA REST API&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        url = <span class="string">&quot;https://services.cancerimagingarchive.net/services/v4/TCIA/query/getImage&quot;</span></span><br><span class="line">        </span><br><span class="line">        param_combinations = [</span><br><span class="line">            &#123;<span class="string">&#x27;SeriesInstanceUID&#x27;</span>: series_uid, <span class="string">&#x27;format&#x27;</span>: <span class="string">&#x27;zip&#x27;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;SeriesInstanceUID&#x27;</span>: series_uid&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;series&#x27;</span>: series_uid, <span class="string">&#x27;format&#x27;</span>: <span class="string">&#x27;zip&#x27;</span>&#125;,</span><br><span class="line">        ]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, params <span class="keyword">in</span> <span class="built_in">enumerate</span>(param_combinations):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;      尝试API参数组合 <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(param_combinations)&#125;</span>&quot;</span>)</span><br><span class="line">                </span><br><span class="line">                response = <span class="variable language_">self</span>.session.get(url, params=params, stream=<span class="literal">True</span>, timeout=<span class="number">300</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                    filename = <span class="string">f&quot;<span class="subst">&#123;series_uid&#125;</span>_method2.zip&quot;</span></span><br><span class="line">                    filepath = os.path.join(download_dir, filename)</span><br><span class="line">                    </span><br><span class="line">                    total_size = <span class="built_in">int</span>(response.headers.get(<span class="string">&#x27;content-length&#x27;</span>, <span class="number">0</span>))</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">with</span> <span class="built_in">open</span>(filepath, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                        downloaded = <span class="number">0</span></span><br><span class="line">                        <span class="keyword">for</span> chunk <span class="keyword">in</span> response.iter_content(chunk_size=<span class="number">8192</span>):</span><br><span class="line">                            <span class="keyword">if</span> chunk:</span><br><span class="line">                                f.write(chunk)</span><br><span class="line">                                downloaded += <span class="built_in">len</span>(chunk)</span><br><span class="line">                                </span><br><span class="line">                                <span class="keyword">if</span> total_size &gt; <span class="number">0</span>:</span><br><span class="line">                                    progress = (downloaded / total_size) * <span class="number">100</span></span><br><span class="line">                                    <span class="built_in">print</span>(<span class="string">f&quot;\r      下载进度: <span class="subst">&#123;progress:<span class="number">.1</span>f&#125;</span>%&quot;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">                    </span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;\r      下载完成: <span class="subst">&#123;downloaded&#125;</span> bytes&quot;</span>)</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> downloaded &gt; <span class="number">1000</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span>, filepath</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        os.remove(filepath)</span><br><span class="line">                        </span><br><span class="line">                <span class="keyword">elif</span> response.status_code == <span class="number">404</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;      系列未找到 (404)&quot;</span>)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">elif</span> response.status_code == <span class="number">401</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;      需要身份验证 (401)&quot;</span>)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;      HTTP错误: <span class="subst">&#123;response.status_code&#125;</span>&quot;</span>)</span><br><span class="line">                    </span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;      API错误: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span>, <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">try_method_3_direct_dicom</span>(<span class="params">self, series_uid, download_dir</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;方法3: 尝试直接DICOM下载&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;    方法3: 直接DICOM下载&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 首先获取系列中的图像列表</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            url = <span class="string">&quot;https://services.cancerimagingarchive.net/services/v4/TCIA/query/getSOPInstanceUIDs&quot;</span></span><br><span class="line">            params = &#123;<span class="string">&#x27;SeriesInstanceUID&#x27;</span>: series_uid, <span class="string">&#x27;format&#x27;</span>: <span class="string">&#x27;json&#x27;</span>&#125;</span><br><span class="line">            </span><br><span class="line">            response = <span class="variable language_">self</span>.session.get(url, params=params, timeout=<span class="number">60</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                sop_instances = response.json()</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> sop_instances <span class="keyword">and</span> <span class="built_in">len</span>(sop_instances) &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;      找到 <span class="subst">&#123;<span class="built_in">len</span>(sop_instances)&#125;</span> 个DICOM实例&quot;</span>)</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 创建系列目录</span></span><br><span class="line">                    series_dir = os.path.join(download_dir, <span class="string">f&quot;<span class="subst">&#123;series_uid&#125;</span>_dicom&quot;</span>)</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(series_dir):</span><br><span class="line">                        os.makedirs(series_dir)</span><br><span class="line">                    </span><br><span class="line">                    downloaded_count = <span class="number">0</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 下载前几个实例作为测试</span></span><br><span class="line">                    <span class="keyword">for</span> i, instance <span class="keyword">in</span> <span class="built_in">enumerate</span>(sop_instances[:<span class="number">3</span>]):  <span class="comment"># 只下载前3个作为测试</span></span><br><span class="line">                        sop_uid = instance.get(<span class="string">&#x27;SOPInstanceUID&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">                        <span class="keyword">if</span> sop_uid:</span><br><span class="line">                            success = <span class="variable language_">self</span>.download_single_dicom(series_uid, sop_uid, series_dir)</span><br><span class="line">                            <span class="keyword">if</span> success:</span><br><span class="line">                                downloaded_count += <span class="number">1</span></span><br><span class="line">                            </span><br><span class="line">                            <span class="keyword">if</span> i &gt;= <span class="number">2</span>:  <span class="comment"># 只测试前3个</span></span><br><span class="line">                                <span class="keyword">break</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> downloaded_count &gt; <span class="number">0</span>:</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">f&quot;      成功下载 <span class="subst">&#123;downloaded_count&#125;</span> 个DICOM文件&quot;</span>)</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span>, series_dir</span><br><span class="line">                        </span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;      DICOM下载错误: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span>, <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">download_single_dicom</span>(<span class="params">self, series_uid, sop_uid, series_dir</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;下载单个DICOM文件&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            url = <span class="string">&quot;https://services.cancerimagingarchive.net/services/v4/TCIA/query/getImage&quot;</span></span><br><span class="line">            params = &#123;</span><br><span class="line">                <span class="string">&#x27;SeriesInstanceUID&#x27;</span>: series_uid,</span><br><span class="line">                <span class="string">&#x27;SOPInstanceUID&#x27;</span>: sop_uid</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            response = <span class="variable language_">self</span>.session.get(url, params=params, stream=<span class="literal">True</span>, timeout=<span class="number">120</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                filename = <span class="string">f&quot;<span class="subst">&#123;sop_uid&#125;</span>.dcm&quot;</span></span><br><span class="line">                filepath = os.path.join(series_dir, filename)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(filepath, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    <span class="keyword">for</span> chunk <span class="keyword">in</span> response.iter_content(chunk_size=<span class="number">8192</span>):</span><br><span class="line">                        <span class="keyword">if</span> chunk:</span><br><span class="line">                            f.write(chunk)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> os.path.getsize(filepath) &gt; <span class="number">100</span>:  <span class="comment"># 至少100字节</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    os.remove(filepath)</span><br><span class="line">                    </span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;        单个DICOM下载错误: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_download_methods</span>(<span class="params">self, series_list, download_dir, test_count=<span class="number">3</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;测试不同的下载方法&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n测试前 <span class="subst">&#123;test_count&#125;</span> 个系列的下载方法...&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        working_methods = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, series_uid <span class="keyword">in</span> <span class="built_in">enumerate</span>(series_list[:test_count]):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;\n测试系列 <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;test_count&#125;</span>: <span class="subst">&#123;series_uid&#125;</span>&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 方法1: NBIA Servlet</span></span><br><span class="line">            success, filepath = <span class="variable language_">self</span>.try_method_1_nbia_servlet(series_uid, download_dir)</span><br><span class="line">            <span class="keyword">if</span> success:</span><br><span class="line">                working_methods.append((<span class="string">&#x27;method1&#x27;</span>, filepath))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;  ✓ 方法1 成功&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 方法2: TCIA API</span></span><br><span class="line">            success, filepath = <span class="variable language_">self</span>.try_method_2_tcia_api(series_uid, download_dir)</span><br><span class="line">            <span class="keyword">if</span> success:</span><br><span class="line">                working_methods.append((<span class="string">&#x27;method2&#x27;</span>, filepath))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;  ✓ 方法2 成功&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 方法3: 直接DICOM</span></span><br><span class="line">            success, filepath = <span class="variable language_">self</span>.try_method_3_direct_dicom(series_uid, download_dir)</span><br><span class="line">            <span class="keyword">if</span> success:</span><br><span class="line">                working_methods.append((<span class="string">&#x27;method3&#x27;</span>, filepath))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;  ✓ 方法3 成功&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;  ✗ 所有方法都失败&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> working_methods</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">download_all_series</span>(<span class="params">self, series_list, download_dir, working_method</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;使用找到的有效方法下载所有系列&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n使用方法 <span class="subst">&#123;working_method&#125;</span> 下载所有 <span class="subst">&#123;<span class="built_in">len</span>(series_list)&#125;</span> 个系列...&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        successful_downloads = <span class="number">0</span></span><br><span class="line">        failed_downloads = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, series_uid <span class="keyword">in</span> <span class="built_in">enumerate</span>(series_list):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;\n下载系列 <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(series_list)&#125;</span>: <span class="subst">&#123;series_uid&#125;</span>&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            success = <span class="literal">False</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> working_method == <span class="string">&#x27;method1&#x27;</span>:</span><br><span class="line">                success, _ = <span class="variable language_">self</span>.try_method_1_nbia_servlet(series_uid, download_dir)</span><br><span class="line">            <span class="keyword">elif</span> working_method == <span class="string">&#x27;method2&#x27;</span>:</span><br><span class="line">                success, _ = <span class="variable language_">self</span>.try_method_2_tcia_api(series_uid, download_dir)</span><br><span class="line">            <span class="keyword">elif</span> working_method == <span class="string">&#x27;method3&#x27;</span>:</span><br><span class="line">                success, _ = <span class="variable language_">self</span>.try_method_3_direct_dicom(series_uid, download_dir)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> success:</span><br><span class="line">                successful_downloads += <span class="number">1</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;  ✓ 下载成功&quot;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                failed_downloads += <span class="number">1</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;  ✗ 下载失败&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 添加延迟避免服务器过载</span></span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n=== 下载完成统计 ===&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;成功: <span class="subst">&#123;successful_downloads&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;失败: <span class="subst">&#123;failed_downloads&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;总计: <span class="subst">&#123;<span class="built_in">len</span>(series_list)&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> successful_downloads, failed_downloads</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extract_zip_files</span>(<span class="params">self, download_dir, extract_dir=<span class="string">&quot;tcia_extracted&quot;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;解压下载的ZIP文件&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(extract_dir):</span><br><span class="line">            os.makedirs(extract_dir)</span><br><span class="line">        </span><br><span class="line">        zip_files = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(download_dir) <span class="keyword">if</span> f.endswith(<span class="string">&#x27;.zip&#x27;</span>)]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> zip_files:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;没有找到ZIP文件&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n解压 <span class="subst">&#123;<span class="built_in">len</span>(zip_files)&#125;</span> 个ZIP文件...&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> zip_file <span class="keyword">in</span> zip_files:</span><br><span class="line">            zip_path = os.path.join(download_dir, zip_file)</span><br><span class="line">            extract_path = os.path.join(extract_dir, zip_file[:-<span class="number">4</span>])</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">with</span> zipfile.ZipFile(zip_path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> zip_ref:</span><br><span class="line">                    zip_ref.extractall(extract_path)</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;✓ 解压完成: <span class="subst">&#123;zip_file&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;✗ 解压失败 <span class="subst">&#123;zip_file&#125;</span>: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;主函数&quot;&quot;&quot;</span></span><br><span class="line">    manifest_file = <span class="string">&quot;Vestibular-Schwannooma-MC-RC manifest August 2023.tcia&quot;</span></span><br><span class="line">    download_dir = <span class="string">&quot;tcia_downloads&quot;</span></span><br><span class="line">    extract_dir = <span class="string">&quot;tcia_extracted&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;TCIA数据下载器 v3.0&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">60</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 检查manifest文件</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(manifest_file):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;错误: 找不到manifest文件: <span class="subst">&#123;manifest_file&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;请确保manifest文件在当前目录下&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建下载目录</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(download_dir):</span><br><span class="line">        os.makedirs(download_dir)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 初始化下载器</span></span><br><span class="line">        downloader = TCIADownloader()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 读取manifest文件</span></span><br><span class="line">        config, series_list = downloader.read_manifest(manifest_file)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;从manifest文件读取到 <span class="subst">&#123;<span class="built_in">len</span>(series_list)&#125;</span> 个系列&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> series_list:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;错误: manifest文件中没有找到系列UID&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 测试下载方法</span></span><br><span class="line">        working_methods = downloader.test_download_methods(series_list, download_dir, test_count=<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> working_methods:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\n❌ 所有下载方法都失败了&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;可能的原因:&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;1. 网络连接问题&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;2. TCIA服务器暂时不可用&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;3. 需要登录TCIA账户&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;4. 数据集可能已被移除或限制访问&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n✅ 找到 <span class="subst">&#123;<span class="built_in">len</span>(working_methods)&#125;</span> 种有效的下载方法&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 选择最佳方法</span></span><br><span class="line">        best_method = working_methods[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># 使用第一个成功的方法</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 询问是否继续下载所有文件</span></span><br><span class="line">        choice = <span class="built_in">input</span>(<span class="string">f&quot;\n是否使用找到的方法下载所有 <span class="subst">&#123;<span class="built_in">len</span>(series_list)&#125;</span> 个系列? (y/n): &quot;</span>).lower().strip()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> choice == <span class="string">&#x27;y&#x27;</span>:</span><br><span class="line">            successful, failed = downloader.download_all_series(series_list, download_dir, best_method)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> successful &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># 询问是否解压文件</span></span><br><span class="line">                extract_choice = <span class="built_in">input</span>(<span class="string">f&quot;\n成功下载了 <span class="subst">&#123;successful&#125;</span> 个文件。是否解压ZIP文件? (y/n): &quot;</span>).lower().strip()</span><br><span class="line">                <span class="keyword">if</span> extract_choice == <span class="string">&#x27;y&#x27;</span>:</span><br><span class="line">                    downloader.extract_zip_files(download_dir, extract_dir)</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;\n文件已解压到: <span class="subst">&#123;extract_dir&#125;</span>&quot;</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;\n✅ 下载完成!&quot;</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;📁 下载目录: <span class="subst">&#123;download_dir&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> extract_choice == <span class="string">&#x27;y&#x27;</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;📁 解压目录: <span class="subst">&#123;extract_dir&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;\n❌ 没有成功下载任何文件&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;下载已取消&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">except</span> KeyboardInterrupt:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\n\n⚠️  下载被用户中断&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n❌ 发生未知错误: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">import</span> traceback</span><br><span class="line">        traceback.print_exc()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_help</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示帮助信息&quot;&quot;&quot;</span></span><br><span class="line">    help_text = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">TCIA数据下载器使用说明:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">1. 准备工作:</span></span><br><span class="line"><span class="string">   - 将TCIA manifest文件保存为 &#x27;manifest-1692206474218.tcia&#x27;</span></span><br><span class="line"><span class="string">   - 确保网络连接正常</span></span><br><span class="line"><span class="string">   - 安装Python依赖: pip install requests</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2. 运行方式:</span></span><br><span class="line"><span class="string">   python tcia_downloader.py</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">3. 下载过程:</span></span><br><span class="line"><span class="string">   - 程序会自动测试多种下载方法</span></span><br><span class="line"><span class="string">   - 找到有效方法后会询问是否继续下载全部文件</span></span><br><span class="line"><span class="string">   - 下载完成后可选择是否解压文件</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">4. 输出目录:</span></span><br><span class="line"><span class="string">   - tcia_downloads/     : 下载的原始文件</span></span><br><span class="line"><span class="string">   - tcia_extracted/     : 解压后的DICOM文件</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">5. 注意事项:</span></span><br><span class="line"><span class="string">   - 医学影像文件通常很大，确保有足够磁盘空间</span></span><br><span class="line"><span class="string">   - 下载可能需要很长时间，请保持网络连接稳定</span></span><br><span class="line"><span class="string">   - 如果某些文件下载失败，可以重新运行程序（会跳过已下载的文件）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">6. 故障排除:</span></span><br><span class="line"><span class="string">   - 如果所有方法都失败，可能需要先登录TCIA网站</span></span><br><span class="line"><span class="string">   - 检查防火墙设置是否阻止了连接</span></span><br><span class="line"><span class="string">   - 尝试使用VPN或更换网络环境</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(help_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_dependencies</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;检查依赖项&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">import</span> requests</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">except</span> ImportError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;❌ 缺少依赖项: requests&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;请运行: pip install requests&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_disk_space</span>(<span class="params">download_dir, estimated_size_gb=<span class="number">50</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;检查磁盘空间&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">import</span> shutil</span><br><span class="line">        free_space = shutil.disk_usage(download_dir)[<span class="number">2</span>] / (<span class="number">1024</span>**<span class="number">3</span>)  <span class="comment"># GB</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> free_space &lt; estimated_size_gb:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;⚠️  磁盘空间可能不足&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;可用空间: <span class="subst">&#123;free_space:<span class="number">.1</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;预估需要: <span class="subst">&#123;estimated_size_gb&#125;</span> GB&quot;</span>)</span><br><span class="line">            choice = <span class="built_in">input</span>(<span class="string">&quot;是否继续? (y/n): &quot;</span>).lower().strip()</span><br><span class="line">            <span class="keyword">return</span> choice == <span class="string">&#x27;y&#x27;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;✅ 磁盘空间充足: <span class="subst">&#123;free_space:<span class="number">.1</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_download_summary</span>(<span class="params">download_dir, series_list, successful_count, failed_count</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建下载摘要文件&quot;&quot;&quot;</span></span><br><span class="line">    summary_file = os.path.join(download_dir, <span class="string">&quot;download_summary.txt&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(summary_file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="string">&quot;TCIA数据下载摘要\n&quot;</span>)</span><br><span class="line">            f.write(<span class="string">&quot;=&quot;</span> * <span class="number">50</span> + <span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line">            f.write(<span class="string">f&quot;下载时间: <span class="subst">&#123;time.strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)&#125;</span>\n&quot;</span>)</span><br><span class="line">            f.write(<span class="string">f&quot;总系列数: <span class="subst">&#123;<span class="built_in">len</span>(series_list)&#125;</span>\n&quot;</span>)</span><br><span class="line">            f.write(<span class="string">f&quot;成功下载: <span class="subst">&#123;successful_count&#125;</span>\n&quot;</span>)</span><br><span class="line">            f.write(<span class="string">f&quot;失败数量: <span class="subst">&#123;failed_count&#125;</span>\n&quot;</span>)</span><br><span class="line">            f.write(<span class="string">f&quot;成功率: <span class="subst">&#123;(successful_count/<span class="built_in">len</span>(series_list)*<span class="number">100</span>):<span class="number">.1</span>f&#125;</span>%\n\n&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 列出下载的文件</span></span><br><span class="line">            downloaded_files = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(download_dir) </span><br><span class="line">                              <span class="keyword">if</span> f.endswith(<span class="string">&#x27;.zip&#x27;</span>) <span class="keyword">or</span> f.endswith(<span class="string">&#x27;.dcm&#x27;</span>)]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> downloaded_files:</span><br><span class="line">                f.write(<span class="string">&quot;已下载文件列表:\n&quot;</span>)</span><br><span class="line">                f.write(<span class="string">&quot;-&quot;</span> * <span class="number">30</span> + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">                <span class="keyword">for</span> file <span class="keyword">in</span> <span class="built_in">sorted</span>(downloaded_files):</span><br><span class="line">                    file_path = os.path.join(download_dir, file)</span><br><span class="line">                    file_size = os.path.getsize(file_path) / (<span class="number">1024</span>*<span class="number">1024</span>)  <span class="comment"># MB</span></span><br><span class="line">                    f.write(<span class="string">f&quot;<span class="subst">&#123;file:&lt;<span class="number">50</span>&#125;</span> <span class="subst">&#123;file_size:&gt;<span class="number">8.1</span>f&#125;</span> MB\n&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;📋 下载摘要已保存到: <span class="subst">&#123;summary_file&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;⚠️  无法创建下载摘要: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">import</span> sys</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 检查命令行参数</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sys.argv) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> sys.argv[<span class="number">1</span>] == <span class="string">&#x27;--help&#x27;</span> <span class="keyword">or</span> sys.argv[<span class="number">1</span>] == <span class="string">&#x27;-h&#x27;</span>:</span><br><span class="line">            show_help()</span><br><span class="line">            sys.exit(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 检查依赖项</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> check_dependencies():</span><br><span class="line">        sys.exit(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 运行主程序</span></span><br><span class="line">    main()</span><br><span class="line"></span><br><span class="line">                </span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="训练截图"><a href="#训练截图" class="headerlink" title="训练截图"></a>训练截图</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-08-01_17-34-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-08-01_17-34-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-08-01_17-34-30"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-08-01_18-43-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-08-01_18-43-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-08-01_18-43-52"></p><h2 id="融合结果"><a href="#融合结果" class="headerlink" title="融合结果"></a>融合结果</h2><p><strong>CT-MRI</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="0"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="1"></p><p><strong>PET-MRI</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/PET1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/PET1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="PET1"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/PET2.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/PET2.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="PET2"></p><p><strong>SPECT-MRI</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/SPECT1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/SPECT1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="SPECT1"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/SPECT2.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/SPECT2.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="SPECT2"></p>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BSAFusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-modal disease segmentation with continual learning and adaptive decision fusion</title>
      <link href="/post/multi-modal-disease-segmentation-with-continual-learning-and-adaptive-decision-fusion/"/>
      <url>/post/multi-modal-disease-segmentation-with-continual-learning-and-adaptive-decision-fusion/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Multi-modal disease segmentation</strong> is essential for the diagnosis and treatment of patients. Advanced algorithms have been proposed, however, two challenging issues remain unsolved, i.e., lacked knowledge share and limited modal relation. To this end, we develop a novel framework for multi-modal disease segmentation. It is based on improved continual learning and adaptive decision fusion. Specifically, continual learning with 𝑘-means sampling is developed to highlight knowledge share from multi-modal medical images. In addition, we propose an adaptive decision fusion technique that uses the Naive Bayesian algorithm to improve the relationship between different modalities. To evaluate our proposed model, we chose two typical tasks, i.e., myocardial pathology segmentation and brain tumor segmentation. Four benchmark datasets, i.e., myocardial pathology segmentation challenge 2020 (MyoPS 2020), brain tumor segmentation challenge 2018 (BraTS 2018), BraTS 2019, and BraTS 2020, are utilized to train and test our framework. Both the qualitative and quantitative results demonstrate that our proposed model is effective and has advantages over peer state-of-the-art (SOTA) methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>多模态疾病分割对于患者的诊断和治疗至关重要。尽管已经提出了先进的算法，但仍有两个挑战性问题未解决，即缺乏知识共享和模态关系有限。为此，我们开发了一种新颖的多模态疾病分割框架，该框架基于<strong>改进的连续学习和自适应决策融合</strong>。具体来说，利用𝑘-means采样的连续学习被开发用于突出多模态医学图像中的知识共享。此外，我们提出了一种自适应决策融合技术，利用朴素贝叶斯算法改善不同模态之间的关系。为了评估我们提出的模型，我们选择了两个典型任务，即心肌病理分割和脑肿瘤分割。四个基准数据集，即心肌病理分割挑战赛2020 (MyoPS 2020)、脑肿瘤分割挑战赛2018 (BraTS 2018)、BraTS 2019和BraTS 2020被用于训练和测试我们的框架。定性和定量结果均表明我们提出的模型是有效的，并且相较于同行的先进方法（SOTA）具有优势。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p><strong>多模态疾病分割</strong>对患者的诊断和治疗至关重要，但目前仍面临两大挑战。一方面，现有基于监督学习网络和自监督学习的疾病分割模型，前者依赖大量手动标注，耗费人力和时间；后者虽能在标签有限时提升分割性能，但存在知识共享不足的问题。另一方面，不同模态间的关系有限，使得模型在处理多模态数据时，因模态差异大导致性能下降。</p><p>当前，持续学习和自适应融合技术为解决上述问题带来新思路。持续学习可帮助模型保留知识，自适应融合能增强模态间的关系。然而，尚未有研究将持续学习应用于多模态疾病分割。</p><p>基于此，作者开发了一种名为CLBF - Net的新型框架，该框架基于改进的持续学习和自适应决策融合，旨在突出多模态医学图像的知识共享和模态关系，以提高多模态疾病分割的效果，这也是撰写本文的目的。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><h3 id="多模态医学图像分割方法"><a href="#多模态医学图像分割方法" class="headerlink" title="多模态医学图像分割方法"></a>多模态医学图像分割方法</h3><ul><li><strong>基于监督学习网络的方法</strong>：该类方法通常基于监督学习网络自动提取特征并决策，如Martin等人提出的两阶段网络用于准确心肌分割和小感兴趣区域检测；Zhou等人开发的多阶段模型利用额外约束信息提高分割精度；Li等人采用两个并行卷积神经网络提取各模态特定特征。不过，这些方法依赖大量手动标签，收集过程耗时费力。</li><li><strong>基于自监督学习的方法</strong>：此类型通过自监督学习将多模态图像编码为嵌入空间，例如Wang等人开发的自动加权框架利用强化学习优化自监督学习层之间的交互；Yang等人引入的灵活模型能够整合多个多模态输入；Shi等人开发的自蒸馏方法在像素和语义层面平衡不同模态的优化。虽然自监督学习在标签有限时能提升分割性能，但仍未解决知识共享不足和模态关系有限的问题。</li></ul><h3 id="持续学习和自适应融合的发展"><a href="#持续学习和自适应融合的发展" class="headerlink" title="持续学习和自适应融合的发展"></a>持续学习和自适应融合的发展</h3><ul><li><strong>持续学习</strong>：能帮助模型随时间保留知识，常见策略是基于排练的方法，如黑暗经验回放（DER），通过在缓冲区存储先前数据或特征并在后续训练中重放来保留知识。</li><li><strong>自适应融合</strong>：被证明可有效增强模态关系，例如Zhu等人基于图卷积设计的自适应融合块用于脑肿瘤分割；Mu等人应用的自适应融合策略结合互补预测，有效处理病变因颜色不一致导致的显著变异性。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><h3 id="框架架构"><a href="#框架架构" class="headerlink" title="框架架构"></a>框架架构</h3><p>主要包含多模态持续预训练学习和自适应决策融合微调两个组件：</p><ul><li><strong>多模态持续预训练学习</strong>：各模态输入按顺序输入到具有持续预训练学习的骨干网络，生成不同的模态模型$C_n$。使用掩码图像预训练任务从多模态数据中提取通用表示，为防止灾难性遗忘，用基于排练的持续学习策略，通过K-means 采样将先前模态数据的一小部分存储在缓冲区。</li><li><strong>自适应决策融合微调</strong>：采用朴素贝叶斯算法的决策融合技术，增强不同模态之间的关系。将多模态数据并行输入到预训练的编码器和解码器以得到预测可能性掩码，通过贝叶斯推理计算每个预测掩码的可靠性，将可靠性值归一化转换为权重，通过加权求和得到像素分数。</li></ul><h3 id="骨干网络"><a href="#骨干网络" class="headerlink" title="骨干网络"></a>骨干网络</h3><p>使用三个特定维度的分词器将医学数据转换为令牌序列，标准视觉Transformer（ViT）作为编码器进行序列到序列的表示学习。遵循掩码自动编码器（MAE）方法，随机掩码50%的令牌序列，仅将未掩码的令牌输入到编码器，然后通过基于Transformer的解码器重建缺失的令牌，使用均方误差（MSE）损失确保原始图像和重建图像在掩码区域的一致性。</p><h3 id="多模态持续学习"><a href="#多模态持续学习" class="headerlink" title="多模态持续学习"></a>多模态持续学习</h3><p>采用顺序预训练方法，每个阶段专注于特定成像模态，防止模态数据冲突，但可能导致灾难性遗忘，因此使用基于排练的持续学习保留先验知识。在每个阶段，预训练包括使用当前模态数据的掩码图像建模（MIM）和辅助特征蒸馏任务。</p><h3 id="自适应决策融合"><a href="#自适应决策融合" class="headerlink" title="自适应决策融合"></a>自适应决策融合</h3><p>基于不同模态的预测掩码，使用贝叶斯推理确定其可靠性，将可靠性值归一化为权重，结合预测概率计算背景样本分数，以提高多模态疾病分割的鲁棒性。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_16-07-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_16-07-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-31_16-07-48"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_16-08-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_16-08-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-31_16-08-15"></p><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ol><li><strong>实验数据集</strong>：使用了四个高质量且开放获取的数据集，分别是<strong>MyoPS 2020</strong>、<strong>BraTS 2018</strong>、<strong>BraTS 2019</strong>和BraTS 2020。其中，MyoPS 2020用于心肌病理分割，其余三个数据集用于脑肿瘤分割。</li><li><strong>评估标准</strong>：采用了Dice分数、Hausdorff距离（HD）、准确率、精确率、灵敏度、特异度和F1分数等指标来评估模型性能。同时，通过浮点运算次数（FLOPs）、参数数量和每秒帧数（FPS）来衡量模型的时间复杂度和计算成本。</li><li>实验设置<ul><li><strong>硬件和软件环境</strong>：硬件使用NVIDIA RTX - 3090，软件环境为PyTorch 11.1和Python 3.7。</li><li><strong>预训练阶段</strong>：设置训练轮数（epoch）为500，优化器为AdamW，初始学习率为1e - 4，在第200和400个epoch后学习率降低为原来的0.1倍。批量大小为24，采样数K固定为5。所有图像调整为512×512像素。</li><li><strong>微调阶段</strong>：继续使用AdamW优化器，根据具体下游任务调整超参数，损失权重λu和λf分别设置为0.4和0.6。混合策略（mixup strategy）中，持续掩码向量λc设置为0.5。</li></ul></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><ul><li><strong>分割实验</strong>：在四个数据集上进行多模态疾病分割实验，分为心肌病理分割和脑肿瘤分割两个任务。设置十个随机种子评估模型的鲁棒性。结果显示，模型在脑肿瘤分割数据集上的表现优于心肌病理分割数据集。</li><li><strong>对比实验</strong>：将模型与其他相关工作在心肌病理和脑肿瘤分割任务上进行对比。定量结果表明，该模型在多个指标上优于之前的SOTA方法；定性结果显示，模型的预测掩码与真实标签的相似度更高。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_16-10-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_16-10-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-31_16-10-25"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_16-10-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_16-10-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-31_16-10-54"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li>消融实验<ul><li><strong>持续学习的贡献</strong>：分析了有无持续学习、特征蒸馏（FD）、k - means采样和混合策略（MS）对模型性能的影响。结果表明，持续学习能够有效促进多模态知识共享，提升多模态疾病分割性能。</li><li><strong>自适应决策融合的贡献</strong>：分析了有无自适应决策融合（ADF）以及与其他融合方法（如平均权重（AW）、多特征推理块（MFIB）和深度标签融合（DLF））的对比。结果显示，基于贝叶斯算法的决策融合能够有效融合多模态掩码的不确定分数，提高分割性能。</li></ul></li><li><strong>超参数优化</strong>：分析了Transformer编码器中单通道特征提取模块的数量NS和分类分支的数量NM对模型性能的影响。结果表明，NS &#x3D; 8和NL &#x3D; 8时模型性能最佳。同时，模型在计算资源使用上实现了性能和效率的平衡。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_16-11-27.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_16-11-27.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-31_16-11-27"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>者提出了基于持续学习和贝叶斯融合的CLB -Net框架用于多模态疾病分割，得出以下结论：</p><ol><li>实验在心肌病理分割和脑肿瘤分割两个典型任务、四个基准数据集上进行，结果表明该框架有效，相比其他先进算法具有优势。</li><li>持续学习结合k - 均值采样增强了多模态知识共享，贝叶斯自适应决策融合技术提升了不同模态间的关系，有效改善了多模态疾病分割效果。</li><li>指出当前研究仍存在处理多模态数据计算资源和时间需求大、涉及患者隐私保护和模型安全等挑战。未来计划结合多中心临床数据、减少模型参数、提高临床数据传输安全性来优化框架。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Continual Learning </tag>
            
            <tag> Bayesian Fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-modality medical image segmentation via adversarial learning with CV energy functional</title>
      <link href="/post/multi-modality-medical-image-segmentation-via-adversarial-learning-with-cv-energy-functional/"/>
      <url>/post/multi-modality-medical-image-segmentation-via-adversarial-learning-with-cv-energy-functional/</url>
      
        <content type="html"><![CDATA[<blockquote><p>部分难懂</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Medical image processing methods based on deep learning have gradually become mainstream. Automatic segmentation of brain tumor from multi-modality magnetic resonance images (MRI) using deep learning method is the key to the diagnosis of gliomas. In our hybrid network, the proposed neural network framework consists of Segmentor and Critic. A new Transformer-CV-Unet (TCUnet) is introduced to gain more semantic features. We employ the new TCUnet as the generator of GAN to complete the segmentation task to increase robustness and efficiency. With a generator to segment the target images, Critic is then built to tightly merge the latent representation with hierarchical characteristics from each modality. Moreover, a hybrid adversarial with multi-phase CV energy functional is introduced. Our hybrid network, AdvTCUnet, combines the advantages of both methods. Furthermore, extensive experiments on BraTs 19–21 show that the proposed model performs better than existing state-of-the-art techniques for segmenting brain tumor MRI (e.g., the Dice Similarity Coefficient of ET, WT and TC on BraTs 21 can reach 0.8642, 0.9303 and 0.9060, respectively).</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>基于深度学习的医学图像处理方法逐渐成为主流。使用深度学习方法对多模态磁共振成像（MRI）进行脑肿瘤自动分割是诊断胶质瘤的关键。在我们的混合网络中，提出的神经网络框架由Segmentor和Critic组成。引入了一种新的Transformer-CV-Unet（TCUnet），以获得更多语义特征。我们采用新的TCUnet作为GAN的生成器来完成分割任务，以提高鲁棒性和效率。通过生成器分割目标图像，随后构建Critic以紧密融合每种模态的层次特征的潜在表示。此外，还引入了带有多相CV能量泛函的混合对抗。我们的混合网络AdvTCUnet结合了两种方法的优点。此外，在BraTs 19-21上的大量实验表明，所提出的模型在分割脑肿瘤MRI方面优于现有的最先进技术（例如，在BraTs 21上的ET、WT和TC的Dice相似系数分别可以达到0.8642、0.9303和0.9060）。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于多模态脑肿瘤磁共振图像（MRI）分割，旨在解决现有方法的局限性，提升分割性能，其研究背景如下：</p><ul><li><strong>多模态成像的重要性</strong>：多模态图像分割在医学图像分析中表现出色，能提高多种图像处理任务的性能。MRI作为常用的临床检查方法，不同模态（T1、T1ce、T2、FlAIR）可提供不同信息，有助于全面理解病变情况。</li><li><strong>传统方法的局限性</strong>：目前多数医学图像分割算法使用单模态数据和传统变分技术，传统变分方法在语义图像分割上存在局限，缺乏特征表示能力。</li><li><strong>深度学习方法的发展</strong>：基于深度学习的医学图像处理方法逐渐成为主流，生成模型的分割技术受到关注。将传统变分方法与卷积神经网络结合，能取得较好结果，因此作者认为结合传统变分技术与生成对抗网络（GAN）可带来更优效果。</li><li><strong>Transformer的应用</strong>：Transformer在自然语言处理和计算机视觉领域表现良好，相关改进模型不断涌现。但现有方法在3D医学图像分割中处理局部和全局信息存在挑战。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多模态学习</strong>：在多模态融合方面取得进展，如Wan等人提出新的无监督多视图表示学习方法，Han等人提出多模态动态分类技术。</li><li><strong>图像分割</strong>：传统变分方法推动了图像分割技术发展，CV模型等有效辅助图像分割。基于深度学习的医学图像处理方法逐渐成为主流，生成模型的分割技术受关注。</li><li><strong>医学图像分割</strong>：多模态或跨模态分割技术取得成功，一些方法在脑肿瘤、肺部肿瘤等分割任务中应用，Transformer方法在医学图像分割中表现良好。</li><li><strong>生成对抗网络</strong>：在医学图像分割领域广泛应用，如Chen等人提出双注意力域自适应分割网络。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_09-14-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_09-14-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-31_09-14-14"></p><p>AdvTCUnet由Segmentor（分割器）和Critic（判别器）两部分组成。Segmentor采用改进的Unet网络TCUnet作为生成器完成分割任务，Critic用于将各模态的潜在表示与分层特征紧密融合。</p><h3 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h3><ul><li>Transformer - CV - Unet（TCUnet）：<ul><li><strong>特征处理</strong>：使用双层ViT模块扩大感受野，获取全局特征信息，克服卷积层的局限性。通过TransBTS中的特征表示方法处理输入，将得到的特征表示序列输入到典型的Transformer块（由Multi - Head Attention（MHA）块和Feed Forward Network（FFN）组成）。</li><li><strong>3D注意力机制</strong>：将注意力从2D转换为3D，嵌入到Segmentor的解码器部分，负责通道注意力和空间注意力。编码器提取的第i个特征与解码器的第i + 1个特征相连，经过通道注意力和空间注意力后输入到第i个解码层。</li></ul></li><li>基于多相CV能量泛函的损失函数</li></ul><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><h3 id="1-数据集"><a href="#1-数据集" class="headerlink" title="1. 数据集"></a>1. 数据集</h3><p>使用了MICCAI脑肿瘤分割竞赛的BraTs数据集，该数据集包含Flair、T1、T1ce和T2四种成像模态。各数据集样本数量不同，如BraTs 21包含1251个训练图像、219个验证图像和530个测试图像；BraTs 20训练集有369个样本，其中295个用于训练和验证，74个用于测试；BraTs 19训练集随机选取268个样本用于训练和验证，67个用于测试。</p><h3 id="2-评估指标和对比方法"><a href="#2-评估指标和对比方法" class="headerlink" title="2. 评估指标和对比方法"></a>2. 评估指标和对比方法</h3><ul><li><strong>评估指标</strong>：选择Dice和Hausdorff作为评估指标。Dice系数衡量分割结果与真实标签的重叠程度；Hausdorff距离表示真实标签与分割结果之间的最大不匹配度。</li><li>对比方法<ul><li><strong>多模态分割方法</strong>：包括Unet、Att - Unet、Li等人的多步级联网络、互惠对抗学习脑肿瘤分割方法（Peiris和Chen）和E1D3。</li><li><strong>基于Transformer的模型</strong>：UNetFormer、TransBTS和nnFormer。</li></ul></li></ul><h3 id="3-实现细节"><a href="#3-实现细节" class="headerlink" title="3. 实现细节"></a>3. 实现细节</h3><ul><li><strong>框架</strong>：使用PyTorch实现多模态分割的神经网络。</li><li><strong>优化器</strong>：采用Ranger优化器，学习率为1e−4。</li><li><strong>训练参数</strong>：训练轮数（epochs）设为200，批次大小（batch size）设为2。在2个NVIDIA TITAN RTX GPU上训练AdvTCUnet，在1个相同GPU上进行测试。</li><li><strong>损失函数超参数</strong>：设置$\lambda_1&#x3D;0.5$，$\lambda_2&#x3D;1$。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><ul><li><strong>模型性能</strong>：在BraTs 19 - 21数据集上的实验表明，该模型具有较好的鲁棒性和适用性。例如，在BraTs 21数据集上，增强肿瘤（ET）、全肿瘤（WT）和肿瘤核心（TC）的Dice系数分别达到0.8642、0.9303和0.9060。</li><li><strong>与其他方法对比</strong>：与多种先进的多模态分割方法和基于Transformer的模型相比，该模型在大多数情况下表现更优，且能大大减少模型训练时间。</li><li><strong>超参数分析</strong>：通过调整损失函数中各项的系数，发现适当减小$\lambda_1$同时保持$\lambda_2$不变，能提高分割效果。最终确定训练过程中$\lambda_1&#x3D;0.5$， $\lambda_2&#x3D;1$。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_09-20-55.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_09-20-55.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-31_09-20-55"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_09-21-10.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_09-21-10.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-31_09-21-10"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>不同损失组合</strong>：对比了Dice损失、Dice联合$L_1$损失、Dice联合ENE损失以及Dice联合$L_1$和ENE损失等不同组合下的模型性能，验证了各损失项的有效性。</li><li><strong>不同模态组合</strong>：从单一模态（T1）开始，逐步添加不同模态组合，结果表明使用提出的分割模型能提高分割效率和准确性，证明了各模态数据在模型中的有效性。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_09-23-02.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-31_09-23-02.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-31_09-23-02"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出基于对抗学习的多模态脑肿瘤MRI分割模型，得出以下结论：</p><ol><li><strong>模型效果显著</strong>：在BraTs 21上分割结果的Dice系数（ET、WT和TC）分别达0.8642、0.9303和0.9060，表现优于现有技术。</li><li><strong>损失函数有效</strong>：基于CV能量函数的损失函数不仅提升分割效果，还使网络更稳定、泛化能力更强。</li><li><strong>对抗学习有益</strong>：对抗学习可防止分割器过拟合，分割器和判别器通过极小 - 极大博弈达成一致，提升了分割网络性能。</li><li><strong>待改进方向</strong>：模型虽整体效果好，但处理不同区域边缘位置欠佳，且应对数据集不平衡能力不足，未来考虑引入平衡模块和集成学习解决问题。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Entropy-aware dynamic path selection network for multi-modality medical image fusion</title>
      <link href="/post/entropy-aware-dynamic-path-selection-network-for-multi-modality-medical-image-fusion/"/>
      <url>/post/entropy-aware-dynamic-path-selection-network-for-multi-modality-medical-image-fusion/</url>
      
        <content type="html"><![CDATA[<p>上海大学</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Deep learning has achieved significant success in multi-modality medical image fusion (MMIF). Nevertheless, the distribution of spatial information varies across regions within a medical image. Current methods consider the medical image as a whole, leading to uneven fusion and susceptibility to artifacts in edge regions. To address this problem,we delve into regional information fusion and introduce an entropy-aware dynamic path selection network (EDPSN). Specifically, we introduce a novel edge enhancement module (EEM) to mitigate artifacts in edge regions through central concentration gradient (CCG). Additionally, an entropy-aware division (ED) module is designed to delineate the spatial information levels of distinct regions in the image through entropy convolution. Finally, a dynamic path selection (DPS) module is introduced to enable adaptive fusion of diverse spatial information regions. Experimental comparisons with some state-of-the-art image fusion methods illustrate the outstanding performance of the EDPSN in three datasets encompassing MRI-CT, MRI-PET, and MRI-SPECT. Moreover, the robustness of the proposed method is validated on the CHAOS dataset, and the clinical value of the proposed method is validated by sixteen doctors and medical students.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>深度学习在多模态医学图像融合（MMIF）中取得了显著成功。然而，医学图像中各区域的空间信息分布存在差异。现有方法将医学图像视为整体，导致融合不均匀，并在边缘区域易出现伪影。为解决这一问题，我们深入研究区域信息融合，并引入一种熵感知动态路径选择网络（EDPSN）。具体来说，我们引入了一种新颖的边缘增强模块（EEM），通过中心集中梯度（CCG）来减轻边缘区域的伪影。此外，设计了一个熵感知分割（ED）模块，通过熵卷积来勾画图像中不同区域的空间信息水平。最后，引入了动态路径选择（DPS）模块，以实现对多样空间信息区域的自适应融合。与一些最先进的图像融合方法的实验比较表明，EDPSN在涵盖MRI-CT、MRI-PET和MRI-SPECT的三个数据集中的表现非常出色。此外，在CHAOS数据集上验证了所提出方法的鲁棒性，并通过十六位医生和医学生验证了所提出方法的临床价值</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>多模态医学图像融合（MMIF）技术对临床诊断至关重要。不同模态的医学图像，如<strong>CT、MRI、PET和SPECT</strong>，能提供不同的关键信息，但因硬件限制，这些图像在临床应用中存在分辨率低、纹理模糊和有噪声等问题。因此，医学图像处理技术被广泛应用以提升临床诊断准确性，MMIF技术作为其中一种图像增强方法，可将不同来源的医学图像融合，弥补单模态图像信息的局限。</p><p>目前，MMIF方法主要包括基于空间域、变换域和深度学习的方法。然而，现有的深度学习方法未充分考虑图像内不同区域空间信息分布的差异，导致图像融合不均，融合图像的边缘易出现伪影。</p><p>为解决这一问题，本文提出了熵感知动态路径选择网络（EDPSN）。该网络通过边缘增强模块（EEM）减少边缘伪影，利用熵感知划分（ED）模块探索不同区域的空间信息分布，设计动态路径选择（DPS）模块对不同空间信息区域进行自适应融合，旨在有效缓解融合不均的问题，提高多模态医学图像融合的质量。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><p><strong>传统方法</strong>：主要包括空间域和变换域方法。空间域方法如PCA、IHS等适用于模态特征相近的图像，但易在融合边界产生伪影；变换域方法如DWT、NSCT等通用性强，但可能引入模糊。</p><p><strong>深度学习方法</strong>：应用广泛且成果显著。如CNN、GAN等被用于图像融合，部分方法通过设计不同图像、模型结构或创新融合任务来提升效果。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-25-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-25-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-30_14-25-32"></p><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-26-31.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-26-31.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-30_14-26-31"></p><h3 id="模型整体概述"><a href="#模型整体概述" class="headerlink" title="模型整体概述"></a>模型整体概述</h3><p>EDPSN通过三个阶段实现多模态医学图像的均匀融合，即增强图像边缘、划分不同信息程度的区域以及自适应融合不同信息分布的区域。该网络主要由<strong>边缘增强模块（Edge Enhancement Module，EEM）</strong>、**熵感知划分模块（Entropy-aware Division，ED）<strong>和</strong>动态路径选择模块（Dynamic Path Selection，DPS）**组成。</p><h3 id="模型模块介绍"><a href="#模型模块介绍" class="headerlink" title="模型模块介绍"></a>模型模块介绍</h3><ol><li><strong>边缘增强模块（EEM）</strong>：为减少多模态医学图像融合中边缘区域的伪影，提出了基于中心集中梯度（Central Concentration Gradient，CCG）的EEM。该模块将PET图像转换到HSI空间，对I通道进行CCG处理，计算梯度值并得到梯度图，将梯度图与I通道相加得到增强结果，最后将增强后的I通道与H和S通道堆叠并逆变换到RGB颜色空间，从而有效增强PET图像的边缘信息，为多模态医学图像融合提供更准确的边缘特征。</li><li><strong>熵感知划分模块（ED）</strong>：该模块利用熵卷积来捕捉图像中不同区域的空间信息分布。具体做法是计算每个像素附近3×3范围内的整体熵值，根据熵值使用自适应阈值将图像直方图划分为低、中、高三个熵区域，这些区域作为滤波器与浅层特征进行逐元素相乘，得到具有不同空间信息的特征图。</li><li><strong>动态路径选择模块（DPS）</strong>：为解决不同空间分布区域融合不均匀的问题，提出了DPS模块。该模块采用路径选择结构，每个分支使用自适应融合模块（Adaptive Fusion Module，AFM）进行特征提取。AFM由多个多尺度特征融合块（Multi-scale Feature Fusion Block，MFB）组成，通过组合不同感受野的特征，可获得更丰富、准确的图像表示。多个MFB能从PET和MRI图像中提取图像颜色和结构的深层语义特征，对提高融合图像的质量有显著效果。</li></ol><h3 id="模型损失函数"><a href="#模型损失函数" class="headerlink" title="模型损失函数"></a>模型损失函数</h3><p>为监督训练过程，模型使用了回归损失“均方误差（Mean Square Error，MSE）”，同时引入梯度损失和感知损失。梯度损失用于模拟融合图像的物理精细细节，感知损失用于模拟融合图像与输入图像之间的高层语义相似性。最终的总损失通过计算输入图像、融合特征和融合图像之间的损失得到。</p><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li><strong>数据集</strong>：使用了<strong>哈佛全脑图谱（Harvard Whole Brain Atlas）医学图像融合数据集</strong>，包含MRI - CT（205对）、MRI - PET（311对）和MRI - SPECT（417对）三种多模态图像对，图像大小均为<strong>256×256</strong> ，MRI和CT为单通道图像，PET和SPECT为三通道图像，像素强度范围为[0, 255]。将所有数据集按9:1分为训练数据和测试集，训练数据再按8:2分为训练集和验证集。</li><li><strong>参数设置</strong>：使用Adam作为优化器，学习率设为0.0001，模型以100个批次进行训练。将EDPSN与传统方法（PCA、DWT、Curvelet、NSCT、DTCWT）和深度学习方法（IFCNN、U2Fusion、DILRAN、MATR、CDDFuse、FATF、BSA、LFDT）进行比较，所有方法均使用作者设置的默认参数。实验环境使用Pytorch在NVIDIA TITAN XP GPUs上实现。选择熵（EN）、标准差（SD）、相关差异总和（SCD）、$Q_{AB&#x2F;F}$、视觉信息保真度（VIF）、峰值信噪比（PSNR）和结构相似性指数（SSIM）七个指标客观评估融合结果。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><ul><li><strong>MRI - CT融合</strong>：需要考虑生理和解剖信息的最佳组合。传统方法融合相对均匀，但难以平衡生理和解剖细节；深度学习方法能实现不同模态结构信息的互补，但将图像作为整体处理，边缘和光滑区域的融合效果有差异。EDPSN针对不同信息分布区域采用不同融合策略，能均匀融合边缘和光滑区域，充分结合MRI的软组织结构和CT的血管钙化信息，辅助医生准确诊断脑血管疾病。</li><li><strong>MRI - PET和MRI - SPECT融合</strong>：需关注空间信息和颜色信息，重点是结构信息和代谢信息的结合。传统方法融合性能稳定，但融合图像易偏向彩色图像而丢失解剖细节；深度学习方法表现较好，但边缘和光滑区域融合效果仍有差异。EDPSN能均匀融合不同区域，清晰反映PET和SPECT的代谢信息，同时保留MRI的结构信息，辅助医生准确识别脑肿瘤。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-31-12.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-31-12.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-30_14-31-12"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-30-56.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-30-56.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-30_14-30-56"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-30-47.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-30-47.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-30_14-30-47"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-31-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-31-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-30_14-31-40"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><p>在<strong>MRI-PET</strong>上进行，设置6个消融实验，分别验证边缘增强模块（EEM）、熵感知划分模块（ED）和动态路径选择模块（DPS）的作用。结果表明，单独使用EEM和ED对提高融合质量有一定影响，结合使用效果更显著；DPS结构对结果影响大，利用不同层次的网络结构处理不同信息区域能使自适应融合模块（AFM）发挥最大作用。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-32-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-32-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-30_14-32-25"></p><h2 id="鲁棒性研究"><a href="#鲁棒性研究" class="headerlink" title="鲁棒性研究"></a><strong>鲁棒性研究</strong></h2><p>在CHAOS数据集上进行实验，该数据集包含190对腹部器官的MRI - CT图像。结果显示，EDPSN在腹部数据集上具有良好的泛化能力，能清晰显示腹部器官结构和软组织信息，适应不同解剖结构和图像特征的变化。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-33-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-33-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-30_14-33-58"></p><h2 id="用户研究"><a href="#用户研究" class="headerlink" title="用户研究"></a><strong>用户研究</strong></h2><p>咨询16位医生和医学生，对12种方法的融合图像进行评估。每种方法向参与者展示18张图像，包括每种模态的两对融合前后结果。调查问卷包含6个问题，参与者根据这些问题对融合方法进行评估，评分范围为1 - 5分。结果显示，与其他方法相比，EDPSN获得更多“5分”和“4分”评价，更少“1分”和“2分”评价，表明该方法在真实性、空间保真度和融合准确性方面能实现最佳主观视觉效果，可应用于临床诊断。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-34-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-30_14-34-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-30_14-34-52"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><blockquote><p>作者提出了一种新颖的**熵感知动态路径选择网络（EDPSN）**来解决医学多模态图像因空间信息分布导致的融合不均问题。通过边缘增强模块（EEM）对融合图像进行预处理，并提出了熵感知划分（ED）模块和动态路径选择（DPS）模块。实验结果表明，EDPSN不仅能有效去除边缘伪影，还能确保同一图像内不同空间区域的信息充分融合。在三种模态上的实验验证了EDPSN的出色性能，其基于不同区域的自适应融合机制为提高多模态医学图像协同诊断的准确性提供了新的技术途径。未来，作者旨在设计一个能适应多模态成像不同场景的通用模型，以提高其泛化能力。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-Modality Medical Image Fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multiscale Frequency-Guided Image Analyses for Mixed-Modality Medical Image Segmentation</title>
      <link href="/post/multiscale-frequency-guided-image-analyses-for-mixed-modality-medical-image-segmentation/"/>
      <url>/post/multiscale-frequency-guided-image-analyses-for-mixed-modality-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<p>广东工业大学</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Medical images generated by different imaging instruments often have different modalities. An architecture that can bridge the instrument gap and provide unified training for different modalities of images is the development direction of medical image segmentation. However, existing methods seldom pay attention to the mutual interference caused by the distribution differences of features between different modalities, which is a key factor in the training of mixed-modality datasets. Differences in diseases and imaging methods result in variations in the distribution of lesion size and frequency information. When training with data from different modalities mixed together, these differences typically lead to a decrease in network performance. To address this, we propose a mixed-modality segmentation network (MMSNet), which consists of three key components: multiscale frequency guidance (MSFG), modality feature adaptor (MFA), and frequency enhancement prompt (FEP). The MSFG refines the process of spatial feature extraction by incorporating multiscale frequency features. Moreover, MFA is designed to adjust pre-trained patch embeddings and transformer layers to reduce the cost of acquiring unknown modality features. Finally, FEP captures multiresolution frequency features of the image and fuses them with the frequency feature map of the image, which can enhance the network’s ability to extract frequency information across multiple spatial domains to a certain extent. In addition, we also introduced Mix1 and Mix2, which are composed of medical images from four different modalities to test the segmentation performance of MMSNet. Our experiment demonstrates that MMSNet can effectively alleviate the interference caused by differences in image frequency distribution, ultimately improving the segmentation quality of medical images. Our code will be made public at <a href="https://github.com/linzijin1238/MMSNet">https://github.com/linzijin1238/MMSNet</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>不同成像仪器生成的医学图像通常具有不同的模态。能够弥补仪器间差距并为不同模态图像提供统一训练的架构是医学图像分割的发展方向。然而，现有方法很少关注由于不同模态特征分布差异导致的相互干扰，这在混合模态数据集的训练中是一个关键因素。疾病和成像方法的差异导致病变大小和频率信息分布的变化。当混合不同模态的数据进行训练时，这些差异通常会导致网络性能下降。为了解决这一问题，我们提出了一种混合模态分割网络（MMSNet），由三个关键组件组成：多尺度频率引导（MSFG）、模态特征适配器（MFA）和频率增强提示（FEP）。MSFG通过结合多尺度频率特征完善空间特征提取过程。此外，MFA旨在调整预训练的补丁嵌入和Transformer层，以降低获取未知模态特征的成本。最后，FEP捕获图像的多分辨率频率特征，并将其与图像的频率特征图融合，能够在一定程度上增强网络跨多个空间域提取频率信息的能力。此外，我们还引入了由四种不同模态医学图像组成的Mix1和Mix2，以测试MMSNet的分割性能。我们的实验表明，MMSNet可以有效缓解由于图像频率分布差异导致的干扰，最终提高医学图像的分割质量。我们的代码将在<a href="https://github.com/linzijin1238/MMSNet%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/linzijin1238/MMSNet公开。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>在医疗领域，不同成像仪器生成的医学图像具有不同模态，当前主流医学分割模型多针对单模态图像分析，支持多模态的网络也多以灰度图像融合形式呈现。不同成像方法在捕获感兴趣区域上存在差异，会干扰网络对空间域信息的判断，导致不同领域医生使用的数据缺乏相关性，给临床实践带来不便。</p><p>因此，训练单一的混合模态数据集来处理多模态分割任务是未来分割模型的发展方向和关键挑战。混合模态训练的主要困难在于克服不同模态图像间病变大小和频率信息分布的差异，这些差异会导致图像纹理、亮度等信息不同，严重影响网络对病变敏感区域的判断。</p><p>尽管已有能处理多模态医学图像的网络，如U - Net及其变体，但U形架构在处理混合模态数据集时不够鲁棒；基于注意力机制的模型虽有发展，但大多聚焦空间域特征分析，忽略了不同模态间频率信息的差异，且在样本数量较少时无法解决特征分布混乱的问题。所以，本文旨在设计一个专注捕捉混合模态图像差异的网络，以增强其应对复杂模态训练的能力。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多尺度频率分割</strong>：多尺度信息在语义分割领域愈发重要，如特征金字塔结构、高效多尺度点卷积等都体现了多尺度的作用，部分研究还将多尺度与注意力机制结合。但当前医学图像分割方法多侧重单一方面，在混合模态数据集上表现不佳。</li><li><strong>视觉提示调优</strong>：视觉变换器使提示概念扩展到视觉领域，如视觉提示调优（VPT）等方法提升了模型性能。然而，这些提示常需手动标注，依赖人类先验知识，限制了网络实际性能。</li><li><strong>适配器</strong>：适配器最初用于自然语言处理，后应用于计算机视觉，如ViT Adaptor能使常规ViT执行下游任务，但此前未用于频率引导的模型微调。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>文章提出了一种用于混合模态医学图像分割的网络模型——混合模态分割网络（Mixed - Modality Segmentation Network，MMSNet），该模型由模态特征适配器（Modality Feature Adaptor，MFA）、频率增强提示（Frequency Enhancement Prompt，FEP）和多尺度频率引导（Multiscale Frequency Guidance，MSFG）三个关键组件构成。</p><ol><li>模态特征适配器（MFA）<ul><li><strong>设计目的</strong>：不同模态的图像特征存在显著差异，MFA旨在通过微调的方式，从预训练的视觉Transformer（ViT）中学习不同模态的医学图像特征，以优化图像特征提取过程。</li><li><strong>具体步骤</strong>：分为特征提取调整、图像嵌入调整和适配器三个步骤。特征提取调整模块用混合模态医学图像微调预训练模型，提取图像特征并学习额外的块嵌入层；图像嵌入调整模块对预训练视觉Transformer中的图像嵌入进行微调；适配器将图像嵌入和频率元素的特征结合，生成每个对应Transformer层的信息传递结果，并添加到主干各阶段提取的图像特征中，生成编码器的输出。</li></ul></li><li>频率增强提示（FEP）<ul><li><strong>设计目的</strong>：为解决混合模态数据集中频率分布混乱的问题，FEP通过将图像从空间域转换到频率域，生成额外的频率信息，促使网络关注特定频率分量，减少不同模态之间的干扰。</li><li><strong>具体方法</strong>：采用拉普拉斯金字塔（LP）和方向滤波器组（DFB）级联的轮廓波变换方法，将输入图像分解为低通和高通子带，高通子带再通过方向滤波器进一步分解为多个方向子空间，实现多尺度和多方向的频率分解，为网络捕捉图像的边缘、细节和纹理信息提供有效提示。</li></ul></li><li>多尺度频率引导（MSFG）<ul><li><strong>设计目的</strong>：使网络能够识别不同模态之间病变大小和频率的细微差异，提高模型在混合模态分割中的性能。</li><li><strong>具体步骤</strong>：分为尺度分解、频率通道引导和多尺度空间引导三个步骤。尺度分解通过多个路径对特征图进行降采样，提取多尺度属性；频率通道引导利用二维离散余弦变换（2D DCT）将信号从空间域转换到频率域，生成通道注意力图，对特征图进行重新校准；多尺度空间引导引入可学习参数控制前景和背景之间的信息流，引导网络关注不同尺度的前景区域，为病变检测提供更可靠的判别边界线索。</li></ul></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-27-36.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-27-36.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-28_17-27-36"></p><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ol><li><strong>评估指标</strong>：使用Dice相似度系数（DSC）、交并比（IoU）和平均绝对误差（MAE）这三个标准指标，来衡量模型的分割准确性。</li><li>数据集设置：<ul><li><strong>Mix1数据集</strong>：涵盖眼底（ORIGA数据集，500张训练图像、150张测试图像）、结肠镜检查（Kvasir数据集，1450张训练图像、100张测试图像）、超声（BUSI数据集，547张训练图像、100张测试图像）和皮肤镜检查（ISIC2018数据集，2594张训练图像、1000张测试图像）四种模态。</li><li><strong>Mix2数据集</strong>：同样包含四种模态，分别为眼底（REFUGE数据集，320张训练图像、80张测试图像）、结肠镜检查（CVC - 300数据集，1450张训练图像、60张测试图像）、超声（Thyroid - tn3k数据集，2879张训练图像、614张测试图像）和皮肤镜检查（ISIC2017数据集，2000张训练图像、600张测试图像）。所有图像均调整为352×352分辨率，训练时将四种模态的训练图像组合成包含5091张图像的混合模态训练集，训练完成后在各模态的测试集上验证模型性能。</li></ul></li><li>实现细节：<ul><li><strong>硬件与优化器</strong>：实验在单张NVIDIA RTX 4090（24G内存）上进行，使用AdamW优化器，学习率设为5e - 4，最大训练轮数为50，训练集批量大小为8。</li><li><strong>模型设置</strong>：MMSNet使用SegFormer - B4作为预训练编码器；在训练SAM相关模型时，为降低计算成本，不训练编码器部分，使用vit - b作为编码器权重，随机选取掩码内的点作为输入提示以模拟临床环境。其他超参数遵循原作者默认设置。</li></ul></li><li>超参数选择：<ul><li><strong>MFA中的参数r</strong>：对比不同r值（从8到1），发现r减小时各模态的准确率提高，但需训练的参数数量也增加。考虑到7.51M训练参数相对多数医学分割模型不算多，选择r &#x3D; 1作为默认超参数较为合理，r &#x3D; 4时模型仍具一定竞争力。</li><li><strong>频率分量数量K</strong>：研究发现，在单模态分割任务中K &#x3D; 16时网络性能最佳，但在混合模态数据集上，K &#x3D; 32时MMSNet的平均性能最优。</li></ul></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-29-35.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-29-35.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-28_17-29-35"></p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><ol><li>与现有模型对比：<ul><li><strong>与U形网络对比</strong>：MMSNet在四种模态评估中均优于U - Net、U - Net++及其变体DCSAUNet、UACANet和M2SNet。U形架构在眼底和皮肤镜检查中表现较好，但在结肠镜检查和超声模态中，因缺乏对频率信息的多尺度分析，易受频率差异导致的分布混乱影响，而MMSNet能收集多尺度的频率信息，准确分割不同大小的病变。</li><li><strong>与SAM架构对比</strong>：Self - prompt SAM在混合模态训练中因依赖单一线性像素分类，需大量同分布样本构建优秀分类器，难以实现；传统SAM因主要处理光谱域信息，缺乏频率信息的获取和处理，易受不同模态特征干扰。MMSNet通过引导和关注多尺度频率信息，解决了模态特征信息缺失问题，在除皮肤镜检查外的模态中优于SAM - Med2D，且无需手动标注。</li></ul></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-30-12.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-30-12.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-28_17-30-12"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-30-16.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-30-16.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-28_17-30-16"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-30-35.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-30-35.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-28_17-30-35"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-30-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-30-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-28_17-30-44"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>MSFG</strong>：对比有无MSFG的网络变体，发现缺少多尺度频率信息提取能力时网络性能不佳，MSFG能有效捕捉不同模态的尺度和频率差异，减轻不同尺度下疾病大小和频率不规则分布导致的性能下降。通过设计四种配置（SFG - SSG、MFG - SSG、SFG - MSG、MFG - MSG），验证了频率通道引导和多尺度空间引导的必要性，最终选择MFG - MSG作为MSFG的最终设计。</li><li><strong>FEP</strong>：使用FEP训练模型在各模态的DSC指标上高于仅集成MFA和MSFG或不使用FEP的情况。对比不同自提示生成方法（快速傅里叶变换、小波变换、轮廓波变换），发现使用轮廓波变换作为自提示生成方法性能最佳，证明了FEP在混合模态医学图像分割中的潜力。</li><li><strong>MFA</strong>：去除MFA后，模型在不同模态的得分明显下降，表明MFA对转移预训练知识和提取多尺度频率信息至关重要。与全调优编码器相比，除皮肤镜检查模态外，使用SegFormer编码器作为骨干并全调优在其他三种模态中无显著改进，且会大幅增加训练参数数量，因此使用MFA进行微调更高效。</li><li><strong>激活映射分析</strong>：不使用MFA微调冻结编码器时，网络无法准确定位病变；仅使用MSFG时，网络在某些模态能部分完成分割任务，但在眼底模态中注意力无法聚焦目标区域；缺少FEP组件时，网络对图像边缘判断模糊，这是由于缺乏FEP的高频信息提示，导致不同病变的轮廓信息丢失。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-31-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-28_17-31-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-28_17-31-28"></p><h2 id="不确定性分析"><a href="#不确定性分析" class="headerlink" title="不确定性分析"></a>不确定性分析</h2><p>基于预测分割结果，使用蒙特卡罗Dropout估计不确定性。观察模型推理过程中产生的不确定区域和最终预测结果，发现模型在高度不确定区域的最终预测有明显模糊现象，但在一些有争议区域，模型虽有一定不确定性，但未将其分割为病变区域，表明模型具有一定的抗干扰能力。临床实践中，专家仍需关注高不确定性区域，重新标注这些区域有助于模型自我纠正预测权重。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者基于混合模态训练设置的实验结果，从三方面总结了MMSNet的有效性：</p><ol><li><strong>MSFG的作用</strong>：在混合模态训练中，MSFG对医学图像分割的多尺度频率信息引导至关重要，能有效应对不同模态下疾病大小和频率分布的差异，提升网络性能。</li><li><strong>MFA的功能</strong>：MFA集成了针对混合模态医学图像的特征提取和优化的预训练块嵌入，可微调预训练编码器，实现对混合模态医学图像特征的自适应编码。</li><li><strong>FEP的能力</strong>：FEP能增强网络捕捉图像频率特征的能力。将其作为自提示融入MFA生成的特征图，可确保网络在处理复杂高频细节图像时具有强鲁棒性。</li></ol><blockquote><p><strong>不足</strong><br>MMSNet在分割小病变区域时表现不佳，可能是由于网络在下采样过程中未能有效关联多尺度信息，导致小病变区域的特征信息丢失或难以准确提取。</p></blockquote><blockquote><p><strong>展望</strong><br><strong>增强特征图间信息共享</strong>：研究如何在降采样过程中增加不同分辨率特征图之间的信息共享，以提高网络对小病灶区域的分割能力。例如，可以探索引入关键节点来整合不同层次的特征，从而更好地捕捉小目标的复杂边缘特征。<br><strong>自适应调整频率分量</strong>：目前频率分量的最佳数量是通过实验确定的。未来希望能够将该超参数调整为可学习的参数，使网络能够自适应更复杂的混合模态情况，从而在更广泛的模态范围内保持良好的性能。<br><strong>探索频率分量适应方法</strong>：研究频率分量的适应方法，增强网络的鲁棒性，使其能够更好地应对不同模态图像的频率差异。<br><strong>自主区分不同模态图像</strong>：虽然本文主要关注缩小不同模态图像的特征差距以提高网络性能，但未来可以探索利用频率信息引导网络自主区分不同模态的图像，并使用不同的网络骨干进行学习，以避免特征之间的纠缠和干扰。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mixed-Modality Segmentation Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Novel 3D Unsupervised Domain Adaptation Framework for Cross-Modality Medical Image Segmentation</title>
      <link href="/post/a-novel-3d-unsupervised-domain-adaptation-framework-for-cross-modality-medical-image-segmentation/"/>
      <url>/post/a-novel-3d-unsupervised-domain-adaptation-framework-for-cross-modality-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>We consider the problem of volumetric (3D) unsupervised domain adaptation (UDA) in cross-modality medical image segmentation, aiming to perform segmentation on the unannotated target domain (e.g. MRI) with the help of labeled source domain (e.g. CT). Previous UDA methods in medical image analysis usually suffer from two challenges: 1) they focus on processing and analyzing data at 2D level only, thus missing semantic information from the depth level; 2) one-to-one mapping is adopted during the style-transfer process, leading to insufficient alignment in the target domain. Different from the existing methods, in our work, we conduct a first of its kind investigation on multi-style image translation for complete image alignment to alleviate the domain shift problem, and also introduce 3D segmentation in domain adaptation tasks to maintain semantic consistency at the depth level. In particular, we develop an unsupervised domain adaptation framework incorporating a novel quartet self-attention module to efficiently enhance relationships between widely separated features in spatial regions on a higher dimension, leading to a substantial improvement in segmentation accuracy in the unlabeled target domain. In two challenging cross-modality tasks, specifically brain structures and multi-organ abdominal segmentation, our model is shown to outperform current state-of-the-art methods by a significant margin, demonstrating its potential as a benchmark resource for the biomedical and health informatics research community.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>我们考虑了<strong>跨模态医学图像分割</strong>中的体积（3D）无监督域适应（UDA）问题，旨在利用有标注的源域（例如CT）对未标注的目标域（例如MRI）进行分割。先前在医学图像分析中的UDA方法通常面临两个挑战：1）它们仅关注于2D层面的数据处理和分析，从而忽略了深度层面的语义信息；2）在风格转换过程中采用一对一映射，导致目标域对齐不足。有别于现有方法，我们在研究中首次探讨了多风格图像翻译，以实现完整的图像对齐，从而缓解域偏移问题，并在域适应任务中引入3D分割以维持深度层面的语义一致性。特别地，我们开发了一种无监督域适应框架，结合了一种新颖的四重自注意力模块，以高效增强高维空间区域中远距离特征之间的关系，从而显著提高未标注目标域的分割准确性。在两个具有挑战性的跨模态任务中，具体是大脑结构和多器官腹部分割，我们的模型表现出色，明显超越了当前的最先进方法，展示了其作为生物医学和健康信息学研究社区基准资源的潜力。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><ul><li><strong>CNN在医学图像应用的局限</strong>：近年来，深度卷积神经网络（CNNs）在医学图像处理中蓬勃发展，但由于不同成像模态、扫描协议和人口统计特性，训练集和测试集的数据分布差异严重，导致训练良好的深度模型在实际场景中表现不佳。</li><li><strong>传统方法的困境</strong>：为减少不同模态间的性能下降，使用标注的目标数据微调预训练模型的方法，因标注过程繁琐和隐私问题，在医学图像中并不适用。无监督域适应（UDA）无需目标域的真实标签，更具吸引力和可行性。</li><li><strong>现有UDA方法的不足</strong>：当前基于UDA的医学图像分析方法存在缺陷。一是倾向于在2D层面处理数据，忽略医学图像中的3D信息，导致深度层面的语义信息缺失；二是风格迁移过程采用一对一映射，仅考虑各域的平均风格，导致部分图像对齐，削弱模型在目标域边缘样本上的泛化能力。</li></ul><p>基于以上背景，作者提出了一种新颖的<strong>3D无监督域适应框架</strong>，以解决现有方法的问题。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-25-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-25-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-27_16-25-53"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>深度学习应用</strong>：近年来，深度卷积神经网络（CNNs）在医学图像处理领域发展迅速，但因成像方式、扫描协议和人口统计学特性等因素，导致训练集和测试集数据分布差异大，模型泛化能力受限。</li><li><strong>无监督域适应（UDA）方法</strong>：当前医学图像分析中的UDA方法主要包括图像对齐和特征对齐。图像对齐通过非配对图像到图像的转换，将源域图像风格转换为目标域；特征对齐则通过对抗训练提取域不变特征。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-29-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-29-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-27_16-29-17"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-27-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-27-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-27_16-27-52"></p><h3 id="2D转换网络"><a href="#2D转换网络" class="headerlink" title="2D转换网络"></a>2D转换网络</h3><p>用于内容 - 风格解缠，旨在最小化源域和目标域之间的域差距。</p><ul><li><strong>网络组件</strong>：由一个共享内容编码器<code>Genc_c</code>、两个特定于域的风格编码器<code>Genc_sa</code>和<code>Genc_sb</code>、一个共享解码器<code>Gdec</code>、一个内容判别器<code>Dc</code>以及两个图像判别器<code>Da</code>和<code>Db</code>构成。</li><li><strong>工作过程</strong>：编码器将输入图像分解为内容表示和风格表示，解码器通过自适应实例归一化（AdaIN）层注入风格表示，从内容表示中重建图像。</li><li><strong>损失函数：</strong><ul><li><strong>重建损失</strong>：在像素级、内容级和风格级分别采用三种重建损失，以确保图像与内容和风格表示之间的双射映射，并保留两个域的内容和风格信息。</li><li><strong>对抗损失</strong>：包括像素级对抗训练和内容级对抗训练，通过生成对抗网络（GAN）使翻译图像的分布与目标数据分布相匹配，进一步对齐两个域的内容表示。</li></ul></li></ul><h3 id="3D分割网络（DAR-UNet）"><a href="#3D分割网络（DAR-UNet）" class="headerlink" title="3D分割网络（DAR - UNet）"></a>3D分割网络（DAR - UNet）</h3><p>利用翻译后的图像（体积）进行训练，以实现3D医学图像的分割。</p><ul><li><strong>Voxel - Wise Attention Module（VAM）</strong>：在U - Net架构的解码器部分添加体素级注意力模块，使解码器专注于特征图的关键区域，增强特征质量。</li><li><strong>Quartet Attention Module（QAM）</strong>：在每个残差块中采用四重注意力模块，捕获输入张量不同维度之间的相邻语义信息，有效保持深度级别的语义一致性。</li><li><strong>Anisotropic Resolution Network</strong>：采用各向异性设计，减少内存成本。网络的前两层和后三层在深度、宽度和高度上分别使用不同的步幅，能够处理平面外分辨率是平面内分辨率4倍的图像，在不降低性能的情况下显著减少内存占用。</li><li><strong>分割损失</strong>：使用深度监督策略，结合软Dice损失和Focal损失，缓解训练过程中的梯度消失问题，克服前景和背景之间的不平衡问题。</li></ul><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li>数据集<ul><li><strong>前庭神经鞘瘤分割数据集</strong>：包含来自不同临床站点的105个对比增强T1（ceT1）和105个高分辨率T2（hrT2）磁共振成像（MRI）体积。仅在“ceT1到hrT2”方向上完成适应实验，分割标签包含耳蜗和前庭神经鞘瘤两个心脏结构。</li><li><strong>腹部多器官数据集</strong>：包括从[34]收集的30个体积的CT数据和从ISBI 2019 CHAOS挑战赛[35]收集的20个体积的T2 - SPIR MRI训练数据。在“CT到MRI”和“MRI到CT”两个方向进行实验，要分割的腹部器官有肝脏、右肾、左肾和脾脏。对数据集进行了空间归一化、填充、最小 - 最大归一化等预处理操作。</li></ul></li><li>实现细节<ul><li>使用一个RTX 3090 GPU（24G内存）进行实验。</li><li>对于风格迁移训练，使用LSGANs稳定训练，使用AdaBelief优化解缠GAN的参数，训练50个epoch，判别器和生成器学习率分别为2e - 4和1e - 4。</li><li>对于分割训练，使用AdaBelief优化参数，训练100个epoch，初始学习率为5e - 4，采用余弦学习率衰减策略。以2的批量大小训练DAR - UNet，训练时随机裁剪32 × 256 × 256体素的子体积，并进行数据增强。</li><li>推理时采用滑动窗口策略。</li></ul></li><li>评估指标<ul><li><strong>Dice相似系数（Dice）</strong>：衡量预测掩码和真实掩码之间的体素级重叠率。</li><li><strong>平均对称表面距离（ASD）</strong>：计算三维中两个表面之间的平均距离。Dice值越高、ASD值越低表示分割性能越好。</li></ul></li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>将提出的方法与<strong>前庭神经鞘瘤和腹部多器官分割</strong>任务中的现有技术方法进行比较，包括SynSeg - Net、AdaOutput、CycleGAN等经典2D UDA方法。结果表明，本文方法在大多数情况下显著优于其他方法，尤其是在前庭神经鞘瘤数据集上。经典2D UDA方法由于背景占比大、类不平衡问题而表现不佳，利用图像和特征对齐的方法比单一对齐方法效果更好，本文方法即使不使用特征对齐，也能通过完整的图像对齐取得良好结果。不过，在腹部多器官数据集的MRI - CT方向上，ASD结果比其他方法差，可能是由于MRI和CT图像的注释标准不同。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-32-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-32-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-27_16-32-43"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-32-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-32-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-27_16-32-48"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-33-47.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-33-47.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-27_16-33-47"></p><h2 id="3-完整图像对齐的效果"><a href="#3-完整图像对齐的效果" class="headerlink" title="3. 完整图像对齐的效果"></a>3. 完整图像对齐的效果</h2><p>在腹部多器官数据上进行不同风格迁移设置的实验，分别使用纠缠GAN、平均风格生成的解缠GAN和多风格生成的解缠GAN生成的转移图像来训练DAR - UNet。结果显示，纠缠GAN的Dice值最低，多风格转换在减少域差距方面最有效，证明了多风格转换策略在减少域差距方面的有效性</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-34-10.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-34-10.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-27_16-34-10"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><p>在腹部数据集的CT到MRI适应任务上，对提出的DAR - UNet的五个变体进行消融实验。结果表明： - 加入各向异性架构可显著降低内存成本并提高性能。 - QAM和VAM注意力模块能显著提升3D分割任务性能，两者结合比单独使用效果更好。 - 深度监督策略可通过缓解梯度消失问题进一步提升性能，且额外内存成本较小。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-34-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-27_16-34-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-27_16-34-33"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种用于<strong>跨模态医学图像分割</strong>的新型<strong>3D无监督域适应</strong>框架，经研究得出以下结论：</p><ol><li><strong>方法性能出色</strong>：结合多风格转换和双注意力模块，该框架在减轻域偏移问题上表现出色。在两个跨模态任务（前庭神经鞘瘤和腹部多器官分割）中，显著超越了现有最先进的方法。</li><li><strong>模块互补有效</strong>：精心设计的VAM和QAM两个注意力模块相互补充，适用于3D UNet架构，能使DAR - UNet在类目标源数据集上训练，并在目标域良好泛化。</li><li><strong>存在一定局限</strong>：采用<strong>2D图像到图像的转换</strong>可能导致轴向样式不一致，且作为两步框架，特征对齐和语义分割无法同时优化，未来需探索全端到端的3D无监督域适应框架。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unsupervised Domain Adaptation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MATR Multimodal Medical Image Fusion via Multiscale Adaptive Transformer</title>
      <link href="/post/matr-multimodal-medical-image-fusion-via-multiscale-adaptive-transformer/"/>
      <url>/post/matr-multimodal-medical-image-fusion-via-multiscale-adaptive-transformer/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Owing to the limitations of imaging sensors, it is challenging to obtain a medical image that simultaneously contains functional metabolic information and structural tissue details. Multimodal medical image fusion, an effective way to merge the complementary information in different modalities, has become a significant technique to facilitate clinical diagnosis and surgical navigation. With powerful feature representation ability, deep learning (DL)-based methods have improved such fusion results but still have not achieved satisfactory performance. Specifically, existing DL-based methods generally depend on convolutional operations, which can well extract local patterns but have limited capability in preserving global context information. To compensate for this defect and achieve accurate fusion, we propose a novel unsupervised method to fuse multimodal medical images via a multiscale adaptive Transformer termed MATR. In the proposed method, instead of directly employing vanilla convolution, we introduce an adaptive convolution for adaptively modulating the convolutional kernel based on the global complementary context. To further model long-range dependencies, an adaptive Transformer is employed to enhance the global semantic extraction capability. Our network architecture is designed in a multiscale fashion so that useful multimodal information can be adequately acquired from the perspective of different scales. Moreover, an objective function composed of a structural loss and a region mutual information loss is devised to construct constraints for information preservation at both the structural-level and the feature-level. Extensive experiments on a mainstream database demonstrate that the proposed method outperforms other representative and state-of-the-art methods in terms of both visual quality and quantitative evaluation. We also extend the proposed method to address other biomedical image fusion issues, and the pleasing fusion results illustrate that MATR has good generalization capability. The code of the proposed method is available at <a href="https://github.com/tthinking/MATR">https://github.com/tthinking/MATR</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>由于成像传感器的限制，同时获取包含功能代谢信息和结构组织细节的医学图像具有挑战性。多模态医学图像融合是一种将不同模态中的互补信息合并的有效方法，已成为促进临床诊断和手术导航的重要技术。借助强大的特征表示能力，基于深度学习（DL）的方法改善了这种融合结果，但仍未达到令人满意的性能。具体来说，现有的基于DL的方法通常依赖于卷积操作，虽然可以很好地提取局部模式，但在保持全局上下文信息方面能力有限。为弥补这一缺陷并实现精确融合，我们提出了一种新的无监督方法，通过多尺度自适应Transformer（称为MATR）融合多模态医学图像。在所提出的方法中，我们引入了一种自适应卷积，以根据全局互补上下文自适应地调整卷积核，而不是直接使用普通卷积。为了进一步建模长距离依赖关系，采用自适应Transformer以增强全局语义提取能力。我们的网络架构以多尺度方式设计，以便从不同尺度的视角充分获取有用的多模态信息。此外，设计了一种由结构损失和区域互信息损失组成的目标函数，以在结构层次和特征层次上构建信息保留的约束。在主流数据库上进行的大量实验表明，所提出的方法在视觉质量和量化评估方面优于其他具有代表性和最先进的方法。我们还扩展了所提出的方法以解决其他生物医学图像融合问题，令人满意的融合结果表明MATR具有良好的泛化能力。所提出方法的代码可在<a href="https://github.com/tthinking/MATR">https://github.com/tthinking/MATR</a> 获取。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于多模态医学图像融合，写作背景基于以下原因：</p><ul><li><strong>临床需求</strong>：功能代谢信息与结构组织细节难以同时呈现于单张医学图像，而多模态医学图像融合可结合不同模态图像的互补信息，在肿瘤分割、细胞分类等临床应用中意义重大。例如，SPECT图像能反映代谢信息但分辨率低，MRI图像含丰富解剖信息且分辨率高，二者融合有助于准确诊断。</li><li><strong>传统方法局限</strong>：以往多模态医学图像融合方法，如基于变换域、稀疏表示、混合及其他方法，需手动设计特征提取和融合策略，使方法复杂且耗时。</li><li><strong>深度学习方法不足</strong>：虽深度学习在图像融合中表现良好，但现有基于卷积运算的方法，难以捕捉长距离上下文依赖，且多采用单尺度网络、基于像素级损失函数，导致全局信息提取受限、重要信息丢失及噪声影响等问题。</li></ul><p>为解决上述挑战，作者提出基于多尺度自适应Transformer的多模态医学图像融合方法MATR，以提升融合效果和性能。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>传统方法</strong>：<strong>多尺度变换</strong>（MST）等传统方法遵循“分解-融合-重建”规则，但在处理不同输入模态时未考虑其自身特征，手动设计的融合策略保留互补信息能力有限。</li><li><strong>深度学习方法</strong>：<strong>基于深度学习</strong>（DL）的方法凭借强大的特征表示能力被广泛应用于多模态医学图像融合。**如卷积神经网络（CNN）、生成对抗网络（GAN）**等方法可将活动级测量和融合规则视为整体，避免手动设计，但大多基于卷积操作。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>文章提出了一种基于多尺度自适应Transformer的无监督深度学习方法MATR（Multiscale Adaptive Transformer），用于多模态医学图像融合。以下是该模型的详细介绍： </p><ol><li><strong>框架概述</strong></li></ol><ul><li>为解决通道不匹配问题，将SPECT图像从RGB色彩空间转换为YUV色彩空间，然后将Y分量$I_{Y}^{SPE}$与MRI图像在通道维度上拼接后输入到多尺度自适应Transformer网络。   </li><li>由于MATR是端到端模型，可避免手动设计融合策略。网络输出融合后的Y分量$I_{Y}^{F}$，最后通过YUV - RGB颜色转换生成最终的融合结果$I_{F}$。</li></ul><ol start="2"><li><strong>网络架构</strong></li></ol><ul><li><strong>基本模块（BM）</strong>：由自适应卷积（AC）、批量归一化（BN）层和修正线性单元（ReLU）组成，用于初步特征提取。公式为$F_{Out}^{BM} &#x3D; ReLU(BN(AC(F_{In}^{BM})))$，其中$F_{In}^{BM}和F_{Out}^{BM}$分别表示输入和输出特征。    </li><li><strong>多尺度结构</strong>：为了提取多模态尺度间的互补特征，模型采用多尺度设计，设置了三个分支处理BM的输出。       <ul><li>顶部分支：包含一个BM和三个自适应Transformer模块（ATM），用于表示潜在特征。       </li><li>中间分支：有两个BM和三个ATM。       </li><li>底部分支：拥有三个BM和三个ATM。分支中BM越多，提取的特征越深，信息提取能力越强。    - <strong>自适应Transformer模块（ATM）</strong>：有两个加法操作。       </li><li>第一个加法操作：$F_{Out}^{ATM1} &#x3D; MSA(LN(F_{In}^{ATM})) + F_{In}^{ATM}$，其中$F_{In}^{ATM}$和$F_{Out}^{ATM1}$分别表示输入和第一个加法操作的结果，LN是层归一化，MSA是多头自注意力。       </li><li>第二个加法操作：$F_{Out}^{ATM} &#x3D; MLP(LN(F_{Out}^{ATM1})) + F_{Out}^{ATM1}$，$F_{Out}^{ATM}$表示ATM的输出，MLP是多层感知器。</li></ul></li></ul><ol start="3"><li><strong>损失函数</strong>：由于多模态医学图像融合缺乏真实标签，损失函数从结构级和区域级两个角度设计。    </li><li><strong>结构级损失$(L_{SSIM})$</strong>：使用结构相似性指数测量（SSIM）来约束融合图像与源图像的相似性，确保融合结果具有足够的结构细节。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-26_15-16-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-26_15-16-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-26_15-16-58"></p><h2 id="数据集和训练细节"><a href="#数据集和训练细节" class="headerlink" title="数据集和训练细节"></a>数据集和训练细节</h2><ul><li><p><strong>数据集</strong>：从主流医学图像数据库Harvard下载354对256×256像素的SPECT和MRI图像。</p></li><li><p><strong>数据划分</strong>：随机分为训练集（319对）、验证集（15对）和测试集（20对）。</p></li><li><p><strong>数据增强</strong>：对训练集采用重叠裁剪策略，将图像裁剪成120×120的补丁对，共得到15631对补丁用于网络训练，并将所有样本归一化到[0, 1]。</p></li><li><p><strong>训练设置</strong>：使用PyTorch框架，在NVIDIA GeForce RTX 3090 GPU上进行实验。采用Adam优化器，学习率为0.001，批量大小为64，训练轮数为10。损失函数中的超参数设置为α &#x3D; 1，β &#x3D; 1，γ &#x3D; 2.5，λ固定为0.5。</p></li><li><p><strong>对比方法</strong>：选取7种具有代表性和最先进的方法进行定性和定量比较，包括基于局部拉普拉斯滤波（LLF）的方法、基于非下采样剪切波变换域参数自适应脉冲耦合神经网络（NSST - PAPCNN）的方法、基于PMGI的方法、基于U2Fusion的方法、基于DDcGAN的方法、基于EMFusion的方法和基于SwinFuse的方法。</p></li><li><p><strong>评估指标</strong>：采用9种广泛使用的评估指标进行全面客观评估，包括归一化互信息QMI、Tsallis熵QTE、非线性相关信息熵QNCIE、基于梯度的指标QG、基于图像特征的指标QP、Chen - Varshney指标QCV、局部互信息（LMI）、视觉信息保真度（VIF）和多尺度结构相似性指数（MS - SSIM）。</p></li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><ul><li><strong>定性比较</strong>：在6对具有代表性的SPECT和MRI图像上，将MATR与7种对比方法的融合结果进行比较。结果表明，MATR在保留源图像互补信息方面表现出更好的融合特性。</li><li><strong>定量比较</strong>：在测试集上使用9种评估指标进行定量比较。结果显示，MATR在大多数指标上优于其他代表性和最先进的方法，客观性能更好。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-26_15-22-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-26_15-22-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-26_15-22-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-26_15-22-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-26_15-22-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-26_15-22-23"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>网络结构分析</strong>：通过在验证集上进行消融实验，验证了自适应卷积（AC）、自适应Transformer模块（ATM）和多尺度结构在方法中的有效性。结果表明，完整模型的性能明显优于其他退化模型。</li><li><strong>损失函数分析</strong>：通过去除结构级损失和区域级损失，验证了这两种损失在训练过程中的重要性。结果表明，同时使用这两种损失时，MATR具有更准确和自然的融合性能。</li><li><strong>参数设置分析</strong>：通过在验证集上进行大量实验，最终将损失函数中的权衡参数固定为α &#x3D; 1，β &#x3D; 1，γ &#x3D; 2.5。结果表明，在这些参数设置下，模型在验证集上达到了最佳融合性能。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-26_15-23-00.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-26_15-23-00.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-26_15-23-00"></p><h2 id="扩展实验"><a href="#扩展实验" class="headerlink" title="扩展实验"></a>扩展实验</h2><ul><li><strong>PET和MRI图像融合</strong>：将MATR扩展到正电子发射断层扫描（PET）和MRI图像融合任务，无需微调。结果表明，MATR在同时利用PET图像的功能信息和MRI图像的组织细节方面表现出最佳融合性能，具有良好的泛化能力。</li><li><strong>GFP和PC图像融合</strong>：将MATR扩展到绿色荧光蛋白（GFP）和相差（PC）图像融合任务，无需微调。结果表明，MATR在主观和客观比较方面均优于其他竞争方法，进一步证明了其良好的泛化能力。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出一种基于多尺度自适应Transformer的深度学习多模态医学图像融合方法MATR，并得出以下结论：</p><ol><li><strong>方法有效性</strong>：引入自适应卷积和自适应Transformer提取全局互补上下文信息，多尺度设计捕捉有用的尺度间信息，从结构和特征层面设计目标函数进行无监督训练。大量实验表明，该方法在视觉质量和客观评价上优于其他代表性和先进方法。</li><li><strong>泛化能力</strong>：将该方法扩展到处理其他生物医学功能和结构图像融合问题，取得了令人满意的结果，说明MATR具有良好的泛化能力。</li><li><strong>应用价值</strong>：该方法具有实际工程应用价值，可促进后续的诊断、治疗规划和手术导航等任务。作者相信MATR模型能够处理更多类型的图像融合问题，并为探索新的图像融合方法提供思路。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Adaptive Transform </tag>
            
            <tag> Adaptive Convolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hybrid cross-modality fusion network for medical image segmentation with contrastive learning</title>
      <link href="/post/hybrid-cross-modality-fusion-network-for-medical-image-segmentation-with-contrastive-learning/"/>
      <url>/post/hybrid-cross-modality-fusion-network-for-medical-image-segmentation-with-contrastive-learning/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Medical image segmentation has been widely adopted</strong> in artificial intelligence-based clinical applications. The integration of medical texts into image segmentation models has significantly improved the segmentation performance. It is crucial to design an effective fusion manner to integrate the paired image and text features. Existing multi-modal medical image segmentation methods fuse the paired image and text features through a non-local attention mechanism, which lacks local interaction. Besides, they lack a mechanism to enhance the relevance of the paired features and keep the discriminability of unpaired features in the training process, which limits the segmentation performance. To solve the above problem, we propose a hybrid cross-modality fusion network (HCFNet) based on contrastive learning for medical image segmentation. The key designs of our proposed method are a multi-stage cross-modality contrastive loss and a hybrid cross-modality feature decoder. The multi-stage cross-modality contrastive loss is utilized to enhance the discriminability of the paired features and separate the unpaired features. Furthermore, the hybrid cross-modality feature decoder conducts local and non-local cross-modality feature interaction by a local cross-modality fusion module and a non-local cross-modality fusion module, respectively. Experimental results show that our method achieved state-of-the-art results on two public medical image segmentation datasets.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>医学图像分割已广泛应用于基于人工智能的临床应用中。将医学文本整合到图像分割模型中显著提高了分割性能。设计有效的融合方式以整合成对的图像和文本特征至关重要。现有的多模态医学图像分割方法通过非局部注意机制融合成对的图像和文本特征，但缺乏局部交互。此外，它们缺乏增强成对特征相关性和在训练过程中保持未配对特征可辨性的机制，这限制了分割性能。为解决上述问题，我们提出了一种基于对比学习的<strong>混合跨模态融合网络</strong>（HCFNet）用于医学图像分割。我们提出方法的关键设计是多阶段跨模态对比损失和混合跨模态特征解码器。多阶段跨模态对比损失用于增强成对特征的可辨性并分离未配对特征。此外，混合跨模态特征解码器分别通过局部跨模态融合模块和非局部跨模态融合模块进行局部和非局部跨模态特征交互。实验结果表明，我们的方法在两个公共医学图像分割数据集上达到了最先进的结果。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>医学图像分割在基于人工智能的临床应用中至关重要，能辅助诊断、治疗规划和个性化医疗发展。但单模态（仅基于医学图像）的分割方法存在不足，如受图像质量和标注数据稀缺的影响。</p><p>为解决这些问题，研究人员开始采用多模态学习框架，将医学文本融入图像分割模型，以提高分割性能。然而，现有的多模态医学图像分割方法存在局限性：一是通过非局部注意力机制融合图像和文本特征，缺乏局部交互；二是在训练过程中缺乏增强配对特征相关性和区分非配对特征的机制，限制了分割性能。</p><p>基于此，本文提出了一种基于对比学习的混合跨模态融合网络（HCFNet）用于医学图像分割，旨在设计有效的融合方式，增强配对特征的可区分性，分离非配对特征，同时实现局部和非局部跨模态特征交互，以提升医学图像分割的性能。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>单模态医学图像分割</strong>：CNN和Transformer等深度学习技术广泛应用，如U-Net、UNet++、基于Transformer的方法等。为解决数据不足问题，半监督技术被引入。</li><li><strong>多模态医学图像分割</strong>：利用文本辅助图像分割受关注，CLIP可通过对比学习连接图像与自然语言，LViT、GLoRIA等方法将医学文本融入分割过程。</li><li><strong>对比学习</strong>：在多模态学习中广泛应用，如MMGL框架在半监督心脏图像分割中取得显著效果。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-25_11-08-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-25_11-08-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-25_11-08-58"></p><p>论文提出了一种基于对比学习的混合跨模态融合网络（Hybrid Cross-Modality Fusion Network，HCFNet）用于医学图像分割。</p><ol><li><strong>整体架构</strong>：主要由双模态特征编码器、混合跨模态特征解码器和分割头三部分组成。同时还设计了多阶段跨模态对比损失，用于增强配对特征的可区分性并分离非配对特征。在训练阶段，冻结了文本编码器模块的权重以减少训练参数的数量。</li><li>双模态特征编码器<ul><li><strong>图像编码器</strong>：采用ConvNeXt - Tiny作为视觉编码器，提取输入图像不同阶段的特征。</li><li><strong>文本编码器</strong>：使用CXR - BERT从文本输入中提取文本特征。</li><li><strong>多阶段跨模态对比损失</strong>：利用图像特征和文本特征计算该损失，通过最小化该损失，使模型提高对图像 - 文本特征对的理解能力，增强匹配对的相似性，降低不匹配对的相似性。</li></ul></li><li>混合跨模态特征解码器：包含三个混合跨模态特征融合层，每层由非局部跨模态融合模块（NLCFM）、局部跨模态融合模块（LCFM）和融合块组成。<ul><li><strong>非局部跨模态融合模块（NLCFM）</strong>：利用多头交叉注意力（MHCA）机制，将语言和图像特征进行融合，增强视觉表示。</li><li><strong>局部跨模态融合模块（LCFM）</strong>：利用文本特征生成缩放和平移调制矩阵，对图像特征进行调制和增强，使模型更关注局部细节。</li><li><strong>融合块</strong>：将非局部特征和局部特征进行拼接和上采样操作，结合局部和非局部融合结果，得到更全面的特征表示。</li></ul></li><li><strong>分割头</strong>：主要由子像素卷积和1×1卷积组成。子像素卷积将解码器输出的特征上采样到原始图像大小，1×1卷积调整通道数量，得到最终的预测结果。</li><li><strong>损失函数</strong>：最终的训练损失函数包括交叉熵损失、Dice损失和多阶段跨模态对比损失，通过综合这些损失，优化模型性能。</li></ol><h2 id="数据集和实验设置"><a href="#数据集和实验设置" class="headerlink" title="数据集和实验设置"></a>数据集和实验设置</h2><h3 id="1-数据集和评估指标"><a href="#1-数据集和评估指标" class="headerlink" title="1. 数据集和评估指标"></a>1. 数据集和评估指标</h3><ul><li>数据集：<ul><li><strong>MosMedData+</strong>：由莫斯科市医院提供，包含2729张肺部感染的CT扫描切片，其中训练集2183张，验证集273张，测试集273张，且有受影响肺部区域的文本描述。</li><li><strong>QaTa - COV19</strong>：由卡塔尔大学和坦佩雷大学的研究人员整理，包含9258张显示COVID - 19感染区域的X射线图像，训练集5716张，验证集1429张，测试集2113张，有肺部感染区域的文本描述。</li></ul></li><li><strong>评估指标</strong>：使用Dice值和平均交并比（mIoU）作为评估指标。</li></ul><h3 id="2-实现细节"><a href="#2-实现细节" class="headerlink" title="2. 实现细节"></a>2. 实现细节</h3><p>使用PyTorch，采用单块NVIDIA 3090 24G GPU进行训练和测试。使用由Dice损失、交叉熵损失和多阶段跨模态对比损失组成的复合损失函数，网络使用AdamW优化器进行训练，批量大小为32，采用余弦退火学习率策略，初始学习率为3e - 4，最小学习率为1e - 6，还实现了早停机制。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>将提出的HCFNet与现有的单模态和多模态医学图像分割方法在QaTa - COV19和MosMedData+数据集上进行比较。结果表明，HCFNet在两个数据集上都取得了最优性能，在Dice值和mIoU方面优于其他方法，且在计算成本和速度方面也有较好表现。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-25_11-13-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-25_11-13-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-25_11-13-59"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-25_11-14-03.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-25_11-14-03.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-25_11-14-03"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><ul><li><strong>不同模块对分割性能的影响</strong>：在基线模型基础上分别引入多阶段跨模态对比损失（MCCL）和局部跨模态融合模块（LCFM），结果显示两者都能提升模型性能，且结合使用效果更佳。</li><li><strong>非局部跨模态融合模块中融合方式的影响</strong>：NLCFM和LCFM结合使用能取得最佳结果，单独使用LCFM会使Dice值下降。</li><li><strong>骨干网络选择的影响</strong>：将图像编码器ConvNeXt - Tiny替换为ResNet18会使Dice和mIoU分别降低4.0%和5.32%。</li></ul><h3 id="可视化结果比较"><a href="#可视化结果比较" class="headerlink" title="可视化结果比较"></a>可视化结果比较</h3><p>在QaTa - COV19和MosMedData+数据集上，将HCFNet与Threads、TGANet和UCTransNet等方法进行可视化结果比较，HCFNet的分割结果更优，边界更准确，对小目标的分割效果更好。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-25_11-15-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-25_11-15-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-25_11-15-25"></p><h2 id="泛化实验"><a href="#泛化实验" class="headerlink" title="泛化实验"></a>泛化实验</h2><p>将HCFNet应用于遥感实体分割任务，在RefSegRS数据集上进行实验，并与LAVT、CrossVLT等最新方法比较，结果表明HCFNet在oIoU和mIoU指标上取得了最优性能，证明了其良好的泛化能力。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-25_11-16-41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-25_11-16-41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-25_11-16-41"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了<strong>基于对比学习的混合跨模态融合网络</strong>（HCFNet）用于医学图像分割，并得出以下结论：</p><ul><li><strong>模型有效性</strong>：多阶段跨模态对比损失增强了配对特征的可区分性，分离了未配对特征；混合跨模态特征解码器中的非局部和局部跨模态融合模块能有效融合特征，提升模型对多模态特征的理解。</li><li><strong>实验验证</strong>：在MosMedData+和Qata - COV19数据集上的实验结果表明，HCFNet的分割性能优于现有单模态和多模态医学图像分割方法，且计算成本较低。</li><li><strong>泛化能力</strong>：HCFNet在遥感实体分割任务中也取得了最优结果，展现出良好的泛化能力。</li></ul><p>总体而言，HCFNet为医学图像分割提供了有效的解决方案，具备实际应用价值。</p><blockquote><p>展望：在未来的工作中，我们打算与重庆大学肿瘤医院合作，收集其他解剖结构的多模态医学图像分割数据集。此外，我们将更加注重构建更高效的分割结构。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 多医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cross-Modality Interaction Network for Medical Image Fusion</title>
      <link href="/post/cross-modality-interaction-network-for-medical-image-fusion/"/>
      <url>/post/cross-modality-interaction-network-for-medical-image-fusion/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Multi-modal medical image fusion</strong> maximizes the complementary information from diverse modality images by integrating source images. The fused medical image could offer enhanced richness and improved accuracy compared to the source images. Unfortunately, the existing deep learning-based medical image fusion methods generally rely on convolutional operations, which may not effectively capture global information such as spatial relationships or shape features within and across image modalities. To address this problem, we propose a unified AI-Generated Content (AIGC)-based medical image fusion, termed Cross-Modal Interactive Network (CMINet). The CMINet integrates a recursive transformer with an interactive Convolutional Neural Network. Specifically, the recursive transformer is designed to capture extended spatial and temporal dependencies within modalities, while the interactive CNN aims to extract and merge local features across modalities. Benefiting from cross-modality interaction learning, the proposed method can generate fused images with rich structural and functional information. Additionally, the architecture of the recursive network is structured to reduce parameter count, which could be beneficial for deployment on resource-constrained devices. Comprehensive experiments on multi-model medical images (MRI and CT, MRI and PET, and MRI and SPECT) demonstrate that the proposed method outperforms the state-ofthe-art fusion methods subjectively and objectively.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>多模态医学图像融合</strong>通过整合源图像最大化来自不同模态图像的互补信息。与源图像相比，融合后的医学图像可以提供更丰富和更准确的信息。不幸的是，现有的基于深度学习的医学图像融合方法通常依赖于卷积操作，这可能无法有效捕捉图像模态内部和跨模态的空间关系或形状特征等全局信息。为了解决这个问题，我们提出了一种基于AI生成内容（AIGC）的统一医学图像融合方法，称为跨模态交互网络（CMINet）。CMINet集成了递归Transformer和交互卷积神经网络。具体来说，递归Transformer旨在捕捉模态内的扩展空间和时间依赖性，而交互CNN则旨在提取和合并跨模态的局部特征。得益于跨模态交互学习，所提出的方法能够生成具有丰富结构和功能信息的融合图像。此外，递归网络的架构被设计为减少参数数量，这对资源受限设备上的部署可能有益。在多模态医学图像（MRI和CT、MRI和PET、MRI和SPECT）上的综合实验表明，所提出的方法在主观和客观上都优于现有的融合方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于医学图像融合领域，其研究背景主要源于以下几方面：</p><ol><li><strong>多模态医学成像的需求</strong>：随着AI生成内容（AIGC）的发展，多模态医学成像成为提供人体组织和结构丰富信息的重要手段。不同模态图像（如CT、MRI、PET、SPECT）聚焦不同类型信息，为克服单模态图像的局限性，图像融合技术应运而生，可将不同模态图像数据整合为统一图像，满足人类视觉感知和机器检测需求。</li><li><strong>现有图像融合方法的不足</strong>：传统图像融合方法通常需通过复杂的数学变换将原始图像映射到变换域，其变换技术复杂，难以在医疗系统上实现实时计算。深度学习驱动的图像融合方法虽有进展，但基于卷积神经网络（CNN）的方法一般依赖卷积运算，难以有效捕捉图像模态内和跨模态的全局信息，如空间关系或形状特征。</li><li><strong>研究目的</strong>：为解决上述问题，作者提出基于AIGC的医学图像融合方法——跨模态交互网络（CMINet），旨在充分利用局部和全局信息，实现更好的特征互补，同时降低模型参数数量和复杂度，以在资源受限的设备上部署。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>传统方法</strong>：通过数学变换将原图像映射到变换域，如 NSCT 和 NSST，再进行活动测量和制定融合规则实现图像融合，但变换技术复杂，难以在医疗系统实现实时计算。</li><li><strong>深度学习方法</strong>：成为主流，可分为 CNN 与 GAN 两类。CNN 方法用并行卷积网络提取特征并融合；GAN 方法由生成器和判别器构成，通过对抗学习生成融合图像。</li><li><strong>Transformer 方法</strong>：因强大的长程建模能力，在图像融合领域得到应用，部分研究将 CNN 与 Transformer 结合，以提升提取全局语义信息的能力。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_10-59-34.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_10-59-34.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-24_10-59-34"></p><p>给定两个源图像 $I_a \in R^{C_a×H×W}$ 和 $I_b \in R^{C_b×H×W}$（$H$、$W$ 和 $C$ 分别代表图像的高度、宽度和通道数），CMINet的处理流程如下：</p><ol><li><strong>全局特征提取</strong>：通过视觉Transformer模块 $H_a^{TF}(·)$ 和 $H_b^{TF}(·)$ 从预处理后的源图像 $I_a$ 和 $I_b$ 中提取全局特征：${F_{a_{global}}, F_{b_{global}}} &#x3D; {H_a^{TF}(I_a), H_b^{TF}(I_b)}$ 。</li><li><strong>浅层局部特征提取</strong>：将全局特征连接起来，并传递给头部模块以提取浅层局部特征：$F_{shallow} &#x3D; H_{head}(Cat(F_{a_{global}}, F_{b_{global}}))$ ，其中 $Cat(·)$ 表示通道连接，$H_{head}$ 是浅层特征提取模块。</li><li><strong>深层局部特征提取</strong>：将浅层局部特征 $F_{shallow}$ 发送到交互式CNN进行深层局部特征提取：$F_{deep} &#x3D; H_{cnn}(F_{shallow})$ ，其中 $H_{cnn}(·)$ 代表交互式CNN，$F_{deep}$ 表示深层局部特征。交互式CNN由三对参数共享的局部特征提取（LFE）模块和通道注意力（CA）模块组成。</li><li><strong>融合图像重建</strong>：设计尾部模块 $H_{tail}$ 来重建融合图像：$I_f &#x3D; H_{tail}(F_{deep})$ ，其中 $I_f$ 是包含丰富结构和功能信息的融合图像。</li></ol><h3 id="模型各部分组件及作用"><a href="#模型各部分组件及作用" class="headerlink" title="模型各部分组件及作用"></a>模型各部分组件及作用</h3><h4 id="模态内知识交互"><a href="#模态内知识交互" class="headerlink" title="模态内知识交互"></a>模态内知识交互</h4><p>引入基于双分支递归Transformer的模态内交互部分，每个阶段的Transformer（TF）模块由多头自注意力（MHSA）、多层感知机（MLP）和两个层归一化（LN）层组成。通过结合RNN和Transformer，该框架可以利用Transformer的全局上下文理解能力和RNN的局部顺序处理优势。</p><h4 id="模态间知识交互"><a href="#模态间知识交互" class="headerlink" title="模态间知识交互"></a>模态间知识交互</h4><p>基于交互式CNN设计模态间交互部分，包括：</p><ol><li><strong>局部特征提取（LFE）模块</strong>：由卷积层和门控双注意力（GDA）单元组成，采用密集连接模式，同时引入局部残差学习，以促进信息的传递和特征的重用，增强模型捕捉医学图像详细信息的能力。</li><li><strong>门控双注意力（GDA）单元</strong>：由1×1卷积层、双注意力机制（空间注意力SA和通道注意力CA）和门控自适应双归一化（GADN）块组成。利用多分支结构提取不同层次的特征，并通过注意力机制选择重要的特征信息。</li><li><strong>门控自适应归一化（GADN）块</strong>：引入三个可学习参数 $\alpha$、$\beta$ 和 $\gamma$ 来调节每个通道的权重，通过门控自适应操作调整特征的权重，使模型能够选择性地强调或弱化不同特征。</li></ol><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><ul><li><strong>结构保留</strong>：使用结构相似性指数（SSIM）建立结构损失函数，限制融合图像与源图像之间的结构相似性：$L_{SSIM} &#x3D; 1 - SSIM(I_f, max{I_a, I_b})$ 。</li><li><strong>纹理保留</strong>：使用梯度算子来增强对源图像纹理细节的捕捉和保留：$L_{TEX} &#x3D; |\nabla I_f - max{\nabla I_a, \nabla I_b}|_2$ 。</li><li><strong>强度调节</strong>：设计像素损失函数，使融合结果的像素强度分布与源图像相近：$L_{PIX} &#x3D; \frac{1}{HW}(|I_f - I_a|_F^2 + |I_f - I_b|_F^2)$ 。</li><li><strong>总损失函数</strong>：$L &#x3D; \lambda_1L_{SSIM} + \lambda_2L_{TEX} + \lambda_3L_{PIX}$ ，其中 $\lambda_1$、$\lambda_2$ 和 $\lambda_3$ 是用于控制各子损失项平衡的超参数。</li></ul><h2 id="实验数据集和设置"><a href="#实验数据集和设置" class="headerlink" title="实验数据集和设置"></a>实验数据集和设置</h2><ol><li>数据集与训练细节<ul><li><strong>数据集</strong>：基于<strong>哈佛医学院的全脑图谱数</strong>据库构建训练和测试数据集。训练集包含160对CT与MRI图像、245对PET与MRI图像、333对SPECT与MRI图像，所有图像均调整为64×64尺寸。测试集从三个数据集中各选24对图像。</li><li><strong>训练设置</strong>：学习率初始化为0.0002，批量大小为16，采用Adam进行网络优化。超参数λ1、λ2和λ3分别设为10、100和1。实验在NVIDIA GeForce RTX 3090 GPU上借助PyTorch框架完成。</li></ul></li><li>实验配置<ul><li><strong>对比方法</strong>：将CMINet与9种先进方法（CSF、DensFuse、FusionGAN、PMGI、RFN - Nest、SDNet、STDFusionNet、U2Fusion和UMF - CMGR）进行对比。</li><li><strong>评估指标</strong>：采用Mutual Information（MI）、Spatial Frequency（SF）、Visual Information Fidelity（VIF）和$Q_{abf}$这四个指标进行定量评估，指标值越高表明融合性能越好。</li></ul></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><ol><li>多模态医学图像融合实验<ul><li><strong>CT和MRI图像融合</strong>：定性结果显示，其他9种融合技术在保留CT图像密集结构完整性方面存在明显不足，而CMINet能有效保留CT图像的高密度结构特征和MRI图像的纹理细节信息。定量结果表明，CMINet在MI、SF、VIF和Qabf指标上排名第一。</li><li><strong>PET和MRI图像融合</strong>：定性结果显示，多数融合结果中MRI的边缘信息受PET图像背景影响，而CMINet能保留MRI图像细节并避免颜色信息失真。定量结果表明，CMINet在SF和Qabf指标上表现显著优越，在MI和VIF指标上位居第二。</li><li><strong>SPECT和MRI图像融合</strong>：定性结果显示，CMINet在融合结果中能有效保留MRI图像的细节和边缘，不受SPECT图像背景干扰。定量结果表明，CMINet在MI、SF、VIF和Qabf指标上表现优越。</li></ul></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_11-03-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_11-03-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-24_11-03-37"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_11-03-41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_11-03-41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-24_11-03-41"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_11-03-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_11-03-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-24_11-03-59"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>模态知识交互消融实验</strong>：设计三种不同配置评估模态内和模态间知识交互的作用。结果表明，同时应用模态内和模态间知识交互在MI、SF、VIF和Qabf指标上取得最佳效果，说明该方法能有效利用不同模态间的互补信息。</li><li><strong>递归步骤消融实验</strong>：使用1 - 4不同级别的递归，评估模型在CT和MRI图像融合任务上的定量性能。结果显示，模型在两个递归步骤时达到最佳性能，递归步骤过多或过少都会导致性能下降，因此合理选择递归步骤数量很重要。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_11-04-39.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_11-04-39.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-24_11-04-39"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_11-04-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-24_11-04-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-24_11-04-43"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了基于AIGC的医学图像融合方法CMINet，并得出以下结论：</p><ol><li><strong>优势显著</strong>：CMINet利用双分支递归变压器和交互式CNN，充分利用全局和局部信息，克服了CNN网络的局限性。递归结构有效减少了模型的参数数量。</li><li><strong>信息互补</strong>：该方法能有效利用不同模态图像的互补信息，递归变压器捕获模态内的长期依赖关系，交互式CNN聚焦模态间的深度局部信息。</li><li><strong>效果良好</strong>：在多种多模态医学图像融合任务中，CMINet能有效保留源图像的结构信息和丰富的纹理细节。</li></ol><p>未来，作者计划提升医学图像融合的实时性能。</p>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> Dual Attention Encoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A cascaded framework with cross-modality transfer learning for whole heart segmentation</title>
      <link href="/post/a-cascaded-framework-with-cross-modality-transfer-learning-for-whole-heart-segmentation/"/>
      <url>/post/a-cascaded-framework-with-cross-modality-transfer-learning-for-whole-heart-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Automatic and accurate segmentation of the <strong>whole heart structure</strong> from 3D cardiac images plays an important role in helping physicians diagnose and treat cardiovascular disease. However, the time-consuming and laborious manual labeling of the heart images results in the inefficiency of utilizing the existing CT or MRI for training the deep learning network, which decrease the accuracy of whole heart segmentation. However, multi-modality data contains multi-level information of cardiac images due to different imaging mechanisms, which is beneficial to improve the segmentation accuracy. Therefore, this paper proposes a cascaded framework with cross-modality transfer learning for whole heart segmentation (CM-TranCaF), which consists of three key modules: modality transfer network (MTN), U-shaped multi-attention network (MAUNet) and spatial configuration network (SCN). In MTN, MRI images are transferred from MRI domain to CT domain, to increase the data volume by adopting the idea of adversarial training. The MAUNet is designed based on UNet, while the attention gates (AGs) are integrated into the skip connection to reduce the weight of background pixels. Moreover, to solve the problem of boundary blur, the position attention block (PAB) is also integrated into the bottom layer to aggregate similar features. Finally, the SCN is used to finetune the segmentation results by utilizing the anatomical information between different cardiac substructures. By evaluating the proposed method on the dataset of the MM-WHS challenge, CM-TranCaF achieves a Dice score of 91.1% on the testing dataset. The extensive experimental results prove the effectiveness of the proposed method compared to other state-of-the-art methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>从三维心脏图像中自动且准确地分割整个心脏结构在帮助医生诊断和治疗心血管疾病方面起着重要作用。然而，对心脏图像进行耗时且费力的人工标注，导致现有的CT或MRI在训练深度学习网络时效率低下，从而降低了整个心脏分割的准确性。然而，由于不同的成像机制，多模态数据包含心脏图像的多层次信息，有助于提高分割精度。因此，本文提出了一种用于全心脏分割的跨模态迁移学习级联框架（CM-TranCaF），该框架由三个关键模块组成：模态迁移网络（MTN）、U形多注意力网络（MAUNet）和空间配置网络（SCN）。在MTN中，采用对抗训练的思想将MRI图像从MRI域转移到CT域，以增加数据量。MAUNet基于UNet设计，将注意力门（AGs）集成到跳跃连接中，以降低背景像素的权重。此外，为解决边界模糊问题，还在底层集成了位置注意块（PAB）以聚合相似特征。最后，SCN通过利用不同心脏子结构之间的解剖信息来微调分割结果。在MM-WHS挑战的数据集上评估所提出的方法，CM-TranCaF在测试数据集上获得了91.1%的Dice分数。广泛的实验结果证明了该方法相较于其他最先进方法的有效性。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>心血管疾病是全球主要死因之一，3D成像技术（CT和MRI）在心脏病研究和治疗中至关重要，常需进行全心脏分割。然而，全心脏分割面临诸多挑战，促使作者开展此项研究：</p><ol><li><strong>数据标注难题</strong>：心脏图像手动标注耗时费力，且涉及患者隐私，导致训练深度学习网络时现有CT或MRI数据利用效率低，影响全心脏分割准确性。</li><li><strong>单模态局限</strong>：多数现有方法基于单模态数据，无法充分利用多模态数据因不同成像机制带来的多层次信息，限制了分割性能。</li><li><strong>现有多模态方法不足</strong>：现有多模态分割方法通常在心脏二维切片上进行模态迁移后拼接成三维图像，会造成信息丢失，产生模态迁移误差，影响最终分割效果。</li></ol><p>为解决上述问题，本文提出一种具有跨模态迁移学习的级联框架（CM - TranCaF）用于全心脏分割，旨在利用多模态数据提升分割准确性，避免传统方法的局限。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>单模态分割</strong>：主要基于深度学习的卷积神经网络，如 3D FCN、UNet 等，虽有一定效果，但单模态数据难以利用多模态信息，影响分割性能。</li><li><strong>多模态分割</strong>：通过图像配准和模态转换解决模态不一致问题，如将 LGE 和 cine MR 图像注册融合后分割。同时，转移学习也为多模态分割带来新思路，如 STAR 方法、基于互知识蒸馏的跨模态分割方法等。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-23_09-14-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-23_09-14-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-23_09-14-09"></p><p>文提出了一种用于全心脏分割的级联框架CM-TranCaF，该框架由三个关键模块组成：</p><ol><li>模态转换网络（Modality Transfer Network，MTN）<ul><li><strong>设计基础</strong>：基于3D CycleGAN，旨在利用其在无配对图像上的转换能力。</li><li><strong>网络结构</strong>：由两个生成器$G_c$、$G_m$和两个判别器$D_c$、$D_m$组成。生成器是编码器 - 解码器结构，对图像进行域转换；判别器是编码器结构，对生成器的输出图像进行真假判断，并将判断结果返回给生成器进行对抗训练。</li><li><strong>作用</strong>：将MRI图像从MRI域转换到CT域，通过对抗训练的思想<strong>增加数据量</strong>。同时，确保转换后的图像内容不变，保留心脏的解剖结构。</li><li><strong>损失函数</strong>：$L &#x3D; L_{GAN}+\lambda L_{cycle}+L_{identity}$，其中$L_{GAN}$是对抗损失，用于优化生成器和判别器的训练；$L_{cycle}$是循环一致性损失，确保转换后图像内容不变；$L_{identity}$是识别损失，用于训练生成器的识别能力；$\lambda$是权重系数，实验中设为10。</li></ul></li><li>U形多注意力网络（U-shaped Multi-Attention Network，MAUNet）<ul><li><strong>设计基础</strong>：受UNet启发，基于编码器 - 解码器结构，并集成了残差结构和多个注意力模块。</li><li><strong>网络结构</strong>：编码和解码部分根据输入图像大小分为4层。编码部分主要由4个残块组成，通过卷积和下采样操作提取多尺度信息；解码部分主要由卷积层和注意力门（Attention Gates，AGs）模块组成。</li><li><strong>作用</strong>：接收带有全心脏标签的CT图像和MTN生成的CT图像作为输入，提取感兴趣区域（ROI），准确分割全心脏的七个子结构。</li><li>注意力模块<ul><li><strong>AGs</strong>：集成到跳跃连接中，用于计算注意力系数$\alpha_i\in[0,1]$，抑制无关背景区域的特征响应，使网络更关注有效信息。</li><li><strong>位置注意力块（Position Attention Block，PAB）</strong>：集成到底层，通过计算空间注意力矩阵$S$，聚合相似特征，解决边界模糊问题。</li></ul></li><li><strong>损失函数</strong>：结合Dice损失和Focal损失形成新的混合损失$L_{MAUNet}&#x3D;L_{Dice}+\lambda L_{Focal}$，其中$\lambda$表示$L_{Dice}$和$L_{Focal}$的相对重要性，实验表明$\lambda$为0.1时分割效果最佳。</li></ul></li><li>空间配置网络（Spatial Configuration Network，SCN）<ul><li><strong>设计灵感</strong>：受Payer等人的启发。</li><li><strong>作用</strong>：对MAUNet的预测结果进行微调，基于不同类别的预测结果和它们之间的解剖关系，使预测更加精确，缓解某些类别结构相似带来的歧义。</li><li><strong>网络结构</strong>：首先通过三个$3\times3\times3$卷积核的卷积层进行处理，然后采用空间配置块，利用$9\times9\times5$卷积核和特定的下采样因子进一步处理。</li><li><strong>工作原理</strong>：将中间分割结果输入SCN，学习不同心脏子结构之间的相对位置信息，根据这些关系数据推断每个类别的位置，从而细化中间分割结果。</li></ul></li></ol><h2 id="实验设置及评估指标"><a href="#实验设置及评估指标" class="headerlink" title="实验设置及评估指标"></a>实验设置及评估指标</h2><ol><li><strong>数据集</strong>：采用MM - WHS2017挑战赛的数据集，该数据集提供了60张3D CT图像和60张3D MRI图像用于全心分割。每个模态有20张图像用于训练，40张图像用于测试。数据来自多家医院的不同设备，图像质量和分辨率各异，模拟了临床实际情况。每张图像涵盖整个心脏子结构，包括<strong>左心室（LV）、右心室（RV）、左心房（LA）、右心房（RA）、心肌（Myo）、升主动脉（AA）和肺动脉（PA）</strong>。</li><li><strong>评估指标</strong>：采用七个心脏子结构的<strong>平均Dice分数</strong>、<strong>Jaccard分数</strong>和**豪斯多夫距离（HD）**来定量评估所提方法。Dice和Jaccard是衡量两个样本相似性的指标，值越大表示样本越相似；HD用于测量两个样本子集之间的距离，即两个样本集中最近点之间的最大距离。此外，还采用了MM - WHS挑战赛中使用的全心分割分数（WHS）来评估全心的分割性能，并报告了WHS分数的标准差。</li><li>实现细节<ul><li><strong>数据预处理</strong>：由于数据由不同设备收集，图像质量差异大，对训练数据集采用纹理增强方法，并将原始心脏图像重采样到间距索引为3。为使网络更关注心脏结构部分，抑制背景区域的影响，使用预训练的定位网络定位心脏中心，从每张图像中裁剪出64×64×64的感兴趣区域（ROI）作为MAUNet的输入。</li><li><strong>模型训练</strong>：提出的网络在TensorFlow上实现，使用Tesla V100 GPU。训练前，通过缩放、平移、翻转、旋转等技术随机增加数据集，以提高模型的泛化能力和鲁棒性。训练阶段采用小批量自适应矩估计（Adam）更新网络权重，批量大小为8，初始学习率为0.00001。经过总共40,000次迭代训练后，在MM - WHS挑战赛的40张无标签测试图像上评估所提网络。</li></ul></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p><strong>与其他方法的比较</strong>：将所提方法与MM - WHS挑战赛中的先进方法进行比较，包括Payer、Wang、Yang、Li等提出的方法。实验结果表明，所提的CM - TranCaF在所有三个评估指标（Dice分数、Jaccard分数和HD）上都取得了最佳性能（除了Wang的方法），且Dice和Jaccard值与Wang的方法非常接近，标准差更小。在HD指标上，所提方法优于Wang的方法。此外，所提方法在右心房（RA）和肺动脉（PA）的分割准确性最高，证明了该方法的优势。同时，所提方法在不同病例上的分割结果比其他先进方法更稳定，证明了其鲁棒性</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-23_09-20-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-23_09-20-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-23_09-20-59"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>MTN的有效性</strong><ul><li><strong>生成图像比例实验</strong>：通过添加不同比例（10% - 100%）的生成图像进行实验，发现添加生成数据的分割性能总体上优于不添加生成数据的情况。随着生成数据比例的增加，分割性能逐渐提高，直到生成图像比例达到50%，之后在三个评估指标上大多数目标的性能开始下降。这是因为生成图像的数据分布只能无限接近CT图像的数据分布，不能完全一致，过多的生成图像会给学习网络带来更多干扰，导致性能下降。因此，添加50%生成图像时可获得最佳分割性能。</li><li><strong>λ的敏感性分析</strong>：对公式（1）中的权重系数λ进行敏感性分析，实验结果表明，当λ的值设置为10时，可获得最佳性能，优于λ为5和20的情况，因此在本文工作中λ的值设置为10。</li></ul></li><li><strong>AGs和PAB的有效性</strong>：在MAUNet中移除AGs或PAB模块，并在训练数据集中添加50%的生成图像进行实验。结果表明，采用AGs模块的网络在所有评估指标上的性能均优于未采用AGs模块的网络（无论是否采用PAB模块），表明注意力模块能有效抑制无关像素。对于PAB模块，在不考虑AGs模块带来的改进时，采用PAB模块也能在所有三个指标上取得更好的性能，表明PAB能够改善相似特征之间的关联关系，聚合相似特征，解决子结构之间边界模糊的问题。总体而言，同时采用AGs和PAB模块在大多数结构的三个指标上都能实现最佳分割性能。</li><li><strong>SCN的有效性</strong>：在训练数据集中添加50%的生成图像，评估SCN模块的有效性。实验结果表明，采用SCN模块的网络在划分不同子结构之间的边界时更准确，分割结果优于未采用SCN模块的网络。表5显示，采用SCN模块可以在三个评估指标上显著提高不同结构的分割性能，表明SCN模块能够学习心脏子结构之间的相对位置，进一步改善分割结果，证明了SCN模块的有效性。</li><li><strong>不同模块的协同作用</strong>：对MTN、AGs、PAB和SCN之间的协同相互作用进行分析，通过两两组合提出的模块进行实验。结果表明，固定MTN模块与其他三个组件组合时，与SCN模块集成产生的分割性能最佳；固定AGs模块和PAB模块时，与SCN模块组合也显示出最佳性能。这些结果证实了SCN模块在该实验框架中的有效性，并且SCN模块与MTN模块的组合能产生最显著的结果，体现了这两个模块之间的协同作用。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出用于全心脏分割的级联框架CM-TranCaF，并得出以下结论：</p><ol><li><strong>模块有效性</strong>：MTN模块可解决多模态医学图像数据分布不一致问题；MAUNet模块集成AGs和PABs能解决类别不平衡和边界模糊问题；SCN模块可利用心脏不同子结构间的解剖关系，辅助建模复杂空间关系和预测类别位置。</li><li><strong>实验结果</strong>：在MM - WHS挑战数据集上训练和测试，该框架取得91.1%的Dice分数和14.386mm的豪斯多夫距离，证明其能有效提高分割精度，且MTN、MAUNet和SCN模块有效。</li><li><strong>未来方向</strong>：MTN<strong>迁移能力</strong>待增强；需平衡各心脏结构的分割性能；可将方法扩展到其他模态以解决医学<strong>数据集不足</strong>问题。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Whole Heart Structure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Diff-IF Multi-modality image fusion via diffusion model with fusion knowledge prior</title>
      <link href="/post/diff-if-multi-modality-image-fusion-via-diffusion-model-with-fusion-knowledge-prior/"/>
      <url>/post/diff-if-multi-modality-image-fusion-via-diffusion-model-with-fusion-knowledge-prior/</url>
      
        <content type="html"><![CDATA[<p>武汉大学</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Multi-modality image fusion (MMIF)</strong> aims to aggregate the complementary information from diverse source image domains. As the generative adversarial network-based methods have been the primary choice and demonstrated satisfactory fusion performance, they suffer from the unstable training and mode collapse. To tackle this challenge, we propose a novel diffusion model for MMIF incorporating fusion knowledge prior, termed as Diff-IF. Diff-IF proposes a trainable diffusion model paradigm for multi-modality image fusion, resolving the issue of lacking the ground truth for the diffusion model in image fusion tasks. It decomposes the diffusion-based image fusion method into conditional diffusion model and fusion knowledge prior with the targeted search to derive the prior distribution for the specific image fusion task. In particular, the forward diffusion process is guided by the fusion knowledge prior distribution through targeted search, while the reverse diffusion process is designed to generate high-quality fused images. Extensive experiments demonstrate that Diff-IF achieves outstanding performance, including exemplary visual preservation, and good preservation of weak textures, across various MMIF tasks such as infrared-visible image fusion and medical image fusion. The code will be available at <a href="https://github.com/XunpengYi/Diff-IF">https://github.com/XunpengYi/Diff-IF</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>多模态图像融合</strong>（<strong>MMIF</strong>）旨在聚合来自不同源图像域的互补信息。尽管基于生成对抗网络的方法已成为主要选择并表现出令人满意的融合性能，它们仍面临不稳定训练和模式崩溃的问题。为了解决这一挑战，我们提出了一种新颖的结合融合知识先验的扩散模型用于MMIF，称为Diff-IF。Diff-IF提出了一种可训练的扩散模型范式用于多模态图像融合，解决了图像融合任务中扩散模型缺乏真实值的问题。它将基于扩散的图像融合方法分解为条件扩散模型和融合知识先验，通过目标搜索得出特定图像融合任务的先验分布。特别是，前向扩散过程通过目标搜索由融合知识先验分布引导，而反向扩散过程旨在生成高质量的融合图像。大量实验表明，Diff-IF在各种MMIF任务（如红外-可见光图像融合和医学图像融合）中实现了卓越的性能，包括出色的视觉保留和对弱纹理的良好保留。代码将在<a href="https://github.com/XunpengYi/Diff-IF">https://github.com/XunpengYi/Diff-IF</a> 提供。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_16-21-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_16-21-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-21_16-21-48"></p><p>文章聚焦于<strong>多模态图像融合（MMIF）领域</strong>，其研究背景源于现有方法的不足及扩散模型应用面临的挑战：</p><ol><li><strong>MMIF的重要性</strong>：由于单一图像传感器成像原理的局限，MMIF技术应运而生，旨在整合不同源图像的互补信息，生成高质量融合图像。经典的MMIF任务包括红外 - 可见光图像融合（IVF）和医学图像融合（MIF）。</li><li><strong>现有方法的问题</strong>：基于生成对抗网络（GAN）的方法虽在图像融合中取得了较好的效果，但存在训练不稳定和模式崩溃的问题，导致融合图像分布不合理、质量低，影响了其实用性。</li><li><strong>扩散模型应用的挑战</strong>：近年来，去噪扩散概率模型（DDPM）在图像生成等领域取得了巨大成功，但在MMIF中，由于缺乏真实标签，基线扩散模型无法直接实现图像融合。现有的基于预训练扩散模型的方法存在适应性差、效率低和设计繁琐等问题。 为解决上述问题，作者提出了一种具有融合知识先验的新型扩散模型Diff - IF，以提供更定制化、高效的MMIF解决方案。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>传统深度学习方法</strong>：早期基于预训练自动编码器的融合策略广泛应用，如CSR、MedCNN等；后来出现基于CNN的端到端融合结构，如U2Fusion；此外，基于transformer、高级语义任务、图像配准与融合相互促进的技术也取得进展。</li><li><strong>生成式深度学习方法</strong>：基于GAN的方法在图像融合任务中因能从分布进行融合，具有高质量和良好视觉感知的优势，但存在训练不稳定和模式崩溃问题；扩散模型在图像生成等领域表现出色，但在多模态图像融合中，现有方法存在设计繁琐、不针对特定任务等问题。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><h3 id="模型原理"><a href="#模型原理" class="headerlink" title="模型原理"></a>模型原理</h3><ul><li><strong>模型拆解</strong>：Diff - IF将图像融合扩散模型分解为带有融合知识先验的条件扩散模型。利用有针对性的搜索技术，为扩散模型提供针对特定图像融合任务的最优先验分布。</li><li><strong>融合知识先验</strong>：代表一般融合结果的通用分布，涵盖了各种融合策略的结果，源图像构成了先验分布的端点。在实际应用中，可以通过定义定制的子集融合先验分布来近似它。</li><li><strong>有针对性的搜索</strong>：旨在在图像融合知识先验的高维流场中找到满足融合任务要求的子集先验分布。通过人工定义或度量评估来确定有针对性的搜索函数，以找到最优的分布解。</li></ul><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><h4 id="正向扩散过程"><a href="#正向扩散过程" class="headerlink" title="正向扩散过程"></a>正向扩散过程</h4><p>从经过有针对性搜索得到的最优融合分布开始，逐步向数据中添加高斯噪声，直到数据分布接近标准高斯分布。该过程由融合知识先验分布通过有针对性的搜索进行引导。</p><h4 id="反向扩散过程"><a href="#反向扩散过程" class="headerlink" title="反向扩散过程"></a>反向扩散过程</h4><p>从标准高斯分布开始，通过网络条件输入（如红外 - 可见光图像融合任务中的可见光图像和红外图像）进行采样，逐步去除噪声，生成符合融合知识先验概率分布的高质量融合图像。</p><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>去噪网络$\epsilon_{\theta}$和细化网络$n_{\theta}$通过时间步$t$调制的网络实现，具体由时间步编码层和图像编码器 - 解码器组成。去噪网络使用SR3的骨干网络，细化网络修改了Restormer的骨干网络，记为R - 块，以实现双输入和有效耦合。</p><h3 id="模型优势"><a href="#模型优势" class="headerlink" title="模型优势"></a>模型优势</h3><ul><li><strong>训练稳定性</strong>：与基于GAN的方法相比，Diff - IF的训练过程更加稳定，避免了模式崩溃的问题。</li><li><strong>高质量融合结果</strong>：通过融合知识先验和定制化的融合任务再训练，Diff - IF能够产生高质量的图像融合结果，具有出色的视觉保留、良好的弱纹理保留和强大的抗噪声能力。</li><li><strong>定制化设计</strong>：Diff - IF避免了无效的预训练模型参数和繁琐的手动融合设计，更适合多模态图像融合任务。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-00-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-00-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-21_17-00-30"></p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-03-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-03-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-21_17-03-24"></p><h3 id="红外-可见光图像融合（IVF）"><a href="#红外-可见光图像融合（IVF）" class="headerlink" title="红外 - 可见光图像融合（IVF）"></a>红外 - 可见光图像融合（IVF）</h3><ol><li><strong>实验细节</strong>：在配备两块NVIDIA Geforce RTX 3090 GPU的机器上使用PyTorch进行实验。根据IVF任务特点设置超参数，训练图像裁剪为128×128，迭代次数40K，批量大小16，采用Adam优化器，学习率为1e - 4。</li><li><strong>数据集和指标</strong>：采用MSRS、RoadScene、LLVIP等公开数据集，还使用GUN数据集进行极端验证。评估指标包括互信息（MI）、差异相关性总和（SCD）、视觉信息保真度（VIF）、$Q_{AB&#x2F;F}$和结构相似性指数（SSIM），指标值越高表示融合图像质量越好。</li><li><strong>融合知识先验和目标搜索</strong>：融合知识先验包含TarDAL、DDFM等现有先进融合方法。采用简单通用样本搜索，以SSIM评估相似性、VIF评估视觉保真度、$Q_{AB&#x2F;F}$评估纹理信息保留，根据任务需求设置权重。</li><li>与SOTA方法比较<ul><li><strong>定性比较</strong>：在多个数据集上，Diff - IF相比其他方法具有明显优势，能保持可见光丰富一致的纹理和场景物理信息，突出红外信息的显著目标和弱热辐射信息，同时具备强大的红外噪声抑制能力。</li><li><strong>定量比较</strong>：Diff - IF在多个指标上取得最佳结果，VIF和SSIM表现良好，MI和$Q_{AB&#x2F;F}$指标表明其能有效融合源图像信息，具有出色的纹理梯度表达能力。</li><li><strong>检测性能比较</strong>：使用YOLOv8作为目标检测骨干网络，Diff - IF在LLVIP数据集上的目标检测任务中表现最优，展示了其在下游任务中的有效性。</li><li><strong>分割性能比较</strong>：在FMB数据集上进行语义分割实验，Diff - IF的融合结果在分割得分和可视化方面表现出色，证明了其高质量的语义保留能力。</li></ul></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-05-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-05-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-21_17-05-44"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-05-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-05-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-21_17-05-58"></p><h3 id="医学图像融合（MIF）"><a href="#医学图像融合（MIF）" class="headerlink" title="医学图像融合（MIF）"></a>医学图像融合（MIF）</h3><ol><li><strong>实验细节</strong>：实验设备和环境与IVF任务一致。根据MIF任务特点修改超参数，训练图像裁剪为128×128，迭代次数16K，批量大小16，采用Adam优化器，学习率为1e - 4。</li><li><strong>数据集和其他配置</strong>：采用哈佛大学医学图像数据集，包括MRI - CT、MRI - PET和MRI - SPECT图像。网络分别用672张图像训练、58张图像测试，评估指标、融合知识构建和目标搜索配置与IVF任务相同。</li><li><strong>定性比较</strong>：在MRI - CT图像中，Diff - IF能更好地保留源图像的纹理，突出高密度成像区域，同时有效保留软组织纹理信息，优于其他方法。</li><li><strong>定量比较</strong>：Diff - IF在大多数评估指标上优于其他先进方法，在信息保留、可视化和纹理保存方面表现出色，展示了其卓越的融合信息保存能力。</li></ol><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><p>通过去除融合知识和目标搜索模块进行消融实验，使用与IVF和MIF任务相同的指标评估性能。结果表明，去除不同模块后，方法性能均不如Diff - IF，证明了融合知识和目标搜索的有效性。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-06-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-21_17-06-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-21_17-06-48"></p><h2 id="基于扩散的多模态图像融合成本比较"><a href="#基于扩散的多模态图像融合成本比较" class="headerlink" title="基于扩散的多模态图像融合成本比较"></a>基于扩散的多模态图像融合成本比较</h2><p>在RTX 3090 GPU上比较Diff - IF和基于预训练扩散的图像融合方法（如DDFM）的资源消耗。结果显示，在960×1440尺寸图像上，DDFM因内存溢出无法运行，而Diff - IF可以处理。此外，Diff - IF在参数数量、计算量和运行时间方面具有明显优势。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出Diff - IF模型为多模态图像融合提供了新的扩散模型范式，解决了缺乏真实标签的问题。该模型有以下优势：</p><ol><li><strong>定制化适配</strong>：与无条件扩散和基于分数的方法不同，Diff - IF更具定制性，避免了无效的预训练模型参数和繁琐的手动融合设计。</li><li><strong>融合效果好</strong>：通过融合知识先验和目标搜索，Diff - IF在红外 - 可见光图像融合和医学图像融合任务中取得了出色的融合效果。</li><li><strong>性能优越</strong>：实验证明Diff - IF具有有效性、可靠性和优越性。</li></ol><p>不过，基于扩散的图像融合方法计算消耗大、运行时间长，Diff - IF虽支持定制设计和加速采样，但仍有改进空间，后续可探索加速扩散过程，使其更适合图像融合任务。</p>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-modality image fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Brain tumor segmentation based on the dual-path network of multi-modal MRI images</title>
      <link href="/post/brain-tumor-segmentation-based-on-the-dual-path-network-of-multi-modal-mri-images/"/>
      <url>/post/brain-tumor-segmentation-based-on-the-dual-path-network-of-multi-modal-mri-images/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Because of the tumor with infiltrative growth, the <strong>glioma boundary is usually fused with the brain tissue</strong>, which leads to the failure of accurately segmenting the brain tumor structure through single-modal images. The multi-modal ones are relatively complemented to the inherent heterogeneity and external boundary, which provide complementary features and outlines. Besides, it can retain the structural characteristics of brain diseases from multi angles. However, due to the particularity of multi-modal medical image sampling that increases uneven data density and dense structural vascular tumor mitosis, the glioma may have atypical boundary fuzzy and more noise. To solve this problem, in this paper, the dualpath network based on multi-modal feature fusion (MFF-DNet) is proposed. Firstly, the proposed network uses different kernels multiplexing methods to realize the combination of the large-scale perceptual domain and the non-linear mapping features, which effectively enhances the coherence of information flow. Then, the over-lapping frequency and the vanishing gradient phenomenon are reduced by the residual connection and the dense connection, which alleviate the mutual influence of multi-modal channels. Finally, a dual-path model based on the DenseNet network and the feature pyramid networks (FPN) is established to realize the fusion of low-level, middle-level, and high-level features. Besides, it increases the diversification of glioma non-linear structural features and improves the segmentation precision. A large number of ablation experiments show the effectiveness of the proposed model. The precision of the whole brain tumor and the core tumor can reach 0.92 and 0.90, respectively.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>由于肿瘤呈浸润性生长，胶质瘤的边界通常与脑组织融合，这导致无法通过单模态图像准确分割脑肿瘤结构。多模态图像在固有异质性和外部边界方面具有相对互补性，提供了互补的特征和轮廓。此外，它可以从多个角度保留脑部疾病的结构特征。然而，由于多模态医学图像采样的特殊性，增加了不均匀的数据密度和密集的结构性血管肿瘤有丝分裂，胶质瘤可能具有非典型的边界模糊和更多的噪声。为了解决这个问题，本文提出了一种基于多模态特征融合的双路径网络（MFF-DNet）。首先，所提出的网络使用不同的核复用方法，实现大规模感知域与非线性映射特征的结合，有效增强信息流的一致性。然后，通过残差连接和密集连接减少重叠频率和梯度消失现象，缓解多模态通道的相互影响。最后，建立了基于DenseNet网络和特征金字塔网络（FPN）的双路径模型，实现低级、中级和高级特征的融合。此外，它增加了胶质瘤非线性结构特征的多样性，提高了分割精度。大量消融实验显示了所提出模型的有效性。整个脑肿瘤和核心肿瘤的精度分别可达0.92和0.90。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>这篇文章聚焦脑肿瘤分割问题，其研究背景主要基于以下几点：</p><ol><li><strong>脑肿瘤特性与诊断需求</strong>：神经胶质瘤是常见中枢神经系统疾病，分为高低不同等级，恶性程度与预后差异大。获取肿瘤位置及分级对治疗至关重要。磁共振成像（MRI）能提供多角度多模态图像，广泛用于脑疾病检测，但肿瘤浸润生长使边界与脑组织融合，单模态图像难以准确分割肿瘤结构。</li><li><strong>传统方法的局限性</strong>：传统深度学习模型虽在脑肿瘤分割上有进展，如卷积神经网络（CNN）、全卷积网络（FCN）、Unet网络、DenseNet网络和特征金字塔网络（FPN）等，但存在诸多不足。如卷积过程丢失边界特征，上采样操作使特征图不完整；不同大小卷积核在采样时易出现重叠不均，导致梯度消失或爆炸；缺乏上下文信息和局部感受野特征，难以实现层间和层内特征融合，影响分类精度。</li><li><strong>本文研究目的</strong>：为克服上述网络的缺点，提高脑肿瘤分割精度，本文提出基于多模态特征融合的双路径网络（MFF - DNet）模型。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多模态 MRI 应用广泛</strong>：多模态 MRI 图像能从多角度保留脑疾病结构特征，在脑肿瘤检测中发挥重要作用，如 FLAIR、T1、T2 和 T1c 图像各具优势，为脑肿瘤检测和诊断提供了丰富信息。</li><li><strong>深度学习模型不断改进</strong>：传统深度学习模型在脑肿瘤分割领域取得显著进展，如 CNN、FCN、Unet、DenseNet 和 FPN 等网络模型。这些模型在提取肿瘤特征和分割肿瘤方面各有特点，但也存在一些不足。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-07-51.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-07-51.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-20_11-07-51"></p><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-07-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-07-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-20_11-07-11"></p><h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><ol><li>不同大小的卷积核复用：<ul><li>使用大卷积核（1×1@128）和小卷积核（1×1@32）训练多模态MRI图像和掩码图像。大卷积核具有大感受野和空间信息，小卷积核具有强非线性映射能力，二者结合可获得肿瘤的低级特征，减少训练参数数量和采样过程中的信息损失，增强信息流的有效性和连贯性。</li><li>1×1卷积的两步操作可实现跨模态交互和多模态信息的集成特征。采用最大池化层，有效减少训练数据量和网络复杂度，增强模型泛化能力。</li></ul></li><li>残差连接和密集连接：<ul><li>将单模态信息视为特征通道，通过密集连接结合多模态局部信息和全局特征，预测像素归属。例如，利用FLAIR图像和T1c图像的特点，结合T2图像获取核心肿瘤的非线性特征并对像素进行分类。</li><li>对输入元素进行标准化处理，调整公式以实现层间标准化，激活特征的移动和缩放。使用ReLU激活函数使特征值落入线性区域，缓解过拟合导致的梯度消失问题；残差连接可自适应地校正大数据的复杂影响，细化网络。</li></ul></li><li>双路径网络：<ul><li>结合DenseUnet和FPN构建双路径模型。DenseUnet将低级特征与中级特征融合，保留大量原始特征，减少参数数量，缓解梯度消失问题；FPN将中级信息与高级特征结合，突出肿瘤结构，增加胶质瘤非线性结构特征的多样性，提高分割精度。</li><li>设置初始学习率为5 * 10⁻⁴，通过自适应运动估计优化器计算偏差校正后的学习率，最后使用softmax函数回归得到输出模型。</li></ul></li></ol><h3 id="模型工作流程"><a href="#模型工作流程" class="headerlink" title="模型工作流程"></a>模型工作流程</h3><ol><li>数据预处理：<ul><li><strong>偏置场校正</strong>：使用N4ITK方法校正MRI图像的偏置场，遍历所有像素计算类内方差，去除磁噪声干扰，确保同一MRI单模态序列的最终强度分布在相似范围内。</li><li><strong>区域生长算法</strong>：通过比较像素的颜色和纹理特征，选择初始种子点，利用区域生长算法得到掩码图像，反映脑肿瘤的形状、外观以及与周围组织的关系，输入网络以快速掌握病变与周围组织的时空关系。</li><li><strong>差异图像（DI）</strong>：计算MRI图像序列相邻特征的差异，去除与脑肿瘤结构识别无关的冗余区域，减少过拟合现象和泛化误差。</li></ul></li><li>特征提取与融合：<ul><li>利用不同大小的卷积核复用获取肿瘤的低级特征，通过残差连接和密集连接调整多模态图像中的特征权重，减少重叠现象的影响。</li><li>双路径网络将低级、中级和高级特征进行融合，增强模型对肿瘤特征的识别和分析能力。</li></ul></li><li><strong>模型训练与优化</strong>：使用BraTS数据集进行训练和测试，设置相关参数，通过自适应运动估计优化器调整学习率，使用softmax函数回归得到输出模型。</li><li><strong>实验评估</strong>：采用Dice、Sensitivity和Specificity指标评估模型的分割精度，通过消融实验从多模态图像、掩码图像和补丁大小三个方面分析模型的有效性，并与其他脑肿瘤分割算法进行比较。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><ul><li><strong>数据集</strong>：使用<strong>BraTS 2015数据集</strong>，包括220例高级别胶质瘤（HGG）和54例低级别胶质瘤（LGG），分为训练集和测试集。</li><li><strong>评估指标</strong>：使用<strong>Dice、Sensitivity和Specificity</strong>指标评估模型的分割精度。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-10-02.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-10-02.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-20_11-10-02"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-10-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-10-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-20_11-10-14"></p><ul><li><strong>与其他算法比较</strong>：与其他脑肿瘤分割算法相比，MFF - DNet模型的全肿瘤和核心肿瘤精度分别可达0.92和0.90，在精度、灵敏度和特异性方面表现更优。</li></ul><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li>消融实验分析：<ul><li><strong>多模态图像</strong>：单模态T1c图像分割精度较高，基于T1c图像测试其他双模态和多模态图像的精度，多模态FLAIR、T1c和T2图像的全肿瘤和核心肿瘤精度可达0.92和0.90。</li><li><strong>掩码图像</strong>：掩码图像提供位置特征，可克服小卷积核导致的信息流不连贯问题，提高分割精度。</li><li><strong>图像补丁</strong>：不同大小的补丁对分割性能影响较小，验证了模型的鲁棒性。</li></ul></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-11-56.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-20_11-11-56.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-20_11-11-56"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出基于<strong>多模态MRI脑肿瘤图像的双路径MFF - DNet模型</strong>，该模型融合低、中、高层特征，增加了胶质瘤非线性结构特征的多样性，提高了分割精度；通过不同内核复用实现了更广泛的感受野和非线性映射能力的结合，解决了边界特征图不完整问题；利用残差连接和密集连接，获得更准确的生理组织和软组织对比结构特征，克服了MRI脑肿瘤图像重叠不均问题。消融实验验证了模型三个创新点在BraTS数据集上的有效性，表明其较现有脑肿瘤分割方法有明显优势。后续工作将聚焦于<strong>特征权重自适应调整和多模态特征的进一步融合</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> Multi-modal MRI Images </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FPL+ Filtered Pseudo Label-Based Unsupervised Cross-Modality Adaptation for 3D Medical Image Segmentation</title>
      <link href="/post/fpl-filtered-pseudo-label-based-unsupervised-cross-modality-adaptation-for-3d-medical-image-segmentation/"/>
      <url>/post/fpl-filtered-pseudo-label-based-unsupervised-cross-modality-adaptation-for-3d-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Adapting a medical image segmentation model to a new domain is important for improving its cross-domain transferability, and due to the expensive annotation process, Unsupervised Domain Adaptation (UDA) is appeal-<br>ing where only unlabeled images are needed for the adaptation. Existing UDA methods are mainly based on image or feature alignment with adversarial training for regularization, and they are limited by insufficient supervision in the target domain. In this paper, we propose an enhanced Filtered Pseudo Label (FPL+)-based UDA method for 3D medical image segmentation. It first uses cross-domain data augmentation to translate labeled images in the source domain to a dual-domain training set consisting of a pseudo source-domain set andapseudo target-domain set. To leverage the dual-domain augmented images to train a pseudo label generator, domain-specific batch normalization layers are used to deal with the domain shift while learning the<br>domain-invariant structure features, generating high-quality pseudo labels for target-domain images. We then combine labeled source-domain images and target-domain images with pseudo labels to train a final segmentor, where image-level weighting based on uncertainty estimation and pixel-level weighting based on dual-domain consensus are proposed to mitigate the adverse effect of noisy pseudo labels. Experiments on three public multi-modal datasets for Vestibular Schwannoma, brain tumor and whole heart segmentation show that our method surpassed ten state-of-the-art UDA methods, and it even achieved better results than fully supervised learning in the target domain in some cases.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>将医学图像分割模型适应到新的领域对于提高其跨领域的可迁移性至关重要</strong>，由于标注过程昂贵，无监督领域适应（UDA）显得极具吸引力，因为只需要未标注的图像进行适应。现有的UDA方法主要基于图像或特征对齐，通过对抗训练进行正则化，但在目标领域中由于监督不足而受到限制。本文提出了一种增强的基于过滤伪标签（FPL+）的UDA方法，用于3D医学图像分割。首先，使用跨领域数据增强将源领域的标注图像转换为由伪源领域集和伪目标领域集组成的双域训练集。为了利用双域增强图像训练伪标签生成器，使用特定领域的批归一化层来处理领域偏移，同时学习领域不变的结构特征，为目标领域图像生成高质量的伪标签。随后，我们结合带有伪标签的源领域标注图像和目标领域图像训练最终分割器，其中基于不确定性估计的图像级加权和基于双域一致性的像素级加权被提出以减轻噪声伪标签的不利影响。在三个公共多模态数据集上的实验，涉及前庭神经鞘瘤、脑肿瘤和全心分割，显示我们的方法超越了十种最先进的UDA方法，甚至在某些情况下在目标领域中取得了比完全监督学习更好的结果。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>这篇文章聚焦于3D医学图像跨模态分割模型的无监督域适应方法研究，其研究背景主要源于以下两方面问题： </p><ol><li><strong>跨模态性能差异大</strong>：医学图像存在多种模态，不同模态间有显著的域差距，如前庭神经鞘瘤分割中的增强T1（ceT1）和高分辨率T2（hrT2）磁共振成像。用一种模态训练的模型在其他模态图像上表现差，且为每种模态手动标注医学图像耗时费力，直接应用训练模型或分别训练新模型都不现实。 </li><li><strong>现有方法局限性</strong>：早期域适应方法需在源域和目标域进行标注，无监督域适应（UDA）虽有潜力，但现有UDA方法多基于图像或特征对齐与对抗训练正则化，主要针对2D医学图像分割，在3D医学图像分割上性能有限。此外，伪标签在UDA中的应用研究较少，因其受源域和目标域间显著域偏移影响，难以生成可靠伪标签，且直接使用含大量噪声的伪标签会误导目标域分割模型训练。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>深度学习在医学图像分割取得进展</strong>：如脑胶质瘤和前庭神经鞘瘤分割算法性能接近手动分割，但不同模态医学图像间存在显著领域差距，模型跨模态表现不佳。</li><li><strong>领域适应方法涌现</strong>：早期领域适应方法需源域和一定程度的目标域标注，半监督DA利用少量标注和大量未标注图像进行适应。无监督领域适应（UDA）成为热门，现有方法主要基于图像或特征对齐，结合对抗训练进行正则化。</li><li><strong>伪标签学习受关注</strong>：伪标签广泛用于训练标注不足或弱标注的分割模型，但在UDA中的应用研究较少。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>本文提出了一种基于<strong>增强过滤伪标签</strong>（FPL+）的**无监督域自适应（UDA）**方法用于3D医学图像分割，以下是该模型的主要构成和关键步骤：</p><h3 id="1-跨域数据增强（Cross-Domain-Data-Augmentation-CDDA）"><a href="#1-跨域数据增强（Cross-Domain-Data-Augmentation-CDDA）" class="headerlink" title="1. 跨域数据增强（Cross - Domain Data Augmentation, CDDA）"></a>1. 跨域数据增强（Cross - Domain Data Augmentation, CDDA）</h3><ul><li><strong>目的</strong>：为了缓解源域和目标域之间的域差距，生成更多可用于训练的样本。</li><li><strong>具体操作</strong>：利用图像风格转换器$T_t$和辅助目标风格转换器$T_{at}$将标记的源域图像$X_{s_i}$转换为两个伪目标域图像$X_{s\rightarrow t_i}$和$X_{s\rightarrow at_i}$，再将它们分别通过$T_s$转换回两个伪源域图像$X_{s’_i}$和$X_{s’’_i}$。这样每个标记的源域图像可以得到四个增强图像，且它们共享相同的分割标签$Y_{s_i}$。增强后的源域训练集$D_{ss} &#x3D; {X_{s_i}, X_{s’_i}, X_{s’’_i}}$和伪目标域训练集$D_{st} &#x3D; {X_{s\rightarrow t_i}, X_{s\rightarrow at_i}}$用于训练伪标签生成器。</li><li><strong>训练损失</strong>：图像转换器$T_s$和$T_t$基于CycleGAN实现，训练涉及两个对抗损失$L_{t_{gan}}$、$L_{s_{gan}}$和一个循环一致性损失$L_{cyc}$。</li></ul><h3 id="2-双域伪标签生成器（Dual-Domain-pseudo-label-Generator-DDG）"><a href="#2-双域伪标签生成器（Dual-Domain-pseudo-label-Generator-DDG）" class="headerlink" title="2. 双域伪标签生成器（Dual - Domain pseudo label Generator, DDG）"></a>2. 双域伪标签生成器（Dual - Domain pseudo label Generator, DDG）</h3><ul><li><strong>目的</strong>：学习增强后的双域图像，为目标域训练集提供高质量的伪标签。</li><li><strong>具体操作</strong>：使用双域批量归一化（Dual - BN）层处理域偏移，同时学习域不变的结构特征。在某一层中，从源域提取的特征由源域BN层归一化，从目标域提取的特征由目标域BN层归一化。</li><li><strong>损失函数</strong>：使用Dice损失$L_{dice}$在$D_{ss}$和$D_{st}$上训练$DDG$。</li></ul><h3 id="3-伪标签过滤（Pseudo-Label-Filtering）"><a href="#3-伪标签过滤（Pseudo-Label-Filtering）" class="headerlink" title="3. 伪标签过滤（Pseudo Label Filtering）"></a>3. 伪标签过滤（Pseudo Label Filtering）</h3><ul><li>目的：抑制不可靠的伪标签，提高最终分割器的训练效果。<ul><li><strong>基于尺寸感知不确定性估计的图像级加权</strong>：使用蒙特卡罗（MC）Dropout进行不确定性估计，计算每个目标域图像的方差图$V_j$，并根据估计的不确定区域大小$\eta_j$对图像级不确定性$v_j$进行归一化，得到图像级不确定性$u_j$，进而计算图像级权重$w_j$。</li><li><strong>基于双域一致性的像素级加权</strong>：将目标域图像$X_{t_j}$通过$T_s$转换为伪源域图像$X_{t\rightarrow s_j}$，并使用$DDG$得到另一个伪标签$\hat{Y}_{t\rightarrow s_j}$。通过比较$\hat{Y}_{t\rightarrow s_j}$和$\hat{Y}_{t_j}$，将一致和不一致的区域分别视为可靠和不可靠的预测，定义像素级权重图$M_j$。最后将图像级权重$w_j$和像素级权重图$M_j$组合成单一权重图$A_j &#x3D; M_j \cdot w_j$，并使用加权Dice损失$L_{w - dice}$进行训练。</li></ul></li></ul><h3 id="4-最终分割器学习（Final-Segmentor-Learning）"><a href="#4-最终分割器学习（Final-Segmentor-Learning）" class="headerlink" title="4. 最终分割器学习（Final Segmentor Learning）"></a>4. 最终分割器学习（Final Segmentor Learning）</h3><ul><li><strong>目的</strong>：结合源域标记图像和带有伪标签的目标域图像进行联合训练，学习域不变特征，提高在目标域的分割性能。</li><li><strong>具体操作</strong>：最终分割器$S$采用与伪标签生成器$G$相同的架构，基于双域批量归一化层设计。其训练损失结合了源域的Dice损失和目标域的加权Dice损失。为了加速训练，最终分割器$S$使用伪标签生成器$G$的权重进行初始化。在测试阶段，直接使用训练好的最终分割器$S$进行推理。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-50-38.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-50-38.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-18_16-50-38"></p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p><strong>前庭神经鞘瘤分割数据集（Vestibular Schwannoma Segmentation Dataset）</strong>：使用公开的包含242名患者的3D MRI图像数据集，有对比增强T1加权（ceT1）和高分辨率T2加权（hrT2）两种模态。将其随机分为200个训练样本、14个验证样本和28个测试样本，进行双向适应实验。实验前对图像进行裁剪和归一化预处理。</p></blockquote><blockquote><p><strong>脑肿瘤分割数据集（BraTS Dataset）</strong>：采用2020年多模态脑肿瘤分割挑战赛数据集，使用T2和FLAIR图像进行双向适应，目标是分割整个肿瘤。从官方训练集中选取样本，部分用于验证和测试。对图像进行强度归一化和去除无肿瘤切片的预处理。</p></blockquote><blockquote><p><strong>心脏分割数据集（MMWHS Dataset）</strong>：该数据集包含20个3D CT扫描和20个3D MRI扫描，分割目标包括升主动脉（AA）、左心房血腔（LAC）、左心室血腔（LVC）和左心室心肌（MYO）。指定MRI为源域，CT为目标域，划分训练、验证和测试集，并对图像进行裁剪和归一化。</p></blockquote><blockquote><p><strong>实现细节</strong>：伪标签生成器G和最终分割器S基于改进的2.5D网络实现，添加额外的BN层用于双域批量归一化。使用Adam优化器训练，G训练200个周期，S初始化后训练100个周期。设置不同的补丁大小和批量大小，按照CycleGAN实现训练图像翻译器Ts和Tt。蒙特卡罗dropout的超参数K设为5，熵阈值e设为0.2。使用PyTorch 1.8.1在NVIDIA GeForce RTX 2080Ti GPU上实现实验，通过Dice分数和平均对称表面距离（ASSD）定量评估分割性能。</p></blockquote><p><strong>将FPL+与十种最先进的无监督域适应（UDA）方法进行比较，同时还与“w&#x2F;o DA”（直接应用源域训练的模型到目标域）、“labeled target”（使用目标域全注释图像训练）和“strong upbound”（使用源域和目标域的标注图像训练双域分割网络）进行对比。</strong></p><ol><li><strong>前庭神经鞘瘤分割结果</strong>：“w&#x2F;o DA”方法在两个方向上的平均Dice分数极低，表明两种模态间存在显著的域差距。所有UDA方法均有改进，FPL+在两个方向上的平均Dice分数分别达到82.92%和91.98%，显著高于其他方法，甚至在某些情况下优于“labeled target”。</li><li><strong>脑胶质瘤分割结果</strong>：在“FLAIR到T2”和“T2到FLAIR”两个方向上，FPL+的平均Dice分数和ASSD均优于其他现有UDA方法，在“T2到FLAIR”方向上超过“labeled target”，略低于“strong upper bound”。</li><li><strong>心脏分割结果</strong>：“w&#x2F;o DA”的Dice分数远低于“labeled target”，表明MR和CT模态间存在明显的域差距。FPL+的平均Dice分数和ASSD分别为73.70%和2.61mm，显著优于现有UDA方法，能更准确地分割心脏子结构。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-53-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-53-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-18_16-53-40"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-53-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-53-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-18_16-53-57"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><p>在VS数据集上对双域伪标签生成器（DDG）和最终分割器S进行全面的消融实验，验证FPL+各组件的有效性，并将伪标签过滤方法与几种现有的抗噪声学习方法进行比较。</p><ol><li><strong>CDDA和Dual - BN对DDG的有效性</strong>：以仅使用Ds→t训练伪标签生成器为基线，实验表明结合Ds→t和Ds训练且不使用Dual - BN时性能有所提升，引入Dual - BN进一步提高性能，使用CDDA生成的双域增强图像结合Dual - BN能获得最高的平均Dice分数。</li><li><strong>最终分割器训练的消融实验</strong>：以标准监督学习使用DDG生成的目标域伪标签为基线，逐步引入添加源域标注图像、使用Dual - BN、从G初始化S、图像级加权和像素级加权等组件。结果显示每个组件都能有效提高分割性能，结合图像级和像素级加权训练最终分割器能获得最佳效果。</li><li><strong>与其他伪标签学习方法的比较</strong>：将FPL+训练S的策略与Co - teaching、GCE Loss和TriNet三种最先进的从噪声标签学习的方法进行比较，结果表明FPL+的性能优于这些方法。</li><li><strong>超参数的有效性</strong>：研究了图像级加权的熵图阈值e和辅助翻译器训练周期数对性能的影响。结果显示e设置为0.2时性能最佳，辅助翻译器训练到200个周期时，伪标签生成器的性能达到峰值。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-54-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-54-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-18_16-54-32"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-54-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-18_16-54-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-18_16-54-52"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了基于<strong>增强过滤伪标签（FPL+）的无监督跨模态适应方法</strong>用于3D医学图像分割，并得出以下结论：</p><ol><li><strong>方法有效性</strong>：通过跨域数据增强和图像、像素级加权等策略，有效对齐源域和目标域图像，筛选可靠伪标签，提升了模型跨域转移能力。在三个公共多模态数据集上的实验表明，该方法超越了十种最先进的无监督域适应（UDA）方法，在某些情况下甚至优于目标域的全监督学习。</li><li><strong>方法局限性</strong>：方法涉及训练伪标签生成器和最终分割器两个步骤，增加了一定复杂性；因GPU内存和图像翻译伪影问题，难以实现3D图像的端到端生成和分割；要求源域和目标域的分割目标可见且拓扑相似。未来，该方法有望应用于其他分割任务。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unsupervised Domain Adaptation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Brain tumor segmentation in MRI with multi-modality spatial information enhancement and boundary shape correction</title>
      <link href="/post/brain-tumor-segmentation-in-mri-with-multi-modality-spatial-information-enhancement-and-boundary-shape-correction/"/>
      <url>/post/brain-tumor-segmentation-in-mri-with-multi-modality-spatial-information-enhancement-and-boundary-shape-correction/</url>
      
        <content type="html"><![CDATA[<p>重庆邮电大学</p><blockquote><p>fluid-attenuated inversion recovery (FLAIR)</p></blockquote><blockquote><p>T1-weighted (T1)</p></blockquote><blockquote><p>contrast-enhanced T1-weighted(T1ce)</p></blockquote><blockquote><p>T2-weighted (T2)</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Brain tumor segmentation</strong> is currently of a priori guiding significance in medical research and clinical diagnosis. Brain tumor segmentation techniques can accurately partition different tumor areas on multi-modality images captured by magnetic resonance imaging (MRI). Due to the unpredictable pathological process of brain tumor generation and growth, brain tumor images often show irregular shapes and uneven internal gray levels. Existing neural network-based segmentation methods with an encoding&#x2F;decoding structure can perform image segmentation to some extent. However, they ignore issues such as differences in multi-modality information, loss of spatial information, and under-utilization of boundary information, thereby limiting the further improvement of segmentation accuracy. This paper proposes a multimodal spatial information enhancement and boundary shape correction method consisting of a modality information extraction (MIE) module, a spatial information enhancement (SIE) module, and a boundary shape correction (BSC) module. The above three modules act on the input, backbone, and loss functions of deep convolutional networks (DCNN), respectively, and compose an end-to-end 3D brain tumor segmentation model. The three proposed modules can solve the low utilization rate of effective modality information, the insufficient spatial information acquisition ability, and the improper segmentation of key boundary positions can be solved. The proposed method was validated on BraTS2017, 2018, and 2019 datasets. Comparative experimental results confirmed the effectiveness and superiority of the proposed method over state-of-the-art segmentation methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>脑肿瘤分割在医学研究和临床诊断中具有优先指导意义。脑肿瘤分割技术可以准确地在磁共振成像（MRI）捕获的多模态图像上划分不同的肿瘤区域。由于脑肿瘤生成和生长的病理过程不可预测，脑肿瘤图像常常表现出不规则形状和内部灰度不均匀。现有基于神经网络的分割方法采用编码&#x2F;解码结构，能够在一定程度上进行图像分割。然而，它们忽视了多模态信息的差异、空间信息的丢失和边界信息的利用不足等问题，从而限制了分割精度的进一步提高。本文提出了一种多模态空间信息增强和边界形状校正方法，该方法由模态信息提取（MIE）模块、空间信息增强（SIE）模块和边界形状校正（BSC）模块组成。上述三个模块分别作用于深度卷积网络（DCNN）的输入、骨干网络和损失函数，并构成一个端到端的三维脑肿瘤分割模型。这三个提出的模块可以解决有效模态信息利用率低、空间信息获取能力不足以及关键边界位置分割不当的问题。所提出的方法在<strong>BraTS2017、2018和2019</strong>数据集上进行了验证。比较实验结果证实了该方法相较于最先进的分割方法的有效性和优越性。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于<strong>脑肿瘤磁共振成像</strong>（MRI）分割，其研究背景主要源于脑肿瘤的高风险及精准分割的重要性，现有方法存在的不足，具体如下：</p><ol><li><strong>脑肿瘤的危害与精准分割的重要性</strong>：脑肿瘤是脑组织癌变引发的异常细胞生长，会导致头痛、认知问题等症状，严重时危及生命。准确识别脑肿瘤位置和形态对临床至关重要，而MRI是诊断和治疗脑肿瘤的重要工具，可提供软组织解剖结构的高分辨率图像。</li><li><strong>现有方法的不足</strong>：当前基于神经网络的分割方法虽有一定效果，但存在诸多局限性。一方面，序列图像结构边界模糊、特征不清晰，且现有方法忽视多模态信息差异，仅叠加多模态MRI扫描结果，导致有价值的多模态信息利用不充分；另一方面，基于卷积神经网络（CNNs）的方法主要捕捉局部信息，难以有效保留表征三维空间全局上下文关系的空间信息，同时忽略边界信息的利用，降低了分割准确性。现有方法尚未能很好地解决这些问题，因此本文提出多模态空间信息增强和边界形状校正方法，以提高脑肿瘤分割的精度。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>医学图像分割方法</strong>：传统方法基于机器学习，如阈值、区域、边缘分割法，但易受噪声影响。深度学习方法通过神经网络训练分类像素，能解决部分问题，综合性能佳。</li><li><strong>维度处理</strong>：包括2D和3D处理方法。2D先切片再拼接，经典网络如FCN、UNet最初用此方法；3D能提供更全面空间信息和立体效果，典型网络有V - Net。</li><li><strong>多模态脑肿瘤图像分割</strong>：BraTS挑战赛推动了相关研究，提出多种创新网络结构，但对多模态和边界稳定性研究不足</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-50-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-50-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-17_15-50-07"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-52-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-52-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-17_15-52-09"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-52-16.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-52-16.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-17_15-52-16"></p><p>文章提出了一种用于脑肿瘤分割的<strong>多模态空间信息增强和边界形状校正方法</strong>，该方法由模态信息提取（MIE）模块、空间信息增强（SIE）模块和边界形状校正（BSC）模块组成，形成了一个端到端的3D脑肿瘤分割模型。</p><h3 id="模态信息提取（MIE）模块"><a href="#模态信息提取（MIE）模块" class="headerlink" title="模态信息提取（MIE）模块"></a>模态信息提取（MIE）模块</h3><ul><li><strong>作用</strong>：解决多模态信息差异问题，通过注意力机制在网络训练期间为关键模态信息分配更高权重，增强多模态信息的融合效果。</li><li><strong>工作流程</strong>：首先将所有模态的三维数据在数据通道维度上合并转换为四维数据，利用两组3D卷积、正则化和激活操作提取各模态的图像特征；然后引入sc - SENet结构，对输出特征数据进行空间和通道重要性加权，得到加权特征作为骨干分割网络的输入数据。</li></ul><h3 id="空间信息增强（SIE）模块"><a href="#空间信息增强（SIE）模块" class="headerlink" title="空间信息增强（SIE）模块"></a>空间信息增强（SIE）模块</h3><ul><li><strong>作用</strong>：解决深度卷积分割网络中空间信息丢失的问题，通过多尺度扩张卷积组扩大空间感受野，增强特征图中的空间信息，使网络有效探索3D图像数据各层之间的全局关系。</li><li><strong>工作流程</strong>：该模块位于基线编码&#x2F;解码过程的卷积层之后、池化层之前，接收卷积层输出的特征图像作为输入，通过扩张卷积组生成$V_i$，再将$V_i$融合到原始特征图中，增强原始输入特征图像的空间信息。输入和输出可表示为$f_{i_{out}}&#x3D;(f_{i_{in}}\otimes V_i)\oplus f_{i_{in}}$。</li></ul><h3 id="边界形状校正（BSC）模块"><a href="#边界形状校正（BSC）模块" class="headerlink" title="边界形状校正（BSC）模块"></a>边界形状校正（BSC）模块</h3><ul><li>作用：解决边界信息利用不足的问题，通过空间关键边界点选择算法和边界校正损失函数，提高预测图像在关键边界位置的分割性能。<ul><li><strong>空间关键边界点选择算法</strong>：在不规则形状的三维图像中选择一组空间关键边界点，有效恢复肿瘤形状。具体步骤包括边界表面提取、边界点选择、通过迭代选择最佳点集。</li><li><strong>边界校正损失函数</strong>：使用空间关键边界点选择算法生成关键边界点，将关键边界点图像与预测图像的重叠关系设置为损失函数，通过梯度反向传播最大化两类图像之间的相关性，实现边界的不断修正。</li></ul></li></ul><p>该模型通过上述三个模块分别对输入、骨干和损失函数进行改进，进一步提高了脑肿瘤分析的分割性能。实验在BraTS2017、2018和2019数据集上进行，对比实验结果证实了该方法相对于现有先进分割方法的有效性和优越性。此外，该模型在细胞分割和皮肤病变分割等其他医学图像分割问题上也表现出良好的适应性。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p><strong>实验准备</strong>：使用Pytorch程序框架实现网络，训练时使用8块TP100 GPU，采用Adam作为优化器，优化器动量为0.9，初始学习率为1×10⁻³，权重衰减为1×10⁻⁵，批量大小为8。对模型进行L2范数正则化，加权衰减率为1×10⁻⁵。从三个分割区域（WT、TC、ET）分别选择一定数量的点，迭代次数t为100，BSC模块中的R分别设置为2（WT）、1（TC）和1（ET）。广义Dice损失和定义的边界校正损失共同训练网络，至少需要500个训练周期，最短训练时间为12小时。</p></blockquote><blockquote><p><strong>数据集</strong>：训练和测试数据集来自<strong>BraTS2017、BraTS2018和BraTS2019</strong>，这些数据集用于医学图像分割竞赛。每个病例有四种模态的3D体积数据，可直接由所提网络处理。采用Dice指数和Hausdorff95距离评估分割的准确性和性能。</p></blockquote><blockquote><p>实现细节<br> <strong>预处理</strong>：对原始数据进行标准化和归一化，使用数据增强技术，包括随机裁剪、随机镜像翻转、随机旋转和随机强度变换等，以减少GPU工作量、提高训练速度和增加训练数据。<br><strong>测试阶段</strong>：将MRI数据扩展后作为网络输入，采用测试时间增强（TTA）方法进行数据增强，计算多个版本的平均输出作为最终结果。在ET区域分割时设置阈值过滤噪声。</p></blockquote><p><strong>对比实验</strong>：将所提网络模型与多个先进的分割网络模型在BraTS2017、BraTS2018和BraTS2019数据集上进行对比实验。结果表明，所提方法在多个分割区域获得了更好的指标分数，尤其在肿瘤区域有显著提升。与先进的2D分割网络相比，所提的3D分割方法在客观评估指标上表现更优。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-54-41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-54-41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-17_15-54-41"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-55-12.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-55-12.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-17_15-55-12"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-54-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-54-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-17_15-54-48"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><p>在相同基线（DMFNet）上逐个添加或减去模块进行消融实验。结果显示，三个功能模块在各分割区域的性能均有提升，不同模块在不同分割区域的贡献不同。所提的BSC损失函数能有效提高三个分割区域的分割性能。此外，添加模块后模型在可接受的模型大小和计算成本增加的情况下取得了最佳性能。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-56-12.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-56-12.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-17_15-56-12"></p><h2 id="扩展到其他医学图像分割问题"><a href="#扩展到其他医学图像分割问题" class="headerlink" title="扩展到其他医学图像分割问题"></a><strong>扩展到其他医学图像分割问题</strong></h2><p>选择细胞分割和皮肤病变分割两个医学图像分割问题进行测试，采用两个公开数据集。实验结果表明，所提模型在这两个问题上具有较好的适应性和竞争力。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-57-12.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-17_15-57-12.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-17_15-57-12"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出由MIE、SIE和BSC三个功能模块构成的新型结构框架，以解决脑肿瘤分割中的多模态、空间信息丢失和关键边界位置问题。研究结果表明：</p><ol><li>各模块有效，该框架在脑肿瘤分割上优于现有方法。 </li><li>该模型对其他医学图像分割问题（如细胞和皮肤病变分割）有良好适应性。</li><li>模型大小和计算成本在实际应用中总体可接受。 未来，作者将从性能和计算效率方面进一步优化分割模型，设计更有效架构和训练策略，并将框架扩展到更多医学图像分割问题，提升其在医学成像领域的适用性和影响力。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multi-modality spatial information </tag>
            
            <tag> boundary shape correction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Adaptive Cross-Feature Fusion Network With Inconsistency Guidance for Multi-Modal Brain Tumor Segmentation</title>
      <link href="/post/adaptive-cross-feature-fusion-network-with-inconsistency-guidance-for-multi-modal-brain-tumor-segmentation/"/>
      <url>/post/adaptive-cross-feature-fusion-network-with-inconsistency-guidance-for-multi-modal-brain-tumor-segmentation/</url>
      
        <content type="html"><![CDATA[<p>深圳大学</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>In the context of contemporary artificial intelligence, increasing deep learning (DL) based segmentation methods have been recently proposed for <strong>brain tumor segmentation (BraTS)</strong> via analysis of multi-modal MRI. However, known DL-based works usually directly fuse the information of different modalities at multiple stages without considering the gap between modalities, leaving much room for performance improvement. In this paper, we introduce a novel deep neural network, termed ACFNet, for accurately segmenting brain tumor in multi-modal MRI. Specifically, ACFNet has a parallel structure with three encoder-decoder streams. The upper and lower streams generate coarse predictions from individual modality, while the middle stream integrates the complementary knowledge of different modalities and bridges the gap between them to yield fine prediction. To effectively integrate the complementary information, we propose an adaptive cross-feature fusion (ACF) module at the encoder that first explores the correlation information between the feature representations from upper and lower streams and then refines the fused correlation information. To bridge the gap between the information frommulti-modal data, we propose a prediction inconsistency guidance (PIG) module at the decoder that helps the network focus more on error-prone regions through a guidance strategy when incorporating the features from the encoder. The guidance is obtained by calculating the prediction inconsistency between upper and lower streams and highlights the gap between multi-modal data. Extensive experiments on the BraTS 2020 dataset show that ACFNet is competent for the BraTS task with promising results and outperforms six mainstream competing methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>在当代人工智能背景下，近年来提出了越来越多基于深度学习（DL）的分割方法，用于通过分析多模态MRI进行脑肿瘤分割（BraTS）。然而，已知的DL方法通常在多个阶段直接融合不同模态的信息，而没有考虑模态之间的差距，留下了很大的性能提升空间。本文中，我们介绍了一种新颖的深度神经网络，称为ACFNet，用于准确分割多模态MRI中的脑肿瘤。具体来说，ACFNet具有三条编码器-解码器流的并行结构。上下流从单个模态产生粗略预测，而中间流整合不同模态的互补知识并弥合它们之间的差距，以产生精细预测。为了有效整合互补信息，我们在编码器中提出了一种自适应交叉特征融合（ACF）模块，该模块首先探索来自上下流的特征表示之间的相关信息，然后优化融合的相关信息。为弥合多模态数据的信息差距，我们在解码器中提出了一种预测不一致性指导（PIG）模块，通过引导策略帮助网络在整合来自编码器的特征时更加关注易错区域。该指导通过计算上下流之间的预测不一致性获得，并突出多模态数据之间的差距。对BraTS 2020数据集的大量实验表明，ACFNet在BraTS任务中表现出色，取得了令人满意的结果，并超过了六种主流竞争方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>脑肿瘤严重威胁患者生命，其智能监测与诊断水平有待提高。磁共振成像（MRI）是脑肿瘤监测与诊断的常用手段，准确的脑肿瘤分割有助于疾病诊断和治疗方案制定，但人工标注主观、繁琐且负担重，因此设计自动化分割方法十分必要。 </p><p>近年来，深度学习助力医学图像分析取得显著进展，为脑肿瘤分割提供了大量灵感。然而，脑肿瘤MRI包含多种模态，如FLAIR、T2、T1和T1ce，如何有效挖掘这些模态的信息是设计多模态脑肿瘤分割方法的关键。现有方法通常采用编码器 - 解码器结构，但大多直接融合不同模态信息，忽略了模态间的差异，即“模态差距”，这会导致各模态细节孤立，影响模型性能，也阻碍了准确描绘肿瘤边界的能力。 </p><p>为解决这些问题，本文提出了自适应交叉特征融合网络（ACFNet），旨在自适应地整合和细化不同模态的特征，解决模态差距问题，实现更准确的多模态脑肿瘤分割。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多模态学习</strong>：多模态数据能提供互补信息，简单拼接多模态图像输入DNN效果不佳，近期更多研究关注基于DNN或病变特征融合多模态数据。</li><li><strong>医学图像分割</strong>：CNN 方法在医学图像分割中占重要地位，U-Net及其变体应用广泛，Vision Transformers也展现出良好性能，部分方法结合二者优势。</li><li><strong>脑肿瘤分割</strong>：早期采用生成概率模型融合多模态信息，目前主要是2D和3D CNN方法。多模态脑肿瘤分割方法分为早期融合、晚期融合和多层融合三类，多层融合效果更好。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-13-41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-13-41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-16_15-13-41"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-16-56.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-16-56.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-16_15-16-56"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-17-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-17-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-16_15-17-07"></p><p>本文提出了一种名为自适应跨特征融合网络（Adaptive Cross-feature Fusion Network，ACFNet）的新型深度神经网络，用于多模态脑肿瘤分割</p><ol><li><strong>整体架构</strong>：ACFNet具有三个并行的编码器 - 解码器流结构，类似于U - Net。将T1和T1ce、T2和FLAIR这两对相关的成像模态分别作为上、下两个流的输入，以更好地学习跨模态特征表示。中间流用于整合不同模态的互补知识。</li><li><strong>自适应跨特征融合模块（ACF模块）</strong></li></ol><ul><li><strong>相关特征探索单元（CFE单元）</strong>：该单元有两个并行分支，通过交叉引导策略探索不同模态特征表示之间的相关性。以一个分支为例，先计算两个模态特征的差值，经全局平均池化（GAP）得到通道统计信息，再通过两个全连接层挖掘通道依赖关系，最终得到包含通道权重的特征，用于突出不同模态之间的差异。另一个分支则交换输入特征的角色，以从不同角度突出差异。    </li><li><strong>跨特征细化单元（CFR单元）</strong>：将CFE单元两个分支的输出沿通道方向拼接，通过空间注意力（SA）块获得空间域的像素权重，对拼接特征进行加权，再经过拼接、卷积操作得到细化后的特征。同时，将该特征与相邻低层CFR单元的输出整合，得到最终输出。</li></ul><ol start="3"><li><strong>预测不一致引导模块（PIG模块）</strong>：利用上、下两个流预测结果的不一致性来引导网络关注易出错区域。在解码阶段，将中间流对应ACF模块的输出与预测不一致信息沿通道方向拼接，经过卷积等操作，使网络聚焦于易出错区域，减少不确定分割，最终生成准确的预测结果。</li><li><strong>损失函数</strong>：ACFNet的总损失由三个流的损失组成，每个流的基本损失Lb是经典的Dice损失和交叉熵损失的线性组合。总损失公式为Lt &#x3D; Lb(P1, G) + Lb(P2, G) + Lb(Po, G)，其中G是每个场景的真实标签集，包含整个肿瘤、增强肿瘤和肿瘤核心的标签。 实验结果表明，ACFNet在多模态脑肿瘤分割任务中表现出色，优于六种主流的医学图像分割方法。消融实验也验证了ACF模块和PIG模块对整体性能提升的重要贡献。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><ol><li><strong>数据集</strong>：选用BraTS 2020数据集评估ACFNet。因该数据集测试部分不可用，仅使用训练部分，包含369个不同对象的多模态MR扫描，每个扫描有T1、T1ce、T2和FLAIR四种模态。从每个体积的轴向平面提取2D切片重建数据集，裁剪为224×224并归一化到[0, 1]，按切片原始对象ID划分为80%训练集和20%测试集。</li><li><strong>评估指标</strong>：采用四个广泛用于医学图像分割的评估指标，即Dice相似度系数、交并比（IoU）、灵敏度和第95百分位豪斯多夫距离（HD95）。</li><li><strong>实现细节</strong>：在PyTorch框架上实现实验，在配备NVIDIA 3090 GPU和Intel 4210R CPU的Ubuntu服务器上训练和测试模型。使用SGD优化器，动量设为0.9，权重衰减设为1e - 4，初始学习率0.01并采用多项式衰减策略，批量大小12，训练100个epoch。</li></ol></blockquote><ol><li><strong>定量结果</strong>：将ACFNet与U - Net、U - Net++等六种主流医学图像分割方法比较。结果显示，ACFNet在多数评估指标上优于竞争对手。例如，在分割全肿瘤（WT）、增强肿瘤（ET）和肿瘤核心（TC）时，Dice值、IoU值和灵敏度值均有提升。单模态方法通常不如多模态方法，可能是直接级联所有模态会忽略其内在相关性，而在网络中间层融合多模态特征有助于发现和利用这些相关性以提高分割性能。此外，ACFNet计算效率方面的浮点运算（FLOPs）和推理时间高于多数竞争方法，但平行结构及ACF和PIG模块对捕捉模态差异、提高分割精度很关键。</li><li><strong>定性结果</strong>：通过可视化结果直观比较分割性能。单模态方法处理背景噪声能力有限，难以准确定位不同大小和形状的脑肿瘤，处理小病灶时易出现漏检。多模态方法因在多模态融合上的努力产生更准确结果，ACFNet在定位小病灶、区分病灶区域与周围环境方面比IVD - Net更准确，且在不同大小和形状的病灶上表现稳定，这得益于其有效的多模态特征融合和减少模态差距的策略。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-19-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-19-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-16_15-19-40"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-19-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-19-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-16_15-19-48"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><p>对ACFNet的两个关键模块ACF和PIG进行消融实验，每次仅移除考虑的模块，保持网络其余部分不变学习新模型。结果表明，ACF模块的CFE和CFR单元以及PIG模块对整体性能提升有积极贡献，可视化结果也显示这两个模块有助于获得准确的分割结果。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-20-36.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-20-36.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-16_15-20-36"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-20-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-20-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-16_15-20-54"></p><h2 id="失败案例"><a href="#失败案例" class="headerlink" title="失败案例"></a>失败案例</h2><p>ACFNet在一些具有挑战性的场景中无法准确分割脑肿瘤，如无法准确定位大肿瘤的精细细节，在肿瘤边界模糊时分割性能不佳。可能原因是ACFNet在这些场景中对复杂肿瘤的表示能力有限，特别是肿瘤与非肿瘤区域相似度高以及肿瘤区域内部差异大时。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-21-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-16_15-21-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-16_15-21-48"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>近年来，多模态数据中的自动脑肿瘤分割受到了越来越多的关注。本文介绍了一种名为ACFNet的新型深度神经网络，用于在磁共振成像（MRI）中实现准确的脑肿瘤分割，其动机主要体现在两个方面：</p><ol><li>直接整合来自不同模态的中间特征可能会导致信息掩蔽，高效融合多模态数据有助于获得更具判别性的特征表示并提高分割性能。</li><li>正确发现多模态数据中的差异并利用这些差异来引导网络，有助于减少可能的假阴性或假阳性结果。 为了有效融合多模态数据，ACFNet使用了自适应交叉特征融合（ACF）模块来探索中间层的内在互补性；为了获得准确的分割性能，ACFNet利用了预测不一致性引导（PIG）模块，该模块能根据不同模态预测结果之间的不一致性，引导网络关注容易出错的区域。</li></ol><p>实验结果表明，ACFNet可以有效解决<strong>多模态脑肿瘤分割问题</strong>，取得了令人满意的结果。此外，消融实验结果也验证了ACF模块和PIG模块的有效性。 基于本文的研究，未来的研究方向主要有两个：</p><blockquote><ol><li>探索ACFNet在除MRI之外的其他多模态图像分割任务中的性能。 </li><li>开发降低ACFNet计算需求的方法，使其更有可能在临床环境中部署。</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACFNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MsgFusion Medical Semantic Guided Two-Branch Network for Multimodal Brain Image Fusion</title>
      <link href="/post/msgfusion-medical-semantic-guided-two-branch-network-for-multimodal-brain-image-fusion/"/>
      <url>/post/msgfusion-medical-semantic-guided-two-branch-network-for-multimodal-brain-image-fusion/</url>
      
        <content type="html"><![CDATA[<p>作者单位</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Multimodal image fusion plays an essential role in medical image analysis and application, where computed tomography (CT), magnetic resonance (MR), single-photon emission computed tomography (SPECT), and positron emission tomography (PET) are commonly-used modalities, especially for brain disease diagnoses. Most existing fusion methods do not consider the characteristics of medical images, and they adopt similar strategies and assessment standards to natural image fusion. While distinctive medical semantic information (MS-Info) is hidden in different modalities, the ultimate clinical assessment of the fusion results is ignored. Our MsgFusion first builds a relationship between the key MS-Info of the MR&#x2F;CT&#x2F;PET&#x2F;SPECT images and image features to guide the CNN feature extractions using two branches and the design of the image fusion framework. For MR images, we combine the spatial domain feature and frequency domain feature (SF) to develop one branch. For PET&#x2F;SPECT&#x2F;CT images, we integrate the gray color space feature and adapt the HSV color space feature (GV) to develop another branch. A classification-based hierarchical fusion strategy is also proposed to reconstruct the fusion images to persist and enhance the salient MS-Info reflecting anatomical structure and functional metabolism. Fusion experiments are carried out on many pairs of MR-PET&#x2F;SPECT andMR-CT images. According to seven classical objective quality assessments and one new subjective clinical<br>quality assessment from 30 clinical doctors, the fusion results of the proposed MsgFusion are superior to those of the existing representative methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>多模态图像融合</strong>在医学图像分析和应用中起着至关重要的作用，其中计算机断层扫描（CT）、磁共振成像（MR）、单光子发射计算机断层扫描（SPECT）和正电子发射断层扫描（PET）是常用的模态，尤其在脑疾病诊断中。大多数现有的融合方法没有考虑医学图像的特征，采用与自然图像融合类似的策略和评估标准。尽管不同模态中隐藏着独特的医学语义信息（MS-Info），但融合结果的最终临床评估被忽视。我们的MsgFusion首先建立了MR&#x2F;CT&#x2F;PET&#x2F;SPECT图像的关键MS-Info与图像特征之间的关系，以指导使用两个分支的CNN特征提取和图像融合框架的设计。对于MR图像，我们结合空间域特征和频率域特征（SF）来开发一个分支。对于PET&#x2F;SPECT&#x2F;CT图像，我们整合灰度颜色空间特征并调整HSV颜色空间特征（GV）以开发另一个分支。还提出了一种基于分类的分层融合策略，用于重建融合图像，以保持和增强反映解剖结构和功能代谢的显著MS-Info。融合实验在多组MR-PET&#x2F;SPECT和MR-CT图像上进行。根据七种经典客观质量评估和来自30位临床医生的一项新的主观临床质量评估，所提出的MsgFusion的融合结果优于现有的代表性方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦<strong>多模态脑图像融合</strong>方法研究，撰写目的在于解决现有方法的局限，其研究背景主要如下：</p><ol><li><strong>多模态脑图像融合的重要性</strong>：多模态图像融合在医学图像分析与应用中作用重大，特别是在脑部疾病诊断方面。CT、MR、SPECT 和 PET 是常用的成像方式，融合这些图像能让医生更方便观察和诊断病情。 </li><li><strong>现有方法的局限性</strong>：多数现有融合方法未考虑医学图像特性，采用与自然图像融合相似的策略和评估标准，忽略了不同模态中独特的医学语义信息（MS - Info），以及对融合结果的最终临床评估。 </li><li><strong>研究的必要性</strong>：为更好地保留和增强关键 MS - Info，提高脑部疾病诊断的准确性，作者提出 MsgFusion 方法，旨在深入分析医学图像特性，设计专用网络模型，实现多模态脑图像的有效融合。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ol><li><strong>传统融合方法</strong>：受人工认知驱动，依赖手动提取特征，需专业知识和复杂参数调整，且针对特定应用场景。</li><li><strong>深度学习融合方法</strong>：数据驱动，能学习大量样本获取抽象特征，具有较强的鲁棒性和泛化能力，但在医学图像分析领域仍处于早期发展阶段。</li><li><strong>应用情况</strong>：图像融合已广泛应用于计算机视觉、遥感、交通等领域，在医学领域自20世纪90年代开始发展，常用于脑疾病诊断。</li></ol><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>设计了一个双分支网络，包括<strong>SF分支和GV分支</strong>，通过结合多模态医学图像的关键医学语义信息（MS - Info）与图像特征，引导CNN特征提取和图像融合框架的设计。</p><h3 id="分支设计"><a href="#分支设计" class="headerlink" title="分支设计"></a>分支设计</h3><ol><li><strong>SF分支</strong>：<br>主要用于提取MR图像的深度特征。结合空间域和频率域信息，通过傅里叶变换将图像从空间域转换到频率域，更方便地提取与关键MS - Info对应的特征，最大程度保留和增强MR图像的关键MS - Info。</li></ol><ul><li><strong>空间域特征提取</strong>：通过神经网络的卷积操作获取源MR图像的深度特征。设置卷积核大小为7×7，步长为1，填充为3，通道放大到64，经过批量归一化和LeakyReLU激活函数处理，输出记为SF1。   </li><li><strong>频率域特征提取</strong>：对图像进行二维离散傅里叶变换和逆变换，得到频率域处理后的输出记为SF2。通过计算傅里叶变换后复数的振幅和相位，获取图像的全局和局部信息。最后将空间域和频率域的特征进行融合，采用加权图特征融合多尺度深度特征，得到最终的融合特征图。</li></ul><ol start="2"><li><strong>GV分支</strong>：用于提取CT&#x2F;PET&#x2F;SPECT图像的深度特征。结合多尺度灰度图像和HSV颜色空间的亮度信息，采用多尺度级联策略和HSV颜色空间变换，提取全局轮廓、局部形状特征和亮度信息。</li></ol><ul><li><strong>多尺度级联特征提取</strong>：从灰度空间采用多尺度级联策略，通过卷积层、池化层、上采样和跳跃连接操作，提取源图像的全局轮廓和局部形状特征，补偿不同尺度下的信息损失。采用多尺度和跳跃连接策略，捕获不同尺度和层的信息，减少信息损失。   </li><li><strong>HSV颜色空间亮度信息提取</strong>：对PET&#x2F;SPECT图像进行HSV颜色空间变换，计算V分量，并定义一个新的亮度值V′，以突出功能图像的MS - Info。通过后续处理提取HSV颜色空间的深度特征GV2。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-15_16-34-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-15_16-34-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-15_16-34-04"></p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ol><li><strong>实验数据</strong>：实验在不同的脑图像对上进行，包括MR-SPECT、MR-CT和MR-PET图像对。其中555对MR-PET图像对作为训练集，来自ADNI；测试使用了30对MR-CT、MR-SPECT和MR-PET图像对，来自Whole Brain Atlas，涵盖脑血管疾病（中风）和肿瘤疾病（脑肿瘤）。</li><li><strong>对比方法</strong>：将MsgFusion与九种其他代表性方法进行比较，分别是LatLrr、IFCNN、NestFuse、atsIF、FusionDN、FusionGAN、FunFuseAn、WPADCPCNN和OLTPSpS。</li><li><strong>评估指标</strong>：采用六种常用评估指标（EN、SD、MI、rSFe、SM、VIFF）和一个相对新颖的指标RQ来评估融合效果。由于不同指标值差异较大，进行了适当的线性变换以便在同一图中比较。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-15_16-37-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-15_16-37-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-15_16-37-59"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-15_16-38-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-15_16-38-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-15_16-38-06"></p><ul><li><strong>MR - SPECT对融合</strong>：源MR图像清晰显示脑脊液等软组织的纹理细节，SPECT的不同颜色和亮度可反映代谢信息。对比各方法融合结果，MsgFusion在多个区域能保留更清晰的结构和纹理细节，其评估指标（如EN、SD、MI、rSFe和VIFF）表现最佳，能保留和增强重要医学信息。</li><li><strong>MR - CT对融合</strong>：CT图像易显示骨骼等硬轮廓，MR图像显示软组织结构。聚焦六个感兴趣区域（ROI）分析，MsgFusion在各区域能更好地保留和增强重要医学特征，除rSFe外其他评估指标值最佳。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-15_16-39-20.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-15_16-39-20.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-15_16-39-20"></p><h2 id="其他实验"><a href="#其他实验" class="headerlink" title="其他实验"></a>其他实验</h2><ol><li><strong>计算成本</strong>：在特定计算环境下对30对不同数据模态进行测试，记录每种方法的平均运行时间。MsgFusion平均运行时间为2.0048秒，时间成本可接受。</li><li><strong>问卷调查</strong>：向30位不同医院神经内科和医学影像科临床医生发放问卷，对融合效果进行主观评价。问卷基于15组融合实验设计15个问题，每个问题提供6种代表性方法的融合图像作为选项，医生根据临床经验选择最有利于观察和诊断的结果。统计结果显示，MsgFusion在8组实验中最常被选为最佳融合图像，在4组中第二常被选，其融合效果远超其他方法。</li></ol><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><p>为验证SF分支中结合频域和GV分支中结合HSV颜色空间的必要性和有效性，对MR - PET融合进行消融实验。结果表明，不考虑频域处理会丢失更多MR图像信息，不考虑HSV颜色空间改进亮度则无法反映PET的功能信息，同时考虑两者时融合效果明显更好。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-15_16-40-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-15_16-40-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-15_16-40-52"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种由<strong>医学语义信息</strong>（MS - Info）引导的脑部疾病图像<strong>深度特征融合方法MsgFusion</strong>。分析了MR&#x2F;CT&#x2F;PET&#x2F;SPECT的关键MS - Info，设计了包含SF - 分支和GV - 分支的双分支网络。实验表明，与现有方法相比，该方法具有明显优势，具体体现在：在MR - CT、MR - SPECT、MR - PET图像融合实验中效果更佳；临床医生通过问卷调查评估融合结果，统计数据显示MsgFusion的融合效果最佳。未来，作者将考虑扩展框架，整合CT、MR、PET、SPECT、DTI等更多成像模态，并应用于临床诊断。 </p>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像融合 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MR </tag>
            
            <tag> CT </tag>
            
            <tag> PET </tag>
            
            <tag> SPECT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Automatic Brain Segmentation for PET/MR Dual-Modal Images Through a Cross-Fusion Mechanism</title>
      <link href="/post/automatic-brain-segmentation-for-petmr-dual-modal-images-through-a-cross-fusion-mechanism/"/>
      <url>/post/automatic-brain-segmentation-for-petmr-dual-modal-images-through-a-cross-fusion-mechanism/</url>
      
        <content type="html"><![CDATA[<p>作者单位</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>The precise segmentation of different <strong>brain regions and tissues</strong> is usually a prerequisite for the detection and diagnosis of various neurological disorders in neuroscience. Considering the abundance of functional and structural dual-modality information for <strong>positron emission tomography&#x2F;magnetic resonance</strong> (PET&#x2F;MR) images, we propose a novel 3D whole-brain segmentation network with a cross-fusion mechanism introduced to obtain 45 brain regions. Specifically, the network processes PET and MR images simultaneously, employing UX-Net and a cross-fusion block for feature extraction and fusion in the encoder. We test our method by comparing it with other deep learning-based methods, including 3DUXNET, Swin-UNETR, UNETR, nnFormer, UNet3D, NestedUNet, ResUNet, and VNet. The experimental results demonstrate that the proposed method achieves better segmentation performance in terms of both visual and quantitative evaluation metrics and achieves more precise segmentation in three views while preserving fine details. In particular, the proposed method achieves superior quantitative results, with a Dice coefficient of 85.73% ± 0.01%, a Jaccard index of 76.68% ± 0.02%, a sensitivity of 85.00% ± 0.01%, a precision of 83.26% ± 0.03% and a Hausdorff distance (HD) of 4.4885 ± 14.85%. Moreover, the distribution and correlation of the SUV in the volume of interest (VOI) are also evaluated (PCC &gt; 0.9), indicating consistency with the ground truth and the superiority of the proposed method. In future work, we will utilize our whole-brain segmentation method in clinical practice to assist doctors in accurately diagnosing and treating brain diseases.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>不同脑区和组织的精确分割通常是神经科学中检测和诊断各种神经系统疾病的前提。考虑到正电子发射断层扫描&#x2F;磁共振（PET&#x2F;MR）图像的功能和结构双模态信息的丰富性，我们提出了一种新颖的三维全脑分割网络，通过引入交叉融合机制来获得45个脑区。具体而言，该网络同时处理PET和MR图像，在编码器中采用UX-Net和交叉融合块进行特征提取和融合。我们通过与其他基于深度学习的方法进行比较来测试我们的方法，包括3DUXNET、Swin-UNETR、UNETR、nnFormer、UNet3D、NestedUNet、ResUNet和VNet。实验结果表明，所提出的方法在视觉和定量评估指标方面实现了更好的分割性能，并在三个视图中实现了更精确的分割，同时保留了细节。特别是，所提出的方法在量化结果方面表现优异，Dice系数为85.73% ± 0.01%，Jaccard指数为76.68% ± 0.02%，敏感性为85.00% ± 0.01%，精确度为83.26% ± 0.03%，Hausdorff距离（HD）为4.4885 ± 14.85%。此外，还评估了感兴趣区域（VOI）中SUV的分布和相关性（PCC &gt; 0.9），表明与真实值的一致性以及所提出方法的优越性。在未来的工作中，我们将利用我们的全脑分割方法在临床实践中帮助医生准确诊断和治疗脑疾病。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于脑区分割研究，旨在解决现有方法的局限，其研究背景如下：</p><ol><li><strong>脑区分割的重要性</strong>：精准的脑区分割对神经科学研究和临床诊断意义重大，不同脑区的体积、表面积和形态与帕金森病、阿尔茨海默病等多种神经系统疾病相关。</li><li><strong>PET&#x2F;MR成像系统的优势</strong>：正电子发射断层扫描&#x2F;磁共振（PET&#x2F;MR）集成成像系统结合了PET代谢成像和MR高分辨率成像的优点，是诊断脑部疾病的有效工具。 </li><li><strong>现有脑区分割方法的不足</strong>：手动分割脑医学图像耗时费力，且结果易受个体差异和主观因素影响；传统自动分割方法依赖手动特征工程，对图像质量和噪声敏感，对解剖变异的鲁棒性差；现有的深度学习方法多基于单模态医学图像分割，部分融合双模态的方法分割的脑区较少，融合方式简单，缺乏深度和全面的整合。 因此，本文提出一种基于交叉融合机制的自动脑分割方法，充分利用PET和MR医学图像，结合两者的功能和结构信息，以实现更精确、全面的脑区自动分割。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>传统方法</strong>：可分为<strong>阈值和统计方法</strong>、<strong>图像处理和数学模型方法</strong>、<strong>基于图谱的方法</strong>，但依赖手动特征工程，对图像质量和噪声敏感，对解剖变异鲁棒性差。</li><li><strong>深度学习方法</strong>：基于高分辨率MR图像或代谢成像与低分辨率PET的单模态分割方法较多，部分方法开始融合PET和MR双模态信息，但存在分割脑区少、融合方法简单的问题。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-51-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-51-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-10_20-51-30"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-51-36.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-51-36.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-10_20-51-36"></p><h3 id="网络整体结构"><a href="#网络整体结构" class="headerlink" title="网络整体结构"></a>网络整体结构</h3><p>模型采用经典的U形分割结构，由编码器（encoder）和解码器（decoder）组成。PET和MR图像同时作为输入，通过编码器中的特征提取模块（UX - Net block）和融合模块（cross - fusion block）进行特征提取与融合，最后由解码器完成脑分割任务。</p><h3 id="编码器部分"><a href="#编码器部分" class="headerlink" title="编码器部分"></a>编码器部分</h3><ul><li>UX-Net Block：作为特征提取器，是网络的骨干部分。该模块采用分层变压器（hierarchical transformer）的思想，使用大规模深度卷积（large-scale depthwise convolution）在不增加计算成本的情况下获取局部信息，适合分层变压器结构并保留了卷积神经网络（CNN）的归纳偏置等优点。<ul><li>首先通过大卷积核的投影层提取图像块特征。</li><li>分四个阶段提取不同层次的特征，每个阶段包含两个7×7×7的大规模深度卷积和两个1×1×1的逐点卷积，以丰富特征表示并减少通道冗余。</li><li>每个阶段使用2×2×2的标准卷积模块将特征分辨率降低1&#x2F;2，并应用层归一化（layer normalization）和高斯误差线性单元（GELU）作为激活函数。</li><li>最后通过包含两个批量归一化的3×3×3卷积层的残差块稳定提取的特征。</li></ul></li><li>Cross - Fusion Block：主要用于融合经过残差块处理后的PET和MR图像在四个阶段提取的特征，由两个交叉注意力块（cross - attention blocks）组成。<ul><li>利用多通道交叉注意力机制（multichannel cross - attention mechanism）在两个不同来源的特征之间建立复杂关联，实现全面的特征交叉融合。</li><li>具体过程是先计算查询（query）、键（key）和值（value），得到两个特征的注意力分布，实现全局关联，再通过卷积层进一步处理输出特征。</li><li>通过两次交叉注意力计算（CA1和CA2）并拼接结果，最后通过残差连接得到最终的交叉融合结果。</li></ul></li></ul><h3 id="解码器部分"><a href="#解码器部分" class="headerlink" title="解码器部分"></a>解码器部分</h3><p>编码器各阶段生成的多尺度输出通过跳跃连接（skip connections）连接到解码器，形成U形网络用于下游分割任务。</p><ul><li>解码器将各阶段特征上采样并与前一阶段的特征拼接，经过残差块处理。</li><li>网络的原始输入和编码器的输出按从上到下顺序设置为[e0, e1, e2, e3, e4]，解码器的输出按从上到下顺序设置为[d1, d2, d3, d4]。</li><li>以第3个解码器层为例，先对第4个解码器层的输出进行上采样，然后与编码器第2层的输出拼接，经过残差块得到该层输出。</li><li>最后将解码器的输出输入到包含1×1×1卷积层和softmax激活函数的残差块中，预测分割概率。</li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>结合了Dice系数损失（Dice coefﬁcient loss）和交叉熵损失（cross - entropy loss）构建混合损失函数，以充分利用两种损失函数的优点，增强模型在图像分割任务中的性能。</p><ul><li>Dice系数损失用于衡量两个样本之间的相似度，常用于图像分割任务。</li><li>交叉熵损失是图像分割中广泛使用的损失函数。</li><li>总损失L是Dice系数损失Ldice和交叉熵损失Lce的加权和，权重为$ω$。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p><strong>数据集</strong>：使用了110名受试者的18F - FDG PET&#x2F;MR脑图像，所有数据在3.0T的PET&#x2F;MRI集成扫描仪上采集。利用FreeSurfer工具获取45个脑区作为真实标签（GT），并对PET和MR图像进行配准和独热编码处理。</p></blockquote><ol><li>训练实现<ul><li><strong>数据预处理</strong>：对输入图像的像素值进行最小 - 最大归一化，并裁剪前景以消除背景。同时，引入正负样本的随机裁剪，获取96×96×96的图像块。</li><li><strong>数据增强</strong>：采用随机缩放、随机翻转和随机强度调整等技术增加数据多样性，缓解过拟合问题。</li><li><strong>训练设置</strong>：在PyTorch框架、Windows 10系统和NVIDIA GeForce RTX 3090 24GB GPU上进行训练。训练250个轮次，批次大小设为1，学习率为1e - 4，使用AdamW动态调整训练损失，每次训练迭代输入2个96×96×96的图像块，损失权重ω设为1。</li><li><strong>对比方法</strong>：将提出的方法与四种基于Transformer的模型（3DUXNET、SwinUNETR、UNETR、nnFormer）和四种基于CNN的模型（UNet3D、NestedUNet、ResUNet、VNet）进行对比。</li></ul></li><li><strong>评估指标</strong>：采用五种评估指标，包括Dice相似度、Jaccard系数、精确率、灵敏度和Hausdorff距离（HD），全面评估分割性能</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-55-55.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-55-55.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-10_20-55-55"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-56-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-56-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-10_20-56-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-56-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-56-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-10_20-56-24"></p><ul><li><strong>整体定量评估</strong>：可视化全脑分割结果，提出的方法在三个视图上的分割结果更接近GT，能完整分割45个脑区并保留细节。定量结果显示，该方法在各项指标上表现优越，相比最佳对比方法有显著提升。</li><li><strong>特定脑区评估</strong>：分析特定脑区（如壳核、海马体、尾状核和右 inferior - 侧脑室）的分割结果，提出的方法能更好地保留边缘细节，与GT的一致性更高。</li><li><strong>一致性和相关性分析</strong>：分析PET图像分割结果在不同感兴趣体积（VOI）区域的标准摄取值（SUV）分布和相关性，提出的方法的SUV分布与GT最一致，相关系数PCC较高。</li><li><strong>临床定量质量评估</strong>：计算肿瘤与背景比和临床容忍率，提出的方法临床容忍率最低，分割性能显著优于其他方法。</li><li>额外数据评估和验证<ul><li><strong>额外数据测试</strong>：引入40例PET和MR数据进行测试，提出的方法在额外数据集上仍保持优越性能，证明了模型的鲁棒性和通用性。</li><li><strong>噪声数据测试</strong>：向测试集的PET图像中添加不同水平的噪声，提出的方法在噪声环境下仍能准确分割脑区，表现出较强的鲁棒性，而NestedUNet受噪声影响最大。</li></ul></li></ul><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li>消融研究和融合模块分析<ul><li><strong>消融分析</strong>：验证了UX - Net块和交叉融合块在模型分割效果中的关键作用，替换这两个模块会导致定量指标下降。</li><li><strong>交叉融合块有效性分析</strong>：交叉融合机制在Dice分数、Jaccard系数和精确率方面取得最佳定量结果，分割性能出色。</li></ul></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-57-20.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-10_20-57-20.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-10_20-57-20"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出一种基于<strong>交叉融合机制</strong>的自动脑分割方法，将<strong>PET和MR多模态信息</strong>融合用于精确脑分割。实验在PET&#x2F;MR数据集上开展，并与8种深度学习方法对比，从视觉、定量和临床三方面评估分割结果，证明该方法的优越性。具体而言，此方法取得了优异的定量结果，Dice得分85.73% ± 0.01%，Jaccard指数76.68% ± 0.02%等，SUV相关性评估也显示其优于其他方法，临床误差容忍率不超5%。该方法实现了精确的全脑分割，对脑部疾病的临床诊断和分析有益。未来，作者将探索该方法在其他模态图像处理任务中的应用。</p>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> brain regions and tissues </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>.nii数据的预处理</title>
      <link href="/post/mr-duo-mo-tai-shu-ju/"/>
      <url>/post/mr-duo-mo-tai-shu-ju/</url>
      
        <content type="html"><![CDATA[<blockquote><p>.nii数据的预处理，参考F2Net</p></blockquote><p>转换后的.npy文件数量与原始.nii文件数量的关系如下：</p><h3 id="数量关系说明"><a href="#数量关系说明" class="headerlink" title="数量关系说明"></a>数量关系说明</h3><ol><li><p><strong>输入.nii文件</strong>：每个病例包含5个.nii文件</p><ul><li>FLAIR模态 (flair.nii)</li><li>T1模态 (t1.nii)</li><li>T1CE模态 (t1ce.nii)</li><li>T2模态 (t2.nii)</li><li>分割掩码 (seg.nii)</li></ul></li><li><p><strong>输出.npy文件</strong>：每个.nii文件中包含多个切片(通常约155个轴向切片)，但代码只处理<strong>包含肿瘤区域</strong>的切片</p></li><li><p><strong>转换关系公式</strong>：</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.npy文件总数 <span class="operator">=</span> <span class="number">2</span> × 所有病例中包含肿瘤的切片总数</span><br></pre></td></tr></table></figure></li><li><p><strong>具体说明</strong>：</p><ul><li>每个包含肿瘤的切片会生成2个.npy文件：<ul><li>一个在trainImage文件夹(四模态图像数据)</li><li>一个在trainGt文件夹(对应分割掩码)</li></ul></li><li>不包含肿瘤的切片会被跳过，不会生成.npy文件</li></ul></li></ol><h3 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h3><p>假设您有100个病例，每个病例的MRI体积包含155个切片，而每个病例平均有40个切片包含肿瘤区域：</p><ul><li>原始.nii文件总数 &#x3D; 100个病例 × 5个模态文件 &#x3D; 500个文件</li><li>包含肿瘤的切片总数 &#x3D; 100个病例 × 40个包含肿瘤的切片 &#x3D; 4,000个切片</li><li>生成的.npy文件总数 &#x3D; 4,000个切片 × 2个文件&#x2F;切片 &#x3D; 8,000个文件<ul><li>trainImage文件夹: 4,000个文件</li><li>trainGt文件夹: 4,000个文件</li></ul></li></ul><p>因此，转换后的.npy文件数量通常会远多于原始.nii文件的数量，但具体倍数取决于每个病例中包含肿瘤的切片数量。这种转换将3D体积数据转换为适合2D CNN训练的单独切片，只保留了对训练有价值的含肿瘤区域的切片。</p>]]></content>
      
      
      <categories>
          
          <category> 数据集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flexible Fusion Network for Multi-Modal Brain Tumor Segmentation</title>
      <link href="/post/flexible-fusion-network-for-multi-modal-brain-tumor-segmentation/"/>
      <url>/post/flexible-fusion-network-for-multi-modal-brain-tumor-segmentation/</url>
      
        <content type="html"><![CDATA[<blockquote><p>南京理工大学、东南大学</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Automated brain tumor segmentation is crucial for aiding brain disease diagnosis and evaluating disease progress. Currently, <strong>magnetic resonance imaging</strong> (MRI) is a routinely adopted approach in the field of brain tumor segmentation that can provide different modality images. It is critical to leverage multi-modal images to boost brain tumor segmentation performance. Existing works commonly concentrate on generating a shared representation by fusing multi-modal data, while few methods take into account modality-specific characteristics. Besides, how to efficiently fuse arbitrary numbers of modalities is still a difficult task. In this study, we present a flexible fusion network (termed F2Net) for multi-modal brain tumor segmentation, which can flexibly fuse arbitrary numbers of multi-modal information to explore complementary information while maintaining the specific characteristics of each modality. Our F2Net is based on the encoder-decoder structure, which utilizes two Transformer-based feature learning streams and a cross-modal shared learning network to extract individual and shared feature representations. To effectively integrate the knowledge from the multi-modality data, we propose a cross-modal feature enhanced module (CFM) and a multi-modal collaboration module (MCM), which aims at fusing the multi-modal features into the shared learning network and incorporating the features from encoders into the shared decoder, respectively. Extensive experimental results on multiple benchmark datasets demonstrate the effectiveness of our F2Net over other state-of-the-art segmentation methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>自动化脑肿瘤分割对于辅助脑疾病诊断和评估疾病进展至关重要。目前，磁共振成像（MRI）是脑肿瘤分割领域常用的方法，可以提供不同的模态图像。利用多模态图像提升脑肿瘤分割性能是关键。现有研究通常关注通过融合多模态数据生成共享表示，而很少有方法考虑模态特异性特征。此外，如何有效地融合任意数量的模态仍是一个困难的任务。在本研究中，我们提出了一种灵活的融合网络（称为F2Net）用于多模态脑肿瘤分割，该网络可以灵活地融合任意数量的多模态信息，以探索互补信息，同时保持每种模态的特异性特征。我们的F2Net基于编码器-解码器结构，利用两个基于Transformer的特征学习流和一个跨模态共享学习网络来提取个体和共享特征表示。为了有效整合多模态数据的知识，我们提出了一个跨模态特征增强模块（CFM）和一个多模态协作模块（MCM），分别旨在将多模态特征融合到共享学习网络中，并将编码器中的特征整合到共享解码器中。多个基准数据集上的广泛实验结果证明了我们的F2Net在其他最新分割方法上的有效性。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦多模态脑肿瘤分割，旨在提升分割性能，其研究背景如下：</p><ul><li><p><strong>临床需求</strong>：脑肿瘤是全球致命疾病之一，及时检测对临床评估和治疗策略至关重要。磁共振成像（MRI）是分析脑肿瘤的常用工具，包含<strong>T1、T2、T1CE和FLAIR四种成像模态</strong>，各模态能提供大脑结构的独特信息和肿瘤不同子区域的互补信息。但自动脑肿瘤分割面临肿瘤大小、形状和位置多样性大等挑战。</p></li><li><p><strong>现有方法局限</strong>：卷积神经网络（CNNs）在医学分割任务中取得显著成功，如U - Net及其变体，但现有多模态脑肿瘤分割方法存在不足。早期融合策略不能有效保留各模态特征和探索模态间联系；晚期融合模型虽提取独立特征，但未充分利用模态特征提升分割性能；部分方法学习融合信息时忽略了模态编码器特征的重要性。因此，如何有效融合多模态数据，同时挖掘共享信息和捕捉模态特定特征以获得良好的分割结果仍是挑战。</p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_09-56-38.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_09-56-38.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-09_09-56-38"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>医学图像分割</strong>：CNN 算法广泛应用，U - Net 及其变体表现出色，且已拓展到 3D 分割；Transformer 用于计算机视觉任务，部分结合 Transformer 的模型在医学图像分割中取得进展。</li><li><strong>脑肿瘤分割</strong>：从生成概率模型转向神经网络模型，包括 2D 和 3D CNN 模型，部分模型结合 Transformer 提升性能。</li><li><strong>多模态学习</strong>：多模态数据融合受关注，有多种融合算法，如独立深度学习流、模态感知模块、共享特征提取等方法。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_09-58-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_09-58-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-09_09-58-59"></p><p>F2Net主要包含两部分：两个<strong>基于Transformer的特定学习网络</strong>和一个<strong>跨模态共享学习网络</strong>。多模态数据首先输入到两个基于Transformer的特定学习网络中学习多级特征，然后通过提出的跨模态特征增强模块（CFM）对多级特征表示进行融合，融合后的特征以逐层策略通过共享学习网络。同时，使用跳跃连接将编码器路径的原始多级特征表示合并到相应的解码器中。此外，还提出了多模态协作模块（MCM），有选择地将特定编码器和共享编码器的特征集成到共享解码器中，以提供更丰富的信息，提高特征表示能力。</p><ol><li>基于Transformer的特定学习网络<ul><li>以基于Transformer的结构作为骨干网络，如PVTv2，它能捕捉长距离依赖关系。输入图像被划分为不重叠的块，将块特征投影到C维空间，并添加位置嵌入后输入到具有空间缩减注意力（SA）层的基于Transformer的编码器中。</li><li>采用渐进收缩策略生成多尺度特征，得到两组多尺度的四个特征F1和F2。</li><li>解码器路径使用跳跃连接将编码器的特征合并到解码器中，通过简单的级联块（“Cas. Block”）组合分层特征。每个解码器都有监督信号，有助于最终分割结果。</li></ul></li><li>跨模态共享学习网络<ul><li><strong>跨模态特征增强模块（CFM）</strong>：为有效利用不同模态的独特特征而提出。首先将两个模态特定编码器的特征进行拼接，经过3×3卷积层和Sigmoid激活函数生成归一化图wi，作为特征级注意力权重。然后将两个特征与wi相乘，得到增强的特征表示。接着对增强的特征图进行求和、平滑处理，并与原始特征拼接，最后结合上一层的上下文信息。CFM能够有效捕捉多模态之间的互补信息，减少背景噪声。</li><li><strong>多模态协作模块（MCM）</strong>：在跨模态共享学习网络的解码器中使用。将模态特定编码器的特征与共享编码器的特征相乘，再与共享编码器的特征拼接，经过一系列操作得到增强的特征表示。还将模态特定编码器的特征协作相乘，与前面的增强特征拼接，最终得到协作特征。MCM能够利用多模态数据的互补信息，提高脑肿瘤分割结果。</li><li><strong>解码器路径</strong>：利用融合后的特征fMCM1、fMCM2、fMCM3进行解码，通过跳跃连接将编码器的分层特征合并到共享解码器中，最终输出作为最终分割结果。</li></ul></li><li><strong>损失函数</strong>：采用混合损失评估预测结果与真实标签之间的差异，混合损失包括加权Dice损失和交叉熵（CE）损失。F2Net的总损失函数由共享学习网络的损失、两个模态特定解码器的损失组成。</li></ol><h3 id="模型优势"><a href="#模型优势" class="headerlink" title="模型优势"></a>模型优势</h3><ul><li><strong>灵活性</strong>：能够灵活融合任意数量的多模态信息，探索互补信息的同时保留每个模态的特定特征。</li><li><strong>有效性</strong>：在多个基准数据集上的实验结果表明，F2Net优于其他最先进的分割方法。</li><li><strong>创新性</strong>：提出的CFM和MCM模块有效提高了特征表示能力和分割性能。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p><strong>数据集</strong>：在三个数据集上评估F2Net，分别是<strong>BraTS 2019、BraTS 2020和ISLES 2015 SISS</strong>。其中，BraTS 2019有335个多模态磁共振扫描，BraTS 2020有369个多模态磁共振扫描，每个受试者都有T1、T1CE、T2和FLAIR四种模态及对应的分割掩码；ISLES 2015 SISS有28个训练病例，包含T1、T1CE、T2和扩散加权成像（DWI）四种模态。实验采用体积轴向平面的2D切片，对输入图像进行归一化和边界裁剪，并将每个2D切片裁剪为224×224大小。同时，将每个数据集中的所有受试者随机分为80%的训练数据和20%的测试数据。</p></blockquote><blockquote><p><strong>实现细节</strong>：使用在ImageNet - 1K上预训练的PVTv2作为F2Net的骨干网络，在一块NVIDIA RTX 2080Ti GPU上训练。采用SGD优化器，初始学习率为0.01，并按照“Poly”策略降低学习率，批大小设置为12，训练100个epoch。从BraTS 2019和BraTS 2020数据集中采用四种模态训练网络，从ISLES 2015数据集中选择三种模态训练分割方法。代码使用<strong>PyTorch和MindSpore</strong>实现，对于比较方法，除TranSiam外，均使用发布的源代码进行训练，TranSiam则按照原论文设置进行复现。</p></blockquote><blockquote><p><strong>评估指标</strong>：使用四个指标评估所有分割方法的有效性，分别是骰子分数（Dice）、交并比（IoU）、95%豪斯多夫距离（HD95）和敏感度。</p></blockquote><ol><li><strong>定量结果</strong>：选择七种最先进的方法进行比较实验，包括单模态和多模态分割方法。结果表明，F2Net在这些数据集上的分割性能均高于其他比较方法，其关键优势在于能有效利用多模态数据间的相关性和互补信息，提升多模态脑肿瘤分割性能。</li><li><strong>定性比较</strong>：可视化结果显示，F2Net取得了良好进展，与其他方法相比，漏分割组织更少。单模态方法往往无法分割不同类型的脑肿瘤，因为它们未考虑各模态的特征和模态间的关系；与其他多模态分割方法相比，由于提出的CFM和MCM，F2Net能取得更准确的结果和更清晰的边缘。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-03-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-03-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-09_10-03-44"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-03-50.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-03-50.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-09_10-03-50"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-04-31.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-04-31.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-09_10-04-31"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ol><li><strong>CFM的有效性</strong>：通过两个基线实验研究CFM的重要性，结果表明CFM能提高多模态融合的表征能力，提升分割性能，引入MCM时也能观察到性能提升。</li><li><strong>MCM的有效性</strong>：基于一个基线实验研究MCM的益处，结果显示引入MCM组件能有效利用各模态特定编码器提取的特征，准确定位和分割真实肿瘤区域，在四个评估指标上均有更好表现。</li><li><strong>灵活融合策略的有效性</strong>：在BraTS 2020数据集上设计多个消融实验，结果表明使用共享学习网络能获得更好性能，引入CFM和MCM时融合策略能挖掘丰富模式并产生高性能；使用四个完整模态时，分割精度相对更好，说明该融合策略能有效融合多模态数据，探索共享信息和模态特定特征。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-05-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-05-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-09_10-05-33"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-05-42.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-09_10-05-42.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-09_10-05-42"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>文章提出了用于<strong>多模态脑肿瘤分割</strong>的灵活融合网络F2Net，并得出以下结论： </p><ol><li><strong>性能优越</strong>：F2Net能灵活融合任意数量的多模态信息，在三个数据集上的实验表明，相比其他先进的分割方法，它能有效利用多模态数据的相关性和互补信息，取得更高的分割性能。 </li><li><strong>模块有效</strong>：交叉模态特征增强模块（CFM）和多模态协作模块（MCM）可有效捕捉多模态之间的互补信息，抑制背景噪声，提升特征表示能力，从而提高分割性能。</li><li><strong>策略可行</strong>：灵活融合策略能高效融合多模态数据，挖掘丰富模式，明确探索共享信息和模态特定特征，使用更多模态时分割结果更好。不过，该模型在边界模糊和大病变情况下效果欠佳，后续需研究更轻量级骨干网络和自适应多模态融合方法。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MACTFusion Lightweight Cross Transformer for Adaptive Multimodal Medical Image Fusion</title>
      <link href="/post/mactfusion-lightweight-cross-transformer-for-adaptive-multimodal-medical-image-fusion/"/>
      <url>/post/mactfusion-lightweight-cross-transformer-for-adaptive-multimodal-medical-image-fusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p>南华大学</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Multimodal medical image fusion</strong> aims to integrate complementary information from different modalities of medical images. Deep learning methods, especially recent vision Transformers, have effectively improved image fusion performance. However, there are limitations for Transformers in image fusion, such as lacks of local feature extraction and cross-modal feature interaction, resulting in insufficient multimodal feature extraction and integration. In addition, the computational cost of Transformers is higher. To address these challenges, in this work, we develop an adaptive cross-modal fusion strategy for unsupervised multimodal medical image fusion. Specifically, we propose a novel lightweight cross Transformer based on cross multi-axis attention mechanism. It includes cross-window attention and cross-grid attention to mine and integrate both local and global interactions of multimodal features. The cross Transformer is further guided by a spatial adaptation fusion module, which allows the model to focus on the most relevant information. Moreover, we design a special feature extraction module that combines multiple gradient residual dense convolutional and Transformer layers to obtain local features from coarse to fine and capture global features. The proposed strategy significantly boosts the fusion performance while minimizing computational costs. Extensive experiments, including clinical brain tumor image fusion, have shown that our model can achieve clearer texture details and better visual quality than other state-of-the-art fusion methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>多模态医学图像融合</strong>旨在整合来自不同医学图像模态的互补信息。深度学习方法，尤其是近期的视觉Transformer，有效提升了图像融合性能。然而，Transformer在图像融合中存在局限性，例如缺乏局部特征提取和跨模态特征交互，导致多模态特征提取和整合不够充分。此外，Transformer的计算成本较高。为了解决这些挑战，本研究开发了一种用于无监督<strong>多模态医学图像融合的自适应跨模态融合策略</strong>。具体而言，我们提出了一种基于跨多轴注意机制的新型轻量级跨Transformer。它包括跨窗口注意和跨网格注意，以挖掘和整合多模态特征的局部和全局交互。跨Transformer进一步由空间适应融合模块引导，使模型能够聚焦于最相关的信息。此外，我们设计了一个特殊的特征提取模块，该模块结合了多个梯度残差密集卷积层和Transformer层，以从粗到细获取局部特征并捕获全局特征。所提出的策略在显著提升融合性能的同时，最小化了计算成本。大量实验，包括临床脑肿瘤图像融合，表明我们的模型能够比其他先进融合方法实现更清晰的纹理细节和更好的视觉质量。</p><blockquote><p>code: <a href="https://github.com/millieXie/MACTFusion">https://github.com/millieXie/MACTFusion</a></p></blockquote><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>多模态医学影像在疾病诊断中至关重要，但不同模态影像各有优劣。结构影像（如CT、MRI）空间分辨率高，能展现器官解剖结构，但CT缺乏结构组织细节信息，MRI缺乏高对比度信息；功能影像（如PET、SPECT）可反映生物分子血流和代谢活动，但空间分辨率低。多模态影像融合旨在整合不同影像的互补信息，增强视觉感知。 </p><p>现有的医学影像融合方法分为经典方法和深度学习方法。经典方法存在不可逆数据丢失、细节失真、融合规则设计复杂等问题；深度学习方法虽有优势，但CNN框架会忽略长程交互，导致全局上下文特征丢失，而Transformer方法存在局部特征提取和跨模态特征交互不足、计算成本高等问题。 </p><p>受 <strong>Vision Transformer</strong>成功应用的启发，本文提出一种自适应跨模态融合策略，开发了轻量级的MACTFusion跨Transformer架构，旨在以较低计算成本挖掘和整合多模态特征的局部和全局交互，提高融合性能，为后续疾病诊断、治疗规划和手术导航等提供更有价值的影像信息。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多模态医学成像重要性</strong>：多模态医学成像在疾病诊断中发挥关键作用，医学图像分为结构和功能信息图像两类，不同成像方式各有优劣，图像融合可整合互补信息。</li><li><strong>融合方法分类</strong>：现有医学图像融合方法分为经典方法和深度学习方法。经典方法包括多尺度变换、脉冲耦合神经网络和稀疏表示等，但存在不可逆数据损失和细节失真等问题。深度学习方法主要有卷积神经网络（CNNs）、生成对抗网络（GANs）和Transformers等，在融合性能上有提升，但仍有局限。</li><li><strong>Transformer应用</strong>：Transformer在医学图像融合中得到应用，如SwinFusion等，但多数网络融合策略简单，无法充分挖掘和整合跨模态信息，且计算成本高。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-47-08.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-47-08.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-07_19-47-08"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-49-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-49-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-07_19-49-14"></p><p>模型主要聚焦于MRI与其他医学图像（如CT、PET和SPECT图像）的融合，包含特征提取、自适应跨模态融合和图像重建三个部分：</p><ol><li>特征提取<ul><li><strong>浅层特征提取</strong>：由密集特征提取器和梯度密集块（GRDB）组成。密集特征提取器通过1×1卷积降低通道维度，再用三个级联的3×3卷积层提取局部特征并拼接输出。GRDB则通过梯度操作增强对细粒度细节的学习。</li><li><strong>深层特征提取</strong>：由简单深度可分离卷积（SD - Conv）块和多轴注意力块构成。SD - Conv块可作为位置编码生成器，使模型无需显式的位置编码层。多轴注意力块由窗口注意力和网格注意力组成，可降低计算复杂度，同时获取局部和全局交互信息。</li></ul></li><li>自适应跨模态融合策略<ul><li><strong>空间自适应融合模块（SAFM）</strong>：基于空间自适应归一化思想，可自适应地重新映射特征分布，使模型学习到丰富的纹理和合适的强度信息。</li><li><strong>跨模态融合Transformer</strong>：包括跨窗口注意力和跨网格注意力，用于挖掘和整合多模态特征的局部和全局交互。输入特征先经过SD - Conv块处理，再通过跨窗口注意力进行局部交互，接着通过跨网格注意力进行全局交互，最后将两个对称分支的输出特征拼接得到最终的融合特征。</li></ul></li><li><strong>图像重建</strong>：通过图像重建模块从融合特征中重建融合图像。</li></ol><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>综合考虑多个方面定义了联合损失函数，包括结构相似性指数损失（SSIM）、梯度损失（纹理损失）和强度损失，以保证融合图像的视觉保真度和强度信息。公式为：$L &#x3D; \delta_1L_{ssim} + \delta_2L_{grad} + \delta_3L_{int}$，其中$\delta_1$、$\delta_2$和$\delta_3$是控制各损失项权重的参数。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-53-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-53-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-07_19-53-54"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-54-16.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-54-16.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-07_19-54-16"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-54-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-54-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-07_19-54-26"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-54-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-54-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-07_19-54-48"></p><ol><li><p>对比实验与评估指标：</p><ul><li><strong>对比方法</strong>：与CSMCA、EMFusion、FusionGAN、IFCNN、U2Fusion、IFT和SwinFusion等先进方法进行对比。</li><li><strong>评估指标</strong>：选用了包括相关差异之和（SCD）、多尺度结构相似性指数（MS - SSIM）、边缘信息测量（QAB&#x2F;F）、标准差（SD）、加权融合质量指标（Qw）和视觉信息保真度（VIF）在内的六个常见定量指标进行评估。</li></ul></li><li><p>不同模态医学图像融合实验：</p><ul><li><strong>CT - MRI融合</strong>：使用哈佛医学数据集，MACTFusion在保留CT的密集信息和MRI的纹理细节方面表现出色，在各项定量指标上取得最优结果。</li><li><strong>PET - MRI融合</strong>：该模型能较好地保留PET的功能信息和MRI的纹理细节，颜色分布更接近PET图像，在多个定量指标上表现优异。</li><li><strong>SPECT - MRI融合</strong>：MACTFusion获取了更多细节并保留了SPECT图像的适当颜色信息，在多数指标上取得最优结果。</li></ul></li><li><p><strong>临床多模态脑肿瘤图像融合实验</strong>：与临床医生合作，对脑胶质瘤、转移性肿瘤和脑膜瘤的MR - T1和MR - T2图像进行融合。融合结果清晰度更高、纹理信息更丰富，有助于医生评估肿瘤异质性和了解肿瘤对周围脑组织的浸润情况。</p></li><li><p><strong>GFP和PC图像融合实验</strong>：验证了MACTFusion的<strong>泛化能力</strong>，该模型与SwinFusion一样，能有效保留纹理和颜色信息。</p></li><li><p><strong>医学图像分割应用实验</strong>：将融合结果应用于医学图像<strong>分割任务</strong>，MACTFusion的分割结果比单模态图像和SwinFusion的结果更准</p></li></ol><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>网络架构分析</strong>：对浅特征提取模块（SFE）、空间自适应融合模块（SAFM）和跨模态融合Transformer（CMFT）进行消融实验。结果表明，缺少这些模块会导致相应指标下降，影响模型提取细节、映射特征分布和整合补充信息的能力。</li><li><strong>损失函数分析</strong>：对联合损失函数中的结构相似性损失Lssim、梯度损失Lgrad和强度损失Lint进行消融实验。结果显示，缺少任何一个子损失都会影响融合性能，而联合损失函数使MACTFusion在所有评估指标上取得最优结果。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-56-46.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-56-46.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-07_19-56-46"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-56-55.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-56-55.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-07_19-56-55"></p><h2 id="计算复杂度分析"><a href="#计算复杂度分析" class="headerlink" title="计算复杂度分析"></a>计算复杂度分析</h2><p>通过训练参数数量和FLOPs评估实现效率，MACTFusion的FLOPs最小，训练参数数量少于IFT和SwinFusion，训练运行时间约为SwinFusion的四分之一，同时VIF表现最佳，证明该方法高效有效。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-58-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-07_19-58-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-07_19-58-37"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这项工作中，我们提出了一种新的<strong>轻量级自适应交叉Transformer架构</strong>，命名为<strong>MACTFusion</strong>，用于多模态医学图像融合。设计的交叉多轴注意机制包括交叉窗口注意以整合局部特征，以及交叉网格注意以整合来自不同模态的全局特征。具体来说，我们引入了空间自适应融合模块来引导交叉Transformer，使模型聚焦于最相关的信息。此外，我们设计了一个特殊的特征提取模块，该模块结合了多个梯度残差密集卷积和Transformer层，以从粗到细获取局部特征，并有效捕获全局特征。值得注意的是，所提出的模型在保持卓越融合性能的同时，尽量减少了计算成本。大量实验结果表明，我们的模型优于其他先进的融合技术。因此，它在后续诊断、治疗计划和手术导航中具有重要的应用价值。未来，我们将尝试将提出的MACTFusion应用于临床医学图像融合。此外，我们还将扩展其应用至3D图像融合。</p><blockquote><ol><li><strong>方法优势</strong>：设计的交叉多轴注意力机制能整合不同模态的局部和全局特征，空间适应融合模块引导交叉 Transformer 聚焦关键信息，特殊特征提取模块可有效获取局部和全局特征。该模型在降低计算成本的同时，保持了出色的融合性能。</li><li><strong>实验验证</strong>：大量实验结果表明，MACTFusion 优于其他先进融合技术，在后续诊断、治疗规划和手术导航等方面具有重要应用价值。</li><li><strong>未来展望</strong>：未来将把 MACTFusion 应用于临床医疗图像融合，并拓展到 3D 图像融合领域。</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像融合 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multimodal Medical Image Fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BTSegDiff Brain tumor segmentation based on multimodal MRI Dynamically guided diffusion probability model</title>
      <link href="/post/btsegdiff-brain-tumor-segmentation-based-on-multimodal-mri-dynamically-guided-diffusion-probability-model/"/>
      <url>/post/btsegdiff-brain-tumor-segmentation-based-on-multimodal-mri-dynamically-guided-diffusion-probability-model/</url>
      
        <content type="html"><![CDATA[<p>云南大学、锡根大学</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>In the treatment of brain tumors, accurate diagnosis and treatment heavily rely on reliable <strong>brain tumor segmentation</strong>, where multimodal <strong>Magnetic Resonance Imaging</strong> (MRI) plays a pivotal role by providing valuable complementary information. This integration significantly enhances the performance of brain tumor segmen­tation. However, due to the uneven grayscale distribution, irregular shapes, and significant size variations in brain tumor images, this task remains highly challenging. In order to overcome these obstacles, we have introduced a novel framework for automated segmentation of brain tumors that leverages the diverse infor­mation from multi-modal MRI scans. Our proposed method is named BTSegDiff and it is based on a Diffusion Probability Model (DPM). First, we designed a dynamic conditional guidance module consisting of an encoder. This encoder is used to extract information from multimodal MRI images and guide the DPM in generating accurate and realistic segmentation masks. During the guidance process, we need to fuse the diffused generated features with the extracted multimodal features. However, diffusion process itself introduces a significant amount of Gaussian noise, which can affect the fusion results. Therefore, we designed a Fourier domain feature fusion module to transfer this fusion process to Euclidean space and reduce the impact of high-frequency noise on fusion. Lastly, we have taken into account that the DPM, as a generative model, produces non-unique results with each sampling. In the meticulous field of medicine, this is highly detrimental. Therefore, we have designed a Stepwise Uncertainty Sampling module based on Monte Carlo uncertainty calculation to generate unique out­ comes and enhance segmentation accuracy simultaneously. To validate the effectiveness of our approach, we perform a validation on the popular BraTs2020 and BraTS2021 benchmarks. The experimental results show that our method outperforms many existing brain tumor segmentation methods. Our code is available at <a href="https://github.com/jaceqin/BTSegDiff">https://github.com/jaceqin/BTSegDiff</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>在脑肿瘤的治疗中，准确的诊断和治疗严重依赖于可靠的脑肿瘤分割，而多模态磁共振成像（MRI）通过提供有价值的补充信息发挥关键作用。这种整合显著提高了脑肿瘤分割的性能。然而，由于脑肿瘤图像中灰度分布不均、形状不规则以及大小差异显著，这项任务仍然极具挑战性。为了克服这些障碍，我们引入了一种利用多模态MRI扫描的多样信息进行脑肿瘤自动分割的新框架。我们提出的方法名为BTSegDiff，基于扩散概率模型（DPM）。首先，我们设计了一个动态条件指导模块，该模块由编码器组成。编码器用于从多模态MRI图像中提取信息，并指导DPM生成准确和逼真的分割掩码。在指导过程中，我们需要将扩散生成的特征与提取的多模态特征进行融合。然而，扩散过程本身引入了大量的高斯噪声，这可能影响融合结果。因此，我们设计了一个傅里叶域特征融合模块，将该融合过程转移到欧几里得空间，并减少高频噪声对融合的影响。最后，我们考虑到DPM作为生成模型，每次采样产生的结果都不唯一。在医学这个细致的领域中，这非常不利。因此，我们设计了一个基于蒙特卡洛不确定性计算的逐步不确定性采样模块，以生成独特结果并同时提高分割精度。为了验证我们方法的有效性，我们在流行的BraTs2020和BraTS2021基准上进行了验证。实验结果表明，我们的方法优于许多现有的脑肿瘤分割方法。我们的代码可在<a href="https://github.com/jaceqin/BTSegDiff%E3%80%82">https://github.com/jaceqin/BTSegDiff。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于<strong>脑肿瘤分割研究</strong>，其背景源于脑肿瘤发病率上升，准确分割对诊断治疗至关重要。传统手动分割耗人力且依赖医生经验，计算机辅助自动分割方法具有重要临床价值，但面临诸多挑战。 早期基于区域生成、边缘检测和阈值分割的自动分割方法虽有进步，但需手动调参，难以处理多模态图像。深度学习发展使基于其的医学图像分割方法展现出强大特征学习能力，U - Net 提升了脑肿瘤分割性能，Transform 能更好学习全局信息。然而，这些方法仍存在不足，如 CNN 难以学习全局特征，Transform 需为不同数据集设计合适架构。 扩散概率模型（DPM）在生成任务和医学成像领域有一定成效，但现有基于 DPM 的脑肿瘤分割方法忽视了多模态信息提取和模型不确定性定量分析。为解决这些问题，作者提出基于 DPM 的脑肿瘤分割框架 BTSegDiff，旨在利用多模态 MRI 信息引导 DPM 生成高质量分割结果，减少噪声影响，解决 DPM 结果不唯一问题，提升分割准确性和稳定性。 </p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-39-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-39-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-06_20-39-04"></p><p>​                                                    Different modalities in the multimodal MRI dataset</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>传统方法</strong>：早期基于区域生成、边缘检测和阈值分割的自动化方法，相比手动方法提高了脑肿瘤分割效率，具有广泛适用性和低成本的优点，但仍需手动调整参数，且难以处理多模态图像。</li><li><strong>深度学习方法</strong>：基于深度学习的医学图像分割方法特征学习能力强。U - Net改进了脑肿瘤分割性能，但学习脑肿瘤图像全局特征的能力不足；Transform能捕捉长距离依赖关系，在脑肿瘤分割领域表现出色，但需为每个数据集专门设计合适的模型。</li><li><strong>扩散概率模型</strong>：扩散概率模型（DPM）在生成任务和医学成像中展现出有效性，现有基于DPM的方法在脑肿瘤分割领域超越了Transform，但忽略了多模态信息提取和模型不确定性的定量分析。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-41-38.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-41-38.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-06_20-41-38"></p><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>使用基于<strong>ResNet</strong>修改的EC和EU编码器对原始图像和当前估计图像进行编码，结合多级残差连接且无批量归一化层，使用组归一化层。每个编码块先进行卷积层下采样，再连接由两个卷积层、组归一化和SiLU激活组成的残差块。解码器Du和Dm基于U - Net，Dm由两个含注意力层的残差块组成，Du先连接三个残差块接收Dm信息，再连接五个下采样卷积层，每层后接三个残差块，部分残差块后有注意力层，编码器各层与解码器同层有跳跃连接。</p><h3 id="多模态动态条件引导"><a href="#多模态动态条件引导" class="headerlink" title="多模态动态条件引导"></a>多模态动态条件引导</h3><p>以往采用固定图像引导扩散概率模型，但脑组织结构复杂，MRI图像存在偏差场效应和噪声，正常与患病组织灰度相似，固定输入图像会影响网络学习能力。因此，该模块将当前步骤的分割信息与原始图像的分割信息融合，公式为$A &#x3D; LN (m_{xt})⨁LN (m_{I})$ ，其中⨁表示逐元素相加，LN ( • )表示归一化。不过当前步骤存在高斯噪声会影响融合结果，为此提出傅里叶域特征融合模块来抑制噪声影响。</p><h3 id="傅里叶域特征融合"><a href="#傅里叶域特征融合" class="headerlink" title="傅里叶域特征融合"></a>傅里叶域特征融合</h3><p>受相关文献启发，该模块将当前特征图和条件输入特征图转换到傅里叶空间以减轻噪声影响。对两个特征图进行二维快速傅里叶变换（2D FFT），与参数化权重图逐元素相乘，增强后的特征图逐元素相加并通过sigmoid激活函数生成傅里叶空间的亲和图，公式为$M &#x3D; Sigmoid ( ( F [ m_{xI,t} ] ⊗W ) ⨁(F [m_{I}] ⊗W ) )$ ，其中⊙表示逐元素相乘，F [ • ]表示2D FFT，m为输入特征图，W为参数化权重图。最后通过二维逆快速傅里叶变换（2D IFFT）将亲和图转换回欧几里得空间，公式为$\tilde{m} &#x3D; F ^{-1}[M ]$ 。</p><h3 id="基于不确定性度量的采样"><a href="#基于不确定性度量的采样" class="headerlink" title="基于不确定性度量的采样"></a>基于不确定性度量的采样</h3><p>扩散模型是生成模型，传统生成任务中多次采样结果不确定，但脑肿瘤分割区域固定。该模块发现时间步越接近$x_0$，预测图像越准确、不确定性越低。借鉴MC Dropout和Diff - UNet计算不确定性，公式为$U_{t} &#x3D; -X_{t} log(X_{t})$ ，其中$X_{t} &#x3D; \frac{1}{S} \sum_{1}^{S} X_{S_{t}}$ ，S为每个扩散步骤生成的结果数量。结合不确定性和预测步数得到最终分割结果，权重公式为$w_{t} &#x3D; e^{sigmoid ( \frac{t}{T} )}×(1 - U_{t})$ ，最终预测结果$Y &#x3D; \sum_{t &#x3D; 1}^{T} w_{t} × X_{t}$ ，其中t为当前预测步骤，T为总预测步数。</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>训练过程依据DPM设置，扩散步数T设为1000。每次迭代随机获取图像I及其真实二值分割图G，从均匀分布中采样迭代步数t，从标准分布中采样噪声$\epsilon$ 。损失函数为$Loss &#x3D; E_{x_0,\epsilon,t} [ | \epsilon - \epsilon_{\theta} ( \sqrt{\alpha_{t}} x_0 + \epsilon \sqrt{ (1 - \alpha_{t}) }, I, t ) |^2 + \frac{1}{2} \sum( x_{I,t} - G )^2 ]$ ，训练时设$x_0 &#x3D; G$ 。采样过程为随机过程，保存每步生成的$x_{I,t}$ 用于基于不确定性度量的采样，最终生成分割掩码。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集:</p></blockquote><ul><li><p><strong>脑肿瘤数据集（BraTs2020 - 2021）</strong>：BraTs2020和BraTs2021数据集分别包含369和1251名患者的训练数据。将训练集按轴向切片，每个患者数据切成155片，选取第80 - 120片，打乱后划分训练集和测试集。BraTs2020使用了来自340 - 1200名患者脑肿瘤MR图像的13940个训练数据，测试集为来自29名患者的1189个MR图像；BraTs2021使用了49200个训练数据，测试集为来自51名患者的2091个MR图像。</p></li><li><p><strong>甲状腺结节数据集（DDTI）</strong>：是一个甲状腺超声图像的开放数据库，包含99例和134张图像。将XML文件转换为JPG格式后，最终数据集包含2878个训练对和613个测试对，对图像大小进行了标准化处理。</p></li><li><p><strong>青光眼数据集（REFUGE - 2）</strong>：由REFUGE竞赛提供，包含1200对RGB眼底图像，训练集和测试集按7:3比例划分。</p></li><li><p><strong>与现有方法对比</strong>：在BraTs2020 - 2021数据集上，与多种脑肿瘤分割方法进行对比。结果表明，该方法在Dice系数、HD95和Jaccard指数等指标上优于传统方法和基于扩散概率模型的最新方法。例如，相较于EnsemDiff，Dice系数提高了5.87% - 6.95%，Jaccard指数提高了6.92% - 8.74%。同时，在Sensitivity和Specificity指标上也表现出色。</p></li><li><p><strong>在其他数据集上的验证</strong>：在REFUGE - 2和DDTI数据集上进行对比。在REFUGE - 2数据集上，该方法在Dice系数和Jaccard指数上优于TransUNet等方法；在DDTI数据集上，虽然该方法在部分指标上优于大多数方法，但由于超声图像的特点，部分特征像素在融合过程中被忽略，导致最终结果未达预期。</p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-46-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-46-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-06_20-46-18"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-46-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-46-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-06_20-46-24"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-46-38.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-46-38.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-06_20-46-38"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><p>对多模态动态条件引导（MDCG）、傅里叶域特征融合（FFF）和基于不确定性测量的采样（UMS）三个关键组件进行消融实验。结果显示，传统DPM不适合脑肿瘤分割，MDCG有效解决了肿瘤位置信息缺失的问题，FFF进一步提高了分割性能，UMS使分割结果更稳定。同时，确定了UMS中最优的S值为3。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-47-36.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-47-36.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-06_20-47-36"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-47-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-06_20-47-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-06_20-47-40"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出基于扩散概率模型（DPM）的脑肿瘤分割框架，得出以下结论： </p><ul><li>利用多模态MRI引导DPM生成高质量分割结果，在傅里叶域进行特征融合操作，降低DPM高频噪声对融合的影响。 </li><li>考虑DPM生成结果的不确定性对脑肿瘤分割性能的影响，结合各步骤的不确定性生成最终分割结果，在多个数据集上验证了方法的有效性。</li></ul><blockquote><p>不足：指出研究存在的问题，如DPM采样过程慢，脑肿瘤数据集切片会丢失肿瘤原始空间位置信息，影响空间体积准确性。后续将采用新加速策略，考虑肿瘤体素信息，将方法应用于3D领域，使分割结果更可靠。 </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion Probability Model (DPM) </tag>
            
            <tag> Brain tumor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Asymmetric Adaptive Heterogeneous Network for Multi-Modality Medical Image Segmentation</title>
      <link href="/post/asymmetric-adaptive-heterogeneous-network-for-multi-modality-medical-image-segmentation/"/>
      <url>/post/asymmetric-adaptive-heterogeneous-network-for-multi-modality-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<p>重庆邮电大学、第三军医大学、重庆医科大学第二附属医院</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Existing studies of <strong>multi-modality medical image segmentation</strong> tend to aggregate all modalities without discrimination and employ multiple symmetric encoders or decoders for feature extraction and fusion. They often over-look the different contributions to visual representation and intelligent decisions among multi-modality images. Motivated by this discovery, this paper proposes an asymmetric adaptive heterogeneous network for multi-modality image feature extraction with modality discrimination and adaptive fusion. For feature extraction, it uses a heterogeneous two-stream asymmetric feature-bridging network to extract complementary features from auxiliary multi-modality and leading single-modality images, respectively. For feature adaptive fusion, the proposed Transformer-CNN Feature Alignment and Fusion (T-CFAF) module enhances the leading single-modality information, and the Cross-Modality Heterogeneous Graph Fusion (CMHGF) module further fuses multi-modality features at a high-level semantic layer adaptively. Comparative evaluation with ten segmentation models on six datasets demonstrates significant efficiency gains as well as highly competitive segmentation accuracy. ((Our code is publicly available at <a href="https://github.com/joker-527/AAHN">https://github.com/joker-527/AAHN</a>)</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>现有的多模态医学图像分割研究往往在不加区分地聚合所有模态，并采用多个对称编码器或解码器进行特征提取和融合时，忽视了多模态图像在视觉表示和智能决策中的不同贡献。受此发现的启发，本文提出了一种用于多模态图像特征提取的<strong>非对称自适应异构网络</strong>，该网络具有模态区分和自适应融合功能。在特征提取方面，使用异构双流非对称特征桥接网络分别从辅助多模态和主要单模态图像中提取互补特征。对于特征自适应融合，提出的Transformer-CNN特征对齐与融合（T-CFAF）模块增强了主要单模态信息，而跨模态异构图融合（CMHGF）模块进一步在高级语义层自适应地融合多模态特征。与六个数据集上的十个分割模型进行比较评估，结果显示显著的效率提升以及极具竞争力的分割准确性。（我们的代码在<a href="https://github.com/joker-527/AAHN%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%EF%BC%89">https://github.com/joker-527/AAHN公开提供）</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-28-08.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-28-08.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-05_16-28-08"></p><p>多模态医学图像能从多个角度提供互补信息，整合不同成像模态可提升成像数据的敏感性、特异性和空间分辨率。然而，现有多模态医学图像分割研究存在两个关键问题：</p><ol><li><strong>特征提取问题</strong>：现有方法倾向于无差别聚合所有模态，采用对称编码器或解码器进行特征提取，忽略了不同模态图像对视觉表示和智能决策的不同贡献，难以有效提取多模态图像中独特且互补的特征。 </li><li><strong>特征融合问题</strong>：常见的特征融合方法是在通道维度上进行初始拼接或求和，然后输入特定融合模块，这种方式可能导致性能瓶颈，无法实现多模态特征的自适应融合，还可能造成独特特征的丢失。</li></ol><p>为解决这些问题，本文提出了一种用于多模态医学图像特征提取的非对称自适应异构网络，旨在实现模态区分和自适应融合，以有效管理多模态特征的异质性，显著提高分割结果。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多模态特征提取</strong>：常见方法有集中提取（如3D AGSE - VNet、MBANet、Swin UNETR）和单独提取（如ME - Net、MAML、NestedFormer、A2FSeg），但大多基于对称特征提取，未考虑模态关系。</li><li><strong>特征融合</strong>：现有方法常采用初始拼接或求和，再通过特定融合模块处理，如IVD - Net、Wang等人和Zhou等人的方法，但可能导致性能瓶颈，且简单使用CNN或Transformer会损失独特特征。</li><li><strong>CNN和Transformer结合</strong>：多种方法将两者结合用于医学图像处理，但常忽略不同模态图像特征的异质性。</li><li><strong>图神经网络应用</strong>：用于分割的图结构模型主要关注单模态，难以处理多模态图像的异质性。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-31-41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-31-41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-05_16-31-41"></p><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>模型的整体架构包括一个非对称双分支特征桥接编码器、Transformer - CNN 特征对齐与融合（T - CFAF）模块、跨模态异质图融合（CMHGF）模块和解码器，主要流程为：</p><ol><li><strong>特征提取</strong>：使用非对称双分支特征桥接编码器从辅助多模态和主导单模态图像中分别提取互补特征。</li><li><strong>特征融合</strong>：T - CFAF 模块增强主导单模态信息，CMHGF 模块在高层语义层自适应地融合多模态特征。</li><li><strong>图像分割</strong>：解码器通过跳跃连接逐步对齐来自 CNN 分支的低层次和高层次特征，实现最终的分割。</li></ol><h3 id="主要组件"><a href="#主要组件" class="headerlink" title="主要组件"></a>主要组件</h3><ul><li>非对称双分支特征桥接编码器：<ul><li><strong>多模态 Transformer 分支</strong>：具有强大的全局上下文信息建模能力，能够更好地捕捉辅助多模态特征。该分支包括嵌入层和骨干结构。嵌入层采用传统卷积结构，可保留位置信息并提取详细特征；骨干结构是 CNN 和 ViT 的混合组合，结合了两者的优势，能够保留 CNN 的准确位置信息和 Transformers 的全局上下文建模能力。</li><li><strong>单模态 CNN 分支</strong>：利用 CNN 保留局部细节的能力处理主导单模态图像。主要由多个残差块（RES）和 T - CFAF 模块组成，通过平均池化形成特征金字塔结构，用于提取多尺度特征。</li></ul></li><li><strong>Transformer - CNN 特征对齐与融合（T-CFAF）模块</strong>：为了增强 CNN 分支上主导单模态信息，该模块将多模态特征图和单模态特征图在通道维度上拼接，通过计算通道注意力和跨模态注意力，调整通道权重，补充多模态信息，增强主导 CNN 特征。</li><li><strong>跨模态异构图融合（CMHGF）模块</strong>：用于缓解不同模态之间的特征异质性，实现跨模态特征的自适应融合。该模块将特征图空间投影到图空间，通过图卷积操作更新节点特征，构建异质图，计算节点间的相似度，更新节点特征向量，最终实现多模态特征的融合。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><ul><li>数据集：在六个数据集上进行实验，涵盖不同医学场景，为模型性能评估提供多样数据支持。<ul><li><strong>Hecktor21</strong>：来自2021年Aicrowd在MICCAI发布的HECKTOR挑战，包含325名患者的18F - FDG PET和CT扫描，其中224例有标注。</li><li><strong>Prostate158</strong>：包含158个前列腺MRI，专家标注，有T2加权和扩散加权图像及表观扩散系数图，目标区域为前列腺周围区（PZ）和过渡区（TZ）。</li><li><strong>BraTS2019和BraTS2023</strong>：来自2019年和2023年脑肿瘤分割挑战，分别有335和1251个标注的MRI图像，每个病例有四种模态，标注区域包括肿瘤的三个子区域。</li><li><strong>CHAOS</strong>：来自联合（CT - MR）健康腹部器官分割挑战的任务5，旨在从20个不同序列的MRI数据集中分割腹部器官。</li><li><strong>BraTS2024</strong>：专注于治疗后神经胶质瘤的自动多区域脑肿瘤分割，使用治疗后的MRI，评估子区域包括非增强肿瘤核心等。</li></ul></li><li><strong>实现细节</strong>：使用Pytorch - 1.7.1实现，在单张24GB NVIDIA GeForce RTX 3090 GPU上从零开始训练300个epoch，批量大小为16，输入图像调整为128×128。采用SGD优化器，初始学习率0.001并使用动态学习率策略，结合交叉熵损失和Dice损失训练网络。根据器官和病变可见性直观选择单模态分支作为CNN分支的输入。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-35-31.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-35-31.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-05_16-35-31"></p><ul><li><strong>头颈部肿瘤分割（Hecktor21数据集）</strong>：该方法在HD95指标上取得最低分2.0793，Dice系数达到最高0.8242。箱线图显示其在Dice和HD95得分上表现更优且稳定，可视化结果表明该方法分割结果接近真实情况，对小目标体积也有良好处理能力。</li><li><strong>前列腺分割（Prostate158数据集）</strong>：在PZ和TZ区域的Dice得分表现更好，PZ区域Dice得分为0.7833，TZ区域为0.8921，相比其他方法有显著提升。在HD95指标上虽未都达到最佳，但与最佳方法差距小，整体表现最优，且具有稳定性。可视化结果显示该方法在PZ区域过分割和欠分割问题较少，3D视角结果更接近真实情况。</li><li><strong>脑肿瘤分割（BraTS2019数据集）</strong>：在WT和TC区域Dice得分最高，分别为0.9280和0.8764，ET区域Dice得分仅次于MAML模型。HD95指标上，ET区域得分最低，WT和TC区域与最佳方法差距不显著，整体平均HD95得分最低。箱线图显示Dice和HD95得分分布更集中，异常值少。可视化结果表明该方法在脑肿瘤和水肿区域分割更准确。</li><li><strong>腹部器官分割（CHAOS数据集）</strong>：在所有评估器官上均优于其他方法，肝脏、右肾、左肾和脾脏的Dice系数分别为0.9206、0.9350、0.9077和0.9208，HD95也表现良好。箱线图显示该方法在两个指标下箱体高度最短，变异性最低，异常值最少，稳定性高。可视化结果表明该方法在不同形状和大小的腹部器官分割上表现更好。</li><li><strong>脑肿瘤分割（BraTS2023数据集）</strong>：在WT、TC和ET区域的Dice和HD95指标上均表现优异，Dice得分分别为0.9371、0.9054和0.8629，HD95值分别为2.7786mm、3.3692mm和2.3758mm。箱线图显示WT区域Dice得分和三个区域HD95得分分布更集中，TC和ET区域Dice得分异常值少。可视化结果表明该方法在脑肿瘤和水肿区域分割更准确，具有良好的泛化能力。</li><li><strong>脑肿瘤分割（BraTS2024数据集）</strong>：除NETC区域的HD95外，在所有评估肿瘤区域表现显著提升，在TC、WT、SNFH和RC区域Dice得分最高，HD95得分较低。可视化结果表明该方法在不同子区域分割更准确。</li><li><strong>在线评估结果（BraTS2023验证集）</strong>：与竞争方法相比，该方法在TC区域取得有竞争力的结果，Dice为0.860，HD95为15.110。该方法属于研究导向方法，注重探索不同模态的异构表示和融合，虽一些竞争方法计算能力和后处理技术强，但该方法在整体分割性能和泛化能力上表现出色。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-36-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-36-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-05_16-36-26"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><p><strong>各模块有效性</strong></p><ul><li><strong>T - CFAF模块</strong>：移除该模块会导致Dice得分和HD95距离显著下降，添加CA和CMA可提高平均Dice得分，降低HD95距离。可视化结果表明添加T - CFAF后模型分割更接近真实情况。该模块有效补充多模态信息，促进特征提取，提高整体分割性能。</li><li><strong>CMHGF模块</strong>：用通道连接结构替代该模块会使平均Dice得分下降，HD95距离增加。在BraTS2019数据集上的详细实验表明，CMHGF模块比CMA模块更能有效整合跨模态特征，增强分割准确性，跳跃连接和GNN结构对模型性能提升起关键作用。</li></ul></li><li><p><strong>不同单模态分支比较</strong>：不同主要模态输入会导致不同结果，但该模型在大多数情况下与无模态区分的方法相比，能产生最高分割准确率。不同模态贡献不同，如头颈部肿瘤分割中高分辨率CT图像局部信息可提高准确性，脑肿瘤分割中T2图像因能清晰描绘病变而表现更好，前列腺和腹部器官分割中不同主要模态结果无明显规律。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-37-31.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-37-31.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-05_16-37-31"></p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-38-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-05_16-38-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-05_16-38-07"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种用于多模态医学图像分割的非对称异构网络，得出以下结论：</p><ol><li>创新性模块有效：特征桥接的Transformer - CNN非对称双编码器分支可高效提取辅助多模态和主导单模态的互补特征；T - CFAF模块促进多模态信息集成到单模态分支；CMHGF模块利用图结构自适应融合双编码器的跨模态特征。</li><li>性能优越：实验结果表明，该方法显著优于现有的多模态分割方法，在多个数据集上的各项指标表现出色。</li><li>具有通用性：该模型可作为通用的多模态研究框架，单模态编码分支易于替换，未来可进一步研究基于文本-图像的多模态模型。</li></ol><blockquote><p>不足：</p><p><strong>(1) 参数数量和模型复杂度</strong>：该方法的参数数量和计算量较大，每批次大小为1时，有4.1137亿个参数和941.001 GFLOPs，相比其他方法规模更大。这一问题导致受限于GPU内存，只能使用二维网络。尽管采用的非对称特征编码结构以及T - CFAF和CMHGF特征对齐融合模块能有效学习多模态信息并取得优于现有先进技术的结果，但大量的参数限制了模型在处理能力或内存受限的实际场景中的应用。 </p><p><strong>(2) 处理不完整模态数据能力</strong>：该方法主要聚焦于对齐的多模态医学图像分割。然而在实际临床环境中，由于设备限制或患者状况等原因，可能会出现模态缺失的情况。未来研究需要解决在模态数据不完整的情况下进行有效分割的问题，即开发出能够适应模态缺失情况，并在关键模态信息缺失时仍能保持高分割精度的鲁棒算法。 </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Asymmetric encoding </tag>
            
            <tag> adaptive heterogeneous graph fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MLFuse Multi-Scenario Feature Joint Learning for Multi-Modality Image Fusion</title>
      <link href="/post/mlfuse-multi-scenario-feature-joint-learning-for-multi-modality-image-fusion/"/>
      <url>/post/mlfuse-multi-scenario-feature-joint-learning-for-multi-modality-image-fusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p>大连大学、北京科技大学、大连理工大学、阿尔斯特大学</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Multi-modality image fusion (MMIF)</strong> entails synthesizing images with detailed textures and prominent objects. Existing methods tend to use general feature extraction to handle different fusion tasks. However, these methods have difficulty breaking fusion barriers across various modalities owing to the lack of targeted learning routes. In this work, we propose a multi-scenario feature joint learning architecture, MLFuse, that employs the commonalities of multi-modality images to deconstruct the fusion progress. Specifically, we construct a cross-modal knowledge reinforcing network that adopts a multipath calibration strategy to promote information communication between different images. In addition, two professional networks are developed to maintain the salient and textural information of fusion results. The spatial-spectral domain optimizing network can learn the vital relationship of the source image context with the help of spatial attention and spectral attention. The edge-guided learning network utilizes the convolution operations of various receptive fields to capture image texture information. The desired fusion results are obtained by aggregating the outputs from the three networks. Extensive experiments demonstrate the superiority of MLFuse for infrared-visible image fusion and medical image fusion. The excellent results of downstream tasks (i.e., object detection and semantic segmentation) further verify the high-quality fusion performance of our method. The code is publicly available at <a href="https://github.com/jialei-sc/MLFuse">https://github.com/jialei-sc/MLFuse</a></p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>多模态图像融合（MMIF）涉及合成具有详细纹理和突出物体的图像。现有方法倾向于使用通用特征提取来处理不同的融合任务。然而，由于缺乏针对性的学习路径，这些方法难以突破各种模态间的融合障碍。在这项工作中，我们提出了一种多场景特征联合学习架构，MLFuse，该架构利用多模态图像的共性来解构融合过程。具体而言，我们构建了一个跨模态知识增强网络，采用多路径校准策略以促进不同图像间的信息交流。此外，开发了两个专业网络以保持融合结果的显著信息和纹理信息。空间-光谱域优化网络可以借助空间注意力和光谱注意力学习源图像上下文的重要关系。边缘引导学习网络利用各种感受野的卷积操作来捕获图像纹理信息。通过聚合三个网络的输出获得期望的融合结果。大量实验表明，MLFuse在红外-可见光图像融合和医学图像融合方面的优越性。后续任务（如目标检测和语义分割）的出色结果进一步验证了我们方法的高质量融合性能。代码公开于 <a href="https://github.com/jialei-sc/MLFuse">https://github.com/jialei-sc/MLFuse</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p><strong>多模态图像融合（MMIF）<strong>旨在合成具有</strong>详细纹理</strong>和<strong>突出对象</strong>的图像，典型任务包括红外-可见图像融合（IVIF）和医学图像融合（MIF）。现有MMIF方法主要分为非生成模型和生成模型两类，但存在明显缺陷：</p><ol><li>多数统一融合方法忽视传感器成像机制差异，未区分单模态与多模态任务。</li><li>缺乏专门学习策略，无法有效引导网络掌握多种融合任务优势。 </li><li>通用特征提取难以同时保留源图像的前景显著对象和背景纹理细节。</li></ol><p>因此，本文旨在探索更合理的融合架构，建立统一的<strong>多模态图像融合网络</strong>以执行IVIF和MIF任务。但面临两个挑战：一是如何有效学习多场景的关键知识，二是如何进行模型优化。为解决这些问题，作者提出多场景特征联合学习架构MLFuse，通过三个有针对性的任务打破不同模态间的融合障碍。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>基于非生成模型方法</strong>（<strong>non-generative models</strong>）：包括自动编码器（AE）、卷积神经网络（CNN）和Transformer（TF）。AE需在大数据集训练后提取特征，常采用手工设计融合策略；CNN构建网络结构与损失函数实现隐式特征学习；TF利用自注意力机制捕捉长程依赖。</li><li><strong>基于生成模型方法</strong> （<strong>generative models</strong>）：主要有生成对抗网络（GAN）和扩散模型（DM）。GAN能学习图像强度与细节分布，但训练易模态失衡；DM克服了GAN训练不稳定问题，在图像融合中有出色表现。 </li><li><strong>多任务学习</strong>：可高效优化模型多目标函数，常直接相加多个损失函数权重，也有算法自动调整损失权重。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>模型的统一公式：</p><p>式中，$L_f$为结构相似损失，$F$为具有可学习参数ωf的多模态图像重构网络。$F$的网络结构如图图所示。$λ$是权衡参数。$x$为结构图像，包括可见光和MRI图像，$y$为功能图像，包括红外、SPECT、CT、PET图像。$u_Ψ$、$u_Φ$和$u_Υ$表示T的三个结果，T由三个网络组成(即跨模态知识强化网络Ψ、空间-频谱域优化网络Υ和边缘引导学习网络Φ)。这是一个混合损失，包括LΨ， LΥ和LΦ。ωt是关于T的一些可训练参数。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_19-52-50.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_19-52-50.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_19-52-50"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_19-49-49.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_19-49-49.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_19-49-49"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_19-49-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_19-49-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_19-49-57"></p><p>本文提出了一种<strong>多场景特征联合学习架构MLFuse</strong>，旨在解决多模态图像融合（MMIF）中的问题，实现红外 - 可见光图像融合（IVIF）和医学图像融合（MIF）任务。其主要由以下部分构成：</p><ol><li><strong>交叉模态知识增强网络</strong>：基于源图像的一致特征和差异特征，提出此网络以充分利用互补信息实现特征聚合。通过构建多路径校准策略，从特征一致性和特征差异的角度强化源图像的特征关系，增强多模态之间的信息交互。</li><li><strong>空间 - 光谱域优化网络</strong>：该优化网络由光谱注意力子网络和空间注意力子网络组成，用于强化融合结果中的显著上下文。光谱注意力优化子网络从光谱方面细化特征，获取各通道权重因子以重新校准原始特征；空间注意力优化子网络从空间角度处理特征，得到空间特征各坐标的加权因子来重新校准初始特征图，最后将两个子网络的输出相加聚合相关特征。 </li><li><strong>边缘引导学习网络</strong>：此网络能够从可见光图像和MRI图像中高效获取纹理特征。对输入图像进行卷积操作后，通过特殊卷积、池化、卷积组及上采样等操作，从不同路径处理特征，最后将两路结果相加并经Sigmoid函数输出。</li><li><strong>图像重建网络</strong>：利用多模态图像的共性，将特征学习阶段分解为三个有针对性的任务，有效提取和合并结构图像的纹理细节与功能图像的显著特征，再通过该网络合成具有美感的融合结果。</li><li><strong>损失函数</strong>：主要分为两部分，第一部分是结构相似性损失，用于优化整个网络，平衡网络学习能力，帮助模型学习源图像的结构元素；第二部分由三个优化损失组成，分别对应交叉模态知识增强网络、空间 - 光谱域优化网络和边缘引导学习网络，通过不同的损失计算方式，使各网络专注于学习相应的特征。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p><strong>数据集</strong>：针对红外与可见光图像融合（IVIF）任务，采用RoadScene进行训练和验证，M3FD和MSRS用于测试；针对医学图像融合（MIF）任务，在Harvard1数据集上开展，按一定比例划分训练集、验证集与测试集。</p></blockquote><blockquote><p><strong>训练细节</strong>：基于特定CPU和GPU，在Pytorch框架下训练500个epoch，数据随机裁剪为128×128的补丁，设置批大小、学习率等参数，训练前将图像转换到Y通道。</p></blockquote><blockquote><p><strong>评估指标</strong>：采用视觉信息保真度（VIF）、相关差异总和（SCD）、QAB&#x2F;F、相关系数（CC）、结构相似性（SSIM）、特征互信息（FMI pixel）六种指标衡量融合效果。</p></blockquote><blockquote><p><strong>对比方法</strong>：引入五种通用融合方法、四种专门融合方法与MLFuse对比。</p></blockquote><ol><li>IVIF实验：<ul><li><strong>定性比较</strong>：在MSRS和M3FD数据集上对比，通过可视化展示，发现MLFuse在捕捉场景纹理和前景物体细节方面优于其他方法。</li><li><strong>定量比较</strong>：运用六种指标从多视角分析，结果表明MLFuse在部分指标上表现最佳，具有显著的泛化能力。</li><li><strong>效率评估</strong>：量化各方法融合效率，MLFuse模型大小适中且融合速率最佳。</li></ul></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-00-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-00-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_20-00-13"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_19-59-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_19-59-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_19-59-48"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-00-41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-00-41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_20-00-41"></p><ol><li>MIF实验：<ul><li><strong>定性比较</strong>：在Harvard数据集上对CT - MRI、PET - MRI和SPECT - MRI三种任务对比，MLFuse融合结果能更好恢复结构和显著元素。</li><li><strong>定量比较</strong>：结果显示MLFuse在多个指标上表现出色，实现了主观与客观评价的统一。</li><li><strong>效率评估</strong>：对比多种模型的融合效率和参数，MLFuse参数第三小，运行时间第二短，对不同分辨率图像有稳定表现。</li></ul></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-01-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-01-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_20-01-24"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-01-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-01-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_20-01-30"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-01-36.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-01-36.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_20-01-36"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-02-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-02-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_20-02-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-02-20.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-02-20.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_20-02-20"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>子任务讨论</strong>：替换网络为卷积块，通过可视化和指标分析表明三个网络对理想融合性能都至关重要，同时应用时融合结果更稳定。</li><li><strong>子模块讨论</strong>：验证空间 - 光谱域优化网络性能，结果表明同时使用空间和光谱注意力块能使模型获得最大收益。</li><li><strong>损失讨论</strong>：验证LΨ、LΥ和LΦ对模型的作用，完整损失配置可获得更符合人眼视觉且纹理结构丰富的融合结果，确定了超参数的最优配置。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-03-12.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-03-12.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_20-03-12"></p><h2 id="IVIF（红外-可见图像融合）下游应用实验"><a href="#IVIF（红外-可见图像融合）下游应用实验" class="headerlink" title="IVIF（红外-可见图像融合）下游应用实验"></a><strong>IVIF（红外-可见图像融合）下游应用实验</strong></h2><ul><li><strong>红外 - 可见光目标检测</strong>：在M3FD数据集上用YOLOv5检测，通过可视化和<a href="mailto:&#109;&#65;&#80;&#64;&#x30;&#x2e;&#53;">mAP@0.5</a>指标评估，MLFuse能提升检测准确性。</li><li><strong>红外 - 可见光语义分割</strong>：在MSRS数据集上用DeeplabV3+分割，以交并比（IoU）衡量，可视化和定量结果表明MLFuse可生成高质量融合图像，但部分目标分数低于原始输入，可能因数据集类别不足和模型对红外数据适配不佳。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-03-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-03-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_20-03-43"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-03-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-03_20-03-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-03_20-03-53"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出一种多场景特征联合学习架构MLFuse，将融合过程分解为三个任务，各构建可学习网络。经对比与消融实验，得出以下结论：该架构具备高质量融合效果。其中，跨模态知识强化网络通过多路径校准策略加强特征关系，实现图像间有效信息交流；空间 - 光谱域优化网络借助空间与光谱注意力增强融合结果的显著物体；边缘引导学习网络提升模型纹理捕捉能力；图像重建网络生成理想融合结果。此外，下游任务（目标检测和语义分割）结果进一步验证了其在红外 - 可见图像融合任务中的性能。不过，MLFuse基于空间严格对齐数据，未来期望开发结合<strong>图像配准与融合</strong>的鲁棒算法。 </p>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-Modality Image Fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A nested self-supervised learning framework for 3-D semantic segmentation-driven multi-modal medical image fusion</title>
      <link href="/post/a-nested-self-supervised-learning-framework-for-3-d-semantic-segmentation-driven-multi-modal-medical-image-fusion/"/>
      <url>/post/a-nested-self-supervised-learning-framework-for-3-d-semantic-segmentation-driven-multi-modal-medical-image-fusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p>云南大学、东南大学、紫金山实验室、阿利亚大学</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>The successful fusion of <strong>3-D multi-modal medical images</strong> depends on both specific characteristics unique to each imaging mode as well as consistent spatial semantic features among all modes. However, the inherent variability in the appearance of these images poses a significant challenge to reliable learning of semantic information. To address this issue, this paper proposes a nested self-supervised learning framework for 3-D semantic segmentation-driven multi-modal medical image fusion. The proposed approach utilizes contrastive learning to effectively extract specified multi-scale features from each mode using U-Net (CU-Net). Subsequently, it employs geometric spatial consistency learning through a fusion convolutional decoder (FCD) and a geometric matching network (GMN) to ensure consistent acquisition of semantic representation within the same 3-D regions across multiple modalities. Additionally, a hybrid multi-level loss is introduced to facilitate the learning process of fused images. Ultimately, we leverage optimally specified multi-modal features for fusion and brain tumor lesion segmentation. The proposed approach enables cooperative learning between 3-D fusion and segmentation tasks by employing an innovative nested self-supervised strategy, thereby successfully striking a harmonious balance between semantic consistency and visual specificity during the extraction of multi-modal features. The fusion results demonstrated a mean classification SSIM, PSNR, NMI,and SFR of 0.9310, 27.8861, 1.5403, and 1.0896 respectively. The segmentation results revealed a mean classification Dice, sensitivity (Sen), specificity (Spe), and accuracy (Acc) of 0.8643, 0.8736, 0.9915, and 0.9911 correspondingly. The experimental findings demonstrate that our approach outperforms 11 other state-of-the-art fusion methods and 5 classical U-Net-based segmentation methods in terms of 4 objective metrics and qualitative evaluation. The code of the proposed method is available at <a href="https://github.com/ImZhangyYing/NLSF">https://github.com/ImZhangyYing/NLSF</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>3D多模态医学图像的成功融合依赖于每种成像模式独特的具体特征以及所有模式之间一致的空间语义特征。然而，这些图像外观的固有变化对语义信息的可靠学习构成了重大挑战。为了解决这一问题，本文提出了一种用于3D语义分割驱动的多模态医学图像融合的嵌套自监督学习框架。所提出的方法利用对比学习通过U-Net (CU-Net)有效地从每种模式中提取指定的多尺度特征。随后，它通过融合卷积解码器（FCD）和几何匹配网络（GMN）进行几何空间一致性学习，以确保在多个模态的相同3D区域内一致地获取语义表示。此外，引入了一种混合多层次损失来促进融合图像的学习过程。最终，我们利用最佳指定的多模态特征进行融合和脑肿瘤病灶分割。所提出的方法通过使用创新的嵌套自监督策略，实现了3D融合和分割任务之间的协同学习，从而在多模态特征提取过程中成功地在语义一致性和视觉特异性之间达到了和谐的平衡。融合结果显示，平均分类SSIM、PSNR、NMI和SFR分别为0.9310、27.8861、1.5403和1.0896。分割结果揭示了平均分类Dice、灵敏度（Sen）、特异性（Spe）和准确性（Acc）分别为0.8643、0.8736、0.9915和0.9911。实验结果表明，我们的方法在4个客观指标和定性评估方面优于其他11种先进的融合方法和5种经典U-Net分割方法。所提出方法的代码可在<a href="https://github.com/ImZhangyYing/NLSF">https://github.com/ImZhangyYing/NLSF</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文的研究背景主要围绕多**模态医学图像融合（MMIF）和分割（MMIS）**展开，现有方法存在诸多不足，具体如下：</p><ol><li><strong>医学影像需求</strong>：在医学成像领域，不同模态影像能提供<strong>互补信息</strong>，综合各模态信息对准确诊断至关重要。3D医学影像相比2D影像，能提供更丰富的空间信息，在手术规划和复杂病例诊断中更具优势。 </li><li><strong>现有MMIF方法局限</strong>：大部分现有方法针对<strong>2D多模态医学图像</strong>，难以保留人体解剖结构的3D空间信息；多聚焦于多模态的视觉特异性，忽视了多模态图像间的空间语义一致性；缺乏对融合和下游任务的统一建模，现有分割方法未将MMIF任务作为有效先验来提升MMIS任务性能。 </li><li><strong>对比学习与语义一致性问题</strong>：现有自监督学习在对比学习中虽能区分各模态特征，但忽视了3D医学图像中<strong>空间语义特征的一致性</strong>，无法准确捕捉图像间的语义信息。 为解决上述问题，本文提出了一种用于3D语义分割驱动的多模态医学图像融合的嵌套自监督学习框架（NSLF）。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>融合方法</strong>：分为传统方法（如基于稀疏表示、多尺度变换）和深度学习方法（如基于CNN、AE）。现有方法在多模态医学图像融合（MMIF）和多模态医学图像分割（MMIS）中取得一定成果，但多针对2D图像，且难以有效捕捉全局依赖。</li><li><strong>对比学习与语义一致性</strong>：现有自监督学习在对比学习方面有进展，能区分各模态独特特征，但忽略了3D医学图像的空间语义相似性，难以捕捉图像间语义信息。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-50-41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-50-41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-02_14-50-41"></p><p>NSLF框架由四个关键组件构成，分别为对比U型网络（Contrastive U-Net，CU-Net）、融合卷积解码器（Fusion Convolutional Decoder，FCD）、几何匹配网络（Geometric Matching Network，GMN）和特征分割解码器（Feature Segmentation Decoder，FSD）。</p><ul><li><strong>CU-Net</strong>：作为特征提取器，通过对比学习从单模态图像中捕获全局和局部特征。它由一组3D U-Net组成，每个U-Net包含多个Maxpool3d层、Conv3d层、BN3d层、激活函数和ConvTranspose3d层。通过对比损失函数约束U-Net参与对比学习，以获取各模态的特定特征。</li><li><strong>FCD</strong>：融合卷积解码器，以CU-Net提取的局部和全局特征为输入，生成源图像的融合概率，进而得到融合结果。它由四个ConvTranspose3d和Conv3d层，以及一个额外的Conv3d层组成。</li><li><strong>GMN</strong>：设计用于确保多模态图像在相同3D语义区域的空间语义一致性。通过结合仿射变换（Affine Transformation，AT）和可变形变换（Deformable Transformation，DT），在多尺度上对融合结果和源图像进行语义对齐。其中，仿射变换用于全局图像对齐，可变形变换用于局部图像对齐。</li><li><strong>FSD</strong>：特征分割解码器，以CU-Net提取的源图像局部特征为输入，进行语义分割并得到分割结果。它包含拼接操作、四个Conv3d层、BN3d层、激活函数和一个Conv3d层。</li></ul><h3 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h3><p>采用多阶段嵌套结构对嵌套自监督学习框架进行训练，具体分为<strong>三个阶段</strong>：</p><ul><li><strong>特征提取阶段</strong>：预训练CU-Net，通过对比损失函数约束其对源图像进行重建，以提取特定的多尺度特征。</li><li><strong>多模态融合阶段</strong>：训练FCD生成融合结果，同时训练GMN以确保语义表示的一致性，并微调预训练CU-Net的参数。</li><li><strong>语义分割阶段</strong>：训练FSD，并进一步微调预训练CU-Net的参数。</li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>为每个训练阶段设计了相应的损失函数，以指导模型的训练过程：</p><ul><li><strong>特征提取阶段损失</strong>：该阶段的损失主要是CU-Net的对比损失，旨在最小化正样本对之间的距离，同时最大化负样本对之间的距离。</li><li><strong>多模态融合阶段损失</strong>：此阶段的损失由两部分组成，FCD的融合损失和GMN的几何一致性损失。融合损失包括保真损失、亮度损失和结构相似性损失，用于约束融合图像与源图像在像素级的视觉相似性；几何一致性损失用于衡量源图像和融合图像在拓扑结构上的相似性，并计算体素位移矢量场（DVF）的平滑损失，以确保模型学习到一致的语义表示。</li><li><strong>语义分割阶段损失</strong>：该阶段的损失为FSD的分割损失，由Dice损失和交叉熵损失组成，用于评估预测掩码与真实标签之间的差异。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><ul><li><strong>数据集</strong>：使用了BraTS 2021挑战赛的开源多模态MRI数据集，包括训练集、验证集和测试集。</li><li><strong>对比实验</strong>：在3D&#x2F;2D多模态医学图像融合和分割任务中，将NSLF与11种最先进的融合方法和5种经典的基于U-Net的分割方法进行了对比。</li><li><strong>评估指标</strong>：采用结构相似性指数（SSIM）、峰值信噪比（PSNR）、归一化互信息（NMI）、空间频率响应（SFR）、Dice系数、灵敏度（Sen）、特异性（Spe）和准确率（Acc）等指标对模型性能进行评估。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-54-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-54-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-02_14-54-14"><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-53-38.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-53-38.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-02_14-53-38"></p><ol><li>二维&#x2F;三维多模态医学图像融合（2-D&#x2F;3-D MMIF）</li></ol><ul><li><strong>训练细节</strong>：使用PyTorch在4090 GPU上训练模型，对CU - Net和FCD应用Adam优化器，采用余弦退火策略调整学习率，权重衰减设为5e - 4，初始学习率1e - 4，最小学习率1e - 5。设置早停机制，耐心值为10。批大小为1，最大训练轮数为30。将所有输入图像中心裁剪为80<em>80</em>64后输入网络。</li><li><strong>对比方法</strong>：与11种先进的融合方法进行比较，包括3种传统方法（CSMCA、BFLGE、NSST - PAPCNN）和8种深度学习方法（IFCNN、MATR、MSDNet、SEDRFuse、STDFusionNet、U2Fusion、ZL、TGFuse）。</li><li><strong>评估指标</strong>：三维融合使用结构相似性指数（SSIM）、归一化互信息（NMI）、峰值信噪比（PSNR）和空间频率响应（SFR）四个客观指标；二维融合使用特征相似性指数（FSIM）、基于像素的视觉信息保真度（VIFP）、归一化互信息（NMI）和信息保真度准则（IFC）。</li><li><strong>定性评估</strong>：三维采用直接体渲染（DVR）对图像进行可视化分析，结果表明所提方法能有效呈现肿瘤病变的空间信息；二维将三维融合结果切片后选取120对进行测试，结果显示其他一些方法在保留病变信息和细节方面存在不足。</li><li><strong>定量评估</strong>：从测试集中选取10个多对比度MRI展示三维融合平均结果，所提方法在所有三维指标上排名第一；将三维融合结果切片为二维后选取120对测试，除PAPCNN的IFC指标外，所提方法在其他指标上表现最佳。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-55-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-55-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-02_14-55-15"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-55-45.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-55-45.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-02_14-55-45"></p><ol><li>二维&#x2F;三维多模态医学图像分割（2-D&#x2F;3-D MMIS）<ul><li><strong>训练细节</strong>：同样使用PyTorch在4090 GPU上训练，训练设置与图像融合类似。输入图像中心裁剪为160<em>160</em>64后输入网络，在验证和测试阶段，根据增强肿瘤（ET）、肿瘤核心（TC）和整个肿瘤（WT）三个肿瘤区域的表现进行评估。</li><li><strong>对比方法</strong>：与5种基于U - Net的先进方法（U - Net、VoxResNet、V - Net、Attention U - Net、U - Net++）进行比较，实验在相同条件下进行以确保公平性。</li><li><strong>评估指标</strong>：使用骰子系数（Dice）、敏感性（Sen）、特异性（Spe）和准确性（Acc）四个指标评估脑肿瘤病变的三维分割性能。</li><li><strong>定性评估</strong>：展示了不同复杂程度的多对比度MRI的分割可视化结果，结果表明U - Net基方法在简单数据上表现较好，但随着数据复杂度增加会出现明显分割错误，而所提方法在不同复杂度图像上都能取得最佳性能。</li><li><strong>定量评估</strong>：在BraTS 2021验证集和测试集上验证，结果显示所提框架在Dice、Sen和Acc指标上优于其他模型，在TC的Spe指标上排名第二。</li></ul></li></ol><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>三维语义分割</strong>：将NSLF退化得到不同版本，实验表明GMN和CU - Net对框架有显著贡献，缺少它们的版本在融合和分割任务中性能变差。</li><li><strong>三维融合</strong>：对比不同版本在三维融合上的表现，结果显示所提方法整体性能最佳，缺少CU - Net和GMN的版本在部分指标上表现较差。</li><li><strong>语义分割驱动的融合</strong>：创建仅使用融合结果训练分割网络的退化版本，实验表明所提方法在综合性能上更优，嵌套自监督学习能提高分割质量。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-56-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-56-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-02_14-56-18"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-56-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_14-56-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-02_14-56-24"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了一种用于**多模态医学图像融合（MMIF）和多模态医学图像分割（MMIS）**任务的嵌套自监督学习框架（NSLF），主要结论如下：</p><ol><li><strong>方法概述</strong>：NSLF由特征提取器（CU - Net）、融合解码器（FCD）、几何匹配网络（GMN）和分割解码器（FSD）组成。通过对比学习从CU - Net获取源图像的特定多尺度特征，并将其输入到FCD、GMN和FSD中，以支持融合和分割任务。GMN对特征提取和融合网络施加几何一致性约束，确保框架提取的多模态特征具有一致的语义信息。</li><li><strong>实验结果</strong>：综合实验结果验证了该方法在视觉效果和定量分析方面的优越性。在3 - D多模态脑MRI融合任务中，该方法在SSIM、PSNR、NMI和SFR等指标上优于11种先进的融合方法；在多模态脑MRI的多病灶3 - D分割任务中，该方法在Dice、Sen、Spe和Acc等指标上优于5种基于U - Net的经典分割方法。</li><li>方法局限性：本方法聚焦于保留多个源图像中的正确语义信息，但也存在一定局限性：<ul><li>与其他融合模型相比，训练过程的计算复杂度较高。</li><li>由于难以同时保持多模态视觉特征和准确的语义信息，对局部细节信息的保留不够重视。</li></ul></li><li><strong>未来研究方向</strong>：未来研究将进一步探索在融合任务中保留语义信息和特定视觉特征之间的微妙平衡。此外，还将考虑替代学习技术，以研究和开发具有高泛化能力、低计算复杂度的通用框架，这些技术可能提供更快的处理速度、更高的可解释性，并且在参数数量、调优要求和资源需求较少的情况下达到可比的性能。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> Dual Attention Encoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mirror U-Net Marrying Multimodal Fission with Multi-task Learning for Semantic Segmentation in Medical Imaging</title>
      <link href="/post/mirror-u-net-marrying-multimodal-fission-with-multi-task-learning-for-semantic-segmentation-in-medical-imaging/"/>
      <url>/post/mirror-u-net-marrying-multimodal-fission-with-multi-task-learning-for-semantic-segmentation-in-medical-imaging/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Positron Emission Tomography (PET) and Computed To-mography (CT)</strong> are routinely used together to detect tumors. PET&#x2F;CT segmentation models can automate tumor delineation, however, current multimodal models do not fully exploit the complementary information in each modality, as they either concatenate PET and CT data or fuse them at the decision level. To combat this, we propose Mirror U-Net, which replaces traditional fusion methods with multi-modal fission by factorizing the multimodal representation into modality-specific decoder branches and an auxiliary multimodal decoder. At these branches, Mirror U-Net assigns a task tailored to each modality to reinforce unimodal features while preserving multimodal features in the shared representation. In contrast to previous methods that use either fission or multi-task learning, Mirror U-Net combines both paradigms in a unified framework. We explore various task combinations and examine which parameters to share in the model. We evaluate Mirror U-Net on the AutoPET PET&#x2F;CT and on the multimodal MSD BrainTumor datasets, demonstrating its effectiveness in multimodal segmentation and achieving state-of-the-art performance on both datasets. Code: <a href="https://github.com/Zrrr1997">https://github.com/Zrrr1997</a></p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>正电子发射断层扫描 (PET) 和计算机断层扫描 (CT) 常常结合使用来检测肿瘤。PET&#x2F;CT 分割模型能够自动进行肿瘤界定，但当前的多模态模型未充分利用每种模态中的互补信息，因为它们要么将 PET 和 CT 数据连接起来，要么在决策层面进行融合。为了解决这一问题，我们提出了 Mirror U-Net，它通过将多模态表示因式分解为模态特定的解码器分支和一个辅助的多模态解码器，来替代传统的融合方法。在这些分支中，Mirror U-Net 为每种模态分配一个量身定制的任务，以强化单模态特征，同时在共享表示中保留多模态特征。与之前仅使用分裂或多任务学习的方法不同，Mirror U-Net 将这两种范式结合在一个统一的框架中。我们探索了各种任务组合，并研究在模型中共享哪些参数。我们在 AutoPET PET&#x2F;CT 和多模态 MSD BrainTumor 数据集上评估了 Mirror U-Net，证明了其在多模态分割中的有效性，并在两个数据集上达到了最先进的性能。代码：<a href="https://github.com/Zrrr1997">https://github.com/Zrrr1997</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>PET和CT扫描常联合用于癌症诊断和治疗，能提供肿瘤大小、位置等信息。深度学习模型可自动分割PET&#x2F;CT扫描中的病变，但肿瘤分割存在挑战： 1. <strong>数据特征局限</strong>：PET中高代谢活动并非肿瘤特有，也可能出现在炎症或感染区域；CT虽能提供解剖信息，但单独使用不足以清晰显示病变。 2. <strong>数据资源有限</strong>：体素级标注的PET&#x2F;CT数据集稀缺，导致当前多模态PET&#x2F;CT分割模型在临床应用中可靠性欠佳。 3. <strong>现有方法不足</strong>：现有多模态PET&#x2F;CT分割模型大多采用早期融合（将各模态数据拼接为单一输入）或晚期融合（合并单模态模型的预测结果），这些方法未能充分利用各模态的互补信息。 基于上述问题，作者提出Mirror U - Net，结合多模态分解和多任务学习，以解决现有方法的不足，实现更有效的多模态分割，并在相关数据集上验证其性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多模态融合</strong>：PET&#x2F;CT、CT&#x2F;MRI、多对比度MRI等多模态自动分割取得显著进展，常见早期和晚期融合方法。早期融合将模态串联成单一输入，晚期融合结合单模态模型预测。</li><li><strong>多模态分解</strong>：通过分解表示学习或显式分离单模态和多模态路径，将数据分解为多模态和单模态信息。</li><li><strong>多任务学习</strong>：在医学图像分割中广泛应用，用于利用不同任务的相关性或正则化分割。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_11-15-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_11-15-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-02_11-15-15"></p><p>文章提出的模型是<strong>Mirror U-Net</strong>，这是一个将多模态分解（multimodal fission）和多任务学习（multi-task learning）相结合的统一框架，用于医学图像的语义分割。以下是该模型的详细信息： </p><ol><li><strong>架构设计</strong>：Mirror U-Net利用两个共享瓶颈权重的3D U-Net模型，类似两个被水平镜子分隔的U-Net。一个模型处理CT数据，另一个处理PET数据。跳跃连接（skip connection）强化特定模态特征，共享层学习多模态特征。 </li><li><strong>多模态分解</strong>：该模型将多模态特征分解为特定模态的解码器分支和一个辅助多模态解码器，从而分离出特定模态特征（如PET&#x2F;CT中的代谢和解剖线索）与多模态特征。</li><li><strong>多任务学习</strong>：Mirror U-Net探索了四种任务组合（v1 - v4），以利用分离的信息。每个版本的任务组合不同，旨在通过不同的任务来学习有用的特征，增强单模态特征的同时保留共享表示中的多模态特征。</li></ol><ul><li><strong>版本1（v1）</strong>：通过L2损失训练CT分支进行重建，同时使用Dice和交叉熵损失训练PET分支进行分割，将CT的高分辨率解剖知识通过多模态表示转移到低分辨率的PET中。  </li><li><strong>版本2（v2）</strong>：在v1的基础上增加了一个瓶颈解码器，用于重建PET数据，以调节多模态特征对高代谢区域的敏感性。    </li><li><strong>版本3（v3）</strong>：在v2的基础上添加了一个二元肿瘤分类器，用于判断患者是否患有肿瘤，以解决训练中的保守预测和假阳性问题。   </li><li><strong>版本4（v4）</strong>：联合训练特定模态分支进行分割，通过加权求和的方式组合它们的逻辑值，是一个单任务的多模态裂变实验。</li></ul><ol start="4"><li><strong>权重共享</strong>：研究了不同的权重共享位置，包括在编码器分支的瓶颈之前和在解码器的瓶颈之后共享层，以确定对所有版本都最优的权重共享方案，并评估每个版本对超参数变化的敏感性。</li><li><strong>实验验证</strong>：在AutoPET PET&#x2F;CT和多模态MSD BrainTumor数据集上进行评估，实验结果表明，Mirror U-Net在多模态分割中表现出色，在两个数据集上均达到了最先进的性能，超越了传统的融合方案以及仅使用裂变或仅使用多任务学习的方法。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><ol><li><strong>任务组合和权重共享实验</strong><ul><li><strong>定量结果</strong>：在AutoPET数据集上，对Mirror U - Net的所有版本（v1） - （v4）和所有权重共享变体L进行实验，观察到三种趋势。一是自我监督方面，在（v1） - （v3）中通过添加噪声或体素重排来正则化重建，能持续提升性能，体素重排效果最佳且最稳健；二是权重共享方面，仅共享瓶颈层（L &#x3D; {5}）在所有多任务设置中效果最佳，共享浅层、深层或多层会显著降低性能；三是添加任务方面，每个Mirror U - Net版本性能依次提升，即（v4） &lt; （v1） &lt; （v2） &lt; （v3），逐步添加有意义的任务可持续改善性能。</li><li><strong>定性结果</strong>：展示了Mirror U - Net（v1） - （v4）各分支的定性结果。（v1） - （v3）中，体素重排使模型恢复结构，提升表示和分割效果，添加分类头能细化分割；（v4）中CT分支虽不适合单独分割，但能为PET分支提供空间指导，结合两者预测结果更接近真实掩码。</li></ul></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_11-18-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-07-02_11-18-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-07-02_11-18-04"></p><ol start="2"><li><p><strong>与其他方法的比较实验</strong></p><ul><li><strong>与基线方法比较</strong>：将Mirror U - Net与传统的早期融合（EF）、中期融合（MF）、晚期融合（LF）以及仅基于CT或PET数据训练的单模态U - Net进行比较。结果显示单模态CT模型性能差，单模态PET模型虽优于传统融合策略但假阳性体积高，所有Mirror U - Net变体在所有指标上均优于基线方法。</li><li><strong>与相关工作比较</strong>：将Mirror U - Net与仅使用裂变、仅使用多任务学习或两者都不使用的相关方法进行比较。还与标准医学分割基准nnUNet和AutoPET 2022挑战获胜者Blackbean进行对比。结果表明Mirror U - Net在AutoPET数据集上始终优于其他模型，展现了结合多模态裂变和多任务学习的有效性。</li></ul></li><li><p><strong>脑肿瘤分割的泛化实验</strong>：将Mirror U - Net（v2）与nnUNet、SegResNet以及Mirror U - Net（v1）和（v2） - rec进行比较。结果表明Mirror U - Net在所有3种肿瘤类别上均优于其他方法，省略瓶颈任务会导致性能显著下降，（v2） - rec在肿瘤核心和水肿分割上取得次佳结果，说明该模型有潜力泛化到其他成像模态和任务。</p></li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了Mirror U-Net模型，首次将多模态裂变与多任务学习相结合，并在多个数据集上进行了实验，主要结论如下： </p><ol><li><strong>性能表现优异</strong>：该模型在2022年AutoPET挑战赛数据集以及MSD BrainTumor数据集上的表现均超越了传统融合方法、仅使用裂变或仅使用多任务学习的方法，展现出最先进的性能，证实了结合多模态裂变与多任务学习的有效性。 </li><li><strong>权重共享方案</strong>：实验结果表明，仅共享瓶颈层的权重是最优方案，共享较浅或较深的层会导致性能下降。 </li><li><strong>任务选择重要</strong>：定性实验显示，选择合适的任务可以提高性能，共享表示能够学习到空间引导信息，从而增强主要分割任务的效果。</li><li><strong>模型具有泛化性</strong>：Mirror U-Net能够有效泛化到多模态MRI扫描的脑肿瘤分割任务中，证明了其在不同任务和成像模态中的适用性。</li><li><strong>临床应用潜力</strong>：该模型对超参数变化具有较强的鲁棒性，且性能良好，为PET&#x2F;CT分割模型在临床实践中的应用迈出了重要一步。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> Dual Attention Encoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BSAFusion A Bidirectional Stepwise Feature Alignment Network for Unaligned Medical Image Fusion</title>
      <link href="/post/bsafusion-a-bidirectional-stepwise-feature-alignment-network-for-unaligned-medical-image-fusion/"/>
      <url>/post/bsafusion-a-bidirectional-stepwise-feature-alignment-network-for-unaligned-medical-image-fusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p>昆明理工大学</p></blockquote><ol><li>医学图像对齐是指将同一患者或不同患者的多幅医学图像进行空间配准，使得图像中的解剖结构在空间位置上达到最佳的几何对应关系的过程。这个过程的核心目标是通过数学变换将不同图像坐标系统中的对应点映射到同一参考坐标系中。</li><li>在临床实践中，医学图像对齐具有重要意义。当患者需要进行多次成像检查时，由于拍摄时间不同、患者体位变化、呼吸运动等因素影响，同一患者的不同时期图像往往存在空间位置偏差。通过图像对齐技术，医生可以准确比较病灶的变化情况，评估治疗效果，制定更精准的诊疗方案。此外，在多模态成像中，比如将<strong>CT图像与MRI图像进行对齐</strong>，可以充分利用<strong>不同成像技术的优势，获得更全面的诊断信息。</strong></li><li>医学图像对齐通常涉及刚性变换和非刚性变换。刚性变换包括<strong>平移、旋转和缩放</strong>，适用于<strong>骨骼等</strong>相对固定的解剖结构</li><li>非刚性变换则可以处理<strong>软组织的形变</strong>，如<strong>呼吸运动导致的器官位移</strong>。现代图像对齐技术还广泛应用于放射治疗计划、手术导航、图像引导治疗等领域，通过精确的空间配准确保治疗的准确性和安全性。随着人工智能和深度学习技术的发展，医学图像对齐的精度和效率正在不断提升，为临床诊疗提供了更加可靠的技术支持。</li></ol><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>If unaligned multimodal medical images can be simultaneously aligned and fused using a <strong>single-stage approach</strong> within a unified processing framework, it will not only achieve mutual promotion of dual tasks but also help reduce the complexity of the model. However, the design of this model faces the challenge of incompatible requirements for feature fusion and alignment. To address this challenge, this paper proposes an unaligned medical image fusion method called Bidirectional Stepwise Feature Alignment and Fusion (BSFA-F)<br>strategy. To reduce the negative impact of modality differences on cross-modal feature matching, we incorporate the Modal Discrepancy-Free Feature Representation (MDF-FR) method into BSFA-F. MDF-FR utilizes a Modality Feature Representation Head (MFRH) to integrate the global information of the input image. By injecting the information contained in MFRH of the current image into other modality images, it effectively reduces the impact of modality differences on feature alignment while preserving the complementary information carried by different images. In terms of feature alignment, BSFA-F employs a bidirectional stepwise alignment deformation field prediction strategy based on the path independence of vector displacement between two points. This strategy solves the problem of large spans and inaccurate deformation field prediction in single-step alignment. Finally, Multi-Modal Feature Fusion block achieves the fusion of aligned features. The experimental results across multiple datasets demonstrate the effectiveness of our method.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>如果可以在统一的处理框架内使用单阶段方法同时对未对齐的多模态医学图像进行对齐和融合，不仅能实现双任务的相互促进，还能帮助降低模型的复杂性。然而，该模型的设计面临着特征融合和对齐要求不兼容的挑战。为了解决这一挑战，本文提出了一种未对齐医学图像融合方法，称为双向逐步特征对齐与融合（BSFA-F）策略。为了减少模态差异对跨模态特征匹配的负面影响，我们将模态无差异特征表示（MDF-FR）方法纳入BSFA-F中。MDF-FR利用模态特征表示头（MFRH）整合输入图像的全局信息。通过将当前图像的MFRH中包含的信息注入其他模态图像，有效减少模态差异对特征对齐的影响，同时保留不同图像所携带的互补信息。在特征对齐方面，BSFA-F采用基于两点之间向量位移路径独立性的双向逐步对齐变形场预测策略。该策略解决了单步对齐中跨度大和变形场预测不准确的问题。最后，多模态特征融合块实现了对齐特征的融合。多个数据集的实验结果证明了我们方法的有效性。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>多模态医学图像融合（MMIF）能整合不同成像模态的医学图像数据，对提升诊断准确性、辅助治疗计划制定等意义重大，因此受到广泛关注并已有众多有效融合算法。</p><p> 然而，多数现有方法假定待融合的源图像在像素层面已严格对齐，但实际中该假设常不成立。处理未对齐图像时，通常先使用配准算法对齐图像再进行融合，这种两阶段方法虽有效，但跨模态图像配准因模态差异和特征不一致面临诸多挑战。 </p><p>近年来，研究者开始探索将多源图像配准与融合集成到统一框架，但这些方法并非专门针对多模态医学图像，或在单任务图像融合中牺牲性能，或依赖生成图像的质量，且常采用两阶段处理模式，需单独成熟的图像配准模型，难以将配准和融合无缝嵌入融合过程。此外，单阶段未对齐融合方法只能处理刚性变换导致的特征未对齐，对弹性变换无效。 为解决上述问题，实现多模态医学图像在单阶段处理模式下的配准与融合，本文提出了一种单阶段框架，以解决特征提取对配准和融合的冲突要求。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多模态医学图像融合算法众多</strong>：基于深度学习的方法广泛应用，可分为基于CNN、Transformer和混合方法。如CNN的残差连接、Transformer的FATMusic等，还有结合两者的混合方法。</li><li><strong>联合处理框架出现</strong>：为解决图像未对齐问题，近年出现将多源图像配准与融合集成于统一框架的方法，如ReCoNet、UMF - CMGR等。</li><li><strong>单阶段未对齐融合方法探索</strong>：提出如IVFWSR、RFVIF等单阶段未对齐融合方法，但仅适用于红外 - 可见光图像融合。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-28_19-58-20.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-28_19-58-20.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-28_19-58-20"></p><ol><li>无模态差异特征表示（Modality Discrepancy - Free Feature Representation，MDF - FR）：<ul><li><strong>特征提取</strong>：利用由Restormer和Transformer层组成的网络作为编码器，从输入的未对齐图像对中提取特征。输入图像经第一个Restormer层输出的浅层特征包含图像的底层细节，直接送入多模态特征融合层以保留融合结果中的边缘细节；经第二个Restormer层输出的特征再经过两个Transformer层，得到用于消除模态差异和预测变形场的特征。</li><li><strong>模态差异消除</strong>：通过最小化交叉熵损失得到模态特征表示头，将其注入到特征中以减少模态差异。同时使用两个Transfer块进一步提取用于预测变形场的特征，并通过交叉熵损失更新参数，确保处理后的特征有效消除模态差异。</li></ul></li><li>双向逐步特征对齐（Bidirectional Stepwise Feature Alignment，BSFA）：<ul><li><strong>向量位移路径独立性</strong>：基于两点间向量位移的路径独立性，提出双向逐步变形场预测策略。该策略可将两点间的变形场表示为中间点变形场的组合，能有效捕捉图像间的相互关系，减少累积误差，提高对齐过程的鲁棒性。</li><li><strong>逐步对齐实现</strong>：从两个方向预测输入图像特征的变形场，通过多次变形场预测操作，在中间位置实现特征的跨模态对齐。为保证变形场质量，引入平滑损失和一致性损失进行模型更新。</li></ul></li><li><strong>多模态特征融合（Multi - Modal Feature Fusion，MMFF）</strong>： 将预测的变形场应用于多模态特征，实现输入图像在特征级别的精确对齐和有效融合。具体过程是将特征经过一系列处理后送入FusionBLK进行特征融合，最后将融合结果与浅层特征拼接，通过重建层得到融合图像。同时引入结构损失、像素强度损失和梯度损失来优化网络参数，确保融合图像与源图像的结构一致性、良好的对比度以及边缘细节的保留。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：CT-MRI, PET-MRI, SPECT-MRI datasets</p></blockquote><ul><li><strong>评估指标</strong>：选择了五个常用的图像质量指标来客观评估融合方法的性能，分别是基于梯度的融合性能（Gradient - based Fusion Performance，$Q_{AB&#x2F;F}$）、Chen - Varshney度量（Chen - Varshney Metric，$Q_{CV}$）、视觉信息保真度（Visual Information Fidelity，$Q_{VIF}$）、基于结构的度量（Structure - based Metric，$Q_{S}$）和结构相似性指数度量（Structural Similarity Index Measure，$Q_{SSIM}$）。其中，$Q_{CV}$值越低表示融合图像质量越好，其他指标值越高表示融合质量越好。</li></ul><p>常见的解决未对齐多源图像融合问题的方法有“Registration + Fusion”（先对要融合的图像进行配准，然后再融合）和“Joint Registration and Fusion”（将配准和融合结合为一个过程的两阶段方法）。为验证所提方法的优越性，将其与这两种方法进行比较，重点对比“Joint Registration and Fusion”方法，与“Registration + Fusion”方法的比较结果包含在补充材料中。具体对比了所提方法与五种联合配准和融合方法（UMF - CMGR、SuperFusion、MURF、IMF和PAMRFuse）的性能。实验结果表明，所提方法在特征对齐、对比度保留和细节保留方面具有显著优势，与现有的两阶段联合处理框架相比，性能更强。此外，通过箱线图展示客观评估结果，所提方法在所有指标上均取得了最佳平均性能，双向对齐策略比IMF的单向对齐策略产生了更好的融合效果。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-28_20-02-38.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-28_20-02-38.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-28_20-02-38"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>MDF - FR的有效性</strong>：设计了设置A（不进行MFRH交换且不使用$L_{ce2}$）和设置B（不进行MFRH交换但使用$L_{ce2}$）来验证MDF - FR的有效性。实验结果表明，包含MDF - FR时，所提方法能实现更好的对齐和融合性能。</li><li><strong>BSFA的有效性</strong>：通过三个实验验证BSFA的有效性。一是完全移除BSFA，评估其对整体对齐性能和融合结果的影响；二是移除BSFA中的正向配准层（FRL），仅保留反向配准层（RRL）；三是仅保留FRL，移除RRL。对齐结果显示，只有完全实施BSFA时，所提方法才能实现出色的对齐效果</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了一种<strong>单阶段的多模态医学图像配准与融合框架</strong>。与传统的两阶段方法不同，该框架通过共享特征编码器降低了模型的复杂度。通过引入模态差异无关特征表示（MDF - FR）方法，该框架解决了跨模态特征对齐中的模态差异问题。为每个输入图像添加的模态特征表示头（MFRH）可以整合全局图像特征，保留不同模态间的互补信息。 此外，基于向量位移原理提出的双向逐步对齐策略用于预测变形场。该方法保留了融合信息的完整性和多样性，在需要精确高效配准与融合的临床应用中展现出了潜力。 </p>]]></content>
      
      
      <categories>
          
          <category> Medical Image Fusion </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> Multi modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态医学图像分割综述</title>
      <link href="/post/duo-mo-tai-yi-xue/"/>
      <url>/post/duo-mo-tai-yi-xue/</url>
      
        <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>多模态医学图像分割</strong>：多模态医学图像分割指<strong>融合多模态图像的信息</strong>以提高分割性能。常见的医学图像主<br>要有计算机断层扫描（Computed Tomography， <strong>CT</strong>）、磁共振成像（Magnetic Resonance Imaging， <strong>MRI</strong>）和正电子发射断层扫描（Positron Emission computed Tomography， <strong>PET</strong>）等。</p><ul><li>CT图像是肌肉及骨骼疾病，如骨肿瘤、骨折等疾病的常用诊断成像</li><li>MRI图像能提供较好的软组织对比度</li><li>功能性影像（例如PET）缺乏解剖特征，但能提供疾病的定量代谢和功能信息</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-25_19-44-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-25_19-44-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-25_19-44-11"></p><blockquote><p> T1w: T1 加权成像 </p><p> T1ce: 对比增强 T1 加权成像          </p><p>  T2w: T2加权成像</p><p> FLAIR:  流体衰减反转恢复</p></blockquote><blockquote><p>融合多模态医学图像进行分割已经成了研究热点</p></blockquote><h2 id="多模态医学图像分割数据集"><a href="#多模态医学图像分割数据集" class="headerlink" title="多模态医学图像分割数据集"></a>多模态医学图像分割数据集</h2><ul><li><p>脑肿瘤分割（Brain Tumor Segmentation， BraTS）数据集</p></li><li><p>缺血性中风病变分割（Ischemic Stroke LEsion Segmentation， ISLES）数据集</p></li><li><p>MR 脑图像分割（MR Brain image Segmentation，MRBrainS）数 据 集</p></li><li><p>新生儿脑分割（Neonatal Brain Segmentation， NeoBrainS）数据集</p></li><li><p>组合（CT-MR）腹部器官分 割 （Combined CT-MR Healthy Abdominal Organ Segmentation， CHAOS）数据集</p></li><li><p>婴儿脑 MRI 分割（6-month Infant brain MRI segmentation， Iseg）数据集</p></li><li><p>多模态MR图像自动椎间盘定位分割（automatic InterVertebral Disc localization and segmentation from 3D MR images， IVDM3Seg）数据集</p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-25_20-03-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-25_20-03-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-25_20-03-33"></p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>数据预处理对后续分割任务有重要影响；尤其对多模态医学图像进行分割时，由于图像具有强度不一、对比度各异、噪声较大等特点，为使图像像素值分布更统一、网络训练更平滑，需要将图像经过预处理操作后再送入分割网络。如图所示，典型的预处理技术主要有<strong>图像配准、偏置场校正、图像重采样、强度值归一化</strong>等。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-25_20-04-42.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-25_20-04-42.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-25_20-04-42"></p><p>数据增强通过对训练数据集中的图像进行变换（旋转、平移、缩放、翻转、扭曲以及加入某些噪声如高斯噪声等），增加了可用的训练数据。数据增强技术在医学图像的分割任务中有着广泛的应用。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-25_20-05-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-25_20-05-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-25_20-05-25"></p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><blockquote><p>交叉熵损失（Cross-Entropy loss， CE）函数：交叉熵损失是最常用的损失函数之一，对每个像素的类别预测进行单独评估，然后对所有像素点进行平均。</p></blockquote><p>$$<br>Loss_{\mathrm{CE}}&#x3D;-\sum_{i\in N}\sum_{l\in L}y_{i}^{l}\log\hat{y}_{i}^{l}<br>$$</p><blockquote><p>加权交叉熵损失（Weighted Cross-Entropy loss， WCE）函数：在医学图像分割任务中由于大部分像素点属于背景类，因此在计算损失时对不同类别的像素点赋予不同的权重是合理的，并可以缓解类别不平衡问题</p></blockquote><p>$$<br>Loss_{\mathrm{WCE}}&#x3D;-\sum_{i\in N}\sum_{l\in L}w_{l}y_{i}^{l}\log\hat{y}_{i}^{l}<br>$$</p><blockquote><p>骰子损失（DICE loss）：骰子损失同样是医学图像分割中常用的损失函数之一。它可以衡量预测结果和真实结果之间重合度</p></blockquote><p>$$<br>Loss_{\mathrm{Dice}}&#x3D;1-2\frac{\sum_{l\in L}\sum_{i\in N}y_{i}^{l}\hat{y}<em>{i}^{l}+\varepsilon}{\sum</em>{l\in L}\sum_{i\in N}\left(y_{i}^{l}+\hat{y}_{i}^{l}\right)+\varepsilon}<br>$$</p><p>焦点损失（Focal loss，FL）：焦点损失最初是为目标检测任务设计的。它通过向交叉熵损失引入一个调节因子γ和一个参数α使模型将训练重点放在困难像素点上<br>$$<br>p_t&#x3D;\begin{cases}p_t,&amp;y&#x3D;1\1-p_t,&amp;\text{其他}\end{cases}<br>$$</p><p>$$<br>Loss_{\mathrm{FL}}(p_{t})&#x3D;-\alpha_{t}(1-p_{t})^{\gamma}\log(p_{t})<br>$$</p><blockquote><p>其中：y ∈ { - 1，+ 1}代表像素点的真实类别；pt衡量了模型预测结果与真实值之间的差异，值越大表示越接近；γ为可调节因子，可以平滑地调整简单样本的权重降低的速度。当γ &gt; 0时，可以减小简单样本的相对损失，把重点放在困难样本上；当γ &#x3D; 0时，焦点损失变为交叉熵损失。</p></blockquote><p>三种融合方法：<strong>输入级融合、中间层融合以及决策级融合</strong></p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p>[1] 杨鸿杰,徐巧枝,于磊. 基于深度学习的多模态医学影像分割研究综述[J]. 计算机应用研究,2022,39(5):1297-1306. DOI:10.19734&#x2F;j.issn.1001-3695.2021.10.0424.</p>]]></content>
      
      
      <categories>
          
          <category> 多模态医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
            <tag> 医学图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rethinking U-Net Task-Adaptive Mixture of Skip Connections for Enhanced Medical Image Segmentation</title>
      <link href="/post/rethinking-u-net-task-adaptive-mixture-of-skip-connections-for-enhanced-medical-image-segmentation/"/>
      <url>/post/rethinking-u-net-task-adaptive-mixture-of-skip-connections-for-enhanced-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>U-Net is a widely used model for medical image segmentation, renowned for its strong feature extraction capabilities and U-shaped design, which incorporates skip connections to preserve critical information. However, its decoders exhibit information-specific preferences for the supplementary content provided by skip connections, instead of adhering to a strict one-to-one correspondence, which limits its flexibility across diverse tasks. To address this limitation, we propose the Task-Adaptive Mixture of Skip Connections (TA-MoSC) module, inspired by the Mixture of Experts (MoE) framework. TA-MoSC innovatively reinterprets skip connections as a task allocation problem, employing a routing mechanism to adaptively select expert combinations at different decoding stages. By introducing MoE, our approach enhances the sparsity of the model, and lightweight convolutional experts are shared across all skip connection stages, with a Balanced Expert Utilization (BEU) strategy ensuring that all experts are effectively trained, maintaining training balance and preserving computational efficiency. Our approach introduces minimal additional parameters to the original U-Net but significantly enhances its performance and stability. Experiments on GlaS, MoNuSeg, Synapse, and ISIC16 datasets demonstrate state-of-the-art accuracy and better generalization across diverse tasks. Moreover, while this work focuses on medical image segmentation, the proposed method can be seamlessly extended to other segmentation tasks, offering a flexible and efficient solution for diverse applications.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>U-Net是一种广泛应用于医学图像分割的模型，以其强大的特征提取能力和u形设计而闻名，u形设计采用跳过连接来保留关键信息。然而，它的解码器对跳过连接提供的补充内容显示了特定于信息的首选项，而不是遵循严格的一对一对应关系，这限制了它在不同任务中的灵活性。为了解决这一限制，我们提出了任务自适应跳跃连接混合(TA-MoSC)模块，灵感来自混合专家(MoE)框架。TA-MoSC创新性地将跳跃连接重新解释为任务分配问题，采用路由机制自适应地选择不同解码阶段的专家组合。通过引入MoE，我们的方法增强了模型的稀疏性，并且在所有跳跃连接阶段共享轻量级卷积专家，并采用平衡专家利用率(BEU)策略确保所有专家都得到有效训练，保持训练平衡并保持计算效率。我们的方法在原有的U-Net基础上引入了最小的附加参数，但显著提高了其性能和稳定性。在GlaS, MoNuSeg, Synapse和ISIC16数据集上的实验证明了最先进的准确性和跨不同任务的更好泛化。此外，虽然本研究的重点是医学图像分割，但该方法可以无缝扩展到其他分割任务，为各种应用提供了灵活高效的解决方案。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>医学图像分割对医疗数据的分析和解读至关重要，能辅助疾病诊断和治疗。U - Net因强大的特征提取能力和U形结构，成为广泛应用的医学图像分割模型，其跳跃连接可保留关键信息，但也存在不足。 原始U - Net的跳跃连接未能有效解决编码器与解码器间的语义差距问题，不同医疗成像任务对语义特征的侧重点不同，简单的一对一连接机制难以满足需求，导致模型性能受限。 尽管已有一些改进方法，如UNet++采用嵌套和密集跳跃路径、部分方法引入注意力机制或使用Transformer架构，但这些方法要么增加了架构复杂度和计算开销，要么缺乏适应不同数据集的灵活性，无法充分考虑解码器对语义信息的阶段特定偏好，泛化能力不足。 基于此，本文通过大量实验探索U - Net中跳跃连接的不同组合，发现每个解码器阶段所需信息并非与跳跃连接严格一对一对应，从而提出任务自适应跳跃连接混合（TA - MoSC）模块，以解决这些问题，提升模型性能和泛化能力。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>U-Net架构多样</strong>：U-Net凭借其强大的特征提取能力和U形结构，成为医学图像分割领域广泛采用的架构，衍生出ResUNet、DenseUNet、UNet++等众多变体，还引入了注意力机制、Transformer架构等。</li><li><strong>Skip Connection改进</strong>：一些方法如UDTransNet、EIU-Net通过基于注意力的重新校准改进了跳跃连接，但缺乏适应不同数据集需求的灵活性。</li><li><strong>Mixture of Experts应用</strong>：稀疏激活的Mixture of Experts（MoE）模型在视觉和文本模型扩展方面取得成功，但在图像分割领域，尤其是跳跃连接方面的应用尚未深入探索。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-43-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-43-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-23_10-43-43"></p><ul><li><p>TA - MoSC模块：受混合专家（Mixture of Experts，MoE）框架启发，将跳跃连接重新定义为任务分配问题。该模块由路由银行（Router Bank）、跳跃连接（Skip - Connection，SC）银行和四个对接器（Dockers）组成。其工作流程如下：</p><ul><li><p><strong>特征聚合</strong>：输入图像经过多层编码器处理，生成不同层次的特征。将前四层特征调整为相同大小后，沿通道维度拼接形成统一的特征表示，并进行特征维度缩减。</p></li><li><p><strong>路由阶段</strong>：路由银行中的每个路由器为定制合适的专家组合，为不同阶段的跳跃连接选择路由方案。专家采用独立的卷积子网络，对输入特征进行专门的非线性变换。使用TopK（K &#x3D; 2）操作进行稀疏选择，确保只有部分专家被激活，降低计算开销。最后，通过对接器模块对跳跃连接进行整形和处理，将其传输到相应的解码器。</p></li><li><p><strong>平衡专家利用</strong>：该模块包含专家方差（Expert Variance，EV）损失和未使用专家处理两个关键机制。</p><ul><li><strong>专家方差损失</strong>：计算不同门控网络中专家使用的方差，避免模型过度依赖某些专家，促使所有专家充分发挥能力。</li><li><strong>未使用专家处理</strong>：当门控网络未选择某些专家时，用输入数据的随机样本对这些未使用的专家进行处理，防止专家闲置，确保每个专家都得到训练。</li></ul></li></ul></li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集</p></blockquote><ul><li><strong>GlaS</strong>：包含<strong>165</strong>张高分辨率苏木精 - 伊红（H&amp;E）染色图像，85张用于训练，80张用于测试。</li><li><strong>MoNuSeg</strong>：由<strong>44</strong>张图像组成，30张用于训练，14张用于测试。</li><li><strong>Synapse</strong>：有<strong>30</strong>个腹部CT扫描，共3779张轴向图像，涵盖8个器官。</li><li><strong>ISIC16</strong>：有<strong>1279</strong>张皮肤镜图像，其中测试集包含379张，有两种疾病类别的真实标注。</li></ul><blockquote><p>训练策略</p></blockquote><p>训练过程分为两个阶段。第一阶段，<strong>使用原始跳跃连接训练编码器和解码器</strong>，使其熟悉数据集并掌握基本的编码和解码能力。之后，<strong>冻结编码器和解码器，专注训练提出的TC - MoSC模块</strong>，使每个专家充分学习其专长的特征，路由器根据各解码阶段的语义需求有效分配任务。冻结编码器和解码器是为防止其参数在整个训练过程中变化，降低训练难度，便于收敛。</p><p><strong>定量比较</strong>：在GlaS、MoNuSeg和ISIC16数据集上，使用Dice系数和交并比（IoU）作为性能指标；在多标签分割的Synapse数据集上，使用95% Hausdorff距离指标。采用5折交叉验证评估模型性能，使用独立Student’s t - 检验（α &#x3D; 0.05）评估统计显著性。结果表明，模型在GlaS和MoNuSeg上显著优于基线模型，实验结果的小标准差显示出模型的高稳定性和鲁棒性，在不同数据集上兼容性良好。<br><strong>定性比较</strong>：如图5所示，红色框突出显示了UTANet在四个数据集上优于其他模型的区域。在MoNuseg数据集上，UTANet能更有效地捕捉细节信息，这得益于TA - MoSC模块中对细节敏感的专家模块。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-48-36.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-48-36.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-23_10-48-36"></p><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>提出模块的消融研究</strong>：结果显示，Baseline + TA - MoSC + BEU始终优于其他基线，表明纳入TA - MoSC模块显著提高了模型的分割性能，应用BEU优化进一步增强了性能。不同数据集在解码阶段需要不同程度的补充信息，为解码器提供适当的补充信息可大幅提高模型的分割性能。</li><li><strong>TopK设置的消融研究</strong>：在推理阶段实验不同的TopK设置，发现单标签数据集上TopK设为3效果最佳，多标签的Synapse数据集上TopK设为4效果最佳。可视化在GlaS数据集上训练的模型中的专家发现，每个专家网络学习的特征和语义不同，经过TA - MoSC模块后，跳跃连接特征的高亮区域与编码器第三和第四阶段的特征紧密对齐，证实了模型对不同数据集或任务的跳跃连接信息有特定偏好，TA - MoSC模块能有效解决跳跃连接中的信息不对称问题。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-49-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-49-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-23_10-49-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-49-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-06-23_10-49-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-06-23_10-49-28"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在U-Net的解码过程中，<strong>不同数据集对补充信息的需求存在差异</strong>。本文通过将<strong>混合专家</strong>（Mixture of Experts，MoE）框架集成到<strong>跳跃连接</strong>（skip connections）中，解决了这一问题，实现了多尺度特征的自适应重新分配，以适应解码器在不同阶段的特定偏好。与传统的固定跳跃连接机制不同，所提出的任务自适应跳跃连接混合（Task-Adaptive Mixture of Skip Connections，TA-MoSC）模块能够动态地对齐编码器和解码器的特征，从而应对特定数据集的分割挑战。 尽管本文以医学图像分割作为案例研究，但所提出的方法可以推广到其他密集预测任务，如自然图像分割和目标检测。在未来的工作中，作者将专注于优化模型的计算效率，使TA-MoSC模块更具可扩展性和高效性，确保其在跨领域的实时应用和大规模任务中的实用性。 </p>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> U-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>配置失败登录尝试限制</title>
      <link href="/post/frp/pei-zhi-shi-bai-deng-lu-chang-shi-xian-zhi/"/>
      <url>/post/frp/pei-zhi-shi-bai-deng-lu-chang-shi-xian-zhi/</url>
      
        <content type="html"><![CDATA[<p>为了增强Windows远程桌面通过FRP和Nginx配置的安全性，限制失败登录尝试是非常重要的一环。我将介绍多个层面上的失败登录限制配置方法。</p><h2 id="1-Nginx层面的失败登录限制"><a href="#1-Nginx层面的失败登录限制" class="headerlink" title="1. Nginx层面的失败登录限制"></a>1. Nginx层面的失败登录限制</h2><h3 id="使用Nginx限流模块"><a href="#使用Nginx限流模块" class="headerlink" title="使用Nginx限流模块"></a>使用Nginx限流模块</h3><p>在Nginx配置中添加限流规则，可以有效防止暴力破解尝试：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在nginx.conf的http部分添加</span></span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line">    <span class="comment"># 定义限制区域</span></span><br><span class="line">    <span class="attribute">limit_req_zone</span> <span class="variable">$binary_remote_addr</span> zone=rdp_login:<span class="number">10m</span> rate=5r/m;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 其他配置...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在rdp.conf的server部分添加</span></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="comment"># ...其他配置</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对RDP登录页面应用限流</span></span><br><span class="line">    <span class="section">location</span> /rdp/ &#123;</span><br><span class="line">        <span class="attribute">limit_req</span> zone=rdp_login burst=<span class="number">10</span> nodelay;</span><br><span class="line">        <span class="comment"># 其他proxy_pass配置...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用fail2ban监控Nginx日志"><a href="#使用fail2ban监控Nginx日志" class="headerlink" title="使用fail2ban监控Nginx日志"></a>使用fail2ban监控Nginx日志</h3><p>安装并配置fail2ban来监控Nginx日志并自动封禁可疑IP：</p><ol><li>安装fail2ban：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install fail2ban -y</span><br></pre></td></tr></table></figure><ol start="2"><li>创建自定义过滤器，例如 <code>/etc/fail2ban/filter.d/nginx-rdp.conf</code>：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Definition]</span></span><br><span class="line"><span class="attr">failregex</span> = ^&lt;HOST&gt; - .* <span class="string">&quot;POST /rdp/api/tokens HTTP/1\.1&quot;</span> <span class="number">401</span></span><br><span class="line">ignoreregex =</span><br></pre></td></tr></table></figure><ol start="3"><li>在fail2ban配置中添加监控规则，修改 <code>/etc/fail2ban/jail.local</code>：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[nginx-rdp]</span></span><br><span class="line"><span class="attr">enabled</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">port</span> = http,https</span><br><span class="line"><span class="attr">filter</span> = nginx-rdp</span><br><span class="line"><span class="attr">logpath</span> = /var/log/nginx/access.log</span><br><span class="line"><span class="attr">maxretry</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">bantime</span> = <span class="number">3600</span>  <span class="comment"># 封禁1小时</span></span><br><span class="line"><span class="attr">findtime</span> = <span class="number">300</span>  <span class="comment"># 5分钟内</span></span><br></pre></td></tr></table></figure><ol start="4"><li>重启fail2ban：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart fail2ban</span><br></pre></td></tr></table></figure><h2 id="2-Windows-RDP层面的失败登录限制"><a href="#2-Windows-RDP层面的失败登录限制" class="headerlink" title="2. Windows RDP层面的失败登录限制"></a>2. Windows RDP层面的失败登录限制</h2><h3 id="账户锁定策略"><a href="#账户锁定策略" class="headerlink" title="账户锁定策略"></a>账户锁定策略</h3><p>在Windows系统上配置账户锁定策略：</p><ol><li>打开本地安全策略（secpol.msc）</li><li>转到”账户策略” &gt; “账户锁定策略”</li><li>配置以下选项：<ul><li>账户锁定阈值：5次（尝试失败5次后锁定账户）</li><li>账户锁定时间：30分钟</li><li>重置账户锁定计数器：30分钟</li></ul></li></ol><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 或者通过PowerShell设置（管理员权限）</span></span><br><span class="line">net accounts /lockoutthreshold:<span class="number">5</span></span><br><span class="line">net accounts /lockoutduration:<span class="number">30</span></span><br><span class="line">net accounts /lockoutwindow:<span class="number">30</span></span><br></pre></td></tr></table></figure><h3 id="使用Windows高级防火墙"><a href="#使用Windows高级防火墙" class="headerlink" title="使用Windows高级防火墙"></a>使用Windows高级防火墙</h3><p>配置Windows高级防火墙来限制连接尝试：</p><ol><li>打开Windows高级防火墙（wf.msc）</li><li>创建新的入站规则</li><li>选择”自定义规则” &gt; “所有程序”</li><li>协议类型选择TCP，本地端口为3389</li><li>在”范围”选项卡中，添加允许连接的IP地址范围</li><li>在”操作”选项卡中，选择”允许连接”</li></ol><h2 id="3-使用RDP-Wrapper配置额外的安全设置"><a href="#3-使用RDP-Wrapper配置额外的安全设置" class="headerlink" title="3. 使用RDP Wrapper配置额外的安全设置"></a>3. 使用RDP Wrapper配置额外的安全设置</h2><p>如果您使用RDP Wrapper，可以启用其内置的安全选项：</p><ol><li>打开RDP Wrapper配置</li><li>启用”Disable Multimon”（禁用多显示器）</li><li>启用”Network Level Authentication”（网络级别身份验证）</li><li>通过编辑<code>rdpwrap.ini</code>文件添加IP过滤规则</li></ol><h2 id="4-服务器层面的安全措施"><a href="#4-服务器层面的安全措施" class="headerlink" title="4. 服务器层面的安全措施"></a>4. 服务器层面的安全措施</h2><h3 id="安装和配置DenyHosts或Fail2ban"><a href="#安装和配置DenyHosts或Fail2ban" class="headerlink" title="安装和配置DenyHosts或Fail2ban"></a>安装和配置DenyHosts或Fail2ban</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装Fail2ban（如果还没安装）</span></span><br><span class="line">apt install fail2ban -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置RDP保护，编辑/etc/fail2ban/jail.local</span></span><br><span class="line">[rdp]</span><br><span class="line">enabled = <span class="literal">true</span></span><br><span class="line">port = 3389</span><br><span class="line">filter = rdp</span><br><span class="line">logpath = /var/log/auth.log</span><br><span class="line">maxretry = 3</span><br><span class="line">bantime = 86400  <span class="comment"># 24小时</span></span><br></pre></td></tr></table></figure><h3 id="使用iptables设置连接限制"><a href="#使用iptables设置连接限制" class="headerlink" title="使用iptables设置连接限制"></a>使用iptables设置连接限制</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 限制每分钟最多接受5个新连接</span></span><br><span class="line">iptables -A INPUT -p tcp --dport 3389 -m state --state NEW -m <span class="built_in">limit</span> --<span class="built_in">limit</span> 5/minute --limit-burst 5 -j ACCEPT</span><br><span class="line">iptables -A INPUT -p tcp --dport 3389 -m state --state NEW -j DROP</span><br><span class="line"></span><br><span class="line"><span class="comment"># 限制每个IP最大同时连接数</span></span><br><span class="line">iptables -A INPUT -p tcp --dport 3389 -m connlimit --connlimit-above 3 -j REJECT</span><br></pre></td></tr></table></figure><h2 id="5-FRP层面的限制"><a href="#5-FRP层面的限制" class="headerlink" title="5. FRP层面的限制"></a>5. FRP层面的限制</h2><p>修改frps.toml配置来添加连接限制：</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加到frps.toml</span></span><br><span class="line"><span class="attr">maxPortsPerClient</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">authentication.method</span> = <span class="string">&quot;token&quot;</span></span><br><span class="line"><span class="attr">webServer.authMethod</span> = <span class="string">&quot;password&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加黑白名单 </span></span><br><span class="line"><span class="attr">allowPorts</span> = [<span class="number">3389</span>, <span class="number">7001</span>, <span class="number">7002</span>, <span class="number">7500</span>, <span class="number">80</span>, <span class="number">443</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 限制连接</span></span><br><span class="line"><span class="attr">transport.maxPoolCount</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">transport.maxPoolSize</span> = <span class="number">100</span></span><br></pre></td></tr></table></figure><h2 id="6-实施多因素认证（MFA）"><a href="#6-实施多因素认证（MFA）" class="headerlink" title="6. 实施多因素认证（MFA）"></a>6. 实施多因素认证（MFA）</h2><h3 id="配置Windows-Hello或智能卡认证"><a href="#配置Windows-Hello或智能卡认证" class="headerlink" title="配置Windows Hello或智能卡认证"></a>配置Windows Hello或智能卡认证</h3><ol><li>通过组策略启用智能卡认证需求</li><li>配置本地安全策略，要求网络级别身份验证</li></ol><h3 id="使用第三方MFA解决方案"><a href="#使用第三方MFA解决方案" class="headerlink" title="使用第三方MFA解决方案"></a>使用第三方MFA解决方案</h3><p>考虑安装和配置以下解决方案：</p><ol><li>DUO Security的Windows RDP插件</li><li>Microsoft的Azure MFA服务</li><li>WiKID强认证系统</li></ol><h2 id="7-监控和审计"><a href="#7-监控和审计" class="headerlink" title="7. 监控和审计"></a>7. 监控和审计</h2><h3 id="配置Windows事件日志监控"><a href="#配置Windows事件日志监控" class="headerlink" title="配置Windows事件日志监控"></a>配置Windows事件日志监控</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启用详细的RDP登录日志</span></span><br><span class="line">wevtutil <span class="built_in">sl</span> <span class="string">&quot;Microsoft-Windows-RemoteDesktopServices-RdpCoreTS/Operational&quot;</span> /e:true</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置事件转发以集中日志</span></span><br><span class="line">wecutil qc /q</span><br></pre></td></tr></table></figure><h3 id="设置登录通知"><a href="#设置登录通知" class="headerlink" title="设置登录通知"></a>设置登录通知</h3><p>创建登录通知脚本，在检测到失败登录尝试时发送电子邮件或其他通知：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建监控脚本</span></span><br><span class="line"><span class="variable">$query</span> = <span class="string">&#x27;*[System[(EventID=4625)]]&#x27;</span>  <span class="comment"># 失败登录事件ID</span></span><br><span class="line"><span class="variable">$subscription</span> = <span class="built_in">Register-WmiEvent</span> <span class="literal">-Query</span> <span class="variable">$query</span> <span class="literal">-SourceIdentifier</span> <span class="string">&quot;LoginFailureAlert&quot;</span> <span class="literal">-Action</span> &#123;</span><br><span class="line">    <span class="comment"># 发送电子邮件或其他通知</span></span><br><span class="line">    <span class="built_in">Send-MailMessage</span> <span class="literal">-From</span> <span class="string">&quot;alert@yourdomain.com&quot;</span> <span class="literal">-To</span> <span class="string">&quot;admin@yourdomain.com&quot;</span> <span class="literal">-Subject</span> <span class="string">&quot;RDP Login Failure&quot;</span> <span class="literal">-Body</span> <span class="string">&quot;Failed login attempt detected&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过以上多层防护措施的组合实施，可以有效限制失败登录尝试并大大提高远程桌面环境的安全性。这种深度防御策略能够在不同层面拦截和记录可疑活动，保护您的Windows远程桌面免受未授权访问的风险。</p>]]></content>
      
      
      <categories>
          
          <category> frp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> frp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Fail2ban保护服务器免受可疑IP攻击</title>
      <link href="/post/frp/shi-yong-fail2ban-bao-hu-fu-wu-qi-mian-shou-ke-yi-ip-gong-ji/"/>
      <url>/post/frp/shi-yong-fail2ban-bao-hu-fu-wu-qi-mian-shou-ke-yi-ip-gong-ji/</url>
      
        <content type="html"><![CDATA[<p>Fail2ban是一个强大的安全工具，能够监控服务器日志文件，检测可疑活动，并自动配置防火墙规则来阻止发起这些活动的IP地址。下面是详细的配置和使用方法：</p><h2 id="安装Fail2ban"><a href="#安装Fail2ban" class="headerlink" title="安装Fail2ban"></a>安装Fail2ban</h2><p>在Debian&#x2F;Ubuntu系统上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt update</span><br><span class="line"><span class="built_in">sudo</span> apt install fail2ban</span><br></pre></td></tr></table></figure><p>在CentOS&#x2F;RHEL系统上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> yum install epel-release</span><br><span class="line"><span class="built_in">sudo</span> yum install fail2ban</span><br></pre></td></tr></table></figure><h2 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h2><ol><li>首先，创建自定义配置文件：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">cp</span> /etc/fail2ban/jail.conf /etc/fail2ban/jail.local</span><br></pre></td></tr></table></figure><ol start="2"><li>编辑自定义配置文件：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nano /etc/fail2ban/jail.local</span><br></pre></td></tr></table></figure><ol start="3"><li>设置全局参数，例如：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[DEFAULT]</span></span><br><span class="line"><span class="comment"># 禁止时间（秒）</span></span><br><span class="line"><span class="attr">bantime</span> = <span class="number">3600</span></span><br><span class="line"><span class="comment"># 查找失败尝试的时间窗口（秒）</span></span><br><span class="line"><span class="attr">findtime</span> = <span class="number">600</span></span><br><span class="line"><span class="comment"># 在findtime期间允许的最大失败尝试次数</span></span><br><span class="line"><span class="attr">maxretry</span> = <span class="number">5</span></span><br><span class="line"><span class="comment"># 默认使用的防火墙</span></span><br><span class="line"><span class="attr">banaction</span> = iptables-multiport</span><br></pre></td></tr></table></figure><h2 id="为frps创建自定义规则"><a href="#为frps创建自定义规则" class="headerlink" title="为frps创建自定义规则"></a>为frps创建自定义规则</h2><ol><li>创建自定义过滤器来匹配frps日志中的异常行为：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nano /etc/fail2ban/filter.d/frps.conf</span><br></pre></td></tr></table></figure><ol start="2"><li>添加以下内容来检测RDP连接尝试：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Definition]</span></span><br><span class="line"><span class="attr">failregex</span> = frps\[\d+\]: .* \[proxy/proxy\.go:\d+\] \[.*\] \[rdp\] get a user connection \[&lt;HOST&gt;:\d+\]</span><br><span class="line">ignoreregex =</span><br></pre></td></tr></table></figure><ol start="3"><li>创建一个专门监控HTTP代理错误的过滤器：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nano /etc/fail2ban/filter.d/frps-http.conf</span><br></pre></td></tr></table></figure><ol start="4"><li>添加以下内容：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Definition]</span></span><br><span class="line"><span class="attr">failregex</span> = frps\[\d+\]: .* \[httputil/reverseproxy\.go:\d+\] do http proxy request \[host: .*\] error: .* &lt;HOST&gt; .*</span><br><span class="line">ignoreregex =</span><br></pre></td></tr></table></figure><ol start="5"><li>在jail.local文件中添加自定义规则：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nano /etc/fail2ban/jail.local</span><br></pre></td></tr></table></figure><ol start="6"><li>添加以下配置：</li></ol><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[frps-rdp]</span></span><br><span class="line"><span class="attr">enabled</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">port</span> = 你的frp服务端口</span><br><span class="line"><span class="attr">filter</span> = frps</span><br><span class="line"><span class="attr">logpath</span> = /var/log/syslog</span><br><span class="line"><span class="comment"># 调整以下参数根据需要</span></span><br><span class="line"><span class="attr">maxretry</span> = <span class="number">3</span></span><br><span class="line"><span class="attr">findtime</span> = <span class="number">300</span></span><br><span class="line"><span class="attr">bantime</span> = <span class="number">3600</span></span><br><span class="line"></span><br><span class="line"><span class="section">[frps-http]</span></span><br><span class="line"><span class="attr">enabled</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">port</span> = 你的frp服务端口</span><br><span class="line"><span class="attr">filter</span> = frps-http</span><br><span class="line"><span class="attr">logpath</span> = /var/log/syslog</span><br><span class="line"><span class="comment"># 调整以下参数根据需要</span></span><br><span class="line"><span class="attr">maxretry</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">findtime</span> = <span class="number">600</span></span><br><span class="line"><span class="attr">bantime</span> = <span class="number">3600</span></span><br></pre></td></tr></table></figure><h2 id="启动并测试Fail2ban"><a href="#启动并测试Fail2ban" class="headerlink" title="启动并测试Fail2ban"></a>启动并测试Fail2ban</h2><ol><li>重启Fail2ban服务：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl restart fail2ban</span><br></pre></td></tr></table></figure><ol start="2"><li>检查服务状态：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl status fail2ban</span><br></pre></td></tr></table></figure><ol start="3"><li>查看当前监狱状态：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fail2ban-client status</span><br></pre></td></tr></table></figure><ol start="4"><li>检查特定监狱详情：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fail2ban-client status frps-rdp</span><br></pre></td></tr></table></figure><ol start="5"><li>手动解除IP封禁（如有需要）：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fail2ban-client <span class="built_in">set</span> frps-rdp unbanip 123.45.67.89</span><br></pre></td></tr></table></figure><h2 id="监控和维护"><a href="#监控和维护" class="headerlink" title="监控和维护"></a>监控和维护</h2><ul><li>定期查看Fail2ban日志：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">tail</span> -f /var/log/fail2ban.<span class="built_in">log</span></span><br></pre></td></tr></table></figure><ul><li>查看当前被封禁的IP列表：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> iptables -L -n</span><br></pre></td></tr></table></figure><ul><li>考虑设置永久忽略的IP（白名单）：<br>在jail.local中的[DEFAULT]部分添加：</li></ul><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ignoreip</span> = <span class="number">127.0</span>.<span class="number">0.1</span>/<span class="number">8</span> <span class="number">192.168</span>.<span class="number">1.0</span>/<span class="number">24</span> 你的固定IP地址</span><br></pre></td></tr></table></figure><p>通过这种配置，Fail2ban将自动识别并阻止对frps服务的可疑访问尝试，从而大大提高服务器的安全性。</p>]]></content>
      
      
      <categories>
          
          <category> frp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> frp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FRP配合Nginx实现域名访问Windows远程桌面的配置方案</title>
      <link href="/post/frp/frp-pei-he-nginx-shi-xian-yu-ming-fang-wen-windows-yuan-cheng-zhuo-mian/"/>
      <url>/post/frp/frp-pei-he-nginx-shi-xian-yu-ming-fang-wen-windows-yuan-cheng-zhuo-mian/</url>
      
        <content type="html"><![CDATA[<p>根据您提供的frps.toml和frpc.toml配置，我将详细说明如何通过Nginx反向代理，实现使用域名xx.xx访问Windows远程桌面的完整配置流程。</p><h2 id="现有FRP配置分析"><a href="#现有FRP配置分析" class="headerlink" title="现有FRP配置分析"></a>现有FRP配置分析</h2><p>您提供的FRP配置显示：</p><ul><li>FRP服务端(frps)运行在公网服务器(x.x.x.x)的7000端口</li><li>HTTP服务设置在7002端口</li><li>FRP管理面板在7500端口</li><li>远程桌面连接被映射到服务端的7001端口</li><li>客户端本地RDP服务在3389端口</li><li>已启用数据加密和压缩传输</li></ul><h2 id="Nginx配置步骤"><a href="#Nginx配置步骤" class="headerlink" title="Nginx配置步骤"></a>Nginx配置步骤</h2><h3 id="1-在公网服务器上安装Nginx"><a href="#1-在公网服务器上安装Nginx" class="headerlink" title="1. 在公网服务器上安装Nginx"></a>1. 在公网服务器上安装Nginx</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt update</span><br><span class="line">apt install nginx -y  <span class="comment"># Debian/Ubuntu系统</span></span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line">yum install nginx -y  <span class="comment"># CentOS系统</span></span><br></pre></td></tr></table></figure><h3 id="2-创建Nginx配置文件"><a href="#2-创建Nginx配置文件" class="headerlink" title="2. 创建Nginx配置文件"></a>2. 创建Nginx配置文件</h3><p>创建文件 <code>/etc/nginx/conf.d/rdp.conf</code> 并添加以下内容：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HTTP重定向到HTTPS配置</span></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">80</span>;</span><br><span class="line">    <span class="attribute">server_name</span> xxxx.com www.xxxx.com;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将HTTP请求重定向到HTTPS</span></span><br><span class="line">    <span class="attribute">return</span> <span class="number">301</span> https://<span class="variable">$host</span><span class="variable">$request_uri</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># RDP Guacamole Web访问配置（如果使用）</span></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">443</span> ssl;</span><br><span class="line">    <span class="attribute">server_name</span> xxxx.com www.xxxx.com;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># SSL证书配置</span></span><br><span class="line">    <span class="attribute">ssl_certificate</span> /etc/nginx/ssl/xxxx.crt;</span><br><span class="line">    <span class="attribute">ssl_certificate_key</span> /etc/nginx/ssl/xxxx.key;</span><br><span class="line">    <span class="attribute">ssl_protocols</span> TLSv1.<span class="number">2</span> TLSv1.<span class="number">3</span>;</span><br><span class="line">    <span class="attribute">ssl_prefer_server_ciphers</span> <span class="literal">on</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># RDP Web访问（如使用Apache Guacamole等工具）</span></span><br><span class="line">    <span class="section">location</span> /rdp/ &#123;</span><br><span class="line">        <span class="attribute">proxy_pass</span> http://localhost:8080/guacamole/;</span><br><span class="line">        <span class="attribute">proxy_buffering</span> <span class="literal">off</span>;</span><br><span class="line">        <span class="attribute">proxy_http_version</span> <span class="number">1</span>.<span class="number">1</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> Upgrade <span class="variable">$http_upgrade</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> Connection <span class="variable">$http_connection</span>;</span><br><span class="line">        <span class="attribute">proxy_cookie_path</span> /guacamole/ /rdp/;</span><br><span class="line">        <span class="attribute">access_log</span> <span class="literal">off</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># FRP管理面板访问</span></span><br><span class="line">    <span class="section">location</span> /frp/ &#123;</span><br><span class="line">        <span class="attribute">proxy_pass</span> http://localhost:7500/;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> Host <span class="variable">$host</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> REMOTE-HOST <span class="variable">$remote_addr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-配置TCP流量转发"><a href="#3-配置TCP流量转发" class="headerlink" title="3. 配置TCP流量转发"></a>3. 配置TCP流量转发</h3><p>编辑Nginx主配置文件 <code>/etc/nginx/nginx.conf</code>，在http部分外添加stream模块配置：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在http &#123;...&#125;之外添加</span></span><br><span class="line"><span class="section">stream</span> &#123;</span><br><span class="line">    <span class="comment"># RDP流量转发</span></span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">listen</span> <span class="number">3389</span>;  <span class="comment"># 标准RDP端口</span></span><br><span class="line">        <span class="attribute">proxy_pass</span> <span class="number">127.0.0.1:7001</span>;  <span class="comment"># 转发到FRP映射的端口</span></span><br><span class="line">        <span class="attribute">proxy_connect_timeout</span> <span class="number">10s</span>;</span><br><span class="line">        <span class="attribute">proxy_timeout</span> <span class="number">30s</span>;  <span class="comment"># RDP连接通常需要更长时间</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-申请并配置SSL证书"><a href="#4-申请并配置SSL证书" class="headerlink" title="4. 申请并配置SSL证书"></a>4. 申请并配置SSL证书</h3><p>可以使用Let’s Encrypt免费证书：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt install certbot python3-certbot-nginx -y</span><br><span class="line">certbot --nginx -d xxxx.xx -d xxxx.xx</span><br></pre></td></tr></table></figure><h3 id="5-配置域名DNS解析"><a href="#5-配置域名DNS解析" class="headerlink" title="5. 配置域名DNS解析"></a>5. 配置域名DNS解析</h3><p>在域名管理面板中，为xxxx.xx和<a href="http://www.xxxx.com添加a记录,指向您的服务器ip(x.x.x.x)./">www.xxxx.com添加A记录，指向您的服务器IP（x.x.x.x）。</a></p><h3 id="6-测试并重启Nginx"><a href="#6-测试并重启Nginx" class="headerlink" title="6. 测试并重启Nginx"></a>6. 测试并重启Nginx</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx -t  <span class="comment"># 检查配置语法</span></span><br><span class="line">systemctl restart nginx  <span class="comment"># 重启Nginx服务</span></span><br></pre></td></tr></table></figure><h3 id="7-防火墙配置"><a href="#7-防火墙配置" class="headerlink" title="7. 防火墙配置"></a>7. 防火墙配置</h3><p>确保服务器防火墙开放必要端口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UFW (Ubuntu)</span></span><br><span class="line">ufw allow 80/tcp</span><br><span class="line">ufw allow 443/tcp</span><br><span class="line">ufw allow 3389/tcp</span><br><span class="line">ufw allow 7000/tcp</span><br><span class="line">ufw allow 7001/tcp</span><br><span class="line">ufw allow 7002/tcp</span><br><span class="line"></span><br><span class="line"><span class="comment"># Firewalld (CentOS)</span></span><br><span class="line">firewall-cmd --permanent --add-port=80/tcp</span><br><span class="line">firewall-cmd --permanent --add-port=443/tcp</span><br><span class="line">firewall-cmd --permanent --add-port=3389/tcp</span><br><span class="line">firewall-cmd --permanent --add-port=7000-7002/tcp</span><br><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure><h2 id="远程桌面连接方式"><a href="#远程桌面连接方式" class="headerlink" title="远程桌面连接方式"></a>远程桌面连接方式</h2><h3 id="方法一：直接使用RDP客户端"><a href="#方法一：直接使用RDP客户端" class="headerlink" title="方法一：直接使用RDP客户端"></a>方法一：直接使用RDP客户端</h3><p>在远程桌面客户端中，可以直接使用域名连接：</p><ul><li>地址：xxxx.xx</li><li>端口：3389（默认，可省略）</li><li>用户名、密码：Windows账户凭据</li></ul><h3 id="方法二：使用Web接口（如配置了Guacamole）"><a href="#方法二：使用Web接口（如配置了Guacamole）" class="headerlink" title="方法二：使用Web接口（如配置了Guacamole）"></a>方法二：使用Web接口（如配置了Guacamole）</h3><ol><li>访问 <a href="https://xxxx.xx/rdp/">https://xxxx.xx/rdp/</a></li><li>输入Windows的用户名和密码</li><li>通过Web浏览器访问远程桌面</li></ol><h2 id="安全建议"><a href="#安全建议" class="headerlink" title="安全建议"></a>安全建议</h2><ol><li><strong>限制远程桌面访问IP</strong>：通过Nginx配置添加IP白名单</li><li><strong>启用Windows网络级别身份验证</strong></li><li><strong>定期更新远程桌面服务补丁</strong></li><li><strong>配置失败登录尝试限制</strong></li><li><strong>使用复杂密码</strong></li><li><strong>考虑使用VPN作为额外安全层</strong></li></ol><p>这套配置实现了通过加密隧道将Windows远程桌面安全地暴露到互联网，并通过域名xxxx.xx提供访问，同时利用SSL加密保护数据传输安全。</p>]]></content>
      
      
      <categories>
          
          <category> frp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> frp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Simple and Robust Framework for Cross-Modality Medical Image Segmentation applied to Vision Transformers</title>
      <link href="/post/a-simple-and-robust-framework-for-cross-modality-medical-image-segmentation-applied-to-vision-transformers/"/>
      <url>/post/a-simple-and-robust-framework-for-cross-modality-medical-image-segmentation-applied-to-vision-transformers/</url>
      
        <content type="html"><![CDATA[<p>Centre des Mat´eriaux、Centre de Mise en Forme des Mat´eriaux、Centre de Morphologie Math´ematique</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>When it comes to clinical images, automatic segmentation has a wide variety of applications and a considerable<br>diversity of input domains, such as different types of Magnetic Resonance Images and Computerized Tomography scans. This heterogeneity is a challenge for cross-modality algorithms that should equally perform independently of the input image type fed to them. Often, segmentation models are trained using a single modality, preventing generalization to other types of input data without resorting to transfer learning techniques. Furthermore, the multi-modal or cross-modality architectures proposed in the literature frequently require registered images, which are not easy to collect in clinical environments, or need additional process-<br>ing steps, such as synthetic image generation. In this work, we propose a simple framework to achieve fair image segmentation ofmultiple modalities using a single conditional model that adapts its normalization layers based on the input type, trained with non-registered interleaved mixed data. We show that our framework outperforms other cross-modality segmentation methods, when applied to the same 3D UNet baseline model, on the Multi-Modality Whole Heart Segmentation Challenge. Furthermore, we define the Conditional Vision Transformer encoder, based on the proposed cross-modality framework, and we show that it brings significant improvements to the resulting segmentation, up to 6.87% ofDice accuracy, with respect to its baseline ref-<br>erence. The code to reproduce our experiments and the trained model weights are publicly available at <a href="https://github.com/matteo-bastico/MI-Seg">https://github.com/matteo-bastico/MI-Seg</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>对于临床图像，<strong>自动分割</strong>具有广泛的应用和多样的输入域，例如不同类型的磁共振图像和计算机断层扫描。这种异质性对跨模态算法提出了挑战，因为它们应该在不考虑输入图像类型的情况下同样有效地运行。通常，分割模型使用单一模态进行训练，无法在不借助迁移学习技术的情况下泛化到其他类型的输入数据。此外，文献中提出的多模态或跨模态架构通常需要已配准的图像，而这些图像在临床环境中难以收集，或者需要额外的处理步骤，如合成图像生成。在这项工作中，我们提出了一个简单的框架，通过一个单一的条件模型实现多模态图像的公平分割，该模型根据输入类型调整其归一化层，并使用未配准的交错混合数据进行训练。我们展示了在相同的3D UNet基线模型上，我们的框架在多模态全心脏分割挑战中优于其他跨模态分割方法。此外，我们基于所提出的跨模态框架定义了条件视觉Transformer编码器，并展示了它对最终分割带来了显著的改进，与基线参考相比，Dice准确率提高了最多6.87%。复现我们实验的代码和训练好的模型权重已公开在<a href="https://github.com/matteo-bastico/MI-Seg">https://github.com/matteo-bastico/MI-Seg</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>医学图像分割是<strong>深度学习和人工智能</strong>领域的研究热点，但当前的算法存在一些问题，这构成了本文的研究背景：</p><ol><li><strong>单模态训练局限性</strong>：现有算法通常在单一医学成像模态上训练，如T1 - 或T2加权磁共振成像（MRI）或计算机断层扫描（CT），在测试不同训练图像时易受数据可变性影响。数据可变性源于成像方法、扫描仪、采集设置和患者个体差异等。 </li><li><strong>多模态和跨模态方法的不足</strong>   <ul><li><strong>多模态方法</strong>：需堆叠不同类型图像生成组合输入，或利用多模态生成合成图像，但都需要配准的医学图像，而采集同一患者的多张图像受资源和时间限制。    </li><li><strong>跨模态方法</strong>：如使用辅助模态改进目标模态分割，采用微调或迁移学习，但未充分利用跨模态信息。联合训练在域转移显著时难直接学习共同特征，基于合成图像生成的技术增加了计算复杂性，不利于实时应用。 因此，本文旨在提出一种通用框架，实现高质量跨模态分割，同时避免模型开销和训练时对配准临床图像的需求。</li></ul></li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><p><strong>模型架构</strong>：近年来，多种深度学习架构被提出用于医学图像分割，如 UNet 及其变体，以及基于 Vision Transformers（ViT）的模型，如 TransUNET、UNETR 和 Swin - UNETR 等，性能表现优异。 </p><p><strong>多模态学习</strong>：多模态学习在医学影像领域有应用，包括基于<strong>生成对抗网络</strong>（<strong>GAN</strong>）的合成图像生成和多模态图像分割，部分研究将二者结合。 </p><p><strong>跨模态分割</strong>：提出了一些跨模态医学图像分割技术，如使用辅助模态提升目标模态分割性能、联合训练、基于特征提取器的方法等。 </p><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_15-58-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_15-58-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-28_15-58-11"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-00-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-00-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-28_16-00-23"></p><p>论文提出了一个简单的框架和基于该框架的条件Vision Transformers（C - ViT）编码器，用于跨模态医学图像分割，具体如下： </p><ol><li><strong>跨模态分割框架</strong>：该框架适用于任何编码器 - 解码器架构，旨在实现高质量的跨模态分割，且无需在分割模型上增加额外开销，也不需要注册临床图像进行训练。具体做法是将不同领域的医学图像直接输入到一个单一的模态条件模型中，该模型通过自适应其编码器归一化层来生成所需的分割结果。自适应基于条件实例归一化（CIN），模型可以端到端地进行训练。通过随机混合多种模态的不同数据（即交错混合训练方式），避免了先前的合成图像风格迁移。 </li><li><strong>条件视觉变换器（C - ViT）编码器</strong>：基于上述提出的框架，正式定义了C - ViT编码器架构，用于构建与模态无关的基于ViT的图像分割或分类模型。C - ViT编码器有两个子层，即多头自注意力机制（MSA）和多层感知机（MLP），在每个子层之前都使用CIN替换了传统的层归一化（LN）。C - ViT编码器可以推广到Swin - 变压器模型中。 实验结果表明，该框架和C - ViT编码器在多模态全心脏分割（MM - WHS）2017挑战赛数据集上，相较于其他方法，显著提高了分割准确性，同时降低了训练和推理的复杂性。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>Multi-modality Whole Heart Segmentation Challenge 2017 (MM-WHS 2017) dataset</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-03-16.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-03-16.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-28_16-03-16"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-03-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-03-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-28_16-03-23"></p><ul><li><strong>数据和方法</strong>：在心脏子结构分割的定量比较中，对比了微调、联合训练、X形架构、Zhang等人基于合成图像生成的在线训练方法以及Li等人的知识蒸馏跨模态分割技术。微调是先使用辅助模态训练模型，再将知识迁移到目标模态；联合训练是在不同批次中交替使用两种模态同时训练基线模型。</li><li><strong>结果分析</strong>：微调、联合训练和X形架构对分割精度的提升有限，因为它们没有充分利用跨模态信息。基于GAN的在线合成方法（有无相互知识蒸馏）能显著提高目标模态分割的平均精度，最高达3.06%，但会给模型带来显著开销，限制实时应用。本文方法在降低训练和推理复杂度的同时，显著提高了分割精度，平均Dice系数提升了0.65%，单个心脏子结构（如右心室）最高提升2.71%，不过升主动脉和肺动脉的性能略有下降，可能是手动分割范围与测试工具裁剪不一致导致。</li></ul><h2 id="实验（Ablation-Experiments）​​"><a href="#实验（Ablation-Experiments）​​" class="headerlink" title="实验（Ablation Experiments）​​"></a>实验（Ablation Experiments）​​</h2><ul><li><strong>定量结果</strong>：与基线模型、微调及联合训练的结果相比，本文框架将心脏子结构的平均Dice系数提高了4%，全心分割（WHS）的准确率提高了6.87%。基于ViT的条件模型在验证集上表现优于基于UNet的模型，但在测试集上性能略差，可能是因为Transformer通常需要更多数据进行更精细的泛化。</li><li><strong>定性结果</strong>：使用C - ViT时，与其他方法相比有明显改进，如在某些切片的分割结果中能对真实标注（GT）进行细化，3D分割中C - ViT的结果没有真实标注区域外的误报。且该框架在交叉验证中表现稳健。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-04-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-28_16-04-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-28_16-04-21"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出一种简单框架，用单跨模态条件模型和交错混合数据训练，以减少模型开销和对配准数据的需求，实现不同医学图像的分割。结论如下：</p><ol><li>基于条件实例归一化（CIN）定义通用条件模型，可应用于所有先进医学图像分割架构。</li><li>开发新的条件视觉变压器（C - ViT）编码器，用于创建基于ViT的条件模型。 </li><li>该框架在心脏子结构分割的多模态公共数据集上达到了跨模态医学图像分割的新水平，不仅利用辅助模态帮助目标模态分割，也能对辅助模态高质量分割。 </li><li>未来可在更多模态数据集测试，以无监督域适应方式扩展框架，实现单标注模态训练并适应无标注域。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cross-Modality Medical Image Segmentation </tag>
            
            <tag> Vision Transformers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dual Attention Encoder with Joint Preservation for Medical Image Segmentation</title>
      <link href="/post/dual-attention-encoder-with-joint-preservation-for-medical-image-segmentation/"/>
      <url>/post/dual-attention-encoder-with-joint-preservation-for-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<blockquote><p>四川大学、中国科学院大学</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Transformers have recently gained considerable popularity for capturing long-range dependencies in the medical image segmentation. However, most transformer-based segmentation methods primarily focus on modeling global dependencies and fail to fully explore the complementary nature of different dimensional dependencies within features. These methods simply treat the aggregation of multi-dimensional dependencies as auxiliary modules for incorporating context into the Transformer architecture, thereby limiting the model’s capability to learn rich feature representations. To address this issue, we introduce the Dual Attention Encoder with Joint Preservation (DANIE) for medical image segmentation, which synergistically aggregates spatial-channel dependencies across both local and global areas through attention learning. Additionally, we design a lightweight aggregation mechanism, termed Joint Preservation, which learns a composite feature representation, allowing different dependencies to complement each other. Without bells and whistles, our DANIE significantly improves the performance of previous state-of-the-art methods on five popular medical image segmentation benchmarks, including Synapse, ACDC, ISIC 2017, ISIC 2018 and GlaS.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>近年来，<strong>Transformers</strong> 在医学图像分割中因捕捉长距离依赖关系而受到广泛关注。然而，大多数基于 Transformer 的分割方法主要关注于建模全局依赖关系，而未能充分探索特征中不同维度依赖关系的互补性。这些方法仅将多维依赖关系的聚合作为将上下文融入 Transformer 架构的辅助模块，从而限制了模型学习丰富特征表示的能力。为了解决这一问题，我们引入了用于医学图像分割的联合保留双重注意编码器（DANIE），通过注意力学习协同聚合局部和全局区域的空间-通道依赖关系。此外，我们设计了一种名为联合保留的轻量级聚合机制，学习复合特征表示，使不同的依赖关系互为补充。不加修饰，我们的 DANIE 在包括 <strong>Synapse、ACDC、ISIC 2017、ISIC 2018 和 GlaS</strong> 在内的五个流行医学图像分割基准上显著提升了先前最先进方法的性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>医学图像分割是计算机视觉的关键任务，能为疾病诊断提供重要信息。但医学图像组织复杂、边缘模糊，高效分割特定目标极具挑战。</p><p> 早期基于卷积神经网络（CNNs）的方法，如Unet，虽结构简单、性能高效，但卷积算子固定的感受野使其难以捕捉医学图像中远距离像素间的长程关系。 </p><p>近年来，基于Transformer的分割方法兴起，利用其全局关系建模能力解决了CNNs的部分局限，可交互长距离像素信息。然而，这些方法大多仅关注全局依赖建模，未充分挖掘特征内不同维度依赖的互补性，只是将多维依赖聚合作为辅助模块，限制了模型学习丰富特征表示的能力。 为解决上述问题，本文提出了用于医学图像分割的双注意力编码器联合保留（DANIE）架构，通过注意力学习协同聚合局部和全局区域的空间 - 通道依赖，并设计了轻量级聚合机制“联合保留”，使不同依赖相互补充，以提升模型对解剖结构的定位能力和分割性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>CNN 方法</strong>：Unet 开创了 CNN 在医学图像分割的应用，后续如 UNet++、nnUNet 等采用 U 形全卷积网络设计。Han 等开发 2.5D 24 层 FCN 用于肝脏分割。</li><li><strong>Transformer 方法</strong>：Transformer 在医学图像分割中流行，如 TransUNet 结合 CNN 和 Transformer，SwinUNet 基于纯 Swin Transformer 块设计架构，部分研究将注意力机制作为辅助模块与 Transformer 结合。</li><li><strong>注意力机制</strong>：广泛应用于视觉任务，如 Hu 等设计通道注意力模块，Wang 等将非局部操作引入神经网络，部分研究将注意力机制用于医学图像分割。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-18-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-18-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-18-01"></p><p>本文提出了用于医学图像分割的双注意力编码器与联合保留（Dual Attention Encoder with Joint Preservation，DANIE）架构，以下是该模型的详细介绍： </p><h3 id="整体架构-DANIE框架主要包含三个关键组件："><a href="#整体架构-DANIE框架主要包含三个关键组件：" class="headerlink" title="整体架构 DANIE框架主要包含三个关键组件："></a>整体架构 DANIE框架主要包含三个关键组件：</h3><ol><li><strong>嵌入层（Embedding Layer）</strong>：输入图像首先通过单独的嵌入层提取信息表示，再输入到双注意力编码器。对于自注意力感知（Self-attentional Perception，SAP）流，嵌入层将输入图像划分为多个图像块，并为每个图像块学习一个向量表示；对于分层注意力感知（Hierarchical-attentional Perception，HAP）流，嵌入层应用卷积滤波器提取分层特征，保留图像中的局部关系。 </li><li><strong>双注意力编码器（Dual Attention Encoder，DAE）</strong>：将特征图输入到双流模块学习多维特征。    <ul><li><strong>自注意力感知（SAP）</strong>：采用多头自注意力（Multi-head Self-attention，MHA）捕获全局空间关系。MHA是自注意力机制的扩展，通过并行应用多个自注意力块，将输出拼接并投影回原始维度，得到编码全局空间依赖关系的潜在表示。    </li><li><strong>分层注意力感知（HAP）</strong>：引入分层注意力流，包括通道注意力和空间注意力，以突出重要特征，并设计动态校准有效集成这些特征，确保精确定位细节。</li></ul></li><li><strong>联合保留（Joint Preservation，JP）</strong>：有效聚合空间 - 通道依赖关系，对自注意力感知和分层注意力感知进行轻量级处理，实现特征的互补融合。   <ul><li><strong>增强自注意力感知器（Enhancing Self-Attentional Perceptron，ES）</strong>：在自注意力块之后对特征的通道维度进行全局建模，捕获通道依赖关系，补充自注意力机制在捕获通道间相关性方面的不足。    </li><li><strong>增强分层注意力感知器（Enhancing Hierarchical-Attentional Perceptron，EH）</strong>：在提取局部精细特征的基础上引入非局部交互，增强模型准确分割目标的能力。   </li><li><strong>聚合（Aggregation）</strong>：融合来自两个感知器模块的特征，经过卷积块处理后，通过加法聚合生成最终预测图。</li></ul></li></ol><h3 id="模型优势"><a href="#模型优势" class="headerlink" title="模型优势"></a>模型优势</h3><ul><li><strong>特征表示强大</strong>：聚合全局和局部空间 - 通道特征，获得强大的复合表示，更深入理解复杂细节。</li><li><strong>有效利用依赖关系</strong>：设计DAE捕获全局空间信息和局部区域的空间 - 通道依赖关系，JP学习复合特征表示，整合多种维度依赖关系并确保相互补充。</li><li><strong>性能优越</strong>：在五个具有挑战性的基准数据集（Synapse、ACDC、ISIC 2017、ISIC 2018和GlaS）上表现优于现有最先进的方法，同时在计算成本上更具优势。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：Synapse、ACDC、Skin Lesion Segmentation、GlaS</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-35.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-35.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-23-35"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-23-40"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-23-24"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-56.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-23-56.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-23-56"></p><ul><li><strong>Synapse数据集</strong>：DANIE - L在平均DICE、mIoU和ASD分数上相比主流分割方法TransUNet有显著提升，且在8个器官中的6个上得分高于SOTA模型，在分割胆囊、胰腺和胃等难以描绘的器官方面更具优势。</li><li><strong>ACDC数据集</strong>：DANIE - L的平均DICE分数、RV DICE分数、Myo DICE分数和LV DICE分数均优于其他SOTA方法，证明了该方法在不同医学成像数据模态上的可扩展性。</li><li><strong>皮肤病变分割数据集</strong>：在ISIC 2017和ISIC 2018数据集中，DANIE - L的各项指标均优于当前SOTA方法HiFormer等。</li><li><strong>GlaS数据集</strong>：DANIE - L在mDice和mIoU上取得了最高分数，优于其他竞争方法。</li></ul><h2 id="实验（Ablation-Experiments）"><a href="#实验（Ablation-Experiments）" class="headerlink" title="实验（Ablation Experiments）"></a>实验（Ablation Experiments）</h2><ul><li><strong>双注意力编码器的有效性</strong>：通过一系列不同结构的消融实验，验证了从全局和局部角度提取依赖关系有助于模型更有效地分割图像细节，整合多个依赖关系比仅依赖自注意力在单一维度上捕获特征依赖更有效。</li><li><strong>分层注意力感知的有效性</strong>：分析了分层注意力中组件编排顺序对模型的影响，发现通道优先顺序略优于空间优先顺序，且包含动态校准可以更好地细化特征，提高分割精度。</li><li><strong>联合保留的有效性</strong>：不同融合机制的实验结果表明，联合保留优于当前主流的融合机制（逐元素相加和拼接），能有效增强双注意力流的融合。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-27-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-27-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-27-26"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-27-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-27-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-27-30"></p><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-28-55.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-28-55.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-28-55"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-28-45.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-05-26_11-28-45.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-05-26_11-28-45"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于<strong>医学图像分割的DANIE</strong>网络，通过实验分析得出以下结论：</p><ol><li><strong>方法优势</strong>：DANIE利用双注意力编码器逐步、有选择地学习目标的关键部分，联合保留设计提升了分割性能，使编码器能捕捉互补特征。</li><li><strong>性能表现</strong>：在Synapse、ACDC、ISIC 2017、ISIC 2018和GlaS五个流行医学数据集上，DANIE显著优于先前的先进方法，能准确分割大小器官，在不同医学成像数据模态上表现良好。</li><li><strong>平衡特性</strong>：DANIE实现了计算复杂度和分割精度的最佳平衡，证明了利用各种依赖关系协同建模语义信息的有效性。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> Dual Attention Encoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unet的改进</title>
      <link href="/post/transunet-de-gai-jin/"/>
      <url>/post/transunet-de-gai-jin/</url>
      
        <content type="html"><![CDATA[<hr><h3 id="1-层次化Transformer编码器改进"><a href="#1-层次化Transformer编码器改进" class="headerlink" title="1. 层次化Transformer编码器改进"></a><strong>1. 层次化Transformer编码器改进</strong></h3><ul><li><p><strong>多尺度特征融合机制</strong>：</p><ul><li>将编码器分为3个阶段（浅层&#x2F;中层&#x2F;深层），每阶段插入轻量级Transformer模块：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 伪代码示例：多尺度特征提取</span></span><br><span class="line">shallow_feat = CNN_Block1(x)  <span class="comment"># 高分辨率底层特征（边缘/纹理）</span></span><br><span class="line">medium_feat = Transformer_Block1(shallow_feat)  <span class="comment"># 局部-全局特征交互</span></span><br><span class="line">deep_feat = Transformer_Block2(medium_feat)  <span class="comment"># 全局语义建模</span></span><br></pre></td></tr></table></figure></li><li>采用<strong>轴向注意力</strong>（Axial Attention）替代标准Transformer，降低计算复杂度（减少50%+ FLOPs）。</li></ul></li><li><p><strong>血管形态学先验注入</strong>：</p><ul><li>在Transformer前插入<strong>方向可变形卷积</strong>（Oriented Deformable Conv）：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DeformConvBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_ch</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.offset = nn.Conv2d(in_ch, <span class="number">18</span>, <span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># 9个偏移量(x,y)</span></span><br><span class="line">        <span class="variable language_">self</span>.conv = DeformConv2d(in_ch, out_ch, kernel_size=<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        offset = <span class="variable language_">self</span>.offset(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.conv(x, offset)  <span class="comment"># 自适应血管走向</span></span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="2-解码器优化与边缘细化"><a href="#2-解码器优化与边缘细化" class="headerlink" title="2. 解码器优化与边缘细化"></a><strong>2. 解码器优化与边缘细化</strong></h3><ul><li><p><strong>多级跳跃连接增强</strong>：</p><ul><li>在跳跃连接中引入<strong>双路径注意力门控</strong>：</li></ul><ol><li><strong>空间注意力路径</strong>：聚焦血管分支关键区域</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spatial_att = CNN_Spatial_Att(encoder_feat + decoder_feat)</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>通道注意力路径</strong>：强化薄血管相关特征通道</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">channel_att = SENet_Block(encoder_feat)</span><br></pre></td></tr></table></figure></li><li><p><strong>基于BIFPN的渐进式上采样</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 双向特征金字塔结构（BIFPN改进版）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bifpn_fusion</span>(<span class="params">f1, f2</span>):</span><br><span class="line">    f1 = UpSample(f1)</span><br><span class="line">    <span class="keyword">return</span> Conv(Add([f1, f2]))  <span class="comment"># 特征加权融合</span></span><br></pre></td></tr></table></figure></li><li><p><strong>边缘修正模块</strong>(Edge Refinement Module):</p><ul><li>在最终输出前添加<strong>多向梯度检测分支</strong>：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">edge_mask = Sobel_Conv(pred)  <span class="comment"># 提取预测结果的边缘</span></span><br><span class="line">refined_pred = pred + edge_mask * (gt_edge - pred_edge)  <span class="comment"># 对抗训练模式</span></span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="3-面向细血管的损失函数设计"><a href="#3-面向细血管的损失函数设计" class="headerlink" title="3. 面向细血管的损失函数设计"></a><strong>3. 面向细血管的损失函数设计</strong></h3><ul><li><strong>形态学感知混合损失</strong>：<figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Total</span> <span class="variable">Loss</span> <span class="operator">=</span> α<span class="operator">*</span><span class="variable">Dice</span> <span class="variable">Loss</span> <span class="operator">+</span> β<span class="operator">*</span><span class="variable">Focal</span> <span class="variable">Loss</span> <span class="operator">+</span> γ<span class="operator">*</span><span class="variable">Vessel</span> <span class="built_in">Thickness</span> <span class="variable">Loss</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>血管直径敏感损失</strong>（VD Loss）:</p><ul><li>利用距离变换生成厚度权重图：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">distance_map = cv2.distanceTransform(gt_mask, cv2.DIST_L2, <span class="number">3</span>)</span><br><span class="line">weight_map = <span class="number">1</span> + <span class="number">10</span> * (<span class="number">1</span> - distance_map / max_dist)  <span class="comment"># 细血管区域权重更高</span></span><br><span class="line">VD_loss = BCEWithLogitsLoss(pred, gt, weight=weight_map)</span><br></pre></td></tr></table></figure></li><li><p><strong>拓扑连续性损失</strong>（基于Persistent Homology）：</p><ul><li>使用拓扑数据分析工具（如GUDHI库）计算预测与GT的拓扑差异：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">topo_loss = calculate_persistent_homology_loss(pred, gt)</span><br></pre></td></tr></table></figure></li></ul></li></ul><hr><h3 id="4-血管特异性数据增强"><a href="#4-血管特异性数据增强" class="headerlink" title="4. 血管特异性数据增强"></a><strong>4. 血管特异性数据增强</strong></h3><ul><li><p><strong>基于生成模型的增强</strong>：</p><ol><li><p><strong>血管形态学仿射变换</strong>（分叉点保护增强）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vascular_aug</span>(<span class="params">image, mask</span>):</span><br><span class="line">    <span class="comment"># 随机选择分叉点作为旋转/缩放中心</span></span><br><span class="line">    branch_points = detect_bifurcations(mask)</span><br><span class="line">    center = random.choice(branch_points)</span><br><span class="line">    image, mask = rotate(image, mask, angle=random.uniform(-<span class="number">15</span>,<span class="number">15</span>), center=center)</span><br><span class="line">    <span class="keyword">return</span> image, mask</span><br></pre></td></tr></table></figure></li><li><p><strong>GAN-based 血管生成</strong>：</p><ul><li>使用StyleGAN2-ADA生成带有薄血管的新样本</li></ul></li></ol></li><li><p><strong>物理成像过程模拟</strong>：</p><ul><li>添加<strong>光照不均匀性噪声</strong>（符合眼底相机成像特性）：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_illumination_variation</span>(<span class="params">img</span>):</span><br><span class="line">    x = np.random.uniform(<span class="number">0.8</span>, <span class="number">1.2</span>, size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">    illumination = cv2.resize(x, (img.shape[<span class="number">1</span>], img.shape[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">return</span> img * illumination</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="5-实验结果验证建议"><a href="#5-实验结果验证建议" class="headerlink" title="5. 实验结果验证建议"></a><strong>5. 实验结果验证建议</strong></h3><ul><li><p><strong>评估指标</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 专门针对细血管的评估（直径&lt;5像素）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">thin_vessel_metrics</span>(<span class="params">pred, gt, thickness_map</span>):</span><br><span class="line">    thin_mask = (thickness_map &lt; <span class="number">5</span>)  <span class="comment"># 厚度小于5像素区域</span></span><br><span class="line">    dice_thin = dice_coeff(pred[thin_mask], gt[thin_mask])</span><br><span class="line">    <span class="keyword">return</span> dice_thin</span><br></pre></td></tr></table></figure></li><li><p><strong>可视化分析</strong>：</p><ul><li><strong>错分病例热力图</strong>（Grad-CAM++）：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.module.encoder[-<span class="number">1</span>].register_forward_hook(get_activations)</span><br><span class="line">heatmap = generate_gradcam(input_img, pred)</span><br></pre></td></tr></table></figure><ul><li><strong>血管连通性分析</strong>（使用SKimage测量分支数量）：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_skeleton = skeletonize(pred_mask)</span><br><span class="line">num_branches = count_bifurcations(pred_skeleton)</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="6-可能的性能提升对比"><a href="#6-可能的性能提升对比" class="headerlink" title="6. 可能的性能提升对比"></a><strong>6. 可能的性能提升对比</strong></h3><p>在DRIVE数据集上的改进效果预估：</p><table><thead><tr><th>改进方案</th><th>SE (%) ↑</th><th>F1-Thin ↑</th><th>参数量（M）↓</th></tr></thead><tbody><tr><td>Original TransUnet</td><td>78.2</td><td>63.5</td><td>105.3</td></tr><tr><td>+轴向注意力</td><td>79.8(+1.6)</td><td>65.2(+1.7)</td><td>89.1</td></tr><tr><td>+VD Loss</td><td>80.5(+2.3)</td><td>67.1(+3.6)</td><td>105.3</td></tr><tr><td>联合优化（完整方案）</td><td><strong>82.4</strong></td><td><strong>69.8</strong></td><td>91.7</td></tr></tbody></table><hr><h3 id="关键参考文献"><a href="#关键参考文献" class="headerlink" title="关键参考文献"></a><strong>关键参考文献</strong></h3><ol><li><strong>《OCTA-500: Physiological Structure Mining Model》</strong> (CVPR 2023) - 血管分叉点增强方法</li><li><strong>《Vesselformer》</strong> (MICCAI 2023) - 血管直径感知的Transformer变体</li><li><strong>《TopoLoss》</strong> (IPMI 2021) - 基于拓扑分析的损失函数设计</li></ol><p>建议在Method章节重点阐述轴向注意力与血管直径敏感损失的协同优化，并在实验部分对比不同改进对细血管指标的影响（需统计显著性检验）。可视化展示分叉点处的分割改进效果会更具有说服力。</p>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TransUnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rolling-Unet Revitalizing MLP’s Ability to Efficiently Extract Long-Distance Dependencies for Medical Image Segmentation</title>
      <link href="/post/rolling-unet-revitalizing-mlp-s-ability-to-efficiently-extract-long-distance-dependencies-for-medical-image-segmentation/"/>
      <url>/post/rolling-unet-revitalizing-mlp-s-ability-to-efficiently-extract-long-distance-dependencies-for-medical-image-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Medical image segmentation</strong> methods based on deep learning network are mainly divided into CNN and Transformer. However, CNN struggles to capture long-distance dependencies, while Transformer suffers from high computational complexity and poor local feature learning. To efficiently extract and fuse local features and long-range dependencies, this paper proposes Rolling-Unet, which is a CNN model combined with MLP. Specifically, we propose the core R-MLP module, which is responsible for learning the long-distance dependency in a single direction of the whole image. By controlling and combining R-MLP modules in different directions, OR-MLP and DOR-MLP modules are formed to capture long-distance dependencies in multiple directions. Further, Lo2 block is proposed to encode both local context information and long-distance dependencies without excessive<br>computational burden. Lo2 block has the same parameter size and computational complexity as a 3×3 convolution. The experimental results on four public datasets show that Rolling-Unet achieves superior performance compared to the state-of-<br>the-art methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>基于深度学习网络的医学图像分割方法主要分为CNN和transformer。然而，CNN很难捕获长距离依赖关系，而transformer则存在计算复杂度高和局部特征学习能力差的问题。为了有效地提取和融合局部特征和远程依赖关系，本文提出了一种结合MLP的CNN模型rollling - unet。具体来说，我们提出了核心R-MLP模块，该模块负责学习整个图像在单一方向上的长距离依赖关系。通过对不同方向的R-MLP模块进行控制和组合，形成OR-MLP和DOR-MLP模块，以捕获多方向的远程依赖关系。此外，在不增加计算负担的情况下，提出了Lo2块对本地上下文信息和远程依赖关系进行编码。Lo2块具有与3×3卷积相同的参数大小和计算复杂度。在四个公共数据集上的实验结果表明，RollingUnet的性能优于当前的方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>这篇文章聚焦于医疗图像分割领域，旨在解决现有方法在提取和融合局部特征与长距离依赖关系方面的不足，具体研究背景如下： </p><ol><li><strong>CNN的局限性</strong>：基于卷积神经网络（CNN）的医疗图像分割方法虽有发展，如U-Net及其变体，但卷积操作的固有局部性使其难以学习清晰的全局和远程语义信息。 </li><li><strong>Transformer的问题</strong>：受自然语言处理中Transformer成功的启发，研究者将其引入视觉领域，但它需要大量训练数据，计算复杂度高，且在捕捉局部特征方面表现不佳，如Vision Transformer和Swin Transformer等。 </li><li><strong>CNN与Transformer结合的不足</strong>：一些方法尝试结合CNN和Transformer，但仍无法很好地平衡性能和计算成本。</li><li><strong>MLP的困境</strong>：多层感知器（MLP）理论上是通用逼近器，但计算量大、易过拟合，输入扁平化限制分辨率，虽有改进工作，但在医疗图像分割领域应用较少，且现有模型难以兼顾局部和全局特征。 因此，文章提出Rolling-Unet，结合CNN和MLP，以有效提取和融合局部特征与长距离依赖关系，实现更准确的医疗图像分割。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>CNN 方法</strong>：以 U-Net 为代表，后续有 UNet++、Att - UNet 等改进模型，通过引入注意力机制、图像金字塔等技术提升性能，但受卷积操作局部性限制，<strong>难以学习全局和远程语义信息</strong>。</li><li><strong>Transformer 方法</strong>：如 Vision Transformer、Swin Transformer 等被引入医学图像领域，能捕捉远程依赖，但<strong>计算复杂度高，对训练数据量要求大</strong>，且在捕捉局部特征方面表现不佳。</li><li><strong>CNN 与 Transformer 结合方法</strong>：如 MedT、UCTransNet 等，尝试融合二者优势，但仍难以平衡性能和计算成本。</li><li><strong>MLP 方法</strong>：MLP - Mixer 复兴了 MLP 在图像任务中的应用，后续工作引入局部先验，但大多仅具备局部感受野，在医学图像领域基于 MLP 的分割模型较少。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-04-47.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-04-47.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-04-47"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-46.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-46.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-10-46"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-10-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-10-52"></p><h3 id="核心模块"><a href="#核心模块" class="headerlink" title="核心模块"></a>核心模块</h3><ol><li><strong>R-MLP模块</strong>：负责学习整个图像在单个方向上的长距离依赖关系。对特征矩阵中每个通道层的特征图沿同一方向进行滚动操作（包括移位和裁剪两步），然后在每个空间位置索引处进行通道投影以编码长距离依赖。该操作初步降低了MLP对位置信息的敏感性，使用权重共享进一步减少了这种敏感性。</li><li><strong>OR-MLP模块</strong>：通过先沿宽度方向应用R - MLP，再沿高度方向应用R - MLP，形成正交滚动MLP模块，能够捕获多个方向的远程依赖关系。</li><li><strong>DOR-MLP模块</strong>：将两个互补的OR - MLP模块并行化，可捕获宽度、高度、正对角线和负对角线四个方向的长距离依赖关系。</li><li><strong>Lo2块</strong>：由DOR - MLP模块和深度可分离卷积（DSC）模块并行组成，能够同时提取图像的局部上下文信息和长距离依赖关系，且参数和计算量与3×3卷积处于同一水平。</li><li><strong>Feature Incentive Block (特征激励块)</strong>：用于编码器的第4层和瓶颈层，主要对特征和通道数量变化进行编码。在编码器第4层采用GELU激活函数和LayerNorm；在解码器第4层，由卷积块、RELU激活函数和BatchNorm组成。</li></ol><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>Rolling-Unet采用U-Net的U形框架，包括<strong>编码器-解码器结构</strong>、<strong>瓶颈层</strong>和<strong>跳跃连接</strong>。编码器 解码器有四个下采样和上采样阶段，分别通过最大池化和双线性插值实现。前三层包含标准的3×3卷积块，第四层和瓶颈层使用特征激励块和Lo2块。跳跃连接通过相加融合相同尺度的特征。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：</p><p>ISIC 2018：用于皮肤病诊断的图像数据集，包含多种皮肤病的图像和相应的标签</p><p>BUSI：乳腺超声图像</p><p>CHASEDB1：眼底血管分割</p><p>GlaS：结直肠腺体组织的分割任务</p></blockquote><ul><li><strong>对比方法</strong>：将Rolling - Unet与其他先进方法进行对比，包括基于CNN的U - Net、UNet++、Att - Unet、DconnNet；基于Transformer的UCTransNet、MedT；基于MLP的UNeXt。</li><li><strong>评估指标</strong>：采用交并比（IoU）、F1分数和95%豪斯多夫距离（HD95）作为评估指标。</li><li><strong>实验结果</strong>：Rolling - Unet在所有数据集上均优于其他方法。在BUSI和ISIC 2018数据集上优势显著，能更有效地提取远程依赖关系以提升分割性能。在ISIC 2018上改变图像大小的实验进一步验证了这一点，只有Rolling - Unet和UNeXt在图像尺寸增大时性能保持稳定。在GlaS和CHASEDB1数据集上，虽无方法取得显著优势，但Rolling - Unet表现最佳且标准差小。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-17-07"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><p>在ISIC 2018数据集（图像大小为512）上进行消融实验，以研究各因素对模型性能的影响。</p><ul><li><strong>Lo2模块分析</strong>：Lo2块由DOR - MLP和DSC模块并行组成。实验表明，无论DSC模块是否存在，R - MLP、OR - MLP和DOR - MLP的性能逐步提升，证明了所提模块捕获长距离依赖的有效性，且与DSC模块结合可进一步提升性能，说明融合远程依赖和局部上下文信息至关重要。</li><li><strong>R - MLP作用验证</strong>：将Rolling - Unet中的R - MLP替换为常规MLP，模型失去捕获长距离依赖的能力，性能显著下降。</li><li><strong>模块组合方式探究</strong>：对比DOR - MLP和DSC的不同组合方式（先执行DOR - MLP再执行DSC、先执行DSC再执行DOR - MLP、并行连接），结果表明并行连接效果最佳，说明提取局部特征和远程依赖的顺序不重要，同时提取后融合效果最好。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-45.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-22_20-17-45.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-22_20-17-45"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出的Rolling-Unet模型能在不增加计算成本的情况下捕获长距离依赖关系，且性能优于现有方法。具体结论如下： </p><ol><li>多方向的远程依赖并非严格意义上的全局感受野，是MLP的一种折中，但R - MLP模块灵活，组合使用可捕获大规模区域甚至全局特征，未来将深入探索。 </li><li>在四个不同数据集上，Rolling - Unet在初级和次级模型中表现最佳，尤其在BUSI和ISIC 2018数据集上优势显著，能有效提取目标轮廓，提升分割性能。 </li><li>消融实验表明，融合远程依赖和局部上下文信息至关重要，同时提取并融合二者效果最佳。未来，作者还将研究其在三维医学图像分割及其他图像任务中的潜力。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> MLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C-CAM Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image</title>
      <link href="/post/c-cam-causal-cam-for-weakly-supervised-semantic-segmentation-on-medical-image/"/>
      <url>/post/c-cam-causal-cam-for-weakly-supervised-semantic-segmentation-on-medical-image/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Recently, many excellent weakly supervised semantic segmentation (WSSS) works are proposed based on class activation mapping (CAM). However, there are few works that consider the characteristics ofmedical images. In this paper, we find that there are mainly two challenges of medical images in WSSS: i) the boundary of object foreground and background is not clear; ii) the co-occurrence phenomenon is very severe in training stage. We thus propose a Causal CAM (C-CAM) method to overcome the above challenges. Our method is motivated by two cause-effect chains including category-causality chain and anatomy-<br>causality chain. The category-causality chain represents the image content (cause) affects the category (effect). The anatomy-causality chain represents the anatomical structure (cause) affects the organ segmentation (effect). Extensive experiments were conducted on three public medical image data sets. Our C-CAM generates the best pseudo masks with the DSC of 77.26%, 80.34% and 78.15% on ProMRI, ACDC and CHAOS compared with other CAM-like methods. The pseudo masks ofC-CAM are further used to improve the segmentation performance for organ segmentation tasks. Our C-CAM achieves DSC of 83.83% on<br>ProMRI and DSC of87.54% on ACDC, which outperforms state-of-the-art WSSS methods. Our code is available at <a href="https://github.com/Tian-lab/C-CAM">https://github.com/Tian-lab/C-CAM</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>近年来，人们提出了许多基于<strong>类激活映射</strong>的弱监督语义分割(WSSS)方法。然而，很少有工作考虑到医学图像的特点。在本文中，我们发现医学图像在WSSS中主要存在两个挑战:1)<strong>目标前景和背景的边界不清晰</strong>;Ii)<strong>训练阶段共现现象非常严重</strong>。因此，我们提出了一种因果CAM (C-CAM)方法来克服上述挑战。我们的方法是由两个因果链驱动的，包括范畴因果链和解剖因果链。范畴-因果链表示图像内容(因)影响范畴(果)。解剖-因果链表示解剖结构(因)影响器官分割(果)。在三个公共医学图像数据集上进行了大量的实验。与其他类cam方法相比，我们的C-CAM在ProMRI、ACDC和CHAOS上的DSC分别为77.26%、80.34%和78.15%，生成的伪掩膜效果最好。进一步利用c - cam的伪掩膜来提高器官分割任务的分割性能。我们的C-CAM在ProMRI上的DSC为83.83%，在ACDC上的DSC为87.54%，优于最先进的WSSS方法。我们的代码可在<a href="https://github.com/Tian-lab/C-CAM%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Tian-lab/C-CAM上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦医学图像弱监督语义分割（WSSS），其研究背景主要源于以下方面：</p><ul><li><strong>语义分割现状</strong>：深度学习推动语义分割广泛研究，传统范式依赖大量像素级标注数据，但获取此类标注耗时且成本高，因此WSSS应运而生。其中，图像级标签获取最易却也最具挑战性，现有基于类激活映射（CAM）的WSSS方法多针对自然图像。</li><li><strong>医学图像挑战</strong>：与自然图像相比，医学图像在基于图像级标签的WSSS中存在两大挑战。一是前景与背景边界模糊，使CAM模型难以准确分类；二是训练阶段共现现象严重，不同器官常同时出现在同一图像中，仅依靠图像级标签，CAM模型难以激活正确的共现器官。</li><li><strong>现有方法不足</strong>：多数基于CAM的WSSS方法未考虑医学图像的上述特性，无法在医学图像上取得良好效果。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：基于类激活映射（CAM）的方法是主流，多数聚焦于自然图像，利用图像级标签等弱标注，常见流程为生成种子区域、细化种子生成伪掩码、用伪掩码训练分割模型。</li><li><strong>解剖先验</strong>：在图像分割中融入先验知识可提升性能，医学图像的解剖先验更具影响力，但现有方法需专业知识或复杂模型。</li><li><strong>计算机视觉中的因果关系</strong>：因果关系在计算机视觉任务中应用广泛，有助于提供更好的学习和可解释模型，但在医学图像弱监督语义分割中应用较少。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-28-49.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-28-49.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-16_13-28-49"></p><ul><li><p><strong>全局采样模块（Global Sampling Module）</strong>：将训练图像输入纯CAM（P - CAM）模型生成粗分割掩码，该模块最终输出包含类别和解剖信息的全局上下文图$M_{GC}$。</p></li><li><p>因果模块（Causality Module）</p><p>：基于两条因果链设计，分别为类别因果链和解剖因果链。</p><ul><li><strong>类别因果链（Category - Causality Chain）</strong>：将粗分割掩码和全局上下文图输入重塑层，通过两个卷积层投影到同一空间，计算类别感知注意力向量$A_{category}$，最终得到图像特定的类别因果图$M_{c}$。</li><li><strong>解剖因果链（Anatomy - Causality Chain）</strong>：设计一个0&#x2F;1指示器表示医学图像的解剖信息，计算解剖因果图$M_{S}$，将其与仅包含类别因果的显著图$CAM_{cc}$相乘得到最终显著图$CAM_{ac}$，进而生成伪分割掩码$S_{pseudo}$</li></ul></li></ul><h3 id="模型优势"><a href="#模型优势" class="headerlink" title="模型优势"></a>模型优势</h3><ul><li><strong>引入因果关系</strong>：C-CAM是首个将<strong>因果关系</strong>引入医学图像弱监督语义分割的方法，生成的伪分割掩码边界更清晰、形状更准确。</li><li><strong>解决关键问题</strong>：类别因果链缓解了边界模糊问题，解剖因果链解决了共现问题。</li><li><strong>实验效果好</strong>：在三个公共医学图像数据集（ProMRI、ACDC和CHAOS）上的实验表明，C-CAM生成的伪掩码在DSC指标上表现优异，训练的分割网络<strong>U-Net</strong>达到了最先进的性能。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：ProMRI、ACDC、CHAOS</p></blockquote><ol><li><strong>与其他CAM类方法比较</strong>：将C - CAM与Grad - CAM、Grad - CAM++等CAM类定位方法比较，使用相同的训练基线模型，测试所有背景阈值，展示不同方法伪掩码的最佳DSC结果。结果显示，C - CAM在三个医学图像数据集上生成的伪掩码性能最佳，在CHAOS的所有类别上表现良好。</li><li><strong>参数敏感性实验</strong>：评估背景阈值对生成伪分割掩码的影响，比较几种不同的CAM类方法。多数CAM类方法对背景阈值敏感，而C - CAM在背景阈值范围为0.3 - 0.9时，显著性图的DSC能稳定在较高值，表明其对背景阈值的鲁棒性。</li><li><strong>显著性图可视化</strong>：直观展示C - CAM的优势。结合类别因果关系，C - CAM能解决模糊边界问题，在ProMRI和ACDC数据集上，其显著性图的前景和背景边界清晰；借助解剖因果关系，能显著缓解共现问题，且错误激活的无关背景区域更少。</li><li><strong>与其他WSSS方法比较</strong>：用生成的伪分割掩码在全监督下训练U - Net模型，将测试数据的最终分割结果与其他先进的WSSS方法比较。在ProMRI数据集上，对于整个前列腺，C - CAM的DSC最高（83.83%），标准差最低（5.14%），在平均表面距离（ASD）和平均绝对距离（MAD）指标上也表现最佳；在ACDC数据集上，C - CAM在所有三个指标上均取得最佳性能。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-38-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-38-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-16_13-38-40"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ol><li><strong>消融实验</strong>：分析C - CAM各模块的作用。结果表明，与P - CAM相比，类别因果关系和解剖因果关系都提高了三个数据集上伪掩码的准确性。解剖因果关系在ProMRI上提升2.43%，在ACDC上提升1.79%，在CHAOS多标签分割任务中提升显著（18.3%）；结合类别因果关系后，ProMRI、ACDC和CHAOS数据集的DSC分别进一步提升4.22%、3.46%和5.41%；再训练一个亲和模型后，三个数据集上生成的伪分割掩码的DSC分别达到77.26%、80.34%和78.15%。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-37-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-16_13-37-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-16_13-37-24"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于医<strong>学图像弱监督语义分割</strong>（<strong>WSSS</strong>）的因果类激活映射（C-CAM）方法，得出以下结论：</p><ol><li><strong>方法有效性</strong>：C-CAM集成类别因果链和解剖因果链生成准确的伪分割掩码，能缓解前景与背景边界模糊问题，解决器官共现问题，生成的显著图边界清晰，符合解剖学知识。 </li><li><strong>性能优越性</strong>：C-CAM在ProMRI、ACDC和CHAOS数据集上优于六种先进的类CAM方法；用其伪掩码训练的U-Net分割网络在ProMRI和ACDC数据集上达到了先进水平。 </li><li><strong>局限性与展望</strong>：C-CAM难以分割形状复杂的物体，未来可结合少量强标签和大量弱标签，提供更准确的类别和解剖信息。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Weakly Supervised Semantic Segmentation </tag>
            
            <tag> C-CAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation</title>
      <link href="/post/pixel-wise-reclassification-with-prototypes-for-enhancing-weakly-supervised-semantic-segmentation/"/>
      <url>/post/pixel-wise-reclassification-with-prototypes-for-enhancing-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Refining the seed region to obtain finely annotated <strong>pseudo masks</strong> for training a segmentation model is a crucial step in the multi-stage weakly supervised semantic segmentation (WSSS) framework. One of the most popular refinement methods, IRN, extends seed regions towards the edges in the image. However, we observed that, due to the lack of guidance from semantic information, IRN’s refinement may lead the generation of partially erroneous refinement directions. To address this issue, we leverage prototypes<br>to recover the overlooked category semantic information in the refinement stage. We propose a prototype-based pseudo mask reclassification post-processing (PtReCl) to correct misclassified pixels in the pseudo masks, generating refined pseudo masks with more accurate coverage. Experimental evaluations demonstrate that our post-processing approach brings improvements in both pseudo mask quality and segmentation results on PASCAL VOC and MS COCO datasets, achieving state-of-the-art performance on VOC.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>在多阶段弱监督语义分割(WSSS)框架中，对种子区域进行细化以获得精细标注的<strong>伪掩膜</strong>用于训练分割模型是至关重要的一步。最流行的细化方法之一是IRN，它将种子区域向图像的边缘扩展。然而，我们观察到，由于缺乏语义信息的指导，IRN的细化可能导致部分错误的细化方向的产生。为了解决这个问题，我们利用原型来恢复细化阶段中被忽略的类别语义信息。我们提出了一种基于原型的伪掩码重分类后处理(PtReCl)来纠正伪掩膜中的错误分类像素，生成更精确覆盖的精细伪掩膜。实验评估表明，我们的后处理方法改善了<strong>PASCAL VOC</strong>和<strong>MS COCO</strong>数据集的伪掩码质量和分割结果，实现了最先进的VOC性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法在生成伪掩码时存在的问题，具体研究背景如下：</p><ul><li><strong>WSSS的目标与流程</strong>：WSSS旨在利用图像级标注数据集完成像素级分类任务，以降低数据标注成本。当前主流方法遵循三阶段流程，其中生成高质量伪掩码对最终分割模型的性能至关重要。 </li><li><strong>现有方法的局限性</strong>：最常用的细化方法IRN在细化种子区域时，因缺乏语义信息指导，可能导致部分错误的细化方向，产生大量错误的伪掩码。</li><li><strong>原型学习的潜力</strong>：近年来，研究发现原型学习可助力语义分割，它能从少量类样本中归纳特定类别的特征，实现特征的像素级分类，还能保留更多非学习参数以预测多样特征。 </li><li><strong>本文的研究动机</strong>：基于上述背景，作者提出基于<strong>原型的伪掩码重分类后处理方法</strong>（<strong>PtReCl</strong>），利用原型的类别区分性恢复伪掩码中误分类的像素，以提高伪掩码质量和分割性能。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>多阶段WSSS框架</strong>：主流方法分三步，先训练分类模型生成种子区域，再用细化方法生成伪掩码，最后用伪掩码训练全监督语义分割模型。</li><li><strong>CAM方法</strong>：解决CAM作为种子区域时前景覆盖不足问题，如采用擦除、对抗学习、利用ViT上下文建模等方法。</li><li><strong>细化方法</strong>：主要分为利用显著性检测和随机游走与语义亲和两类，部分方法还借助Transformer中的注意力矩阵。</li><li><strong>原型学习</strong>：在语义分割中，部分研究将原型用于对比学习或自监督学习，部分用原型替换分类器结构。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>多阶段弱监督语义分割框架中，细化种子区域以获得精细注释的伪掩码是训练分割模型的关键步骤。现有流行的细化方法IRN在细化过程中缺乏语义信息的引导，可能导致部分错误的细化方向。为解决这一问题，作者提出了<strong>PtReCl</strong>方法。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-21-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-21-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-21-22"></p><h3 id="模型流程"><a href="#模型流程" class="headerlink" title="模型流程"></a>模型流程</h3><ol><li><strong>种子区域获取</strong>：利用原始的类激活映射（Class Activation Maps, CAM）方法获取种子区域。训练分类模型后，丢弃分类器中的全局平均池化（Global Average Pooling, GAP）层，直接在原始特征图上进行预测，忽略负预测分数并归一化生成CAM。</li><li><strong>伪掩码生成</strong>：使用IRN方法对种子区域进行细化，生成伪掩码。</li><li><strong>伪掩码恢复网络</strong>：参考Deeplab的结构构建伪分割网络，以伪掩码作为像素级注释，通过空洞空间金字塔池化层（Atrous Spatial Pyramid Pooling, ASPP）提取图像特征并获得像素级预测结果。引入标签条件策略（Label Conditioning strategy），根据图像级类别注释保留相关通道，丢弃无关通道，以减轻无关通道对后续原型准确性的影响。</li><li><strong>前景 - 背景原型获取</strong>：依次遍历训练集图像，使用骨干网络提取特征。对于伪掩码中每个类别的前景区域，收集其对应特征到前景特征集；对于非该类别区域，收集其对应特征到背景特征集。使用余弦距离作为度量，采用K - means聚类方法为每个类别获取多个前景和背景原型。</li><li><strong>多原型像素级重新分类</strong>：使用伪掩码恢复网络的骨干提取图像特征，利用特定类别的前景和背景原型对像素特征的语义信息进行重新分类。计算每个位置与前景 - 背景原型的余弦相似度，对相似度进行降序排序，选择前m个距离参与像素分类计算，生成像素级重新分类图。</li><li><strong>重新细化</strong>：将重新分类图替换IRN中的CAM，再次使用IRN进行细化，增强其边缘信息，得到后处理的伪掩码。</li><li><strong>全监督语义分割</strong>：使用后处理的伪掩码训练全监督语义分割模型，如DeeplabV2和UperNet - Swin。</li></ol><h3 id="模型贡献"><a href="#模型贡献" class="headerlink" title="模型贡献"></a>模型贡献</h3><ul><li><strong>解决分类错误</strong>：提出PtReCl后处理方法，利用原型的类别区分性，通过前景 - 背景特征恢复伪掩码中误分类的像素。</li><li><strong>多原型分类</strong>：设计多原型像素级分类方法，利用伪分割网络重建伪掩码并通过聚类方法获取原型，缓解不同类别有效原型数量的差异，获得准确的重新分类图。</li><li><strong>实验验证</strong>：在PASCAL VOC和MS COCO数据集上进行了广泛实验，结果表明PtReCl方法能有效提高伪掩码的准确性，从而提升分割性能，在VOC数据集上取得了最先进的结果。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>PASCAL VOC 2012</strong>、<strong>MS COCO 2014</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-25-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-25-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-25-57"></p><ul><li><strong>伪掩码增强</strong>：与一些先进的WSSS方法相比，经PtReCl处理后的伪掩码在VOC上提升了8.4%，在COCO上提升了3.4%，在VOC上取得了最佳性能，在COCO上也有出色表现。</li><li><strong>分割性能提升</strong>：在使用DeepLab作为全监督分割方法的VOC实验中，PtReCl在两种常用预训练ResNet101骨干网络下均取得了最先进的结果。在基于Transformer的分割方法中，使用UperNet - Swin作为骨干网络时，PtReCl也达到了最先进的性能。在COCO上，尽管受噪声影响，PtReCl仍优于除AMN和LPCAM外的其他方法，与基线IRN相比，在验证集上提升了2.2%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-26-36.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-26-36.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-26-36"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>有效性验证</strong>：PtReCl在VOC和COCO上分别将伪掩码的mIoU提高了8.4%和3.4%。通过对比不使用原型和使用不同数量原型时的像素级分类结果，验证了多原型像素级分类方法的有效性，当M设为[10, 15, 20]时，重分类图的mIoU最高可达70%。</li><li><strong>原型数量影响</strong>：研究了调整每个类别的原型数量K（范围从2到30）对重分类效果的影响。重分类图的mIoU随原型数量增加先上升后稳定，最终将类中心数量设为20。在10到30的范围内，重分类图的mIoU波动仅在1%以内，表明在合理范围内改变原型数量对重分类效果影响不大。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-20.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-20.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-28-20"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-28-28"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-13_19-28-33"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者指出广泛使用的<strong>WSSS</strong>方法IRN在细化策略上存在局限，它在不考虑特定像素级语义信息的情况下将种子区域向图像边缘扩展，导致部分错误细化。基于现有的WSSS三阶段框架，作者引入了基于原型的重分类后处理方法，以纠正伪掩码中的像素错误分类，得到更精确的后处理伪掩码。 通过在<strong>VOC和COCO</strong>数据集上的大量实验，结果表明该后处理阶段有效提高了伪掩码的质量和分割模型的性能，在VOC数据集上取得了最先进的成果。 </p>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pixel-Wise Reclassification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>弱监督语义分割</title>
      <link href="/post/fcn/20250412-ruo-jian-du-yu-yi-fen-ge/"/>
      <url>/post/fcn/20250412-ruo-jian-du-yu-yi-fen-ge/</url>
      
        <content type="html"><![CDATA[<p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-29-41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-29-41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-29-41"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-30-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-30-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-30-22"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-31-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-31-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-31-13"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-32-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-32-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-32-28"></p><p>CAM：图像中的那些像素点对类别的响应比较高（粗糙的伪标签）</p><p>种子区域：粗糙的伪标签中<strong>置信度比较高的区域</strong>，比如某一个区域认为就是猫或狗</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-37-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-37-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-37-57"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-40-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-40-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-40-23"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-42-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_19-42-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_19-42-58"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-16-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-16-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_20-16-09"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-24-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-12_20-24-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-12_20-24-05"></p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/knowledge-transfer-with-simulated-inter-image-erasing-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/knowledge-transfer-with-simulated-inter-image-erasing-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>南京理工大学、地平线机器人</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Though <strong>adversarial erasing</strong> has prevailed in <strong>weakly supervised semantic segmentation</strong> to help activate integral object regions, existing approaches still suffer from the dilemma of under-activation and over-expansion due to the difficulty in determining when to stop erasing. In this paper, we propose a Knowledge Transfer with Simulated Inter-Image Erasing (KTSE) approach for weakly supervised semantic segmentation to alleviate the above problem. In contrast to existing erasing-based methods that remove the discriminative part for more object discovery, we propose a simulated inter-image erasing scenario to weaken the original activation by introducing extra object information. Then, object knowledge is transferred from the anchor image to the consequent less activated localization map to strengthen network localization ability. Considering the adopted bidirectional alignment will also weaken the anchor image activation if appropriate constraints are missing, we propose a self-supervised regularization module to maintain the reliable activation in discriminative regions and improve the inter-class object boundary recognition for complex images with multiple categories of objects. In addition, we resort to intra-image erasing and propose a multi-granularity alignment module to gently enlarge the object activation to boost the object knowledge transfer. Extensive experiments and ablation studies on PASCAL VOC 2012 and COCO datasets demonstrate the superiority of our proposed approach. Source codes and models are available at <a href="https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE">https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>尽管<strong>对抗擦除</strong>在弱监督语义分割中很流行，以帮助激活完整的目标区域，但由于难以确定何时停止擦除，现有的方法仍然存在<strong>激活不足和过度扩展</strong>的困境。在本文中，我们提出了一种基于模拟图像间擦除(KTSE)的弱监督语义分割方法来缓解上述问题。与现有的基于擦除的方法不同，我们提出了一种模拟图像间擦除场景，通过引入额外的目标信息来削弱原始激活。然后，将目标知识从锚点图像转移到随后激活程度较低的定位图中，以增强网络定位能力。考虑到如果缺少适当的约束条件，所采用的双向对齐也会削弱锚点图像的激活，我们提出了一种自监督正则化模块，以保持在判别区域的可靠激活，并改善具有多类别物体的复杂图像的类间物体边界识别。此外，我们采用图像内擦除的方法，并提出了一种多粒度对齐模块来温和地放大目标激活，以促进目标知识的转移。在PASCAL VOC 2012和COCO数据集上进行的大量实验和烧蚀研究证明了我们提出的方法的优越性。源代码和模型可在<a href="https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割在深度学习时代取得了巨大进展，广泛应用于<strong>自动驾驶和图像编辑等领域</strong>。但深度学习模型训练依赖大量标注图像，收集精确的像素级标注耗时耗力。因此，弱监督学习作为减轻标注负担的方向，受到众多研究者关注。 本文聚焦于图像级标签监督下的弱监督语义分割（WSSS）。当前WSSS通常遵循将图像标签转换为像素级粗标签、细化伪标签、用细化标签训练最终分割模型的三步流程。在分割标签生成方面，类激活图（CAM）技术是目标定位的主流范式，但朴素CAM只能突出对象最具判别性的区域，激活小且稀疏，导致对象挖掘不完整。 为解决这一问题，许多工作致力于扩展CAM激活以生成高质量伪标签，其中对抗擦除是主流方法之一。然而，现有的基于对抗擦除的方法难以确定何时停止擦除，过度擦除会导致过度扩展，擦除不足则会导致激活不足。因此，本文提出了一种基于模拟图像间擦除的知识转移（KTSE）方法，以缓解上述问题。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：以图像级标签作为弱监督进行语义分割是热门方向，通常采用<strong>类激活图（CAM）技术定位目标对象</strong>，将图像级标签转化为像素级标注。为扩大CAM激活区域，研究者采用了多种方法，如扩张卷积、对比学习、自监督学习、利用跨图像信息等。</li><li><strong>基于擦除的方法</strong>：通过掩盖训练图像中的区域，迫使网络寻找其他相关部分，以扩大CAM激活区域。其中，对抗擦除方法通过掩盖最具判别性的区域，展现出更有潜力的激活扩展效果。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><h3 id="网络结构图"><a href="#网络结构图" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-25-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-25-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-11_09-25-53"></p><p>本文提出了一种用于弱监督语义分割的模拟图像间擦除知识转移（Knowledge Transfer with Simulated Inter - Image Erasing，KTSE）方法，以缓解现有基于对抗擦除方法的过度扩展和激活不足问题。以下是该模型的详细介绍： </p><h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p>训练一个带有给定图像级弱标签的分类网络，由骨干特征提取器和池化分类头组成。模拟图像间擦除场景，通过拼接配对图像引入额外对象信息，将对象知识从锚图像转移到后续激活较低的定位图，以增强网络的对象定位能力。同时，提出了自监督正则化模块和多粒度对齐模块。</p><h3 id="具体模块"><a href="#具体模块" class="headerlink" title="具体模块"></a>具体模块</h3><ol><li><strong>类激活图（CAM）生成</strong>    <ul><li><strong>网络架构调整</strong>：参考ACoL移除最终全连接层，将骨干网络输出通道设置为C + 1（C为前景类别数，1为背景），直接从类感知CAM特征F生成对象定位图。    </li><li><strong>CAM计算</strong>：对于每个前景类c，将注意力图Fc输入ReLU层，然后归一化到0到1的范围，即$A ^{c} &#x3D;\frac {ReLU\left ( F^{c} \right )}{\max \left ( F^{c} \right )}$。   </li><li><strong>池化头与损失函数</strong>：采用门控金字塔池化（GPP）层作为最终池化头，使用多标签软边缘损失训练分类网络，损失函数为$\mathcal {L} _{cls}&#x3D;-\frac {1}{C} \sum _{ c&#x3D;1}^{C} y^{c} \log \sigma \left (q^{c}\right )+\left (1-y^{c}\right ) \log \left [1-\sigma \left (q^{c}\right )\right ]$，其中$\sigma (·)$是sigmoid函数，$y^{c}$是第c类的图像级标签。</li></ul></li><li><strong>模拟图像间擦除（SIE）</strong>    <ul><li><strong>场景设计</strong>：与现有擦除方法不同，通过拼接锚图像和配对图像创建更大的合成图像，将锚图像视为被擦除的图像。引入额外对象信息使锚图像中突出的对象区域减少，然后将锚图像的对象知识转移到激活较低的合成图像锚部分，以增强网络的对象定位能力。   </li><li><strong>知识转移损失</strong>：损失函数为$\mathcal {L} _{kt} &#x3D; ReLU \left ( \hat {F _{a} } - \hat {F _{s} } \right )$，其中$\hat {F _{a} } &#x3D;CFE\left ( F _{a}, y \right )$，$\hat {F _{s} } &#x3D;CFE\left ( F _{s}, y \right )$，$F _{a}$和$F _{s}$分别表示锚分支和模拟分支的CAM特征，CFE表示类特征提取。</li></ul></li><li><strong>自监督正则化（SSR）</strong>    <ul><li><strong>问题提出</strong>：由于知识转移是双向的，学习模拟分支的稀疏激活会削弱锚分支的对象挖掘。为保持锚分支在判别区域的可靠激活，提出自监督正则化模块。   </li><li><strong>伪标签生成</strong>：使用两个阈值$\beta_{h} &#x3D; 0.3$和$\beta_{l} &#x3D; 0.15$定位置信前景和背景，生成伪标签$\hat {Y} _{i,j}$。   </li><li><strong>损失函数</strong>：使用交叉熵损失$\mathcal {L} _{ce}$直接监督CAM特征的学习，同时设计类间损失$\mathcal {L} _{inter}$鼓励复杂图像中多个前景类的激活一致性，以提高类间对象边界的识别能力。</li></ul></li><li><strong>多粒度对齐（MGA）</strong>   <ul><li><strong>问题提出</strong>：自监督正则化模块虽能促进知识转移，但网络对象定位能力的提升受锚CAM质量限制，因此采用传统的图像内擦除并提出多粒度对齐模块。  </li><li><strong>全局对齐</strong>：将锚特征$F_{a}$和掩码特征$F_{m}$输入类特征提取模块，采用全局平均池化（GAP）操作获得每个分支的最终类置信度，设计图像级全局对齐损失$\mathcal {L} _{global}$。   </li><li><strong>局部对齐</strong>：利用像素级局部激活对齐将擦除图像中新发现的对象信息转移到锚分支，损失函数为$\mathcal {L} _{local}&#x3D; ReLU\left ( \hat {F _{m}} - \hat {F _{a}} \right )$。</li></ul></li></ol><h3 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h3><p>整体训练损失为$\mathcal {L}&#x3D;\mathcal {L} _{cls} + \mathcal {L} _{kt} + \mathcal {L} _{global} + \mathcal {L} _{local} + \mathcal {L} _{ce} + \lambda _{inter}\mathcal {L} _{inter}$，其中$\lambda _{inter} &#x3D; 0.005$是控制类间损失权重的超参数。 </p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：<strong>PASCAL VOC 2012 (20+1)、COCO 2014 (80+1)</strong></p></blockquote><ul><li><strong>伪掩膜的准确性</strong>：KTSE方法的分割种子mIoU达到67.0%，比IRN的基线提高了18.7%，超过了当前最优方法FPR 3.2%；经过IRN进一步细化后，生成的伪掩码mIoU达到73.8%，超过了先前的SOTA方法AEFT和ACR超过1.5%。</li><li><strong>PASCAL VOC 2012分割图的准确性</strong>：使用VGG骨干网络时，KTSE方法在验证集和测试集上的性能分别为67.3%和67.0%，优于仅使用图像级标签的其他现有技术方法，并且与许多依赖显著性图的方法具有竞争力；使用ResNet骨干网络时，验证集和测试集的结果分别提高到73.0%和72.9%，优于最近的SOTA方法，例如在测试集上比OCR和ACR高约1%。</li><li><strong>COCO分割图的准确性</strong>：使用VGG骨干网络时，KTSE方法的mIoU达到37.2%，远优于仅使用图像级标签的先前方法，例如比CONTA高13.5% mIoU，并且与具有额外显著性指导的先前SOTA方法具有竞争力；使用ResNet骨干网络时，KTSE方法达到了45.9% mIoU的最佳结果，分别比ACR和BECO高0.6%和0.8% mIoU。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-33-16.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-11_09-33-16.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-11_09-33-16"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>逐元素组件分析</strong>：通过实验验证了KTSE方法中各个组件对提高伪掩码质量的贡献。模拟图像间擦除（SIE）模块可将分割种子的准确率从基线的54.2%提高到57.1%；自监督正则化（SSR）模块可使mIoU达到57.4%；SIE和SSR的组合可将性能显著提升至63.8% mIoU；多粒度对齐（MGA）模块最终使伪掩码的mIoU达到67.0%。</li><li><strong>MGA与先前基于擦除的方法比较</strong>：MGA模块采用来自锚分支的软类置信度知识来指导掩码图像的温和激活扩展。实验表明，CAM特征的全局对齐可将基线从54.2%提高到58.7%，优于ACoL中的刚性分类指导和AEFT中采用的GPP特征对齐；加上像素级局部对齐后，MGA模块最终将性能提高到60.2%，证明了其温和对齐策略相对于先前基于擦除方法的优势。</li><li><strong>SIE现象分析</strong>：基于经典对抗擦除方法的观察，引入额外的判别性目标信息会使原始高激活区域变得不那么具有判别性并降低激活度，通过从原始CAM学习可以增强这种减弱的激活。当网络学会增加对拼接图像中不那么具有判别性区域的关注时，也会学会激活原始图像中不那么具有判别性的目标区域以定位更多目标。</li><li><strong>SIE与数据增强比较</strong>：虽然像CutMix这样的数据增强方法也会改变锚图像并导致激活扰动，但它们不能保证像SIE那样使拼接图像的锚部分激活减弱（可能导致过度扩展）。SIE的新颖之处在于构建了模拟图像间擦除场景，通过从锚分支的目标知识中学习来提高后续激活减弱的注意力图，从而增强网络的定位能力。实验表明，KTSE方法在VOC和COCO数据集上的性能显著优于基于CutMix的数据增强方法CDA。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于弱监督语义分割的<strong>知识转移</strong>与<strong>模拟图像间擦除</strong>（<strong>KTSE</strong>）方法，并得出以下结论：</p><ol><li>与现有对抗擦除方法不同，KTSE模拟图像间擦除场景，添加额外对象信息，增强网络目标定位能力，缓解过扩展问题。 </li><li>提出自监督正则化模块，维持判别区域可靠激活，提升复杂图像类间目标边界识别能力。 </li><li>提出多粒度对齐模块，通过图像级全局对齐和像素级局部对齐扩大目标激活，促进知识转移。</li><li>在PASCAL VOC 2012和COCO数据集上的大量实验和消融研究表明，KTSE方法优于现有方法，能有效缓解过扩展和激活不足问题。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模拟图像间擦除 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SFC Shared Feature Calibration in Weakly Supervised Semantic Segmentation</title>
      <link href="/post/sfc-shared-feature-calibration-in-weakly-supervised-semantic-segmentation/"/>
      <url>/post/sfc-shared-feature-calibration-in-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Image-level weakly supervised semantic segmentation has received increasing attention due to its low annotation cost.<br>Existing methods mainly rely on <strong>Class Activation Mapping (CAM)</strong> to obtain pseudo-labels for training semantic segmentation models. In this work, we are the first to demonstrate that long-tailed distribution in training data can cause the CAM calculated through classifier weights over-activated for head classes and under-activated for tail classes due to the shared features among head- and tail- classes. This degrades pseudo-label quality and further influences final semantic segmentation performance. To address this issue, we propose a Shared Feature Calibration (SFC) method for CAM generation. Specifically, we leverage the class prototypes that carry positive shared features and propose a Multi-Scaled Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the CAMs generated through classifier weights and class prototypes during training. The MSDW loss counterbalances over-activation and under-activation by calibrating the shared features in head-&#x2F;tail-class classifier weights. Experimental results show that our SFC significantly improves CAM boundaries and achieves new state-of-the-art performances. The project is available at <a href="https://github.com/Barrett-python/SFC">https://github.com/Barrett-python/SFC</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>图像级弱监督语义分割因其标注成本低而受到越来越多的关注。现有方法主要依靠类激活映射(Class Activation Mapping, CAM)获取伪标签，用于训练语义分割模型。在这项工作中，我们首次证明了训练数据中的长尾分布会导致通过分类器权重计算的CAM由于头类和尾类之间的共享特征而对头类过度激活而对尾类激活不足。这降低了伪标签的质量，并进一步影响最终的语义分割性能。为了解决这一问题，我们提出了一种用于CAM生成的共享特征校准(SFC)方法。具体来说，我们利用带有正共享特征的类原型，并提出了多尺度分布加权(MSDW)一致性损失，以缩小训练期间通过分类器权重和类原型生成的cam之间的差距。MSDW损失通过校准头&#x2F;尾类分类器权重中的共享特征来平衡过度激活和欠激活。实验结果表明，我们的SFC显著改善了CAM边界，实现了新的最先进的性能。该项目可在<a href="https://github.com/Barrett-python/SFC%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Barrett-python/SFC上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p><strong>图像级弱监督语义分割（WSSS）<strong>因标注成本低而备受关注，现有方法多依赖</strong>类激活映射</strong>（CAM）生成伪标签来训练语义分割模型。然而，研究发现WSSS的训练数据呈长尾分布，这使得头类和尾类之间的共享特征成分在头类分类器权重中倾向于为正，在尾类分类器权重中倾向于为负。 具体而言，头类权重接收的正梯度多于负梯度，尾类权重则相反。这导致包含共享特征的像素被头类分类器权重激活，而包含尾类特征的像素未被尾类权重激活，使得通过分类器权重计算的CAM对头部类过度激活，对尾部类激活不足，进而降低了伪标签的质量，影响了最终的WSSS性能。 目前，尚未有工作针对长尾分布训练数据导致的过激活和欠激活问题进行研究。因此，本文旨在分析该问题产生的原因，并提出共享特征校准（SFC）方法来解决这一问题，以提高CAM质量和WSSS性能。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：伪标签生成基于注意力映射，关键在于生成高质量的<strong>类激活映射</strong>（CAM）。已有方法采用启发式策略、自监督学习、对比学习等生成CAM，还使用<strong>CRF、IRN等方法对初始映射进行细化</strong>。此外，视觉-语言预训练成为解决下游视觉 - 语言任务（包括WSSS）的流行方法。</li><li><strong>分类中的共享特征</strong>：分类是语义分割的上游任务，现有方法多提取判别性部分特征进行分类，避免共享特征影响分类性能。而WSSS不能仅依赖判别性特征构建完整CAM，部分方法冻结预训练编码器的若干层以避免遗忘非判别性特征。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_13-57-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_13-57-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_13-57-44"></p><p>本文提出了一种名为<strong>共享特征校准</strong>（<strong>Shared Feature Calibration，SFC</strong>）的方法，用于解决弱监督语义分割（Weakly Supervised Semantic Segmentation，WSSS）中因训练数据<strong>长尾分布</strong>导致的类激活映射（Class Activation Mapping，CAM）过激活和欠激活问题。以下是该模型的详细介绍：</p><ol><li><strong>模型背景</strong>：图像级弱监督语义分割因标注成本低而受到广泛关注。现有方法主要依赖CAM获取伪标签来训练语义分割模型，但训练数据的长尾分布会使头类别的CAM过激活，尾类别的CAM欠激活，从而降低伪标签质量，影响最终分割性能。 </li><li><strong>模型结构</strong>    - <strong>图像库重采样（Image Bank Re-sampling，IBR）</strong>：维护一个图像库，存储每个前景类别的最新图像。在训练时，从图像库中均匀采样图像并与原始训练批次拼接，以增加尾类别样本的采样频率，有效校准尾类别分类器权重中的共享特征。   <ul><li><strong>多尺度分布加权一致性损失（Multi-Scaled Distribution-Weighted，MSDW）</strong>：提出两个分布加权一致性损失$L_{P_{DW}}$和$L_{W_{DW}}$，分别用于缩小原型CAM和分类器权重CAM之间的差距，以及缩小原始图像和下采样图像的分类器权重CAM之间的差距。通过计算缩放分布系数$DC_c$对一致性损失进行重新加权，使总需求较高的类别分配更高的一致性损失。</li></ul></li><li><strong>模型推理</strong>：最终的CAM通过将分类器权重CAM和原型CAM相加得到，即$(M_{final})<em>{\tilde{c}} &#x3D; (M</em>{W}(F, W, I))<em>{\tilde{c}} + (M</em>{P}(F, P))_{\tilde{c}}$，共同解决过激活和欠激活问题。 </li><li><strong>实验结果</strong>：在PASCAL VOC 2012和MS COCO 2014两个基准数据集上进行实验，结果表明SFC方法显著提高了CAM的边界质量，在弱监督语义分割任务中取得了新的最先进性能。 综上所述，SFC方法通过图像库重采样和多尺度分布加权一致性损失，有效解决了长尾分布下共享特征导致的CAM过激活和欠激活问题，提高了弱监督语义分割的性能。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：PASCAL VOC 2021、MS COCO 2014</p></blockquote><ul><li><strong>伪标签质量比较</strong>：评估伪标签生成过程中中间和最终结果的质量。比较分类模型生成的初始CAM、经过CRF和IRN后处理的CAM。实验结果表明，SFC生成的CAM明显优于以往方法，在PASCAL VOC数据集上，SFC的CAM比现有方法高2.6%；经过CRF处理后mIoU达到69.4%，再经过IRN处理后mIoU提高到73.7%，比AMN方法高1.5%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-01-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-01-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-01-54"></p><ul><li><strong>弱监督语义分割性能比较</strong>：将经过CRF和IRN后处理的伪掩码作为真实标签，以全监督方式训练语义分割模型。在PASCAL VOC 2012的验证集和测试集上，使用ImageNet预训练骨干网络，SFC方法的mIoU分别达到71.2%和72.5%，优于仅使用图像级标签或同时使用图像级标签和显著性图的其他弱监督语义分割方法。在MS COCO 2014验证集上，使用ResNet101骨干网络，SFC方法的mIoU达到46.8%，比AMN方法高2.1%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-02-00.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-02-00.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-02-00"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-03-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-03-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-03-21"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>SFC组件有效性验证</strong>：验证图像库重采样（IBR）和多尺度分布加权（MSDW）一致性损失（包括LP DW和LW DW）的有效性。结果表明，IBR可提高分类器权重CAM的mIoU，增加尾部类别的采样频率能提升LMSDW的有效性；LW DW可增强LP DW带来的性能提升，但单独使用LW DW无法校准下采样特征空间中的共享特征，性能会显著下降。</li><li><strong>DC系数有效性研究</strong>：研究式（6）中DC系数的有效性。结果显示，DC系数能有效调整每个类别的一致性损失权重，带来显著的性能提升。</li><li><strong>CAM组合性能比较</strong>：比较推理时单独使用MW、MP和组合使用Mfinal的性能。结果表明，组合使用Mfinal的性能最高，说明在SFC中用MP补充MW效果更好。</li><li><strong>不同类别集性能增益分析</strong>：分析有无DC系数时不同类别集的平均性能增益。结果显示，使用DC系数时，头部和尾部类别能获得更多的mIoU增益。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-04-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-10_14-04-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-10_14-04-01"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者得出以下结论：</p><ol><li>首次指出在长尾场景下，<strong>头类和尾类的共享特征</strong>会使头类的分类器权重生成的类激活映射（CAM）扩大，尾类的CAM缩小，导致伪标签质量下降，影响弱监督语义分割（WSSS）最终性能。</li><li>提出共享特征校准（SFC）方法，通过图像库重采样（IBR）和多尺度分布加权（MSDW）一致性损失，平衡不同分类器权重中的共享特征比例，避免共享特征导致的<strong>过激活和欠激活</strong>问题。</li><li>实验表明，SFC显著改善了CAM边界，在<strong>Pascal VOC 2012</strong>和<strong>MS COCO 2014</strong>数据集上仅使用图像级标签就取得了新的最优WSSS性能，为提高图像级弱监督语义分割中CAM的准确性提供了新视角，未来将探索其他可能的解决方案。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SFC </tag>
            
            <tag> semantic segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WeakCLIP Adapting CLIP for Weakly-Supervised Semantic Segmentation</title>
      <link href="/post/weakclip-adapting-clip-for-weakly-supervised-semantic/"/>
      <url>/post/weakclip-adapting-clip-for-weakly-supervised-semantic/</url>
      
        <content type="html"><![CDATA[<p>华中科技大学、西北工业大学</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Contrastive language and image pre-training (CLIP) achieves great success in various computer vision tasks and also presents<br>an opportune avenue for enhancing weakly-supervised image understanding with its large-scale pre-trained knowledge. As<br>an effective way to reduce the reliance on pixel-level human-annotated labels, weakly-supervised semantic segmentation<br>(WSSS) aims to refine the class activation map (CAM) and produce high-quality pseudo masks. Weakly-supervised semantic<br>segmentation (WSSS)aims to refine the class activationmap(CAM)as pseudo masks, but heavily relies on inductive biases like<br>hand-crafted priors and digital image processing methods. For the vision-language pre-trained model, i.e. CLIP, we propose a<br>novel text-to-pixel matching paradigm forWSSS.However, directly applying CLIP toWSSS is challenging due to three critical<br>problems: (1) the task gap between contrastive pre-training and WSSS CAM refinement, (2) lacking text-to-pixel modeling to<br>fully utilize the pre-trained knowledge, and (3) the insufficient details owning to the 1&#x2F;16 down-sampling resolution ofViT. Thus,<br>we proposeWeakCLIP to address the problems and leverage the pre-trained knowledge from CLIP toWSSS. Specifically, we<br>first address the task gap by proposing a pyramid adapter and learnable prompts to extract WSSS-specific representation. We<br>then design a co-attention matching module to model text-to-pixel relationships. Finally, the pyramid adapter and text-guided<br>decoder are introduced to gather multi-level information and integrate it with text guidance hierarchically.WeakCLIP provides<br>an effective and parameter-efficient way to transfer CLIP knowledge to refine CAM. Extensive experiments demonstrate that<br>WeakCLIP achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 74.0% mIoU on the val set of<br>PASCAL VOC 2012 and 46.1% mIoU on the val set of COCO 2014. The source code and model checkpoints are released<br>at <a href="https://github.com/hustvl/WeakCLIP">https://github.com/hustvl/WeakCLIP</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>对比语言和图像预训练(CLIP)在各种计算机视觉任务中取得了巨大的成功，并且利用其大规模的预训练知识为增强弱监督图像理解提供了一个很好的途径。弱监督语义分割(WSSS)是一种减少对像素级人工标注标签依赖的有效方法，其目的是细化类激活图(CAM)并生成高质量的伪掩膜。弱监督语义分割(WSSS)旨在将类激活图(CAM)细化为伪掩膜，但严重依赖于手工制作先验和数字图像处理方法等归纳偏差。对于视觉语言预训练模型，即CLIP，我们提出了一种新的文本到像素的wsss匹配范式。然而，直接应用CLIP toWSSS是具有挑战性的，因为存在三个关键问题:(1)对比预训练与WSSSCAM细化之间的任务差距;(2)缺乏文本到像素的建模以充分利用预训练的知识;(3)由于vit的下采样分辨率为1 16，细节不足。因此，我们提出了weakclip来解决问题，并利用CLIP toWSSS的预训练知识。具体来说，我们首先通过提出金字塔适配器和可学习的提示词符来提取特定于wss的表示来解决任务差距。然后，我们设计了一个共同关注匹配模块来模拟文本到像素的关系。最后，引入金字塔适配器和文本引导解码器，实现多级信息采集，并与文本引导分层集成。WeakCLIP提供了一种有效的、参数高效的方法来传递CLIP知识以改进CAM。大量的实验表明，WeakCLIP在标准基准测试上达到了最先进的WSSS性能，即在PASCAL VOC 2012的val集上达到了74.0%的mIoU，在COCO 2014的val集上达到了46.1%的mIoU。源代码和模型检查点在<a href="https://github.com/hustvl/WeakCLIP%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/hustvl/WeakCLIP上发布。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）任务，旨在解决当前方法在处理类激活图（CAM）种子时面临的问题，具体研究背景如下： </p><ul><li><strong>WSSS的重要性与挑战</strong>：语义分割中像素级标注耗时费力，限制了实际应用。WSSS利用弱监督信息生成伪像素级分割，可减轻标注负担，但仅使用图像级标签的WSSS是该领域最具挑战性的方向。</li><li><strong>现有CAM细化方法的局限性</strong>：现有方法多依赖手工先验和改进的数字图像处理算法来细化CAM，这些方法存在归纳偏差，限制了性能和鲁棒性。 </li><li><strong>CLIP的潜力与应用挑战</strong>：CLIP在计算机视觉任务中取得了巨大成功，为WSSS带来了新的机遇。然而，直接将CLIP应用于WSSS存在任务差距、缺乏文本到像素建模以及细节不足等问题。 基于以上背景，作者提出了WeakCLIP方法，旨在利用CLIP的预训练知识，通过文本到像素匹配范式解决WSSS中的关键问题，提高伪掩码的质量，从而推动WSSS的发展。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：为减轻像素级标注负担，出现多种基于不同弱监督信息（如<strong>边界框、涂鸦、点、图像级标签</strong>）的算法。其中，基于<strong>图像级标签</strong>的WSSS最具挑战性，常使用类激活图（CAM）定位目标，但原始CAM噪声大、易出错，已有多种方法对其进行优化。</li><li><strong>大规模预训练模型</strong>：大规模预训练模型在各领域广泛应用，如CLIP通过对比学习在大量图像-文本对上预训练，展现出强大的知识迁移能力。已有研究尝试将CLIP应用于WSSS，如CLIMS引入辅助损失，CLIP - ES利用文本提示和GradCAM提升CAM质量。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-44-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-44-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-09_08-44-06"></p><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>本文提出了一种名为<strong>WeakCLIP的弱监督语义分割（WSSS）方法</strong>，旨在利用预训练的CLIP模型知识来改进WSSS网络的类激活图（CAM）细化过程。以下是WeakCLIP模型的详细介绍： </p><ol><li><p><strong>文本到像素匹配范式</strong>：与以往基于CLIP的WSSS方法不同，WeakCLIP提出了<strong>文本到像素匹配</strong>范式，以在像素级别查询相似度。具体而言，输入图像通过CLIP预训练的ViT - B网络提取多层特征图，经过投影层后，定义文本到像素匹配操作，得到文本到像素匹配的嵌入。 </p></li><li><p><strong>WeakCLIP框架</strong> </p><ul><li><strong>可学习提示（Learnable Prompt）</strong>：受CoOp和CLIP - Adapter启发，提出可学习嵌入作为自适应提示。将类文本标记并嵌入为类文本嵌入，与随机初始化的可学习嵌入拼接，作为文本编码器的输入，最终投影得到文本嵌入。  </li><li><strong>金字塔适配器（Pyramid Adapter）</strong>：为解决CLIP视觉编码器专注于整体图像内容以及低分辨率问题，提出金字塔适配器。它独立于CLIP图像编码器，对不同分辨率的特征图进行处理，通过上采样和下采样操作，生成不同分辨率的特征，有效融合低级细节和高级表示。   </li><li><strong>协同注意力匹配模块（Co - attention Matching）</strong>：为充分利用CLIP预训练知识，提出协同注意力匹配模块，用于建模双向文本到像素匹配。该模块使用两个交叉注意力模块分别建模文本到像素和像素到文本的关系，并通过残差连接更新文本和图像嵌入，最后进行文本到图像匹配得到协同注意力匹配的嵌入。  </li><li><strong>文本引导解码器（Text - Guided Decoder）</strong>：为解决CLIP ViT - B的分辨率限制问题，引入文本引导解码器。将协同注意力匹配的嵌入插值到与适配器输出特征对应的大小，与适配器输出特征拼接后进行解码，得到分割预测。    </li><li><strong>WSSS损失（WSSS Losses）</strong>：采用DSRG中使用的WSSS损失，包括平衡种子损失和边界损失。平衡种子损失计算分割预测与CAM种子之间的加权交叉熵损失；边界损失先使用条件随机场（CRF）处理分割预测以细化对象边界，然后计算CRF细化结果与分割预测之间的Kullback - Leibler散度损失。</li></ul></li><li><p><strong>伪掩码生成和再训练</strong>：使用训练好的WeakCLIP网络生成高质量的伪掩码。当推理结果中的类别不在图像级标签中时，将其标记为未知标签。最后，使用生成的伪掩码进行全监督分割，采用DeepLabv1网络架构，并尝试使用更先进的基于ViT的分割方法进行再训练。 实验结果表明，WeakCLIP在PASCAL VOC 2012和COCO 2014数据集上取得了优于以往WSSS方法的结果，证明了该方法的有效性和高效性。</p></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-49-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-49-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-09_08-49-54"></p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>PASCAL VOC 2012、COCO 2014</strong></p><ul><li><strong>ASCAL VOC 2012</strong>：在CAM监督方面，WeakCLIP与MCTformer相同，但低于ViT - PCM；在伪掩码质量上，比基线MCTformer提高了8.1%，比AMN提高了5.0%。使用精炼后的伪掩码训练DeepLabV1网络，WeakCLIP在验证集和测试集上的mIoU分别达到74.0%和73.8%，优于其他仅使用图像级监督的方法，以及部分使用额外显著图监督或边界框监督的方法。使用基于ViT的再训练基准（Segmenter和SegFormer）可进一步提升分割结果，混合ViT再训练的WeakCLIP表现最佳。</li><li><strong>COCO 2014</strong>：WeakCLIP在验证集上的mIoU达到46.1%，比基线MCTformer提高了4.1%，优于其他仅使用图像级监督的方法。使用SegFormer和MiT - B2骨干进行再训练，WeakCLIP在COCO 2014验证集上取得最佳性能。</li></ul><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ol><li><ul><li><strong>组件改进</strong>：协同注意力匹配模块将验证集mIoU提高到67.4%；可学习提示将其提高到68.9%；金字塔适配器将性能提升到70.3%；文本引导解码器将验证集mIoU进一步提升到72.6%。</li><li><strong>可学习嵌入数量</strong>：可学习嵌入数量为8时性能最佳。</li><li><strong>可学习温度初始值</strong>：协同注意力匹配中可学习温度初始值为1e - 1时性能最佳。</li></ul></li></ol><h2 id="其他实验（Other-Experiments）-1st-place-medal"><a href="#其他实验（Other-Experiments）-1st-place-medal" class="headerlink" title="其他实验（Other Experiments）:1st_place_medal:"></a>其他实验（Other Experiments）:1st_place_medal:</h2><ol><li><strong>逐类语义分割结果</strong>：在PASCAL VOC 2012的验证集和测试集以及COCO 2014的验证集上，将WeakCLIP与基线MCTformer进行逐类分割结果比较，WeakCLIP在大多数类别中表现更优。</li><li><strong>可视化分析</strong><ul><li>比较MCTformer和WeakCLIP生成的伪掩码，WeakCLIP生成的语义信息更准确、精确，能识别出MCTformer遗漏或识别不准确的对象位置。</li><li>在PASCAL VOC 2012验证集上再训练后的分割结果可视化显示，WeakCLIP对室内和室外场景都能实现准确分割。</li></ul></li><li><strong>参数效率分析</strong>：与MCTformer相比，WeakCLIP仅训练12.4%的参数，训练帧率（FPS）快4.3倍，节省68.4%的GPU内存。</li><li><strong>不同CLIP骨干实验</strong>：使用不同CLIP骨干进行实验，结果表明WeakCLIP - ResNet101性能优于WeakCLIP - ResNet50，WeakCLIP - ViT - B表现最佳。</li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了名为<strong>WeakCLIP</strong>的新方案，旨在利用预训练CLIP模型的知识来增强弱监督语义分割（WSSS）网络的**类激活图（CAM）**细化过程。该框架采用了新的文本到像素匹配范式，有效解决了将CLIP集成到WSSS中存在的三个关键问题。在广泛使用的PASCAL VOC 2012和COCO 2014数据集上的实验结果表明，与以往的WSSS方法相比，WeakCLIP取得了显著改进。引入利用大规模视觉语言预训练的WeakCLIP范式，有望推动WSSS问题的解决。未来，作者计划探索更先进的大规模CLIP，以提升WSSS的像素级理解能力。 </p>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Weakly Supervised Semantic Segmentation </tag>
            
            <tag> CLIP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/all-pairs-consistency-learning-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/all-pairs-consistency-learning-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>澳大利亚国立大学、OpenNLPLab、上海人工智能实验室、厦门大学、OPPO研究院</p><p>::: tip</p><p>启发</p><p>:::</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>In this work, we propose a new <strong>transformer-based regularization</strong> to better localize objects for <strong>Weakly supervised semantic segmentation (WSSS)</strong>. In image-level WSSS, Class Activation Map (CAM) is adopted to generate object localization as pseudo segmentation labels. To address the partial activation issue of the CAMs, consistency regularization is employed to maintain activation intensity invariance across various image augmentations. However, such methods ignore pair-wise relations among regions within each CAM, which capture context and should also be invariant across image views. To this end, we propose a new<br>all-pairs consistency regularization (ACR). Given a pair of augmented views, our approach regularizes the activation intensities between a pair of augmented views, while also ensuring that the affinity across regions within each view remains consistent. We adopt vision transformers as the self-attention mechanism naturally embeds pair-wise affinity. This enables us to simply regularize the distance between the attention matrices of augmented image pairs. Additionally, we introduce a novel class-wise localization<br>method that leverages the gradients ofthe class token. Our method can be seamlessly integrated into existing WSSS methods using transformers without modifying the architectures. We evaluate our method on PASCAL VOC and MS COCO datasets. Our method produces noticeably better class localization maps (67.3% mIoU on PASCAL VOC train), resulting in superior WSSS performances.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>在这项工作中，我们提出了一种新的<strong>基于变换的正则化方法</strong>来更好地定位弱监督语义分割(WSSS)的对象。在图像级WSSS中，采用类激活图(Class Activation Map, CAM)生成目标定位作为伪分割标签。为了解决cam的部分激活问题，采用一致性正则化方法在不同的图像增强中保持激活强度的不变性。然而，这些方法忽略了每个CAM内区域之间的成对关系，这种关系捕获上下文，并且应该在图像视图之间保持不变。为此，我们提出了一种新的全对一致性正则化(ACR)。给定一对增强视图，我们的方法规范了一对增强视图之间的激活强度，同时还确保每个视图中跨区域的亲和性保持一致。我们采用视觉Transformer作为自注意力机制机制，自然嵌入成对的亲和力。这使我们能够简单地正则化增广图像对的注意矩阵之间的距离。此外，我们引入了一种新的类智能定位方法，利用类标记的梯度。我们的方法可以使用Transformer无缝集成到现有的WSSS方法中，而无需修改体系结构。我们在PASCAL VOC和MS COCO数据集上评估了我们的方法。我们的方法产生了明显更好的类定位图(在PASCAL VOC训练上有67.3%的mIoU)，从而获得了卓越的WSSS性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法的局限性，具体研究背景如下：</p><ul><li><strong>WSSS的意义与挑战</strong>：WSSS旨在利用图像级标签、点、涂鸦和边界框等弱标签，缓解像素级标注的繁琐和高成本问题。其中，图像级WSSS仅使用类别标签监督像素级预测，尤为具有挑战性。 </li><li><strong>现有方法的不足</strong>：现有图像级WSSS方法通常依赖基于卷积神经网络的类激活图（CAM）生成伪分割标签，但CAM存在激活不完整和不准确的问题，这是由于图像标签和像素级分割监督之间的差距导致的。</li><li><strong>一致性正则化的局限</strong>：现有工作使用增强不变一致性来改进CAM，考虑了区域激活一致性，但忽略了跨视图的成对一致性，即区域亲和性一致性。激活一致性只能发现新视图中的激活，无法解决未激活区域和背景噪声问题。</li><li><strong>本文的研究动机</strong>：鉴于亲和性是上下文编码的一种方式，且上下文对像素级预测至关重要，本文提出全对一致性正则化（ACR）方法，同时强制区域激活一致性和区域亲和性一致性，以提高WSSS的性能。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><p><strong>弱监督语义分割方法多样</strong>：采用图像级标签、涂鸦、点和边界框等弱标签，避免像素级标注的繁琐。图像级弱监督语义分割常依赖类激活图（CAM）生成伪分割标签，且有多种方法对其进行优化。</p></li><li><p><strong>一致性正则化受关注</strong>：不同类型的一致性被提出用于优化初始种子，如CAM一致性、特征一致性等。</p></li><li><p><strong>亲和性学习细化</strong>：成对亲和性常被用于优化初始种子，在CNN和Transformer时代都有相关研究。</p></li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-44-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-44-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-44-09"></p><p>本文提出了一种名为<strong>全对一致性正则化</strong>（<strong>All-pairs Consistency Regularization，ACR</strong>）的模型，用于弱监督语义分割（Weakly Supervised Semantic Segmentation，WSSS）任务。</p><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>在图像级弱监督语义分割中，<strong>类激活图</strong>（<strong>Class Activation Map，CAM</strong>）常被用于生成目标定位作为伪分割标签，但存在部分激活问题。现有方法采用一致性正则化来保持不同图像增强下的激活强度不变性，但忽略了每个CAM内区域之间的成对关系。因此，ACR模型旨在同时确保区域激活一致性和区域亲和性一致性，以更好地定位目标。</p><h3 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h3><ul><li><strong>基于视觉Transformer</strong>：选择视觉Transformer作为基础模型，因为其自注意力机制可以自然地编码区域之间的依赖关系，适合建模两种一致性，且无需引入额外模块。</li><li>注意力一致性正则化<ul><li><strong>区域激活一致性</strong>：通过比较原始图像和增强图像的类到块注意力（class-to-patch attention），计算两者之间的ℓ1损失，以鼓励网络生成对变换不变的目标定位。</li><li><strong>区域亲和性一致性</strong>：比较原始图像和增强图像的块到块注意力（patch-to-patch attention），计算两者之间的ℓ1损失，以鼓励图像区域之间的成对关系对变换不变。</li><li><strong>变换逆操作</strong>：为了解决图像增强后注意力矩阵空间顺序不一致的问题，引入变换逆操作，恢复注意力矩阵的原始空间顺序，以便直接计算两个注意力矩阵之间的距离。</li></ul></li><li>基于梯度的Transformer类定位图生成<ul><li><strong>梯度计算</strong>：通过反向传播分类分数，计算类到块注意力的类特定梯度，去除负值并重塑为h×w的图，得到类定位图。</li><li><strong>亲和性细化</strong>：利用学习到的块间亲和性对激活图进行细化，结合区域激活一致性和区域亲和性一致性，生成最终的类定位图。</li></ul></li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：<strong>PASCAL VOC、MS COCO</strong></p><h4 id="MS-COCO"><a href="#MS-COCO" class="headerlink" title="MS COCO"></a>MS COCO</h4><p>表1展示了在MS COCO上的分割结果。本文方法实现了45%的分割平均交并比（mIoU），明显优于现有方法。值得注意的是，该结果不依赖任何额外的显著性信息，但超过了所有先前的WSSS方法，包括使用显著性信息的方法。MS COCO是一个更大的数据集，有更多语义类别和包含多个对象的复杂图像。这一结果表明，显著性信息可能会阻碍WSSS方法在复杂场景中的扩展性，因此本文方法未纳入显著性信息。该结果证明了全对一致性正则化（ACR）能够在具有挑战性的场景中生成可靠的类别定位图。MS COCO的每类结果报告在补充材料中。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-50-42.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-50-42.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-50-42"></p><h4 id="PASCAL-VOC"><a href="#PASCAL-VOC" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h4><ul><li><strong>种子性能</strong>：表2报告了类别定位图的mIoU，包括有无亲和性细化的性能。结果显示，即使没有亲和性细化，ACR*仍优于大多数现有的非显著性方法（mIoU为59.4%）。本文的ACR显著改善了初始种子，证明了所提出的ACR的有效性。在没有显著性信息辅助的情况下，先前最佳方法[66]也采用变压器亲和性来细化种子，而ACR比其高出5.2%。图5展示了定性结果。此外，图6展示了在包含多个对象的复杂场景中的种子，ACR学习到精确的亲和性，有助于形成具有精确边界的完整对象形状。</li><li><strong>伪标签性能</strong>：表2的最后一列显示了伪分割标签的性能。遵循常见做法，采用PSA [2]将激活图（种子）处理为像素级伪分割标签。实验发现PSA容易受到误报样本（即过度激活）的影响。为避免过度激活，使用ACR*训练PSA网络，然后训练好的PSA网络将细化ACR种子（67.3%）为伪标签。结果表明，本文方法显著改善了伪标签。</li><li><strong>语义分割性能</strong>：表3展示了在PASCAL VOC上的语义分割结果。ACR在验证集和测试集上分别取得了71.2%和70.9%的有竞争力的结果，优于先前的非显著性方法。图7显示，使用本文伪标签训练的分割模型可以产生准确和完整的预测。PASCAL VOC的每类结果报告在补充材料中。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-51-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-51-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-51-40"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><p>本文提出在分类训练期间同时正则化区域激活和区域亲和性。表4对这两个正则化项进行了消融实验。首先观察到，即使在基线模型中，区域亲和性也能显著提高种子质量，这验证了视觉变压器的上下文编码能力。通过引入这两个正则化项，发现它们分别对性能有显著提升。同时使用两个正则化项取得了最优结果，与普通变压器基线（51.1%）相比，整体mIoU提高了15.8%，证明了ACR的有效性。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-53-12.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-07_13-53-12.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-07_13-53-12"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种名为<strong>全对一致性正则化（ACR）的训练框架</strong>，用于从变压器生成更好的类定位图。研究结果表明，ACR在分类训练中利用区域激活一致性和区域亲和一致性，通过变压器的自注意力机制同时规范这两种一致性。仅使用一个类令牌，ACR就能学习精确的对象定位和准确的成对亲和性，以提取对象范围。其类定位图显著优于先前方法，在PASCAL VOC和MS COCO数据集上取得了最先进的性能。此外，ACR可以无缝集成到视觉变压器网络中，无需额外修改，这有助于其他基于变压器的任务。因此，作者认为ACR是一种简单而有效的方法，能够为弱监督语义分割提供更好的初始种子。 </p>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Weakly Supervised Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Self-supervised vision transformers for semantic segmentation</title>
      <link href="/post/self-supervised-vision-transformers-for-semantic-segmentation/"/>
      <url>/post/self-supervised-vision-transformers-for-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Semantic segmentation is a fundamental task in computer vision and it is a building block of many other vision applications. Nevertheless, semantic segmentation annotations are extremely expensive to collect, so using pre-training to alleviate the need for a large number of labeled samples is appealing. Recently, <strong>self-supervised</strong> <strong>learning (SSL)</strong> has shown effectiveness in extracting strong representations and has been widely applied to a variety of downstream tasks. However, most works perform sub-optimally in semantic segmentation because they ignore the specific properties of segmentation: (i) the need of pixel level fine-grained understanding; (ii) with the assistance of global context understanding; (iii) both of the above achieve with the dense self-supervisory signal. Based on these key factors, we introduce a systematic self-supervised pre-training framework for semantic segmentation, which consists of a hierarchical encoder–decoder architecture MEVT for generating high-resolution features with global contextual information propagation and a self-supervised training strategy for learning fine-grained semantic features. In our study, our framework shows competitive performance compared with other main self-supervised pre-training methods for semantic segmentation on <strong>COCO-Stuff, ADE20K, PASCAL VOC, and Cityscapes</strong> datasets. e.g., MEVT achieves the advantage in linear probing by +1.3 mIoU on PASCAL VOC.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>语义分割是计算机视觉的基础任务，也是众多视觉应用的基础模块。但由于语义分割标注的获取成本极高，通过预训练减少对大量标注样本的依赖显得尤为重要。近年来，自监督学习（SSL）在特征提取方面展现出显著成效，已被广泛应用于各类下游任务。然而，现有方法在语义分割任务中效果欠佳，主要原因在于忽视了该任务的三个核心特性：(i) 需要像素级的细粒度理解；(ii) 需要结合全局上下文信息；(iii) 必须通过密集的自监督信号同时实现上述两个目标。基于这些关键要素，我们开发了系统的自监督预训练框架，包含以下创新：采用 MEVT 分层编码器-解码器架构生成具有全局上下文传播能力的高分辨率特征，以及专门设计的自监督训练策略用于学习细粒度语义特征。实验表明，在 COCO-Stuff、ADE20K、PASCAL VOC 和 Cityscapes 等数据集上，我们的框架相比其他主流自监督预训练方法展现出竞争优势。典型例证如：MEVT 在 PASCAL VOC 的线性探测任务中实现了 1.3 mIoU 的性能提升。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割是计算机视觉的基础任务，在自动驾驶、机器人操作等领域应用广泛，但标注成本高昂，多数语义分割数据集规模远小于分类数据集。因此，利用大规模无标签数据进行预训练以减少对大量标注样本的依赖成为潜在解决方案。 </p><p>近年来，<strong>自监督学习（SSL）及其在视觉Transformer</strong>上的应用在计算机视觉领域取得显著进展，能帮助网络学习通用视觉表示，降低对大规模标注数据的需求。然而，多数自监督学习方法在语义分割任务中表现欠佳，原因在于它们忽略了语义分割的特定属性：需要像素级的细粒度理解、借助全局上下文理解，且要通过密集的自监督信号实现上述两点。 基于这些问题，本文作者探索一种适用于语义分割的自监督预训练方法，提出了一个系统的自监督预训练框架，旨在生成具有全局上下文信息传播的高分辨率特征，并学习细粒度的语义特征，以提升语义分割任务的性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>自监督学习</strong>：提出多种预训练任务，如色彩化、图像修复等，对比学习在下游视觉任务表现良好，基于掩码图像建模的方法取得了不错的成果，部分研究还改进了训练目标和架构。</li><li><strong>密集预测预训练</strong>：利用自监督学习进行密集预测预训练，一些方法聚焦实例级&#x2F;原型对应，部分引入像素&#x2F;区域级自监督预训练方法。</li><li><strong>视觉Transformer</strong>：ViT将Transformer应用于图像识别，Swin Transformer引入卷积风格窗口计算，部分工作构建多分辨率特征图用于密集输出。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-00-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-00-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-14_21-00-59"></p><p>本文提出了一种用于<strong>语义分割的自监督预训练框架</strong>，该框架在网络架构和自监督目标方面都有创新，核心模型是多尺度编码器 - 解码器视觉变压器（Multi-scale Encoder–Decoder Vision Transformer, MEVT），以下是详细介绍： </p><ol><li><strong>模型架构</strong>    <ul><li><strong>多尺度解码器</strong>：为了获得高分辨率的细粒度特征，MEVT在预训练架构中引入了多尺度解码器，并对编码器和解码器进行联合预训练。解码器由全局注意力阶段（Stage 5）和局部注意力阶段（Stage 6）组成，通过“Patch Unmerging”层对特征图进行上采样，同时引入跳跃连接（skip-connections），促进训练过程中浅层的梯度传播。    </li><li><strong>混合注意力机制</strong>：为了融合全局和局部上下文信息，MEVT采用了一种简单而有效的混合注意力策略。在浅层（Stage 1、2和6）使用Swin Transformer的窗口注意力块来处理局部信息，在深层（Stage 3、4和5）使用全局自注意力（ViT块）来增强全局上下文信息的传播。</li></ul></li><li><strong>自监督预训练策略</strong>：MEVT使用图像级自蒸馏损失（来自DINO）对全局平均池化（GAP）特征进行预训练。将图像的两个增强视图分别输入到教师网络和学生网络中，通过最小化交叉熵损失将知识从教师网络蒸馏到学生网络。教师网络通过指数移动平均（EMA）更新。在Stage 4和Stage 6的输出处分别应用自蒸馏损失，并对两个损失项进行等权重加权，以确保编码器和解码器网络得到充分的预训练。</li><li><strong>模型优势</strong></li></ol><ul><li><strong>性能表现</strong>：在多个语义分割数据集（COCO - Stuff、ADE20K、PASCAL VOC和Cityscapes）上的实验结果表明，MEVT在各种设置（线性探测、微调、低样本学习）下均优于大多数现有方法。例如，在PASCAL VOC上，MEVT在线性探测中比其他方法提高了+1.3 mIoU。   </li><li><strong>特征学习</strong>：通过实验和消融研究，证明了MEVT能够学习到具有细粒度和全局上下文感知能力的视觉表示，适用于具有挑战性的语义分割任务。例如，在定性结果中，MEVT在复杂环境中识别小物体的能力优于其他基线方法。</li></ul><h2 id="实验（Compared-with-SOTA）-1st-place-medal"><a href="#实验（Compared-with-SOTA）-1st-place-medal" class="headerlink" title="实验（Compared with SOTA）:1st_place_medal:"></a>实验（Compared with SOTA）:1st_place_medal:</h2><p><strong>数据集</strong>：在ImageNet上进行300个epoch的预训练。</p><ul><li><strong>线性探测结果</strong>：在COCO - Stuff、ADE20K、Cityscapes和PASCAL等数据集上，MEVT在大多数数据集上优于所有基线方法。例如，在具有挑战性的Cityscapes数据集上，MEVT比基于Transformer的DINO方法高出8.4 mIoU，比采用Swin - T的MOBY方法高出2.6 mIoU。</li><li><strong>端到端微调结果</strong>：在ADE20K数据集上，使用线性头时，MEVT比之前最好的方法iBOT高出2.4 mIoU。</li><li><strong>低样本微调结果</strong>：在不同比例标记的ADE20K图像上，MEVT在各种监督水平下均优于现有方法，表明其在实际场景中能实现更高效的语义分割。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-14_21-04-37"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-14_21-04-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-14_21-04-43"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>预训练解码器</strong>：将解码器纳入预训练框架显著提高了线性探测mIoU（+4.9）和微调mIoU（+2.2）。</li><li><strong>解码器深度</strong>：默认使用两个块的解码器，从一个块增加到两个块可提高线性探测和微调性能，增加到三个块时性能下降。</li><li><strong>多尺度融合</strong>：MEVT在低分辨率阶段使用全局自注意力进行多尺度信息融合，比仅依赖窗口注意力的Swin - T + W.A.Dec.在线性探测和微调上分别高出2.9 mIoU和2.8 mIoU。</li><li><strong>跳跃连接</strong>：添加两个跳跃连接时性能最佳，线性探测mIoU从67.8提高到71.5。</li><li><strong>位置编码</strong>：成对相对位置偏差的效果优于其他位置偏差，线性探测mIoU提高了4.3。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种<strong>自监督预训练方法</strong>，用于促进下游<strong>语义分割任务</strong>，得出以下结论：</p><ol><li><strong>方法创新</strong>：该方法在神经网络架构和自监督目标方面均有创新，构建了包含多尺度编码器-解码器架构MEVT和自监督训练策略的框架。</li><li><strong>性能优越</strong>：此框架简单且强大，在<strong>COCO-Stuff、ADE20K、PASCAL VOC和Cityscapes</strong>四个常用数据集的多种语义分割和低样本评估指标上达到了最优性能。 </li><li><strong>应用前景</strong>：<strong>作者希望该简单框架能推动无标签或少量标签语义分割的广泛应用，减少对大量高质量标注数据的依赖。</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> 自监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> semantic segmentation </tag>
            
            <tag> ViT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation</title>
      <link href="/post/a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation/"/>
      <url>/post/a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="南京信息工程大学、青海师范大学、澳门大学、中国科学院-100"><a href="#南京信息工程大学、青海师范大学、澳门大学、中国科学院-100" class="headerlink" title="南京信息工程大学、青海师范大学、澳门大学、中国科学院  :100:"></a>南京信息工程大学、青海师范大学、澳门大学、中国科学院  :100:</h2><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a><strong>摘要：</strong></h2><blockquote><p><strong>Few-shot semantic segmentation (FSS)</strong> aims to generate a model for segmenting novel classes using a limited number of annotated samples. Previous FSS methods have shown sensitivity to background noise due to inherent bias, attention bias, and spatial-aware bias. In this study, we propose a <strong>Transformer-Based Adaptive Prototype Matching Network</strong> to establish robust matching relationships by improving the semantic and spatial perception of query features. The model includes three modules: <strong>target enhancement module (TEM)</strong>, <strong>dual constraint aggregation module (DCAM)</strong>, and <strong>dual classification module (DCM)</strong>. In particular, TEM mitigates inherent bias by exploring the relevance of multi-scale local context to enhance foreground features. Then, DCAM addresses attention bias through the dual semantic-aware attention mechanism to strengthen constraints. Finally, the DCM module decouples the segmentation task into semantic alignment and spatial alignment to alleviate spatial-aware bias. Extensive experiments on <strong>PASCAL-5i</strong> and <strong>COCO-20i</strong> confirm the effectiveness of our approach.</p></blockquote><h2 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a><strong>翻译：</strong></h2><blockquote><p>Few-shot语义分割（FSS）旨在通过少量的标注样本为新的类别生成一个分割模型。以往的FSS方法由于固有偏差、注意力偏差和空间感知偏差，往往对背景噪声过于敏感。在本研究中，我们提出了一种基于Transformer的自适应原型匹配网络，通过增强查询特征的语义和空间感知能力，建立更为稳定的匹配关系。该模型包含三个模块：目标增强模块（TEM）、双重约束聚合模块（DCAM）和双重分类模块（DCM）。其中，TEM通过探索多尺度局部上下文的相关性，增强前景特征，从而减轻固有的偏差。接着，DCAM通过双重语义感知注意力机制解决了注意力偏差问题，强化了约束效果。最后，DCM模块将分割任务拆解为语义对齐和空间对齐，帮助缓解空间感知偏差。我们在PASCAL-5i和COCO-20i数据集上进行了大量实验，验证了该方法的有效性。</p></blockquote><h2 id="研究背景："><a href="#研究背景：" class="headerlink" title="研究背景："></a><strong>研究背景：</strong></h2><p>近年来，由于深度学习在计算机视觉领域的快速发展，所以传统的语义分割取得了飞速进步。在这种情况下，少样本分割(few-shot segmentation, FSS)被提出用于模拟有限数据和多类别的真实世界场景。</p><p>FSS遵循元学习框架，执行过程分特征提取、匹配和分类三个阶段。现有FSS模型虽有成果，但受背景干扰，存在三方面问题：一是特征提取阶段，预训练骨干网络有固有偏差，易优先提取无关特征；二是特征匹配阶段，注意力机制在目标类别内差异大时，会导致注意力偏差；三是分类阶段，现有方法多依赖语义相关性，忽略空间信息，产生空间感知偏差。</p><p>基于上述问题，作者提出一种基于Transformer的自适应原型匹配网络，通过在模型执行的三个阶段进行策略性和高效交互，减轻FSS中的背景干扰，利用查询特征的语义和空间感知，增强模型的鲁棒性，以解决现有FSS模型存在的问题。</p><h2 id="研究现状："><a href="#研究现状：" class="headerlink" title="研究现状："></a><strong>研究现状：</strong></h2><ul><li><strong>Few - Shot Semantic Segmentation（FSS）</strong>：FSS旨在用<strong>少量标注样本</strong>为新类别生成分割模型，基于度量学习的FSS主要分为基于原型和基于像素匹配两类方法。<strong>基于原型的方法</strong>用原型代表目标类信息进行匹配预测；<strong>基于像素匹配</strong>的方法建立支持像素和查询像素的密集关联。</li><li><strong>Transformer应用</strong>：Transformer因能捕捉长距离相关性，在FSS中得到应用，如动态调整分类器权重、过滤无关像素、聚合多级别支持掩码等。</li></ul><h2 id="提出的模型："><a href="#提出的模型：" class="headerlink" title="提出的模型："></a><strong>提出的模型：</strong></h2><p>本文提出了一种基于Transformer的自适应原型匹配网络（Transformer - Based Adaptive Prototype Matching Network），用于<strong>少样本语义分割（Few - Shot Semantic Segmentation，FSS）<strong>任务，以解决现有FSS模型存在的</strong>固有偏差、注意力偏差和空间感知偏差</strong>导致的对背景噪声敏感的问题。该模型主要包含以下三个模块： </p><ol><li><blockquote><p><strong>目标增强模块（Target Enhancement Module，TEM）</strong>    <strong>设计目的</strong>：缓解骨干网络的<strong>固有偏差</strong>，增强前景特征。在特征提取阶段，以往工作依赖预训练骨干网络直接提取的特征，存在固有偏差，倾向于提取与当前任务无关的特征。    <strong>具体方法</strong>：引入基于卷积Transformer架构的多尺度局部感知调制Transformer进行多尺度特征提取，采用多尺度自适应局部注意力增强前景信息、减轻背景干扰；用可逆神经网络（INN）替代标准多层感知器（MLP），在前馈过程中保留更细粒度的特征。 </p></blockquote></li><li><blockquote><p><strong>双约束聚合模块（Dual Constraint Aggregation Module，DCAM）</strong>    <strong>设计目的</strong>：解决特征匹配阶段的<strong>注意力偏差</strong>问题。现有方法利用单层注意力机制建立支持集和查询集的关系，在目标类别存在显著类内差异时，这种关系不足以准确匹配，导致注意力偏差。    <strong>具体方法</strong>：由类内差异表示和双语义感知注意力机制两个关键部分组成。类内差异表示利用一组可学习向量建模支持集和查询集之间的差异；双语义感知注意力机制通过两层约束，先以支持原型为参考在查询特征中选择匹配置信度高的点，再以此为指导在整个查询特征图中寻找特征相似度高的点，生成鲁棒的支持类别原型。 </p></blockquote></li><li><blockquote><p><strong>双分类模块（Dual Classification Module，DCM）</strong>    <strong>设计目的</strong>：解决特征分类阶段的<strong>空间感知偏差</strong>问题。现有方法主要基于语义一致性进行预测，忽略了目标对象的空间一致性，导致难以准确定位目标类别。    <strong>具体方法</strong>：将分割任务解耦为语义对齐和空间对齐两个子任务。通过优化查询特征和类别原型生成基于语义相似度的掩码来识别目标类别；利用查询特征的内在引导，挖掘目标对象自身的空间一致性，得到基于空间分布概率的掩码用于精确的定位，最后将两个掩码相加得到最终的查询前景分割图。 实验结果表明，该模型在PASCAL - 5i和COCO - 20i两个基准数据集上取得了优于现有方法的性能，且参数数量较少。</p></blockquote></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-36-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-36-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-36-07"></p><h2 id="实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）"><a href="#实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）" class="headerlink" title="实验（compared with the state-of-the-art models and ablation experiments）"></a><strong>实验（compared with the state-of-the-art models and ablation experiments）</strong></h2><ul><li><h3 id="Comparison-with-the-State-of-the-Arts"><a href="#Comparison-with-the-State-of-the-Arts" class="headerlink" title="Comparison with the State-of-the-Arts"></a><strong>Comparison with the State-of-the-Arts</strong></h3></li></ul><p>数据集：PASCAL-5${^i}$，COCO-20${^i}$</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-39-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-39-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-39-17"></p><ul><li><h3 id="ablation-experiments"><a href="#ablation-experiments" class="headerlink" title="ablation experiments"></a><strong>ablation experiments</strong></h3></li></ul><ol><li><strong>组件分析</strong>：该方法包含目标增强模块（TEM）、双约束聚合模块（DCAM）和双分类模块（DCM）三个主要模块。与基线相比，单独使用TEM增强查询前景特征可使性能提升0.9%，单独使用DCAM增强类别原型的判别能力可提升2.2%，TEM和DCAM协同作用可提升2.6%，使用DCM实现语义对齐和空间对齐可额外提升1.3%。模型整体比基线提升了3.9%，表明引入的模块有效解决了固有偏差、注意力偏差和空间感知偏差问题，减少了背景干扰，实现了精确分割。 </li><li><strong>目标增强模块（TEM）</strong>：TEM旨在减轻骨干网络的固有偏差并增强查询前景区域。通过与其他方法在计算量和准确性方面进行对比实验，包括采用自对齐模块（SA）、卷积变压器架构（SAM）、多尺度自适应局部注意力（MSLA + MLP）以及用可逆神经网络（INN）代替多层感知器（MLP）作为前馈网络（MSLA + INN）。结果表明，该方法在降低计算复杂度的同时保持了较高的准确性，且前馈网络在略微增加计算成本的情况下保留了更多特征细节。 </li><li><strong>双约束聚合模块（DCAM）</strong>：对DCAM中的关键组件进行了全面分析，通过修改模型采用不同的注意力机制，如原始的普通注意力（VA）、掩码注意力（MA）、双语义感知注意力（DSAA）和类内差异表示（IDR）。结果显示，使用掩码注意力减轻背景噪声干扰对性能提升影响不大，因为支持集和查询集之间的相似度掩码在类内差异较大时准确性存在挑战。而双语义感知注意力机制通过可学习的方式减轻背景干扰，能应对类内差异的敏感性，类内差异表示在三种不同的注意力机制中都有益。</li><li><strong>双分类模块（DCM）</strong>：通过消融实验评估不同的DCM组件。仅使用基于语义相似度的掩码可使模型性能提升1.5%，证明了优化类别原型和查询特征的必要性；仅使用基于空间分布概率的分割图时，性能下降2.3%，这是因为仅依赖查询图像本身的前景分布会使模型偏向已知类别的区域，导致对未知类别的分割失败。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-19.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-19.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-41-19"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-08_10-41-25"></p><h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a><strong>结论：</strong></h2><blockquote><p>作者提出了一种<strong>基于Transformer的自适应原型匹配网络</strong>，以应对少样本语义分割（FSS）中<strong>固有偏差、注意力偏差和空间感知偏差</strong>导致的背景干扰问题。该网络包含目标增强模块（TEM）、双约束聚合模块（DCAM）和双分类模块（DCM）。TEM通过多尺度局部上下文相关性增强前景特征，解决固有偏差；DCAM利用双语义感知注意力机制加强约束，处理注意力偏差；DCM将分割任务解耦为语义对齐和空间对齐，缓解空间感知偏差。实验表明，该方法在PASCAL - 5i和COCO - 20i数据集上以最少的参数达到了最先进的性能，有效减少了背景干扰，实现了精确分割。 </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 少样本语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> Few-Shot Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation</title>
      <link href="/post/dsmf-net-dual-semantic-metric-learning-fusion-network-for-few-shot-aerial-image-semantic-segmentation/"/>
      <url>/post/dsmf-net-dual-semantic-metric-learning-fusion-network-for-few-shot-aerial-image-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Chinese Academy of Sciences、University of Chinese Academy of Sciences</strong></p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Semantic segmentation</strong> of aerial images is crucial yet resource-intensive. Inspired by human ability to learn rapidly, few-shot semantic segmentation offers a promising solution by utilizing limited labeled data for efficient model training and generalization. However, the intrinsic complexities of aerial images, compounded by scarce samples, often result in inadequate feature representation and semantic ambiguity, detracting from themodel’s performance. In this article, we propose to tackle these challenging problems via dual semantic metric learning and multisemantic features fusion<br>and introduce a novel few-shot segmentation Network (DSMF-Net). On the one hand, we consider the inherent semantic gap between the feature of graph and grid structures and metric learning of few-shot segmentation. To exploit multiscale global semantic context, we construct scale-aware graph prototypes from different stages of the feature layers based on graph convolutional networks (GCNs), while also incorporating prior-guided metric learning to further enhance context at the high-level convolution features. On the other hand, we design a pyramid-based fusion and condensa-<br>tion mechanism to adaptively merge and couple the multisemantic information from support and query images. The indication and fusion of different semantic features can effectively emphasize the representation and coupling abilities of the network. We have conducted extensive experiments over the challenging iSAID-5i andDLRSD benchmarks. The experiments have demonstrated our network’s effectiveness and efficiency, yielding on-par performance with the state-of-the-art methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>航空图像的语义分割</strong>是一个非常重要的问题。受人类快速学习能力的启发，少射语义分割通过利用有限的标记数据进行有效的模型训练和泛化，提供了一种很有前途的解决方案。然而，航空图像固有的复杂性，加上稀缺的样本，往往导致特征表示不足和语义模糊，从而降低了模型的性能。在本文中，我们提出通过双语义度量学习和多语义特征融合来解决这些具有挑战性的问题，并引入了一种新的少量样本学习分割网络(DSMFNet)。一方面，我们考虑了图和网格结构特征之间固有的语义差距和少量样本学习分割的度量学习。为了利用多尺度全局语义上下文，我们基于图卷积网络(GCNs)从特征层的不同阶段构建了尺度感知的图原型，同时还结合了先验引导的度量学习来进一步增强高级卷积特征的上下文。另一方面，我们设计了一种基于金字塔的融合与凝聚机制来自适应地融合和耦合来自支持和查询图像的多语义信息。不同语义特征的表示和融合可以有效地强调网络的表示和耦合能力。我们对具有挑战性的iSAID-5i和dlrsd基准进行了广泛的实验。实验证明了我们的网络的有效性和效率，产生了与最先进的方法相当的性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于航空影像少样本语义分割问题，研究背景如下：</p><ul><li><strong>语义分割需求与挑战</strong>：语义分割是计算机视觉基础技术，在城市管理、环境监测等领域应用广泛。但传统航空影像语义分割模型训练需大量标注数据，获取耗时耗力。</li><li><strong>少样本学习的潜力</strong>：少样本学习受人类学习能力启发，利用少量标注数据进行模型训练和泛化，为解决数据获取难题提供了思路。少样本语义分割作为其延伸，通过利用相关任务或领域的先验知识进行分割任务。</li><li><strong>航空影像少样本分割的困难</strong>：航空影像由机载或卫星传感器捕获，具有空间分辨率变化大、覆盖范围广的特点。不同语义对象外观差异大，同一类别对象在尺度和结构上也存在显著差异，导致特征表示不足和语义模糊，增加了少样本语义分割的难度。 </li><li><strong>现有方法的局限性</strong>：现有方法虽有一定进展，但传统卷积神经网络在捕捉航空影像的全局和可变结构关系方面效率较低，特征提取存在特征耦合和细节保留不足的问题，导致特征歧义。 基于以上背景，作者提出DSMF - Net网络，以解决航空影像少样本语义分割中的特征建模不佳和语义模糊问题。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>语义分割</strong>：FCN、U - Net等方法推动了语义分割发展，后续如DeepLabv3 +、PSPNet等通过引入新机制提升性能，但监督分割方法标注要求高。</li><li><strong>少样本语义分割</strong>：出现半监督、弱监督、无监督学习等方法，近期少样本学习受关注，如OSLSM、PL、PANet等方法不断涌现，部分还探索了知识迁移问题。</li><li><strong>航空影像少样本语义分割</strong>：不同方法被提出，如Wang等人的原型队列学习法、Yao等人的多原型框架、DMML - Net的深度特征金字塔比较网络等。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>文章提出了一种名为DSMF - Net（Dual Semantic Metric Learning Fusion Network）的新型少样本分割网络，用于解决航空影像少样本语义分割中特征建模不佳和语义模糊的问题。</p><ol><li><strong>图原型度量学习（Graph Prototypes Metric Learning）</strong><ul><li><strong>图卷积（Graph Convolution）</strong>：图卷积可有效表示图像中不同像素或区域之间的关系，捕捉长距离依赖和全局上下文信息，有助于减少特征耦合，使模型学习和区分图像内不同的结构和语义关系。</li><li><strong>尺度感知图原型（Scale - Aware Graph Prototypes）</strong>：静态网格卷积特征难以捕捉航空影像中的复杂关系，因此引入尺度感知图原型，以图投影的方式利用多尺度全局语义上下文。通过在预训练的ResNet - 50的三个中间阶段输出特征，利用GloRe单元进行全局推理，结合支持掩码加权和全局平均池化生成原型。</li><li><strong>原型度量学习（Prototype Metric Learning）</strong>：利用查询特征与原型之间的余弦距离进行度量学习，对查询特征应用相同的图投影操作，通过最小 - 最大归一化得到图原型概率图。</li></ul></li><li><strong>先验引导度量学习（Prior Guided Metric Learning）</strong>：图卷积特征金字塔为图结构数据的特征嵌入和全局信息捕捉提供了基础，但高级卷积特征中的语义信息也不能忽视。因此引入先验引导度量学习，生成高级查询和支持卷积特征之间的相似性度量，通过添加二进制支持掩码减轻背景影响，计算余弦距离并进行最小 - 最大归一化得到先验引导概率图。</li><li><strong>语义特征融合模块（SFF）</strong>：构建SFF模块解决特征耦合问题，增强模型对变化的鲁棒性。采用金字塔结构对不同尺度的特征进行上采样和下采样，通过1×1卷积合并特征生成中间尺度特征，最后插值和拼接生成新的融合特征，促进不同尺度特征的有效交互和集成。</li><li><strong>损失函数（Loss Function）</strong>：采用交叉熵损失作为主要损失函数$L_{main}$，并引入中间监督$L_{aux}$，总损失L是$L_{main}$和$L_{aux}$的加权和，其中λ设为1.0。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：iSAID-5i、DLRSD</p><ul><li><strong>SAID - 5i数据集</strong>：将提出的模型与其他流行方法进行比较，结果表明该模型明显优于当代少样本分割模型，随着骨干网络的增强，性能提升。在不同设置下，模型在各折数据上均有显著的mIoU提升，且通过配对t检验验证了模型性能提升的显著性。</li><li><strong>DLRSD数据集</strong>：在更具挑战性的DLRSD数据集上进行实验，结果显示该模型的mIoU得分高于其他方法，尤其在处理复杂场景和具有细微视觉差异的对象时表现出色。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-35-15"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-38.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-38.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-35-38"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li>消融实验：使用ResNet - 50骨干网络在1 - shot设置下进行广泛的消融实验，分析提出模块的有效性和不同设置的影响。<ul><li><strong>GPML模块</strong>：通过实验表明，基于图结构的GPML模块相比基于卷积结构的原型学习，能进一步提高模型性能，更好地捕捉航空图像中对象之间的复杂关系。</li><li><strong>SFF模块</strong>：实验验证了SFF模块在不同尺度下的特征融合策略的有效性，特别是引入图结构和合并操作后，模型的平均性能显著提升。</li><li><strong>效率评估</strong>：通过比较不同网络的参数数量和每秒帧数（FPS），证明了提出的模型在保持较低参数数量的同时，实现了较高的FPS，在少样本航空图像分割中具有高效性。</li></ul></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-36-29"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-39.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-39.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_09-36-39"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者聚焦于<strong>航拍影像少样本语义分割</strong>中特征建模不佳和语义模糊的问题，提出了名为<strong>DSMF-Net</strong>的少样本分割网络。该网络采用<strong>双度量学习和多语义信息融合</strong>，增强了模型的解析和表达能力。具体而言，通过尺度感知图原型以图投影方式挖掘多尺度全局语义上下文，集成先验引导度量学习增强高层语义上下文，设计基于金字塔的融合模块更好地提取和浓缩语义特征。在<strong>iSAID - 5i和DLRSD</strong>两个具有挑战性的基准数据集上的实验表明，该方法性能优越，能有效处理有限样本下的密集预测任务。不过，作者也指出，未来需在更大、更多样化的数据集上进一步评估，以全面了解其能力和局限性，后续研究将关注模型对不同航空影像类型的适应性、在大规模数据集上的泛化能力和计算复杂度。 </p>]]></content>
      
      
      <categories>
          
          <category> 少样本语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSMF-Net </tag>
            
            <tag> Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kill Two Birds with One Stone Domain Generalization for Semantic Segmentation via Network Pruning</title>
      <link href="/post/kill-two-birds-with-one-stone-domain-generalization-for-semantic-segmentation-via-network-pruning/"/>
      <url>/post/kill-two-birds-with-one-stone-domain-generalization-for-semantic-segmentation-via-network-pruning/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>浙江大学、内华达大学</strong></p></blockquote><p>::: tip</p><p>启发</p><p>:::</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Deep model</strong>s are notoriously known to perform poorly when encountering new domains with different statistics. To alleviate this issue, we present a new domain generalization method based on network pruning, dubbed NPDG. Our core idea is to prune the filters or attention heads that are more sensitive to domain shift while preserving those domain-invariant ones. To this end, we propose a new pruning policy tailored to improve generalization ability, which identifies the filter and head sensibility of domain shift by judging its activation variance among different domains (unary manner) and its correlation to other filter (binary manner). To better reveal those potentially sensitive filters and heads, we present a differentiable style perturbation scheme to imitate the domain variance dynamically. NPDG is trained on a single source domain and can be applied to both CNN- and Transformer-based backbones. To our knowledge, we are among the pioneers in tackling domain generalization in segmentation via network pruning. NPDG not only improves the generalization ability of a segmentation model but also decreases its computation cost. Extensive experiments demonstrate the state-of-the-art generalization performance of NPDG with a lighter-weight structure.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>众所周知，深度模型在遇到具有不同统计数据的新领域时表现不佳。为了解决这个问题，我们提出了一种新的基于<strong>网络剪枝</strong>的域泛化方法，称为NPDG。我们的核心思想是剪枝过滤器或注意头，更敏感的领域转移，同时保留那些领域不变的。为此，我们提出了一种新的剪枝策略来提高泛化能力，该策略通过判断不同域之间的激活方差(一元方式)和与其他滤波器的相关性(二值方式)来识别滤波器和域漂移的头部敏感性。为了更好地揭示那些潜在的敏感滤波器和头部，我们提出了一种可微风格的摄动方案来动态地模拟域方差。NPDG在单一源域上训练，可以应用于基于CNN和transformer的主干。据我们所知，我们是通过网络剪枝处理分割领域泛化的先驱之一。NPDG不仅提高了分割模型的泛化能力，而且降低了分割模型的计算量。大量的实验证明了具有较轻重量结构的NPDG具有最先进的泛化性能。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>‍深度学习方法在训练和测试数据独立同分布时，能在各种视觉任务中取得显著成功。然而在实际应用中，深度模型部署到统计特性不同的新环境时，性能会大幅下降。为缓解这一问题，<strong>领域泛化</strong>（<strong>DG</strong>）被提出，旨在增强深度神经网络对未见目标分布的泛化能力。 与领域自适应（DA）不同，DG在训练时无法获取目标域数据，更具挑战性。DG研究主要分为多源和单源两种设置，多源DG假设各源域存在共享因素，但多源样本获取和标注耗时费力，单源DG更具现实意义，因此成为研究热点。 现有单源DG方法多通过数据增强或风格迁移创建多个增强域来模拟未见域，但数据生成与下游任务独立，导致结果欠佳。还有方法尝试让模型学习域不变表示或解耦潜在表示，但存在网络结构或损失函数设计复杂，以及域无关特征占用存储空间和推理时间的问题。 基于此，本文提出一种基于网络剪枝的单源领域泛化方法NPDG，旨在解决上述问题，提高模型泛化能力并降低计算成本。 </p><p>‍</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><p>‍</p><ul><li><strong>领域适应与泛化</strong>：领域适应（DA）和领域泛化（DG）旨在让模型在未标记目标域表现良好。DA可获取目标数据，有分布对齐、合成样本等策略；DG无法获取目标数据，更具挑战性，分为多源和单源方法，单源DG因数据收集和标注成本高而受关注。</li><li><strong>网络剪枝</strong>：旨在减少网络复杂度，分为非结构化和结构化剪枝。多数现有方法用于图像分类，未考虑领域差距，部分跨领域剪枝方法聚焦粗粒度视觉任务。</li></ul><h2 id="提出的模型（NPDG）"><a href="#提出的模型（NPDG）" class="headerlink" title="提出的模型（NPDG）"></a>提出的模型（NPDG）</h2><p>‍</p><ol><li>可微风格扰动（Differentiable Style Perturbation, DSP）模块：<ul><li>受AdaIN启发，通过额外的领域变分自编码器（D - VAE）将风格统计信息编码为标准分布，动态生成具有高多样性的域外数据，以匹配模型的剪枝状态。</li><li>训练目标是最小化包含AdaIN损失、KL散度损失和重建损失的总损失。</li><li>在部署阶段，通过干扰采样向量ε生成任意新领域，且梯度可直接反向传播到ε，使整个生成过程可微。</li></ul></li><li>网络剪枝策略：<ul><li>为每个滤波器引入可学习的缩放因子γ，通过联合训练后缩放因子接近零的滤波器被认为是要被修剪的。</li><li>对于基于CNN的骨干网络，将γ重新用于批量归一化（BN）层；对于基于Transformer的模型，为每个注意力激活分配缩放因子。</li><li>训练目标包括任务损失和稀疏正则化项，通过修改稀疏正则化函数F(γ)来重新加权香草L1正则化，以抑制对域敏感的滤波器或注意力头。</li></ul></li><li>滤波器&#x2F;头敏感性度量：<ul><li><strong>一元滤波器&#x2F;头敏感性（Unary Filter&#x2F;Head Sensitivity）</strong>：测量第i个滤波器&#x2F;头在域转移下的激活方差，通过对共享相同内容但不同风格的小批量图像进行前向传播，计算激活图的方差并归一化得到wU i。</li><li><strong>二元滤波器&#x2F;头敏感性（Binary Filter&#x2F;Head Sensitivity）</strong>：考虑域转移下滤波器之间的二元关系，通过计算协方差矩阵并对其行求和得到wB i，以识别与同一层中其他滤波器高度相关的滤波器。</li><li>最终的滤波器敏感性w由一元和二元滤波器敏感性加权求和得到，即w &#x3D; λwU + (1 - λ)wB，其中λ是控制两者相对重要性的超参数。</li></ul></li></ol><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-31-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-31-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-31-14"></p><p>‍</p><p>‍</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p><strong>数据集：GTA5、SYNTHIA、Cityscapes、BDD100K、Mapillary</strong></p></blockquote><p>‍</p><ul><li><strong>与基于DG的方法比较</strong>：在合成到真实、真实到合成和跨真实的DG任务中，NPDG与多个先进的DG方法对比。仅使用DSP就能大幅提升基线方法的mIoU，在此基础上进行域敏感滤波器剪枝可进一步提高泛化能力。与SHADE方法相比，NPDG在多数情况下表现更优，且模型结构更轻、计算成本更低。在使用Transformer作为分割骨干时，NPDG在所有目标数据集上至少比基线模型提高4%，在7个DG任务中的3个达到了最优结果。</li><li><strong>鲁棒性</strong>：通过给出实验的标准差评估NPDG的鲁棒性。各剪枝迭代可能导致模型结构略有不同，使指标有轻微波动，但方差不大。较高的剪枝率会导致更大的偏差，与仅使用DSP的模型相比，域敏感滤波器剪枝带来了显著提升。</li><li><strong>效率</strong>：与现有DG方法相比，NPDG能用更轻量级的模型达到最优的分割精度，节省超过17 GFLOPs和35M参数。但进一步提高剪枝率（超过30%）会损害泛化性能，因为分割是细粒度任务，过多剪枝会降低语义边界的分割精度。</li></ul><p>‍</p><p>‍</p><p>‍<strong>与网络剪枝（NP）方法比较</strong>：选择在普通分类和分割任务中有效的NP方法，在ResNet - 101和VGG - 16上评估剪枝性能。由于这些方法在滤波器或权重剪枝时未考虑域偏移，在新域中的性能比未剪枝的基线模型下降。其中，SFP方法的GFLOPs和内存成本最低，而基于Network Slimming的方法（包括NPDG）在隐式训练过程中剪枝滤波器，大量剪枝滤波器位于浅层。</p><p>‍</p><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-35-11"></p><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-35-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-35-18"></p><p>‍</p><h2 id="实验（Ablation-Experiments）🥇"><a href="#实验（Ablation-Experiments）🥇" class="headerlink" title="实验（Ablation Experiments）🥇"></a>实验（Ablation Experiments）🥇</h2><p>‍对NPDG的核心组件（DSP模块、一元和二元滤波器敏感性）进行消融研究。所有组件都有助于提高基线模型的泛化性能，DSP生成的新变体域比随机采样策略效果更好。结合一元和二元滤波器敏感性可使mIoU达到最高，表明两者具有互补作用。</p><p>‍<img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-37-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_14-37-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-04-02_14-37-26"></p><h1 id="‍超参数研究"><a href="#‍超参数研究" class="headerlink" title="‍超参数研究"></a>‍超参数研究</h1><ul><li><strong>一元和二元权重的比例λ</strong>：通过网格搜索确定λ值，ResNet101和MiT - B5模型在λ &#x3D; 0.4时性能最优，表明一元和二元敏感性剪枝都有贡献，二元剪枝效果略更明显。</li><li><strong>剪枝率r</strong>：剪枝率是灵活参数，在分割任务中，过高的剪枝率会导致边缘模糊，影响分割性能。实验表明，最优剪枝率在20% - 40%之间，约30%时泛化效果最佳。可使用验证集找到剪枝率和mIoU的精确权衡，实际应用中，若没有验证集，使用约30%的剪枝率通常可行。</li><li><strong>剪枝阈值t</strong>：剪枝阈值不是非常敏感的超参数，在一定范围内取值均可。只要稀疏训练迭代次数足够，就能识别出满足要求的滤波器。一般设置t &#x3D; 0.1。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种基于<strong>网络剪枝</strong>的<strong>领域泛化</strong>方法NPDG，得出以下结论：</p><ol><li><strong>创新性</strong>：NPDG通过定制的剪枝策略，从一元和二元分析辨别滤波器对领域偏移的敏感性，同时引入可微风格扰动方案动态模拟领域变化，助力识别敏感滤波器，是利用网络剪枝解决领域泛化问题的先驱。 </li><li><strong>有效性</strong>：在CNN和Transformer架构上的大量实验表明，NPDG能以更轻量级的模型实现语义分割泛化的最优性能。 </li><li><strong>局限性与展望</strong>：当前NPDG主要考虑风格差异导致的领域偏移，难以识别所有因素，未来将全面解决领域偏移问题。此外，目前依赖经验值选择超参数，未来需开发测试时训练的方法确定超参数。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 领域泛化语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Domain Generalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stronger, Fewer, &amp; Superior Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation（DGSS）</title>
      <link href="/post/stronger-fewer-superior-harnessing-vision-foundation-models/"/>
      <url>/post/stronger-fewer-superior-harnessing-vision-foundation-models/</url>
      
        <content type="html"><![CDATA[<h2 id="中国科学技术大学，上海人工智能实验室"><a href="#中国科学技术大学，上海人工智能实验室" class="headerlink" title="中国科学技术大学，上海人工智能实验室"></a><strong>中国科学技术大学，上海人工智能实验室</strong></h2><p><a href="https://github.com/w1oves/Rein.git">https://github.com/w1oves/Rein.git</a></p><blockquote><p>摘要：In this paper, we first assess and harness various Vision Foundation Models (VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS). Driven by the motivation that Leveraging Stronger pre-trained models and Fewer trainable parameters for Superior generaliz- ability, we introduce a robust fine-tuning approach, namely “Rein”, to parameter-efficiently harness VFMs for DGSS. Built upon a set of trainable tokens, each linked to distinct instances, Rein precisely refines and forwards the feature maps from each layer to the next layer within the backbone. This process produces diverse refinements for different categories within a single image. With fewer trainable parameters, Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full parameter fine-tuning. Extensive experiments across various settings demonstrate that Rein significantly outperforms state-of-the-art methods. Remarkably, with just an extra 1% of trainable parameters within the frozen backbone, Rein achieves a mIoU of78.4% on the Cityscapes, without accessing any real urban-scene datasets. Code is available at <a href="https://github.com/w1oves/Rein.git">https://github.com/w1oves/Rein.git</a>.</p></blockquote><blockquote><p>翻译：在本文中，我们首先在领域泛化语义分割（DGSS）任务中，评估并应用了多种视觉基础模型（VFM）。我们提出的动机是：“通过利用更强大的预训练模型和更少的可训练参数，获得更好的泛化能力”。基于此，我们提出了一种高效的微调方法——“Rein”，该方法能够以参数高效的方式利用VFM来解决DGSS任务。Rein方法依赖于一组可训练的标记，每个标记与特定实例对应，能够精确地细化并将特征图从每一层传递到骨干网络的下一层。这样，Rein能够在单张图像中为不同的类别生成多样化的细化结果。通过减少可训练的参数，Rein在微调VFM时，效果出乎意料地优于完全参数微调。通过广泛的实验验证，Rein显著超越了现有的最先进方法。值得一提的是，仅在冻结的骨干网络中增加1%的可训练参数，Rein便在Cityscapes数据集上达到了78.4%的mIoU，而且无需使用任何真实的城市场景数据集。代码已发布，您可以通过<a href="https://github.com/w1oves/Rein.git%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/w1oves/Rein.git访问。</a></p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-26_21-13-37"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-26_21-13-59"></p><p>​                                                                                <strong>模型结构图</strong></p><p><strong>本文的研究背景：</strong> </p><ul><li><strong>传统DGSS方法的局限</strong>：以往DGSS方法着重提升模型在多未见领域的预测准确性，但多采用VGGNet、MobileNetV2和ResNet等经典骨干网络，且依赖复杂数据增强和领域不变特征提取策略，对更强的VFMs在DGSS中的效能探索不足。 </li><li><strong>VFMs的潜力与挑战</strong>：近年来，CLIP、MAE、SAM等大规模VFMs显著提升了计算机视觉任务的性能，其在不同未知场景下展现出强大泛化能力。然而，将VFMs用于DGSS任务存在挑战，常用数据集规模远小于ImageNet，对VFMs大量可训练参数进行微调会导致泛化能力受限，且现有的参数高效微调策略大多不适用于DGSS。-</li><li><strong>研究动机</strong>：基于利用更强预训练模型和更少可训练参数实现更优泛化能力的动机，作者评估并利用VFMs进行DGSS研究，提出“Rein”微调方法，以高效利用VFMs解决DGSS问题。</li></ul><p><strong>研究现状：</strong></p><ul><li><strong>领域广义语义分割（DGSS）</strong>：传统方法聚焦提升模型跨多未见领域的预测准确性，采用复杂数据增强和领域不变特征提取策略，多使用VGGNet、MobileNetV2等旧骨干网络。 </li><li><strong>视觉基础模型（VFMs）</strong>：如CLIP、MAE、SAM等在计算机视觉挑战中表现出色，具有显著的跨场景泛化能力，但在DGSS任务中的表现缺乏专门研究。 </li><li><strong>参数高效微调（PEFT）</strong>：在自然语言处理领域取得成功，部分方法开始应用于计算机视觉，但大多不是为DGSS设计，难以对单张图像中不同实例的特征进行细化。</li></ul><p><strong>研究思路：</strong></p><p>本文聚焦于在领域泛化语义分割（DGSS）中利用视觉基础模型（VFMs），研究思路清晰，具体如下：</p><ol><li><strong>提出问题</strong>：先前DGSS方法多采用传统骨干网络，而大规模VFMs虽在计算机视觉挑战中表现出色，但在DGSS中的性能及利用方式尚不明确。因此，作者提出评估VFMs在DGSS中的性能以及如何有效利用VFMs的问题。</li><li><strong>构建框架</strong>：以利用更强预训练模型和更少可训练参数实现更优泛化能力为动机，作者引入<strong>Rein</strong>微调方法，在骨干网络层间嵌入该机制，以有效利用VFMs的强大能力。</li><li><strong>选择方法</strong>：选择CLIP、MAE、SAM、EVA02和DINOv2等五种不同训练策略和数据集的VFMs进行评估。设置“Full”和“Freeze”两个基本基线，并提出“Rein”方法。采用AdamW优化器，设置特定学习率、迭代次数、批量大小等进行训练。</li><li><strong>分析数据</strong>：在多个数据集和三种泛化设置下进行实验，对比Rein与现有DGSS和参数高效微调（PEFT）方法的性能。通过消融实验分析Rein各组件的有效性、令牌长度和秩对模型性能的影响，以及训练速度、GPU内存使用和模型存储要求。</li><li><strong>得出结论</strong>：实验表明，冻结的VFMs性能优于先前DGSS方法，Rein以更少可训练参数显著增强VFMs的泛化能力，大幅超越现有方法。证明了VFMs在DGSS领域的巨大潜力以及Rein方法的有效性。</li></ol><p><strong>本文的创新点：</strong> </p><ol><li><strong>评估并利用视觉基础模型（VFMs）</strong>：首次在**领域泛化语义分割（DGSS）**中评估多种VFMs，证实其强大泛化能力，为该领域建立重要基准。 </li><li><strong>提出“Rein”微调方法</strong>：通过可学习令牌对特征图进行实例级细化，以较少可训练参数有效利用VFMs，显著提升泛化性，超越现有方法。</li><li><strong>设计优化策略</strong>：采用层共享MLP权重和低秩token序列，减少参数冗余，提高训练效率。</li></ol><blockquote><p>写作启发：<strong>领域泛化语义分割（DGSS）</strong>、<strong>视觉基础模型（VFMs）</strong>、<strong>参数高效微调（PEFT</strong>）</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 领域泛化语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Vision Foundation Models </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LGAD Local and Global Attention Distillation for Efficient Semantic Segmentation</title>
      <link href="/post/lgad-local-and-global-attention-distillation-for-efficient-semantic-segmentation/"/>
      <url>/post/lgad-local-and-global-attention-distillation-for-efficient-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Shaoxing University、Central South University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Efficient <strong>semantic segmentation</strong> is essential for a wide array of computer vision applications, and knowledge distillation has emerged as a promising methodology for model compression and efficiency. However, we observed that an excess of positive pixels can dilute attention weights, hindering the student model’s learning process. To tackle this significant challenge, we introduce the Local and Global Attention Distillation (LGAD) framework, a pioneering block-based technique that distills both local and global attention. The LGAD framework segments feature maps and output probabilities into well-defined local and global blocks, effectively mitigating the dilution of attention weights. By doing so, it enhances the distinction between positive and negative pixels, particularly amplifying the focus on salient regions within each local and global block. We have conducted comprehensive experiments on three benchmark datasets, Cityscapes, CamVid, and Pascal VOC 2012. The experiment results demonstrate the effectiveness of our proposed LGAD and confirm its superiority over several state-of-the-art distillation methods for semantic segmentation.</p><p>::: tip</p><p><strong>正像素：通常表示目标、高亮度、激活区域或有效数据。</strong></p><p><strong>负像素：通常表示背景、低亮度、抑制区域或噪声&#x2F;无效数据</strong></p><p>:::</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>高效的语义分割对于广泛的计算机视觉应用至关重要，知识蒸馏已经成为一种很有前途的模型压缩和效率方法。然而，我们观察到过多的正像素会稀释注意力权重，阻碍学生模型的学习过程。为了应对这一重大挑战，我们引入了局部和全局注意力蒸馏(LGAD)框架，这是一种开创性的基于块的技术，可以提取局部和全局注意力。LGAD框架将特征图和输出概率分割为定义良好的局部和全局块，有效地减轻了注意力权重的稀释。通过这样做，它增强了正像素和负像素之间的区别，特别是放大了对每个局部和全局块内显著区域的关注。我们在cityscape、CamVid和Pascal VOC 2012三个基准数据集上进行了全面的实验。实验结果证明了我们所提出的语义分割方法的有效性，并证实了它比几种最先进的语义分割方法的优越性。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割在<strong>自动驾驶、机器人导航</strong>等众多实际应用中至关重要，深度卷积神经网络（DCNNs）成为主流方法，如PSPNet、DeepLab系列等。然而，这些模型存在存储和计算开销大的问题，限制了其在现实场景中的部署，因此开发紧凑且高效的分割模型成为研究热点。 知识蒸馏是一种有前景的模型压缩技术，可通过将大而复杂模型（教师模型）的知识转移到小模型（学生模型）来提升学生模型性能。已有研究者将知识蒸馏引入高效语义分割并提出多种框架，但现有知识蒸馏方法主要集中于全局交互的蒸馏。 研究发现过多正像素会稀释注意力权重，导致正、负像素注意力值差距小，阻碍学生模型识别和学习特征。如图1所示，正像素注意力值约为0.0004，背景像素近于零。为解决这一挑战，作者提出了**Local and Global Attention Distillation（LGAD）**框架，旨在通过将特征图和输出概率划分为局部和全局块，增强正、负像素注意力值差距，提升学生模型的语义分割性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>语义分割模型</strong>：<strong>深度卷积神经网络（DCNNs）<strong>成为主流方法，如PSPNet、DeepLab系列等取得了不错的性能，但存在存储和计算开销大的问题。为解决此问题，出现了一些</strong>轻量级框架</strong>，如ENet、SegNet等，还有基于<strong>Vision Transformer</strong>的语义分割框架。</li><li><strong>知识蒸馏</strong>：作为模型压缩的有效技术，被广泛应用于语义分割。现有方法主要集中在对齐中间特征图和输出概率，如SKDS、IFVD、IDD等，但大多聚焦于全局交互的蒸馏。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-44-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-44-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-44-32"></p><p>本文提出了用于高效**语义分割的局部和全局注意力蒸馏（Local and Global Attention Distillation，LGAD）**框架。</p><p><strong>模型组件</strong></p><ul><li><strong>局部注意力特征蒸馏（Local Attention Feature Distillation）</strong>：聚焦于蒸馏教师模型中间特征图中的有价值知识，让学生模型关注教师模型表示中的关键区域。</li><li><strong>局部注意力输出蒸馏（Local Attention Output Distillation）</strong>：旨在蒸馏分割输出中的有价值知识，使学生模型获得教师模型关注特定感兴趣区域并进行精确预测的能力。</li><li><strong>全局注意力特征蒸馏（Global Attention Feature Distillation）</strong>：与局部注意力特征蒸馏类似，但侧重于全局层面的特征蒸馏。</li><li><strong>全局注意力输出蒸馏（Global Attention Output Distillation）</strong>：与局部注意力输出蒸馏类似，但侧重于全局层面的输出蒸馏</li></ul><p><strong>损失函数</strong>：总损失函数为$L &#x3D; L_{seg} + \lambda_1 \cdot (L_{lafd} + L_{gafd}) + \lambda_2 \cdot (L_{laod} + L_{gaod})$，其中$L_{seg}$是语义分割的交叉熵损失，$\lambda_1 &#x3D; 30$和$\lambda_2 &#x3D; 3$是两个超参数，用于平衡LGAD框架中不同组件的影响。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：Cityscapes、Pascal VOC 2012、CamVid</p><p>评估指标：mIoU、模型参数、浮点运算次数（FLOPs）</p><ul><li><strong>Cityscapes</strong>：在验证集上，对于PSPNet - R18学生模型，LGAD将mIoU提升了7.12%，优于SKDS、IFVD、CWD和APD等方法；对于PSPNet - B0学生模型，LGAD使mIoU提升了7.39%，也超过了对比方法。</li><li><strong>CamVid</strong>：在测试集上，LGAD能提升两个学生模型的性能。对于PSPNet - R18，提升了2.9%，超过SKDS、IFVD和CWD；对于PSPNet - B0，提升了3.5%，同样优于对比方法。</li><li><strong>Pascal VOC 2012</strong>：在验证集上，LGAD大幅提升了两个学生模型的性能。对于PSPNet - R18，mIoU提升了5.39%；对于PSPNet - B0，提升了2.98%，均超过了对比方法。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-49.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-49.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-48-49"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-48-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-48-58"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-03.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-03.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-49-03"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>不同损失项的有效性</strong>：在Cityscapes验证集上，以PSPNet - R18为学生模型进行实验。结果表明，局部和全局注意力输出蒸馏损失（Llaod和Lgaod）能独立提升学生模型性能，且二者结合效果更佳；局部和全局注意力特征蒸馏损失（Llafd和Lgafd）同时应用时也能进一步提升性能；当四个蒸馏损失项都应用时，提升达到7.12%，验证了LGAD的有效性和整合局部与全局注意力蒸馏的重要性。</li><li><strong>窗口大小的影响</strong>：研究了不同局部块数量（P×P）和全局块数量（G×G）对蒸馏的影响，设置P &#x3D; G &#x3D; {1, 2, 4, 8}。结果显示，窗口数量为2×2时带来最高的mIoU值76.22%，随着窗口数量增加，学生模型的mIoU得分略有下降，这表明窗口数量过大时，学生模型可能难以有效学习。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-31_10-49-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-31_10-49-40"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者针对<strong>知识蒸馏</strong>在语义分割中存在的正像素过多稀释注意力权重、阻碍学生模型学习的问题，提出了<strong>Local and Global Attention Distillation（LGAD）框架</strong>。该框架将特征图和输出概率划分为块，设计局部和全局注意力蒸馏方法，增强了学生模型识别和学习判别特征的能力。 通过在Cityscapes、CamVid和Pascal VOC 2012三个基准数据集上的大量实验，验证了LGAD框架的有效性，且其性能优于多个现有最先进的知识蒸馏方法。作者认为该方法在语义分割领域引入了有意义的进展，为开发轻量级且准确的密集预测模型提供了有价值的见解。 </p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 注意力蒸馏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Class Tokens Infusion for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/class-tokens-infusion-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/class-tokens-infusion-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Weakly Supervised Semantic Segmentation (WSSS)</strong> relies on Class Activation Maps (CAMs) to extract spatial information from image-level labels. With the success of Vision Transformer (ViT), the migration of ViT is actively conducted in WSSS. This work proposes a novel WSSS framework with Class Token Infusion (CTI). By infusing the class tokens from images, we guide class tokens to possess class-specific distinct characteristics and global-local consistency. For this, we devise two kinds of token infusion: 1) Intra-image Class Token Infusion (I-CTI) and 2)Cross-image Class Token Infusion (C-CTI). In I-CTI, we infuse the class tokens from the same but differently augmented images and thus make CAMs consistent among var-<br>ious deformations (i.e. view, color). In C-CTI, by infusing the class tokens from the other images and imposing the resulting CAMs to be similar, it learns class-specific distinct characteristics. Besides the CTI, we bring the background (BG) concept into ViT with the BG token to reduce the false positive activation ofCAMs. We demonstrate the effectiveness ofour method on PASCAL VOC 2012 and MS COCO 2014 datasets, achieving state-of-the-art results in weakly supervised semantic segmentation. The code is available at <a href="https://github.com/yoon307/CTI">https://github.com/yoon307/CTI</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>弱监督语义分割(WSSS)依靠类激活图(CAMs)从图像级标签中提取空间信息。随着视觉transformer(ViT)的成功，视觉transformer的迁移在WSSS中得到了积极的开展。本文提出了一种基于类标记注入(CTI)的WSSS框架。通过注入来自图像的类标记，我们引导类标记具有特定于类的独特特征和全局-局部一致性。为此，我们设计了两种标记注入:1)图像内类标记注入(I-CTI)和2)跨图像类标记注入(C-CTI)。在I-CTI中，我们从相同但不同的增强图像中注入类标记，从而使cam在各种变形(即视图，颜色)之间保持一致。在C-CTI中，通过注入来自其他图像的类标记并强制生成的cam相似，它学习特定于类的独特特征。除了CTI之外，我们还通过BG标记将背景(BG)概念引入ViT，以减少cam的误报激活。我们在PASCAL VOC 2012和MS COCO 2014数据集上证明了我们的方法的有效性，在弱监督语义分割中取得了最先进的结果。代码可在<a href="https://github.com/yoon307/CTI%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/yoon307/CTI上获得。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法存在的问题，具体研究背景如下： </p><ul><li><strong>WSSS的兴起</strong>：全监督语义分割虽在多领域表现出色，但标注成本高、耗时长。为减轻标注负担，WSSS应运而生，其利用图像级标签、涂鸦和边界框等弱监督信息进行研究，其中仅利用图像级分类标签的设置最具实用性和挑战性。</li><li><strong>现有方法的局限性</strong>：传统WSSS研究多依赖卷积神经网络（CNNs）生成类激活图（CAMs），但CNN的感受野有限，导致CAMs存在稀疏性问题，仅关注物体的判别区域。Vision Transformer（ViT）虽能缓解该问题，但原始ViT使用单类令牌进行分类，定位图缺乏类别特异性，且基于多类令牌的ViT仍存在类令牌特征表示相关性高、CAMs过度扩展导致假阳性区域增加等问题。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：利用图像级标签、涂鸦、边界框等弱监督信息进行研究，其中基于图像级分类标签的研究最具挑战性。主要通过类激活图（CAMs）定位目标，为提高CAMs精度，出现了对抗擦除、局部 - 全局一致性等方法，也有不少工作对CAMs进行后处理以获取可靠标签。</li><li><strong>基于视觉Transformer（ViT）的WSSS</strong>：ViT凭借自注意力机制能捕捉长距离依赖，缓解了CAMs稀疏性问题。一些工作采用多类令牌或直接用补丁令牌训练分类器来提取特定类别的激活图。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>本文提出了一种基于视觉Transformer（ViT）的弱监督语义分割（WSSS）框架，该框架引入了类令牌注入（Class Token Infusion，CTI）和背景令牌（Background Token，BGT）两种方法，以解决传统多类令牌WSSS方法的局限性，生成更精确的类激活图（Class Activation Maps，CAMs）。</p><h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p>使用三个配对图像进行训练，将每个图像分割成 $N \times N$ 个补丁并嵌入为补丁令牌 $T_{patch}$，同时使用 $C$ 个前景类令牌 $T_{cls - fg}$ 和 1 个背景类令牌 $T_{cls - bg}$ 组成输入类令牌 $T_{cls}$。将类令牌和补丁令牌连接形成输入令牌 $T_{input}$，添加位置嵌入后输入到 $L$ 个Transformer块中。从最后一个Transformer块的补丁令牌输出 $T_{L_{patch}}$ 中获取CAMs $M$，并通过池化类令牌输出 $T_{L_{cls}}$ 得到类预测 $y_{pred}$，使用多标签软边缘损失计算分类损失 $L_{cls}$ 和补丁级分类损失 $L_{cls - patch}$。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-29_21-01-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-29_21-01-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-29_21-01-48"></p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><ol><li><strong>数据集</strong>：选用PASCAL VOC 2012和MS - COCO 2014这两个广泛使用的基准数据集。前者包含20个前景对象类和1个背景类，有10582、1449和1456张图像分别用于训练集、验证集和测试集；后者更具挑战性，有82k训练集和40k验证集，包含80个前景对象类和1个背景类。</li><li><strong>评估指标</strong>：采用平均交并比（mIoU）评估语义分割性能，在验证集上评估语义分割模型，在训练集上评估类激活图（CAMs）性能，PASCAL VOC 2012测试集结果通过在线官方服务器评估。</li></ol></blockquote><ol><li><strong>PASCAL VOC数据集</strong>：在训练集上，所提方法在CAMs（种子）和伪像素级真值（掩码）方面性能均优于其他方法，相比第二好的结果，种子性能提升1.8%p，掩码性能提升0.9%p。在语义分割性能上，基于高质量标签训练的模型在验证集和测试集上均大幅超越现有技术，且语义分割模型性能优于伪标签，比第二好的模型在验证集上有超过1.7%p的提升。</li><li><strong>MS COCO数据集</strong>：在该数据集上训练和评估模型，虽数据集类别多、场景复杂，但所提方法取得45.4%的mIoU，显示出良好的泛化能力，有效减少了ViT - based WSSS方法在该数据集上因错误激活和类间激活重叠导致的性能差距。</li></ol><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ol><li>组件分析<ul><li>引入背景类令牌（BGT）和背景CAMs概念到ViT，相比基线有2.9%p的提升。</li><li>提出的图像内类令牌注入（I - CTI）额外带来1.1%p的性能提升。</li><li>结合图像间类令牌注入（C - CTI），性能提升至69.5%。</li></ul></li><li><strong>背景类令牌的重要性</strong>：训练无BGT但有BG CAM的基线模型，结果比基线下降4.1%p，凸显BGT在训练BG CAM中的重要性。</li><li><strong>类令牌注入的作用</strong>：通过t - SNE可视化不同层的类令牌，表明所提方法的类令牌在各层特征空间区分度好，生成的CAMs更具独特性，不侵犯其他类区域，而基线的类令牌区分度差，CAMs存在错误激活区域。</li><li><strong>注入索引的影响</strong>：改变注入索引L1从2到10（总层数L为12），mIoU性能在L1设为3时最高，虽不同索引有轻微性能差异，但均高于无CTI的情况，因此将注入索引设为3。</li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者旨在增强ViT中类别令牌的特定类别表示能力，以实现图像中物体的清晰定位，得出以下结论：</p><ol><li><strong>提出CTI方法</strong>：提出两种类别令牌注入（CTI）方法，即图像内类别令牌注入（I - CTI）和跨图像类别令牌注入（C - CTI）。I - CTI使激活图具有全局 - 局部一致性，C - CTI让类别令牌和激活图具备跨图像的一致特定类别知识。 </li><li><strong>引入背景令牌</strong>：将背景令牌（BGT）引入ViT，有效解决了激活图过度扩展问题，减少了错误激活。</li><li><strong>实验验证有效性</strong>：在PASCAL VOC 2012和MS COCO 2014数据集上的大量实验结果，支持了所提方法的有效性和泛化性，该方法在两个数据集上均达到了最先进水平。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scribble-Supervised Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation</title>
      <link href="/post/use-universal-segment-embeddings-for-open-vocabulary-image-segmentation/"/>
      <url>/post/use-universal-segment-embeddings-for-open-vocabulary-image-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Bosch Research North America、Bosch Center for Artificial Intelligence (BCAI)</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>The <strong>open-vocabulary image segmentation</strong> task involves partitioning images into semantically meaningful segments and classifying them with flexible text-defined categories. The recent vision-based foundation models such as the Segment Anything Model (SAM) have shown superior performance in generating class-agnostic image segments. The main challenge in open-vocabulary image segmentation now lies in accurately classifying these segments into text-defined categories. In this paper, we introduce the Universal Segment Embedding (USE) framework to address this challenge. This framework is comprised of two key components: 1) a <strong>data pipeline</strong> designed to efficiently curate a large amount of segment-text pairs at various granularities, and 2) a <strong>universal segment embedding model</strong> that enables precise segment classification into a vast range oftext defined categories. The USE model can not only help open-vocabulary image segmentation but also facilitate otherdownstream tasks (e.g., querying and ranking). Through comprehensive experimental studies on semantic segmen-<br>tation and part segmentation benchmarks, we demonstrate that the USE framework outperforms state-of-the-art open-<br>vocabulary segmentation methods.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>开放词汇图像分割任务包括将图像划分为语义上有意义的片段，并使用灵活的文本定义类别对其进行分类。近年来，基于视觉的基础模型(如SAM)在生成与类别无关的图像片段方面表现出了优异的性能。目前，开放词汇表图像分割的主要挑战在于将这些片段准确地分类到文本定义的类别中。在本文中，我们引入了通用段嵌入(USE)框架来解决这一挑战。该框架由两个关键组件组成:1)一个数据解决方案，旨在有效地管理各种粒度的大量片段-文本对;2)一个通用的片段嵌入模型，能够将精确的片段分类到大量文本定义的类别中。USE模型不仅可以帮助开放词汇表图像分割，还可以促进其他下游任务(例如查询和排序)。通过对语义切分和零件切分基准的综合实验研究，我们证明了USE框架优于最先进的开放词汇切分方法。</p><p>::: tip</p><p>启发</p><p>:::</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>开放词汇图像分割的目的是将图像分割成语义上有意义的片段，并用文本定义的任意类对其进行分类。最近的基础模型在将图像的像素分为有意义的片段上效果显著，例如SAM，然而，现有的开放词汇图像分割方法面临着挑战：<strong>端到端的方法</strong>不能将基础模型生成的图像段作为输入或提示来分配类标签；<strong>两阶段的方法</strong>由于人类标签的限制，它们在分类不同粒度的片段方面仍然受到限制。基于上述的存在的问题，本文的作者提出了<strong>通用片段嵌入框架（Universal Segment Embedding，USE）</strong>，该框架由两个关键的组件：数据方案和通用片段嵌入模型。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li>多模态表示学习：</li></ul><p>从大规模图像-文本数据中学习(例如CLIP)在将视觉概念与文本描述联系起来方面显示出了很好的结果，然而，对于文本数据的多模态表示学习，目前的研究还很少。</p><ul><li>开放词汇图像分割：</li></ul><p>在<strong>自动驾驶</strong>等现实世界视觉任务需求日益增长的驱动下，开放词汇图像分割的重要性正在迅速增长，现有的方法可以分为两类:端到端方法和两阶段方法，但是上述两种方法都存在不足，不能满足本文的任务需求。</p><ul><li>改进图像-文本数据集：</li></ul><p>现有的工作可以分为两类:数据过滤和数据改进。数据过滤旨在通过过滤噪声的图像-文本对来提高模型训练的效率和鲁棒性，而数据改进则侧重于改善图像和文本数据的对齐。</p><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><strong>USE Data Pipeline：</strong></p><ol><li><strong>多粒度图像字幕生成</strong>：利用多模态大语言模型（MLLM），通过特定提示生成图像中对象及其属性的详细描述，以获取更丰富的语义信息。</li><li><strong>从字幕中进行指代表达定位</strong>：从字幕中提取名词短语并扩展为指代表达，使用开放词汇定位模型（如Grounding DINO）获取与这些表达对应的边界框，生成框 - 文本对。</li><li><strong>使用框提示生成掩码</strong>：将边界框转换为掩码，使用图像分割模型SAM生成掩码，并进行后处理，最后通过基于掩码的非极大值抑制（NMS）合并段 - 文本对。</li></ol><p><strong>USE Model：</strong></p><ol><li><strong>图像编码器</strong>：利用预训练的视觉变换器（ViTs）提取图像块嵌入，通过多级别特征合并，结合CLIP和DINOv2的信息，同时获取全局图像特征。在训练过程中，CLIP和DINOv2保持冻结，仅线性层归一化（LLN）模块和块尺度参数可训练。</li><li><strong>段嵌入头</strong>：根据输入段从图像块嵌入中提取段嵌入，并将其映射到视觉 - 语言联合空间。通过计算段在每个图像块内的面积并归一化，得到段在每个块内的权重，进而计算加权平均嵌入，最后通过线性层映射为段嵌入。</li><li><strong>训练与损失</strong>：使用段 - 文本对比损失来训练模型，在训练时，每个段随机采样一个文本描述来计算文本嵌入。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>训练集：COCO、Visual Genome (VG)       测试集：ADE20K、Pascal Context</p><blockquote><p>评估方法：使用类无关掩码，通过提示SAM生成掩码，经过滤和合并后，用模型获取掩码嵌入，计算与目标类文本嵌入的相似度，转换为概率，聚合像素上所有片段的概率进行类别预测。</p><p>对比结果：与最先进的开放词汇语义分割方法在<strong>ADE20K和Pascal Context数据集</strong>上对比，以平均交并比（mIoU）评估性能。结果表明，USE方法在所有数据集上大幅优于最先进的两阶段方法，与端到端方法相比平均性能最佳，在COCO和VG图像上训练时性能进一步提升。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-31-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-31-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-26_14-31-25"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>预训练骨干网络选择</strong>：在ADE20K数据集和开放词汇语义分割任务上研究，结果表明结合CLIP和DINOv2可获得性能提升。</li><li><strong>图像编码器架构设计</strong>：研究cls令牌对性能的影响，结果显示包含cls令牌可提高mIoU。</li><li><strong>定性比较</strong>：对比从真实标注和MLLM增强标注中提取的对象，MLLM增强标注能捕获更细粒度的对象和部件。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-33-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-26_14-33-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-26_14-33-30"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出用于<strong>开放词汇图像分割的USE框架</strong>，通过实验研究得出以下结论：</p><blockquote><ol><li><strong>方法有效性</strong>：该框架结合精心设计的数据管道和轻量级嵌入模型，能在无人工标注下以零样本方式有效对图像片段进行分类。 </li><li><strong>性能优越性</strong>：在语义分割和部件分割任务的实验中，USE框架在<strong>ADE20K、Pascal Context</strong>和<strong>PartImageNet</strong>等数据集上，大幅超越现有最先进的两阶段方法，平均性能也优于端到端方法。 </li><li><strong>研究意义</strong>：此工作为构建开放词汇图像分割的基础模型和基于片段的表征学习提供了参考，有望推动相关领域的研究发展。</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 开放词汇图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Universal Segment Embeddings </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation</title>
      <link href="/post/llmformer-large-languagemodel-for-open-vocabulary-semantic/"/>
      <url>/post/llmformer-large-languagemodel-for-open-vocabulary-semantic/</url>
      
        <content type="html"><![CDATA[<p>Hunan University、Monash University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Open-vocabulary (OV) semantic segmentation</strong> has attracted increasing attention in recent years, which aims to recognize<br>objects in an open class set for real-world applications. While prior OV semantic segmentation approaches have relied on<br>additional semantic knowledge derived from vision-language (VL) pre-training, such as the popular CLIP model, this paper<br>introduces a novel paradigm by harnessing the unprecedented capabilities of large language models (LLMs). Inspired by<br>recent breakthroughs in LLMs that provide a richer knowledge base compared to traditional vision-language pre-training, our proposed methodology capitalizes on the vast knowledge embedded within LLMs for OV semantic segmentation. Particularly, we partition LLM knowledge into object, attribute, and relation priors, and propose three novel attention modules-semantic, scaled visual, and relation attentions, to utilize the LLM priors. Extensive experiments are conducted on common benchmarks including ADE20K (847 classes) and Pascal Context (459 classes). The results show that our model outperforms previous state-of-the-art (SoTA) methods by up to 7.2% absolute. Moreover, unlike previous VL-pre-training-based works, our method can even predict OV segmentation results without target candidate classes.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>开放词汇语义分割近年来受到越来越多的关注，其目的是为了在现实应用中识别开放类集中的对象。虽然之前的开放词汇语义分割方法依赖于来自视觉语言(VL)预训练的额外语义知识，例如流行的CLIP模型，但本文通过利用大型语言模型(大语言模型)前所未有的能力引入了一种新的范式。与传统的视觉语言预训练相比，大语言模型提供了更丰富的知识库，受其最新突破的启发，我们提出的方法利用大语言模型中嵌入的大量知识进行开放词汇语义分割。特别地，我们将大语言模型知识划分为对象先验、属性先验和关系先验，并提出了语义关注、尺度视觉关注和关系关注三个新的关注模块来利用大语言模型先验。在包括ADE20K(847个类)和Pascal Context(459个类)在内的常见基准测试上进行了广泛的实验。结果表明，我们的模型比以前的最先进的(SoTA)方法高出7.2%。此外，与之前基于vl预训练的工作不同，我们的方法甚至可以在没有目标候选类的情况下预测开放词汇分割结果。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>基于先验的语义分割方法可以识别固定集的目标种类，但不能够很好的处理真实世界中各种新的目标，由此引出开放词汇语义分割。开放词汇语义分割分为两种：一阶段和两阶段，尽管取得了不错的效果，但是大多数还是利用预训练的视觉语言模型提取embeding，这种方式仅提供有限的语义信息。随着大语言模型的发展，由于其提供了对场景的综合理解能力，本文作者尝试采用LLM的知识解决开放词汇语义分割中的挑战，即利用大语言模型描述中的目标名字、目标属性和目标关系。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_15-58-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_15-58-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_15-58-54"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>固定集语义分割</strong>：早期采用CNN架构，后引入多尺度组合、全局上下文建模和transformer等方法，但难以利用大预训练模型知识，且只能识别固定集对象。</li><li><strong>开放词汇语义分割</strong>：分为单阶段和两阶段方法。两阶段方法依赖训练良好的掩码生成器，计算成本高；单阶段方法虽有改进，但大多仅从视觉 - 语言预训练模型提取知识，语义信息有限。</li><li><strong>大语言模型</strong>：在许多领域取得成功，具备综合复杂推理能力，部分模型可理解视觉内容，但主要用于通用表示和预测</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-00-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-00-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-00-15"></p><ol><li><p>整体架构</p><p>LLMFormer由三个主要部分组成：</p><ul><li><strong>图像特征提取（Image Feature Extraction）</strong>：使用多模态大语言模型（MLLM）中的视觉编码器、适配器（ViT Adapter）和多尺度可变形注意力（MSDA）模块，以捕获多尺度的图像特征。</li><li><strong>LLM先验提取（LLM Prior Extraction）</strong>：通过向LLM输入问题（如“描述图像”），获取图像的全面描述，并利用语言解析工具从中提取对象、属性和关系先验知识。</li><li><strong>LLM先验引导的分割（LLM-Prior-Guided Segmentation）</strong>：引入了三种新型注意力模块，分别是语义注意力（Semantic Attention）、缩放视觉注意力（Scaled Visual Attention）和关系注意力（Relation Attention），以利用LLM的先验知识进行开放词汇语义分割。</li></ul></li><li><p>关键组件</p></li></ol><ul><li><strong>语义注意力（Semantic Attention）</strong>：将对象和属性先验知识嵌入到掩码嵌入中，以增强开放词汇对象的发现、掩码预测和分类能力。该模块通过多头交叉注意力机制，捕捉对象先验与掩码之间的对应关系。</li><li><strong>缩放视觉注意力（Scaled Visual Attention）</strong>：基于属性先验知识，为每个掩码选择合适的视觉特征图，以更好地分割不同大小的对象。具体来说，该模块根据属性先验生成掩码的属性嵌入，并通过多层感知机（MLP）预测尺度选择分数，从而选择合适尺度的特征图。</li><li><strong>关系注意力（Relation Attention）</strong>：利用LLM的关系先验知识，学习掩码之间的关系。该模块通过生成对象关系图，并将其映射到掩码级别，然后通过多头自注意力机制将关系先验知识编码到掩码嵌入中。</li></ul><p>::: tip</p><p>重要</p><p>:::</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：训练：COCO-Stuff，测试：ADE20K、Pascal Context、Pascal VOC</p></blockquote><ul><li><strong>Pascal Context数据集</strong>：与FC - CLIP相比，在PC - 459和PC - 59上分别提高了7.2%和5.8%；与SAN相比，分别提高了8.3%和4.0%。</li><li><strong>ADE20K数据集</strong>：在A - 847和A - 150上均达到了最先进的性能，超过FC - CLIP分别为1.7%和4.4%，超过使用VIT - L骨干的SAN分别为2.8%和5.2%。</li><li><strong>Pascal VOC 2012数据集</strong>：取得了最佳的mIoU，显著超过之前的SOTA方法FC - CLIP 1.4%。在Open IoU指标下也有显著提升，证明了模型的开放词汇能力。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-04-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-04-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-04-53"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-00.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-00.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-05-00"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-24_16-05-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-24_16-05-17"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>主要组件的影响</strong>：语义注意力模块能显著提升性能，基于大语言模型（LLM）属性的尺度选择和关系注意力可进一步提高性能，完整模型在A - 847和A - 150上取得最佳性能。</li><li><strong>语义注意力</strong>：使用对象先验能持续提高开放词汇分割性能，属性先验可进一步增强结果，交叉注意力在整合先验的方法中表现最优。</li><li><strong>缩放视觉注意力</strong>：多尺度方法优于单尺度方法，基于属性先验的尺度选择方法优于其他非选择方法。</li><li><strong>关系注意力</strong>：完整模型通过整合LLM关系先验到自注意力图中，在A - 847和A - 150上分别比仅使用原始注意力图的模型提高了1.7%和2.3%。</li><li><strong>不同注意力方法</strong>：模型的注意力方法显著优于掩码注意力和TSG注意力，因为它们能利用LLM先验来改进开放词汇语义分割。</li><li><strong>成本比较</strong>：与Zegformer和OV - Seg相比，模型的可训练参数更少，同时精度更高。使用LLAVA - Phi2 - 2.7B可进一步降低计算开销，性能仅有轻微下降。</li><li><strong>不同提示</strong>：详细提示（描述所有对象、属性和关系）能进一步提高分割性能，但实验中主要使用简单提示。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><blockquote><p>作者提出了LLMFormer这一利用大语言模型（LLM）知识进行开放词汇语义分割（OV）的新方法，并得出以下结论： </p></blockquote><ol><li><strong>方法有效性</strong>：提出语义、缩放视觉和关系三种注意力模块，利用LLM的对象、属性和关系先验知识进行分割，大量实验证明LLMFormer及各注意力模块有效。 </li><li><strong>性能优势</strong>：在ADE20K、Pascal Context和Pascal VOC等数据集上显著优于现有方法，绝对提升最高达7.2%，还能在无预定义候选类别的情况下预测OV分割结果，更适用于实际应用。</li><li><strong>未来方向</strong>：当前工作虽提升了OV识别能力，但LLM参数多致速度慢，且存在细粒度分类和边界分割问题，未来将研究效率、细粒度分类和分割问题。</li></ol><blockquote><p>不足及展望：使用LLM带来了巨大的参数量，导致速度减小；本文当前的工作关注于提升开放词汇的分类能力，其他的细粒度分类和边界框分割问题没有涉及，这也是未来工作的研究方法。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 开放词汇语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMFormer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation</title>
      <link href="/post/corrmatch-label-propagation-via-correlation-matching-for-semi-supervised-semantic-segmentation/"/>
      <url>/post/corrmatch-label-propagation-via-correlation-matching-for-semi-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Nankai University、NKIARI, Shenzhen Futian、SICE, UESTC</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>This paper presents a simple but performant semi-supervised semantic segmentation approach, called CorrMatch. Previous approaches mostly employ complicated training strategies to leverage unlabeled data but overlook the role ofcorrelation maps in modeling the relationships between pairs oflocations. We observe that the correlation maps not only enable clustering pixels ofthe same category easily but also contain good shape information, which previous works have omitted. Motivated by these, we aim to improve the use efficiency of unlabeled data by designing two novel label propagation strategies. First, we propose to conduct pixel propagation by modeling the pairwise similarities of pixels to spread the high-confidence pixels and dig out more. Then, we perform region propagation to enhance the pseudo labels with accurate class-agnostic masks extracted from the correlation maps. CorrMatch achieves great performance on popular segmentation benchmarks. Taking the DeepLabV3+ with ResNet-101 backbone as our segmentation model, we receive a 76%+ mIoU score on the Pascal VOC 2012 dataset with only 92 annotated images. Code is available at <a href="https://github.com/BBBBchan/CorrMatch">https://github.com/BBBBchan/CorrMatch</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>本文提出了一种简单但高性能的半监督语义分割方法 CorrMatch。现有方法大多采用复杂的训练策略来利用未标注数据，但忽视了关联图在建模像素位置关系中的重要作用。我们发现，关联图不仅能够轻松实现同类像素的聚类，还包含了被以往研究忽略的优质形状信息。基于这些观察，我们设计了两种创新的标签传播策略来提升未标注数据的使用效率。首先，我们提出<strong>像素传播策略</strong>，通过建模像素对的相似性关系来扩展高置信度像素区域，并挖掘更多潜在的高置信度像素。其次，我们开发了<strong>区域传播策略</strong>，通过从关联图中提取精确的类别无关掩码来增强伪标签质量。CorrMatch 在主流分割基准测试中表现优异：当使用 ResNet-101 为主干的 DeepLabV3+ 模型时，在仅含 92 张标注图像的 Pascal VOC 2012 数据集上实现了 76%+ 的 mIoU。代码已开源：<a href="https://github.com/BBBBchan/CorrMatch%E3%80%82">https://github.com/BBBBchan/CorrMatch。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦半监督语义分割领域，旨在解决深度学习方法对大规模像素级标注数据集的依赖问题，具体研究背景如下： </p><ul><li><strong>标注成本高</strong>：基于深度学习的语义分割方法通常需要大量像素级标注图像，但准确标注分割数据集成本高、耗时长，限制了其应用。 </li><li><strong>半监督学习受关注</strong>：为减少对大规模准确标注数据的需求，研究者提出弱监督、半监督和无监督分割方法。其中，半监督语义分割仅需少量标注数据和大量未标注数据进行训练，更接近现实场景，受到广泛关注。 </li><li><strong>现有方法存在不足</strong>：现有半监督语义分割方法多采用复杂训练策略，如Mean Teacher架构或自训练策略，需要额外网络或训练阶段，增加了训练复杂度。此外，常用的固定阈值筛选伪标签方法难以有效利用未标注数据。</li><li><strong>Correlation map的潜力</strong>：像素间的相关性可反映成对相似性，相关性地图不仅能轻松聚类同一类别的像素，还包含良好的形状信息，但以往研究忽略了其在建模位置对关系中的作用。 基于以上背景，作者提出了CorrMatch方法，通过设计两种新颖的标签传播策略，提高未标注数据的使用效率。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>方法多样</strong>：在半监督语义分割领域，已有多种方法被提出，如采用Mean Teacher架构的方法（U2PL、PS - MT等）、基于自训练策略的方法（ST++、SimpleBase等），以及近期的UniMatch等单阶段框架。</li><li><strong>成果显著</strong>：这些方法在一些公开数据集（如Pascal VOC 2012、Cityscapes）上取得了一定成果，推动了半监督语义分割技术的发展。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-03-57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-03-57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-03-57"></p><ol><li><strong>模型框架</strong>：CorrMatch是一个单阶段框架，基于具有弱到强一致性正则化的简单框架构建。对于有标签图像，使用标准的交叉熵损失；对于无标签图像，主要通过强制预测一致性来利用，同时考虑弱增强和强增强图像在高置信区域的对数似然一致性。</li><li>标签传播策略<ul><li><strong>像素传播</strong>：通过计算相关图并将其传播到预测中，增强模型对像素对之间相似性的整体感知，从而提高无标签数据的利用率。具体步骤为：首先通过网络编码器后的线性层提取特征，计算特征向量对之间的相关性得到相关图；然后将相关图传播到模型的对数似然输出中，得到另一种预测表示；最后计算该预测表示与高置信伪标签之间的相关损失作为监督。</li><li><strong>区域传播</strong>：利用相关图中隐含的形状信息来增强伪标签。具体做法是将相关图的每一行进行归一化并转换为二值图，当二值图与高置信区域有较大重叠时，计算高置信形状内每个唯一类别的数量，找到最显著的类别，并将该类别传播到增强的伪标签和扩展的高置信区域中。为了提高效率，采用随机采样的方法。</li></ul></li><li>其他策略<ul><li><strong>动态阈值</strong>：使用与训练过程相关的动态阈值策略，避免固定阈值过严或过松对模型收敛的不利影响。通过指数移动平均（EMA）根据对数似然输出迭代更新阈值。</li><li><strong>损失函数</strong>：整体目标函数是监督损失和无监督损失的组合。监督损失是基本监督损失和监督相关损失的组合；无监督损失包括无监督硬损失、软损失和相关损失。</li></ul></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p>数据集：Pascal VOC 2012 、Cityscapes</p><ul><li><strong>经典Pascal VOC 2012</strong>：在不同分割比例下与其他最先进方法比较，CorrMatch在全分割比例下mIoU达到81.8%，且在各分割比例上均优于现有方法，如比UniMatch在各分割比例上分别高出1.2%、1.3%、0.6%、0.7%和0.6%。</li><li><strong>aug Pascal VOC 2012</strong>：在不同训练尺寸和分割比例下进行实验，结果显示CorrMatch始终优于现有最佳方法。例如，在321×321训练尺寸下，比监督基线在1&#x2F;16、1&#x2F;8和1&#x2F;4分割比例上分别提高12.0%、7.4%和5.5%，比UniMatch在各分割比例上分别高出1.1%、0.8%和1.1%。</li><li><strong>Cityscapes</strong>：采用滑动窗口评估和在线难例挖掘（OHEM）损失技术，CorrMatch在所有分割比例下均优于其他方法，比UniMatch在1&#x2F;16、1&#x2F;8、1&#x2F;4和1&#x2F;2分割比例上分别高出0.7%、0.6%、0.2%和0.9%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-09.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-09.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-08-09"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-08-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-08-15"></p><p>: : : important</p><p>重要！！！</p><p>: : :</p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>组件有效性</strong>：验证了CorrMatch不同组件的有效性，包括硬无监督损失、软损失、标签传播等。完整的CorrMatch在92和1464分割比例下mIoU分别达到76.4%和81.8%，比基线分别提高2.8%和1.8%。动态阈值策略与标签传播策略配合良好。</li><li><strong>标签传播策略的影响</strong>：像素传播策略带来了一定的性能提升，区域传播策略进一步提高了性能。例如，像素传播策略在92、366和1464分割比例上分别提高1.4%、0.4%和0.8%，区域传播策略在此基础上分别再提高0.6%、0.5%和0.5%。</li><li><strong>特征提取位置</strong>：默认从骨干网络提取特征，实验表明使用骨干网络特征的性能始终优于其他位置。</li><li><strong>不同采样策略</strong>：比较了随机采样和均匀采样方法，随机采样效果更好，其中随机采样128个样本时性能最佳。</li><li><strong>不同初始值</strong>：基于EMA的阈值更新策略对不同初始值不敏感，在训练早期所有阈值都会快速趋近相似值。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-09-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-09-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-09-06"></p><h1 id="其他实验"><a href="#其他实验" class="headerlink" title="其他实验"></a>其他实验</h1><ul><li><strong>统计分析</strong>：统计挖掘比例和有效伪标签比例，结果表明使用标签传播策略后，这两个比例显著高于未使用时，说明未标记数据的利用率得到有效提高。</li><li><strong>定性分析</strong>：可视化结果显示，使用标签传播策略后，高置信区域的像素数量和完整性明显优于未使用时，能够有效扩展高置信区域并填充正确类别。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-10-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-22_15-10-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-22_15-10-04"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种名为<strong>CorrMatch的半监督语义分割方法</strong>，通过实验分析得出以下结论： </p><ul><li><strong>策略有效</strong>：重新考虑了相关图的使用，设计了<strong>像素传播和区域传播</strong>两种标签传播策略，能利用相关图中的相似性和形状信息，显著扩大高置信度区域，有效提升伪标签的整体质量。 </li><li><strong>效率提升</strong>：这些策略使模型能更高效地利用未标记数据，解决了传统方法中阈值选择困难、未标记数据利用不充分等问题。 </li><li><strong>性能优越</strong>：在Pascal VOC 2012和Cityscapes等数据集上，CorrMatch始终优于其他现有方法，取得了新的最先进性能，且推理过程无额外计算负担。</li></ul><blockquote><p>我们提出了CorrMatch，它可以利用标签传播和相关匹配来发现更准确的高置信度区域，用于半监督语义分割。CorrMatch的主要贡献是重新考虑相关映射的使用，并设计了两种标签传播策略来丰富伪标签。利用这些策略，CorrMatch显著扩展了高置信度区域，从而可以更有效地利用未标记的数据。实验证明了我们的cormatch算法优于其他方法。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 半监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semi-Supervised Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation</title>
      <link href="/post/cc4s-encouraging-certainty-and-consistency-in-scribble-supervised-semantic-segmentation/"/>
      <url>/post/cc4s-encouraging-certainty-and-consistency-in-scribble-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="Peking-University、Shandong-Universit"><a href="#Peking-University、Shandong-Universit" class="headerlink" title="Peking University、Shandong Universit"></a>Peking University、Shandong Universit</h2><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Deep learning-based</strong> solutions have achieved impressive performance in semantic segmentation but often require large<br>amounts of training data with fine-grained annotations. To alleviate such requisition, a variety of weakly supervised annotation<br>strategies have been proposed, among which scribble supervision is emerging as a popular one due to its user-friendly annotation way. However, the sparsity and diversity of scribble annotations make it nontrivial to train a network to produce deterministic and consistent predictions directly. To address these issues, in this paper we propose holistic solutions involving the design of network structure, loss and training procedure, named <strong>CC4S</strong> to improve Certainty and Consistency for Scribble-Supervised Semantic Segmentation. Specifically, to reduce uncertainty, CC4S embeds a random walkmodule into the network structure to make neural representations uniformly distributed within similar semantic regions, which works together with a soft entropy loss function to force the network to produce deterministic predictions. To encourage consistency, CC4S adopts self-supervision training and imposes the consistency loss on the eigenspace of the probability transition matrix in the random walk module (we named neural eigenspace). Such self-supervision inherits the category-level discriminability from the neural eigenspace and meanwhile helps the network focus on producing consistent predictions for the salient parts and neglect semantically heterogeneous backgrounds. Finally, to further improve the performance, CC4S uses the network predictions<br>as pseudo-labels and retrains the network with an extra color constraint regularizer. From comprehensive experiments, CC4S<br>achieves comparable performance to those from fully supervised methods and shows promising robustness under extreme supervision cases.</p><p>代码： <a href="https://github.com/panzhiyi/CC4S">https://github.com/panzhiyi/CC4S</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>基于深度学习的方法在语义分割中取得了令人印象深刻的性能，但通常需要大量带有细粒度标注的训练数据。为了减少这种需求，研究者提出了多种弱监督标注策略，其中<strong>涂鸦监督</strong>因其用户友好的标注方式而逐渐流行。然而，涂鸦标注的稀疏性和多样性使得直接训练网络生成确定且一致的预测具有挑战性。为解决这些问题，本文提出了包含网络结构设计、损失函数和训练流程的完整解决方案——CC4S（提升涂鸦监督语义分割确定性与一致性的方法）。具体而言，为降低不确定性，CC4S在网络架构中嵌入<strong>随机游走模块</strong>，使神经表征在相似语义区域内均匀分布。该模块与软熵损失函数共同作用，迫使网络生成确定性预测结果。为增强一致性，CC4S采用自监督训练策略，在随机游走模块的概率转移矩阵特征空间（称为神经特征空间）中施加一致性损失。这种自监督机制既继承了神经特征空间的类别级判别能力，又能促使网络专注于对显著区域生成一致预测，同时忽略语义异构的背景区域。为进一步提升性能，CC4S将网络预测结果作为伪标签，通过引入额外的色彩约束正则化项对网络进行重训练。综合实验表明，CC4S取得了与全监督方法相媲美的性能，在极端监督条件下也展现出良好的鲁棒性。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于涂鸦监督语义分割领域，旨在解决该领域存在的问题，其研究背景主要如下：</p><ul><li><strong>数据标注难题</strong>：基于深度学习的语义分割方法虽表现出色，但需大量细粒度标注的训练数据。以Cityscapes为例，手动生成像素级语义分割标注平均耗时3 - 5分钟，收集大规模标注数据集并非易事。</li><li><strong>弱监督方法兴起</strong>：为缓解数据标注压力，多种弱监督标注策略应运而生，如<strong>图像级监督、边界框监督、点监督和涂鸦监督</strong>等。其中，涂鸦监督因<strong>标注方式友好</strong>且能提供有效监督信息，受到越来越多关注。</li><li><strong>涂鸦监督现存问题</strong>：尽管涂鸦监督语义分割取得了一定进展，但仍存在预测结果不确定和不一致的问题。标注稀疏会导致预测结果不确定，而标注的多样性会使网络难以学习到稳定一致的分割模式，从而产生不一致的预测结果。 基于以上背景，作者提出了CC4S方法，以提高涂鸦监督语义分割的确定性和一致性，减少标注稀疏和多样性带来的影响。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-12-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-12-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-12-13"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><blockquote><p>弱监督语义分割：分为四种，图像级监督、边界框监督、点监督和涂鸦监督。图像级监督仅为整个图像提供类别标签，缺乏定位信息；边界框监督仍然缺乏可靠和有效的措施来产生高质量的物体掩膜；点监督通过在每个图像对象内标记带有类别信息的点来完成注释；涂鸦监督是一种用户友好的弱监督形式。</p></blockquote><blockquote><p>涂鸦监督语义分割：现有方法包括利用辅助任务信息、图割算法传播标注、在损失函数引入分割正则化等，但仍存在预测不确定和不一致问题。</p></blockquote><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p>本文提出了一种名为<strong>CC4S（Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation）<strong>的模型，旨在解决涂鸦监督语义分割任务中预测结果的</strong>不确定性和不一致性</strong>问题</p><p>核心网络包含两个模块：</p><ul><li><strong>ResNet骨干网络</strong>：用于提取图像的特征。</li><li><strong>相似度测量模块（SMM）</strong>：计算每两个神经元之间的转移概率，形成转移矩阵。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-24-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-24-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-24-44"></p><h4 id="减少神经表示的不确定性"><a href="#减少神经表示的不确定性" class="headerlink" title="减少神经表示的不确定性"></a>减少神经表示的不确定性</h4><h4 id="神经特征空间的自监督学习"><a href="#神经特征空间的自监督学习" class="headerlink" title="神经特征空间的自监督学习"></a>神经特征空间的自监督学习</h4><h4 id="带有颜色约束的伪标签再训练"><a href="#带有颜色约束的伪标签再训练" class="headerlink" title="带有颜色约束的伪标签再训练"></a>带有颜色约束的伪标签再训练</h4><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：Pascal VOC 2012 and Pascal Context</p><p>比较的方法：Scribblesup, RAWKS, NCL, GraphNet，KCL, BPG, URSS, PSI, SPML, A2GNN, DBFNet, PCE , CCL , TEL and CDL</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-30-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-30-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-30-25"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-29-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-20_16-29-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-20_16-29-58"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者通过研究得出以下结论：</p><ol><li>仅使用涂鸦注释进行语义分割会导致预测结果<strong>不确定和不一致</strong>。为此，提出了两种策略，即减少神经表示的不确定性以产生可靠结果，以及在神经特征空间进行自监督以保证输出的一致性。</li><li>结合<strong>伪标签再训练</strong>，该方法达到了最先进的性能，甚至可与全标签监督方法相媲美，且整个过程无需额外信息或注释准备要求。</li><li>大量的消融实验和中间可视化验证了所提解决方案的有效性。</li><li>该方法在涂鸦<strong>随机丢弃或按比例缩小</strong>的困难情况下也能表现良好，具有较强的鲁棒性。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 涂鸦监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scribble-Supervised Semantic Segmentation </tag>
            
            <tag> CC4S </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings</title>
      <link href="/post/scaling-upmulti-domain-semantic-segmentation-with-sentence/"/>
      <url>/post/scaling-upmulti-domain-semantic-segmentation-with-sentence/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>The <strong>state-of-the-art semantic segmentation methods</strong> have achieved impressive performance on predefined close-set individual datasets, but their generalization to zero-shot domains and unseen categories is limited. Labeling a large-scale dataset is challenging and expensive, Training a robust semantic segmentation model on multi-domains has drawn much attention. However, inconsistent taxonomies hinder the naive merging of current publicly available annotations. <strong>To address this, we propose a simple solution to scale up the multi-domain semantic segmentation dataset with less human effort</strong>. We replace each class label with a sentence embedding, which is a vector-valued embedding of a sentence describing the class. This approach enables the merging of multiple datasets from different domains, each with varying class labels and semantics. We merged publicly available noisy and weak annotations with the most finely annotated data, over 2 million images, which enables training a model that achieves performance equal to that of state-of-the-art supervised methods on 7 benchmark datasets, despite not using any images therefrom. Instead of manually tuning a consistent label space, we utilized a vector-valued embedding of short paragraphs to describe the classes. By fine-tuning the model on standard semantic segmentation datasets, we also achieve a significant improvement over the state-of-the-art supervised segmentation on NYUD-V2 (Silberman et al., in: European conference on computer vision, Springer, pp 746–760, 2012) and PASCAL-context (Everingham et al. in Int J Comput Visi 111(1):98–136, 2015) at 60% and 65% mIoU, respectively. Our method can segment unseen labels based on the closeness of language embeddings, showing strong generalization to unseen image domains and labels. Additionally, it enables impressive performance improvements in some adaptation applications, such as depth estimation and instance segmentation. Code is available at <a href="https://github.com/YvanYin/SSIW">https://github.com/YvanYin/SSIW</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>当前最先进的语义分割方法在预设的封闭数据集上表现出色，但其在零样本（zero-shot）领域和未见过类别上的泛化能力仍然有限。由于大规模数据标注既困难又昂贵，开发跨领域通用的鲁棒语义分割模型成为研究热点。然而，现有公开数据集的不同分类标准阻碍了它们的直接融合。为此，我们提出了一种高效扩展多领域语义分割数据集的方法：用文本嵌入（text embedding）替代传统类别标签，这种向量化的语义表示可以融合不同领域、不同标签体系的数据。通过整合包含 200 万张图像的精细标注数据与公开的带噪声弱标注数据，我们训练的模型在 7 个主流测试集上达到了监督学习的顶尖水平，尽管完全没有使用这些测试集的训练图像。与人工统一标签体系不同，我们通过语言模型生成短文本描述来表征类别语义。在标准数据集微调后，模型在 NYUD-V2 (Silberman et al., 2012) 和 PASCAL-context (Everingham et al., 2015) 上分别取得 60% 和 65% 的平均交并比（mIoU），显著超越现有监督方法。该方法通过计算语义相似度实现未见过标签的分割，展现出优异的跨领域泛化能力，**同时在深度估计、实例分割等下游任务中带来显著性能提升。**代码已开源：<a href="https://github.com/YvanYin/SSIW">https://github.com/YvanYin/SSIW</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割是计算机视觉的基础任务，在自动驾驶、农业机器人和医学等领域应用广泛。当前语义分割方法虽在预定义封闭集数据集上表现出色，但存在明显局限：</p><ol><li><strong>泛化能力不足</strong>：这些方法假设测试集中的所有类别都在训练时出现，然而现实场景并非如此，且模型受限于训练数据集的图像领域，难以泛化到新领域和标签。</li><li><strong>数据集合并难题</strong>：训练多领域语义分割模型是提升模型鲁棒性和泛化能力的自然途径，但直接合并不同领域的数据集会导致标签分类体系冲突，手动统一标签集和重新标注的方法不仅费力，在开放集场景下也存在局限性。</li><li><strong>现有零样本方法的缺陷</strong>：现有解决开放集问题的方法多在小数据集上实验，限制了其在现实场景中的应用潜力。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>语义分割</strong>：深度学习方法在特定高质量数据集上取得显著成果，但泛化能力受限。如FCN开启全卷积方法，后续ResNet、Transformer等推动性能提升。</li><li><strong>零样本语义分割</strong>：分为判别式和生成式方法，前者如Xian等将像素特征转换到语义词嵌入空间，后者如ZS3Net用生成模型生成像素特征。</li><li><strong>跨领域密集预测</strong>：有方法合并分割数据集提升性能和泛化能力，如Ros合并六个驾驶数据集，Lambert提出统一分类法合并多领域数据集。</li><li><strong>零样本学习标签编码</strong>：许多方法为类别标签生成语义嵌入，如Bucher用Word2Vec编码标签，Lseg用语言嵌入监督类别标签。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><ol><li><ul><li><p><strong>创建语言嵌入</strong>：从Wikipedia收集每个类别的简短描述，使用CLIP - ViT语言模型将这些描述编码为向量值的句子嵌入。这种方式能保留标签之间的语义关系，相比单字标签嵌入，更能反映类别间的语义相似性，有助于零样本标签的分割。</p></li><li><p><strong>混合数据的异构约束</strong>：为解决合并数据集中标注质量不平衡的问题，提出了异构损失函数。</p></li></ul></li></ol><ul><li><strong>高质量标注数据集</strong>：对所有样本施加像素级损失。<ul><li><strong>粗标注数据集（如OpenImages）</strong>：通过自适应阈值，对高置信度样本施加损失，忽略噪声较大的部分。</li></ul></li><li><strong>弱标注数据集（如Objects365）</strong>：采用蒸馏方法，利用CLIP分类模型的知识，对分割模型进行监督。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_09-38-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_09-38-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_09-38-37"></p><h2 id="实验（Compared-with-SOTA-and-ablation-experiment）"><a href="#实验（Compared-with-SOTA-and-ablation-experiment）" class="headerlink" title="实验（Compared with SOTA and ablation experiment）"></a>实验（Compared with SOTA and ablation experiment）</h2><h3 id="1-数据集与实现细节"><a href="#1-数据集与实现细节" class="headerlink" title="1. 数据集与实现细节"></a>1. 数据集与实现细节</h3><ul><li><strong>训练数据</strong>：合并了7个高质量语义分割数据集（<strong>ADE20K、Mapillary、COCO Panoptic、IDD、BDD100K、Cityscapes、SUNRGBD</strong>），并从OpenImagesV6和Objects365中采样部分数据用于训练。这些数据集涵盖了不同的标注风格和图像领域，总训练图像约<strong>200万张</strong>。</li><li>测试数据<ul><li><strong>语义分割测试</strong>：在8个零样本数据集（CamVid、KITTI、Pascal VOC、Pascal Context、ScanNet、WildDash1、WildDash2、YoutubeVIS）上进行测试，并在NYUv2和Pascal Context上微调模型以评估性能。</li><li><strong>下游应用测试</strong>：在零样本数据集上创建伪语义标签，用于提升实例分割和单目深度估计的性能。实例分割在COCO数据集上进行测试，深度估计在NYUDv2、KITTI、DIODE、ScanNet和Sintel等数据集上进行评估。</li></ul></li><li>评估指标<ul><li><strong>语义分割</strong>：使用平均交并比（mIoU）进行评估。</li><li><strong>实例分割</strong>：使用平均精度（AP）进行评估。</li><li><strong>深度估计</strong>：采用绝对相对误差（AbsRel）和满足特定条件的像素百分比（δτ）进行评估。</li></ul></li><li><strong>多尺度评估</strong>：在评估语义分割性能时，将测试图像调整为多个尺度（0.5 - 1.75，步长为0.25）输入模型，然后平均得分作为最终预测结果。</li><li><strong>实现细节</strong>：使用HRNet - W48和Segformer两种网络架构进行实验。训练时，采用不同的优化器和学习率衰减策略，并对图像进行数据增强处理。推理时，将图像短边调整为三种分辨率（480&#x2F;720&#x2F;1080），并根据需要采用多尺度或单尺度测试。</li></ul><h3 id="2-实验内容"><a href="#2-实验内容" class="headerlink" title="2. 实验内容"></a>2. 实验内容</h3><ul><li><p><strong>语义分割评估</strong></p><p>   在15个数据集上进行评估，以验证模型的鲁棒性和有效性。</p><ul><li><strong>鲁棒性评估</strong>：与现有最先进的方法在6个零样本数据集上进行比较，结果表明该方法在CamViD、ScanNet和WildDash1上达到了最先进的性能，并且在混合数据集上训练的模型比在单个数据集上训练的HRNet更具鲁棒性。</li><li><strong>Wilddash2评估</strong>：在Wilddash2基准测试中，该方法取得了最先进的性能。</li><li><strong>异质损失聚合效果评估</strong>：提出异质损失来监督合并的数据集，实验结果表明，在聚合OpenImages和Objects365时，使用异质损失可以持续提高所有零样本数据集的性能。</li><li><strong>未见标签泛化能力评估</strong>：通过在YoutubeVIS数据集上采样具有5个零样本标签的约2100张图像进行实验，结果表明该方法比Mseg和JoEm更具鲁棒性，并且使用句子编码可以获得更好的性能。</li><li><strong>小数据集微调评估</strong>：在NYUv2和Pascal Context上微调模型，与其他预训练权重相比，该方法的预训练权重可以显著提升性能，超过现有最先进的方法。</li><li><strong>蒸馏效果评估</strong>：使用CLIP对模型进行知识蒸馏，实验结果表明，即使只在裁剪的边界框区域进行粗略的知识蒸馏，性能仍然可以得到显著提升。</li><li><strong>语言嵌入合并训练数据效果评估</strong>：比较了两种合并训练数据标签的方法，结果表明，使用句子嵌入来表示标签可以更好地解决标签冲突问题，从而获得更好的性能。</li><li><strong>与Lseg比较</strong>：与Lseg方法在不同粒度的标签集上进行比较，结果表明，在细化标签集上，该方法的分割结果更优。</li></ul></li><li><p><strong>下游应用提升评估</strong></p><ul><li><strong>单目深度估计</strong>：在多个零样本深度数据集上创建伪语义标签，将其转换为像素级语言嵌入并输入到深度预测网络中。实验结果表明，与基线方法LeReS相比，添加创建的嵌入可以持续提高所有数据集的性能。</li><li><strong>实例分割</strong>：在采样的Objects365上创建伪实例掩码，用于训练CondInst模型。实验结果表明，使用仅25%的COCO数据，该方法可以达到与基线方法相当的性能，并且在使用完整的COCO数据进行微调时，性能比基线方法高约4% AP。</li></ul></li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出一种语义分割方法，能在多个零样本跨域数据集上取得良好性能。具体结论如下：</p><ol><li><strong>数据融合</strong>：从维基百科收集标签简短描述，编码为向量嵌入替代标签，可轻松合并多数据集，得到强大鲁棒的分割模型。</li><li><strong>损失函数</strong>：提出异质损失，利用噪声和弱标注数据集。 </li><li><strong>性能表现</strong>：在7个跨域数据集上，性能优于或与当前最先进方法相当，模型能分割零样本标签。</li><li><strong>下游应用</strong>：该模型显著提升单目深度估计和实例分割等下游应用的性能。不过，模型性能可能受语言模型表示限制，对训练语言空间外的类别泛化能力不足，但增加数据类别和改进语言模型有望解决。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 多模态语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sentence Embeddings </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scribble Hides Class Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label</title>
      <link href="/post/scribbl-hides-class-promoting-scribble-based-weakly-supervised-semantic-segmentation-with-its-class-label/"/>
      <url>/post/scribbl-hides-class-promoting-scribble-based-weakly-supervised-semantic-segmentation-with-its-class-label/</url>
      
        <content type="html"><![CDATA[<p>Peking University, Beijing, China</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>Scribble-based weakly-supervised semantic segmentation</strong> using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we pro-pose a class-driven scribble promotion network, which uti-<br>lizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction,which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label’s boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness<br>of our method. The code is available at <a href="https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network">https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p> **基于涂鸦的弱监督语义分割（Weakly-supervised Semantic Segmentation, WSSS）**通过使用稀疏涂鸦监督正逐渐受到关注，相较于需要完整标注的替代方案，这种方法能显著降低标注成本。现有方法主要通过基于局部特征线索的像素扩散机制，将已标注像素的特征传播至未标注区域来生成伪标签。然而，这种扩散过程未能有效利用全局语义信息和类别特异性特征线索，而这些要素对实现高质量的语义分割至关重要。本研究提出了一种类驱动的涂鸦增强网络，该网络通过协同利用涂鸦标注和基于图像级类别及全局语义引导的伪标签实现监督。考虑到直接使用伪标签可能误导分割模型，我们特别设计了定位校正模块，用于在特征空间中修正前景目标表示。为了深度融合两种监督方式的优势，我们还开发了距离熵损失函数，通过涂鸦标注与伪标签边界的可靠区域确定机制，动态调整各像素点的置信权重以降低不确定性。在ScribbleSup数据集上采用不同质量涂鸦标注的实验表明，我们的方法在性能表现和鲁棒性方面均优于现有所有方法。相关代码已开源：<a href="https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network%E3%80%82">https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>语义分割领域已经取得很大的进步，但是也面临着一些挑战：手动的处理大量的数据集费时费力，且不能在现实世界中进行语义分割。基于涂鸦的WSSS的内在挑战在于稀疏标签提供的部分监督，现存的方法有三种：正则化损失、一致性学习和标签扩散。基于正则化损失的方法设计了特定的损失函数来提高模型的稳定性，基于一致性学习的方法旨在捕获不变特征，从而通过一致性损失来提高细粒度分割性能。基于标签扩散的方法通过将标记的像素扩散到未标记的像素来生成像素级伪标签，但以上的方法都存在不足，利用涂鸦标签的方法也存在不足。</p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>图像级弱监督语义分割</strong>：早期深度学习图像分类成果推动特征可视化工作，如引入类激活图（CAM）技术，后续有多种方法基于此生成语义伪标签以训练分割网络，还出现利用像素相关性、自注意力机制等的方法。</li><li><strong>涂鸦级弱监督语义分割</strong>：早期采用传统交互式分割方法，近年分为正则化损失、一致性学习和标签扩散三类方法。部分新方法尝试自适应生成伪标签。</li><li><strong>其他弱监督语义分割</strong>：点级和边界框级标注也是常见方式，但在训练监督和人工成本间难以平衡。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_14-48-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_14-48-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_14-48-25"></p><p>利用从<strong>稀疏涂鸦</strong>中提取的图像级类别标签为图像监督分割提供全局线索，生成全局考虑的伪标签，同时引入定位校正模块（Localization Rectification Module，LoRM）和距离熵损失（Distance Entropy Loss，DEL）来结合两种监督的优势。</p><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：PASCAL VOC2012 and SBD</p></blockquote><h3 id="1-ScribbleSup数据集上的比较"><a href="#1-ScribbleSup数据集上的比较" class="headerlink" title="1. ScribbleSup数据集上的比较"></a>1. ScribbleSup数据集上的比较</h3><ul><li><strong>模型配置</strong>：部署resnet101作为骨干网络，deeplabV3+作为分割器，超参数设置为(λs &#x3D; e2, λc &#x3D; e7)以生成最佳结果。</li><li><strong>公平性处理</strong>：对于先前工作RAWKS和NCL采用的CRF后处理，因其耗时较长，在比较中进行了考虑。对于近期工作TEL和AGMM，为确保公平性，使用标准涂鸦重新实现它们。</li><li><strong>实验结果</strong>：该方法优于所有先前方法，比TEL高0.6%，比AGMM高1.6%。测试结果从PASCAL VOC2012网站获取。可视化比较显示，近期方法未能捕捉到正确的全局语义。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-01-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-01-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-01-53"></p><h3 id="2-涂鸦收缩和丢弃实验"><a href="#2-涂鸦收缩和丢弃实验" class="headerlink" title="2. 涂鸦收缩和丢弃实验"></a>2. 涂鸦收缩和丢弃实验</h3><p>由于基于涂鸦的注释具有灵活性，用户注释的涂鸦长度可能不同，有时会丢弃一些对象。因此，评估模型在不同收缩或丢弃比率下的鲁棒性很重要。实验结果表明，随着丢弃或收缩比率的增加，模型性能下降。当涂鸦收缩到点（收缩比率 &#x3D; 1）时，AGMM和TEL的性能下降约10%，而该方法的性能仅下降不到1%，显示出其鲁棒性。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-02-32"></p><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><h3 id="3-组件消融实验"><a href="#3-组件消融实验" class="headerlink" title="3. 组件消融实验"></a>3. 组件消融实验</h3><ul><li><strong>模型配置</strong>：采用resnet50作为骨干网络，deeplabV2作为分割器，使用ScribbleSup数据集进行训练和验证。</li><li><strong>超参数调整</strong>：通过网格搜索找到距离熵损失所有组件的最佳超参数组合，即λs &#x3D; 1, λc &#x3D; 6。</li><li><strong>实验结果</strong>：单独使用涂鸦或伪标签作为基本监督产生的结果不理想（约67%），而同时使用两者产生了更好的结果（72.13%），表明涂鸦和伪标签提供了互补的监督。仅添加Ldc会使模型性能下降到与仅使用Lsegc几乎相同的水平，这是由于模型对伪标签中的噪声标签过拟合，而LoRM可以解决这个问题，将模型性能从67.33%提高到73.64%。与基线相比，所有组件都能提高性能，同时使用所有组件可获得最佳性能。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-02-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-02-52"></p><h3 id="4-伪标签消融实验"><a href="#4-伪标签消融实验" class="headerlink" title="4. 伪标签消融实验"></a>4. 伪标签消融实验</h3><p>使用deeplabV3+作为分割器，对不同伪标签进行实验，以评估其影响。结果表明，随着伪标签基础准确率的提高，该方法的性能也随之提高，这表明该方法直接受益于图像级弱监督语义分割方法，是一个有前途的发展方向。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-03-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_15-03-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-18_15-03-15"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了用于**基于涂鸦的弱监督语义分割（scribble-based WSSS）**问题的类驱动涂鸦提升网络（<strong>CDSP</strong>），并得出以下结论： </p><blockquote><ol><li><p>引入定位校正模块（LoRM）解决模型对噪声标签过拟合问题，通过参考其他前景位置的特征表示，校正被误导的前景特征。 </p></li><li><p>采用距离熵损失（DEL）增强网络鲁棒性，根据涂鸦和伪标签边界确定可靠区域，为预测分配不同置信度。 </p></li><li><p>实验结果表明，该方法优于现有方法，在不同质量涂鸦的实验中表现出卓越的鲁棒性，达到了当前最优性能，证明了利用图像级类别信息生成全局伪标签用于基于涂鸦的弱监督语义分割的有效性。</p></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 涂鸦监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scribble </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Generalized Medical Image Segmentation from Decoupled Feature Queries</title>
      <link href="/post/learning-generalized-medical-image-segmentation-from-decoupled-feature-queries/"/>
      <url>/post/learning-generalized-medical-image-segmentation-from-decoupled-feature-queries/</url>
      
        <content type="html"><![CDATA[<p>Jarvis Research Center、Wuhan University、Guangxi Medical University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Domain generalized medical image segmentation requires models to learn from multiple source domains and generalize well to arbitrary unseen target domain. Such a task is both technically challenging and clinically practical, due to the domain shift problem (i.e., images are collected from different hospitals and scanners). Existing methods focused on either learning shape-invariant representation or reaching consensus among the source domains. An ideal generalized representation is supposed to show similar pattern responses within the same channel for cross-domain images. However, to deal with the significant distribution discrepancy, the network tends to capture similar patterns by mul-tiple channels, while different cross-domain patterns are also allowed to rest in the same channel. To address this issue, we propose to leverage channel-wise decoupled deep features as queries. With the aid of cross-attention mechanism, the long-range dependency between deep and shallow features can be fully mined via self-attention and then guides the learning of<br>generalized representation. Besides, a relaxed deep whitening transformation is proposed to learn channel-wise decoupled features in a feasible way. The proposed decoupled feature query (DFQ) scheme can be seamlessly integrate into the Transformer segmentation model in an end-to-end manner. Extensive experiments show its state-of-the-art performance, notably outperforming the runner-up by 1.31% and 1.98% with DSC metric on generalized fundus and prostate benchmarks, respectively. Source code is available at <a href="https://github.com/BiQiWHU/DFQ">https://github.com/BiQiWHU/DFQ</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p><strong>域泛化医学图像分割</strong>任务要求模型能够从多个源域学习，并有效泛化到任意未知目标域。这种任务不仅在技术上具有挑战性，同时也具有重要的临床应用价值，其核心难点在于域偏移问题 (即医学图像采集自不同医院和扫描设备) 。现有方法主要聚焦于学习形状不变性表征或实现源域间的特征共识。理想的泛化表征应确保跨域图像在相同特征通道上呈现相似的模式响应。然而，面对显著的分布差异，网络往往通过多个通道捕捉相似模式，而不同跨域模式又可能共存于同一通道。针对这一矛盾，我们提出基于通道解耦深度特征的查询机制。通过交叉注意力机制，深度特征与浅层特征间的长程依赖关系可经由自注意力充分挖掘，从而指导泛化表征的学习。此外，我们开发了一种自适应深度白化变换，以更灵活的方式实现通道解耦特征学习。所提出的解耦特征查询 (DFQ) 框架能够以端到端方式无缝集成于Transformer分割模型。大量实验验证了该方案的先进性，在眼底和前列腺的泛化性基准测试中，其Dice相似系数 (DSC) 指标分别以1.31%和1.98%的优势显著超越次优方法。项目代码已开源：<a href="https://github.com/BiQiWHU/DFQ%E3%80%82">https://github.com/BiQiWHU/DFQ。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><ol><li><strong>数据分布差异</strong>：多数现有医学图像分割方法假定训练和测试样本遵循相同统计分布，但实际中，医学图像来自不同医院、由不同水平的标注者标注，存在显著的领域偏移问题，导致模型泛化能力要求高。 </li><li><strong>现有方法不足</strong>：过去医学图像分割的领域适应研究需目标域样本参与训练，只能泛化到训练中见过的目标域。现有领域泛化医学图像分割方法主要分为学习形状不变特征和明确学习多源域间的域间偏移两类，但难以应对不同成像条件下任意未见领域的特征分布变化。 </li><li><strong>特征问题</strong>：领域偏移使深度学习模型同一通道中不同领域的医学图像激活模式差异大，浅层特征的特征不对齐问题明显，网络为捕捉各领域模式会在多通道学习相似模式，导致特征冗余，影响模型对未见领域的泛化能力。</li></ol><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>医学图像分割技术发展</strong>：深度学习技术推动医学图像分割发展，早期U - Net及其变体占主导，后DeepLab及改进模型成为趋势，近期Vision Transformer因强大特征表示能力受关注，且弱监督、半监督和多标注场景下的分割研究也有开展。</li><li><strong>领域泛化研究</strong>：计算机视觉和机器学习领域对领域泛化广泛研究，计算机视觉中领域泛化分割多聚焦驾驶场景，医学图像分割领域泛化旨在从源域学习泛化到任意未见目标域的语义表示，现有方法分学习形状不变特征和学习域间偏移两类</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-06-07.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-06-07.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-06-07"></p><ol><li>减少通道冗余<ul><li><strong>问题</strong>：为应对不同领域间的分布差异，深度神经网络倾向于在多个通道中提取相似模式，导致特征冗余。</li><li><strong>解决方案</strong>：提出松弛深度白化变换（RDWT）。传统的深度白化变换（DWT）在学习解相关表示时存在问题，可能无法有效消除通道相关性。而RDWT通过在计算协方差矩阵之前对特征进行归一化，只关注通道之间的相关性，更有效地减少了特征冗余。</li></ul></li><li>从解耦特征查询中学习<ul><li><strong>问题</strong>：通道解耦特征虽增强了深度神经网络在跨领域场景中的表示能力，但松弛白化变换损失无法保证不同领域的医学图像在同一通道上显示相似的特征响应。</li><li><strong>解决方案</strong>：利用自注意力机制中固有的长距离依赖关系。在解码高层特征时，查询由深层特征生成，键和值基于浅层特征。深层特征查询对不同领域浅层表示的一致性施加了隐式约束。</li></ul></li><li>解码泛化表示<ul><li><strong>方法</strong>：通过一个由权重$W_1$和偏置$b_1$参数化的线性层对学习到的泛化表示进行特征融合，然后将结果输入到语义分割头进行最终预测。</li><li><strong>损失函数</strong>：总损失函数$L$是标准的二元交叉熵损失和Dice损失（记为$L_{seg}$）与每个特征的$L_{i_{RDWT}}$的组合，即$L &#x3D; L_{seg} + λ · \sum_{i&#x3D;1}^{4}L_{i_{RDWT}}$，其中$λ$设置为$1 × 10^{-4}$。</li></ul></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-04-46.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-04-46.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-04-46"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-05-05"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-17_16-05-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-17_16-05-13"></p><ol><li>与现有最优方法对比<ul><li><strong>前列腺分割基准</strong>：论文提出的方法显著优于现有最优方法。与次优方法相比，在第一、二、四、五和六个领域的ASD指标分别提高了0.14%、0.18%、0.49%、0.38%和0.26%；DSC指标在六个领域中的五个领域超过了所有现有方法，最高提升了2.08%。</li><li><strong>眼底图像分割基准</strong>：该方法同样显著优于现有最优方法。与次优的RAM - DSIR方法相比，平均DSC提高了1.63%，ASD改善了0.80%。</li></ul></li><li>对DFQ的理解<ul><li><strong>减少通道冗余</strong>：通过计算并可视化特征查询的协方差矩阵，发现所提出的DFQ方案在消除非对角元素方面表现最佳。</li><li><strong>跨领域特征对齐</strong>：通过t - SNE可视化特征空间，表明DFQ使来自不同领域的样本更均匀地混合，有助于最小化领域差距。</li></ul></li><li><strong>可视化分割结果</strong>：与现有方法相比，所提出的方法显示出更精确和合理的预测。</li></ol><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><ul><li><strong>各组件实验</strong>：DFQ框架由分割骨干网络、特征查询和松弛深度白化变换（RDWT）三个关键组件组成。实验表明，使用特征查询使DSC提高了0.94%，ASD提高了0.88%；RDWT进一步使DSC提高了1.16%，ASD提高了0.68%。</li><li><strong>各尺度实验</strong>：研究了解耦查询和风格不变的键与值的影响。结果显示，使用风格解耦的键和值（F1）以及风格解耦的查询（F2、F3、F4）都对分割结果有积极贡献，其中风格解耦查询的贡献更大。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li><p><strong>方法创新</strong>：为解决跨领域医学图像的特征不对齐问题，提出了放松的深度白化变换（RDWT），增强了通道表示能力并减少通道冗余；创新性地使用解耦的深度特征作为查询，引导整个框架学习不同领域相似的通道特征模式。</p></li><li><p><strong>性能表现</strong>：大量实验表明，该方法在前列腺和眼底图像分割基准测试中显著优于现有最先进的方法，在多个指标上取得了最佳性能，如在眼底和前列腺基准测试中，DSC指标分别至少高出1.31%和1.98%，展现出了卓越的领域泛化能力。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 医学图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Segmentation </tag>
            
            <tag> Decoupled Feature Queries </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation</title>
      <link href="/post/progressive-feature-self-reinforcement-for-weakly-supervised-semantic-segmentation/"/>
      <url>/post/progressive-feature-self-reinforcement-for-weakly-supervised-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p><strong>Zhejiang Lab、Xidian University、Zhejiang University、University of Manchester</strong></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Compared to conventional semantic segmentation with pixel-level supervision, <strong>weakly supervised semantic segmentation(WSSS)</strong> with image-level labels poses the challenge that it commonly focuses on the most discriminative regions, resulting in a disparity between weakly and fully supervision scenarios. A typical manifestation is the diminished precision on object boundaries, leading to deteriorated accuracy of WSSS. To alleviate this issue, we propose to adaptively partition the image content into certain regions (e.g., confident<br>foreground and background) and uncertain regions (e.g., object boundaries and misclassified categories) for separate processing. For uncertain cues, we propose an adaptive masking strategy and seek to recover the local information with self-distilled knowledge.We further assume that confident regions should be robust enough to preserve the global semantics, and introduce a complementary self-distillation method that constrains semantic consistency between confident regions and an augmented view with the same class labels. Extensive experiments conducted on PASCAL VOC 2012 and MS COCO 2014 demonstrate that our proposed single-stage approach for WSSS not only outperforms state-of-the-art counterparts but also surpasses multi-stage methods that trade complexity foraccuracy.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>相较于需要像素级标注的传统语义分割方法，仅使用图像级标签的<strong>弱监督语义分割 (Weakly Supervised Semantic Segmentation, WSSS)</strong> 面临一个关键挑战：这类方法通常会过度关注最具区分度的区域，导致弱监督与全监督方法间存在显著性能差异。这种现象的典型表现是物体边界区域的识别精度下降，从而影响 WSSS 的整体准确性。为解决这一问题，我们提出将图像内容自适应划分为确定区域（如高置信度的前景和背景）与不确定区域（如物体边界和易混淆类别）进行差异化处理。针对不确定区域，我们设计了一种自适应掩码策略，通过<strong>自蒸馏知识 (self-distilled knowledge)</strong> 来恢复局部特征信息。同时我们提出，确定区域应当具备足够的鲁棒性以保持全局语义特征，为此开发了互补式自蒸馏方法，通过约束高置信区域与经过数据增强的同类别图像视图之间的语义一致性来强化模型。在 PASCAL VOC 2012 和 MS COCO 2014 数据集上的大量实验表明，我们提出的单阶段 WSSS 方法不仅超越了当前最先进的同类方案，其性能表现甚至优于那些通过增加模型复杂度来提升准确率的多阶段方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法存在的问题，提升分割性能。研究背景如下：</p><ul><li><strong>WSSS的优势与挑战</strong>：与传统的像素级监督语义分割相比，WSSS使用如边界框、涂鸦、点和图像级标签等“弱”标签，可降低标注成本。其中，图像级标签最为经济，但难以利用。 -</li><li><strong>现有方法的局限性</strong>：基于图像级标签的WSSS常用方法是先训练图像分类网络，生成类激活图（CAMs）作为种子区域，再将其细化为伪分割标签来监督分割网络。然而，CAMs本质上存在缺陷，它主要关注对象最具判别性的区域，导致前景对象与背景的边界区域以及多语义不同对象内的误分类区域存在高度不确定性，影响分割精度。 </li><li><strong>本文的研究目标</strong>：为解决上述问题，本文提出一种渐进式特征自我强化方法，通过自适应划分图像内容为确定区域和不确定区域并分别处理，以明确不确定区域的视觉语义，提高WSSS性能。</li></ul><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><ul><li><strong>弱监督语义分割方法</strong>：多阶段方法先通过分类模型生成类激活图（CAMs）作为伪标签，再训练分割模型评估性能，部分采用视觉变换器提升长程建模能力；单阶段方法将分类、伪标签细化和分割联合训练，但性能常不如多阶段方法。</li><li><strong>自蒸馏方法</strong>：将自监督学习与知识蒸馏结合，动态构建教师网络，简化训练过程并取得不错效果。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-23-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-23-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_10-23-43"></p><p>采用<strong>编码器 - 解码器</strong>架构实现图像级监督的语义分割。</p><ul><li><strong>编码器</strong>：使用在 ImageNet 上预训练的 ViT - B 视觉变压器，由图像级类标签监督。采用补丁令牌对比（PTC）进行亲和性学习，以约束最后一层补丁令牌之间的亲和性，防止过度平滑。</li><li><strong>解码器</strong>：借鉴 DeepLab 中的轻量级卷积解码器，由类激活映射（CAMs）生成的伪分割标签监督。</li><li><strong>聚合模块</strong>：将patch token汇总为一个类token。</li><li><strong>投影器</strong>：由 3 层感知器和权重归一化的全连接层组成，将所有令牌转换到合适的特征空间进行特征学习。</li><li><strong>自蒸馏机制</strong>：通过学生和教师管道实现自蒸馏，以改进模型训练。</li></ul><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><blockquote><p>数据集：PASCAL VOC 2012、MS COCO 2014</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-30.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-30.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_10-26-30"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_10-26-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_10-26-43"></p><ul><li><strong>PASCAL VOC 2012</strong>：提出的特征自强化（FSR）方法在验证集和测试集上的mIoU分别达到75.7%和75.0%，显著优于其他单阶段方法，甚至超过了一些复杂的多阶段方法，如比BECO分别高2.0%和1.5%。与使用图像级标签和现成显著性图的多阶段方法相比，也取得了更优的性能。</li><li><strong>MS COCO 2014</strong>：在验证集上mIoU达到45.5%，优于之前的单阶段解决方案，略高于多阶段的MCTformer + 0.2%，进一步证明了该方法的优越性。</li></ul><h2 id="实验（Alabtion-Experiments）"><a href="#实验（Alabtion-Experiments）" class="headerlink" title="实验（Alabtion Experiments）"></a>实验（Alabtion Experiments）</h2><ul><li><p><strong>不确定特征选择分析</strong>：比较了基于边缘和基于CAM的两种严格选择不确定特征的方法，基于CAM的选择略优于基于边缘的选择，且当基于CAM的选择不那么严格时，性能进一步提升，经验上掩码比率r &#x3D; 0.4效果最佳。不确定特征掩码在大多数情况下比随机特征掩码性能更高，表明强化不确定特征对语义澄清很重要。</p></li><li><p><strong>特征自强化分析</strong></p><p>展示了FSR在不确定区域（unc.FSR）和确定区域（cer.FSR）的消融结果。unc.FSR在伪标签和预测标签上都有显著提升，证明了强化不确定特征的有效性。结合unc.FSR和cer.FSR可以进一步提高伪标签和预测标签的质量，表明强化确定特征与unc.FSR互补，增强了全局理解。</p><ul><li><strong>unc.FSR分析</strong>：通过分析注意力机制，计算每个注意力头在Transformer层上的平均注意力熵。应用unc.FSR时，深层（如第7 - 11层）的熵更高且更集中，表明unc.FSR通过提高深层的上下文程度有利于语义分割。</li><li><strong>cer.FSR分析</strong>：将确定特征的注意力聚合（MCA）与全局平均池化（GAP）和全局最大池化（GMP）两种传统方法进行比较。GAP性能优于GMP，MCA大幅优于GAP，表明注意力加权机制优于平均加权。可视化的类到补丁注意力图显示类令牌可以自适应地学习关注目标区域。</li></ul></li><li><p><strong>数据增强</strong>：与其他数据增强方法的比较结果表明，数据增强对性能的影响有限。例如，加入高斯模糊或日光化处理时，性能在预期范围内波动；使用强大的AutoAugment时，结果略有下降，因为强增强可能会干扰分割目标。</p></li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者提出了一种<strong>基于语义不确定性引导的弱监督语义分割方法</strong>，并将其集成到单阶段框架中，在PASCAL VOC 2012和MS COCO 2014基准上验证了有效性。具体结论如下： 1. <strong>方法设计有效</strong>：设计了基于激活的掩码策略，利用自蒸馏知识恢复局部信息，并引入自蒸馏方法增强语义一致性，能有效估计边界。 2. <strong>性能表现优异</strong>：在两个基准测试中，所提方法显著优于其他单阶段方法，甚至超过了一些复杂的多阶段方法，证明了基于Transformer的单阶段训练的有效性。 3. <strong>消融实验验证</strong>：消融实验表明，增强不确定特征和确定特征对语义分割都很重要，两者结合可进一步提高伪标签和预测标签的质量。 </p><blockquote><p>启发：弱监督语义分割、自蒸馏</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 弱监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Weakly Supervised Semantic Segmentation </tag>
            
            <tag> Feature Self-Reinforcement </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch教程</title>
      <link href="/post/fcn/20250312-pytorch-jiao-cheng/"/>
      <url>/post/fcn/20250312-pytorch-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-02-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-02-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_17-02-44"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-10-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_17-10-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_17-10-25"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-12_14-39-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-12_14-39-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-12_14-39-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_15-05-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-13_15-05-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-13_15-05-58"></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation</title>
      <link href="/post/relevant-intrinsic-feature-enhancement-network-for-few-shot-semantic-segmentation/"/>
      <url>/post/relevant-intrinsic-feature-enhancement-network-for-few-shot-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<p>University of Chinese Academy of Sciences、Chinese Academy of Sciences、Alibaba group</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><p>For few-shot semantic segmentation, the primary task is to extract class-specific intrinsic information from limited labeled data. However, the semantic ambiguity and inter-class similarity of previous methods limit the accuracy of pixel-level foreground-background classification. To alleviate these issues, we propose the Relevant Intrinsic Feature Enhancement Network (RiFeNet). To improve the semantic consistency of foreground instances, we propose an unlabeled branch as an efficient data utilization method, which teaches the model how to extract intrinsic features robust to intra-class differences. Notably, during testing, the proposed unlabeled branch is excluded without extra unlabeled data and computation. Furthermore, we extend the inter-class variability between foreground and background by proposing a novel multi-level prototype generation and interaction module. The different-grained complementarity between global and local prototypes allows for better distinction between similar categories. The qualitative and quantitative performance of RiFeNet surpasses the state-of-the-art methods on PASCAL−5i and COCO benchmarks.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a><strong>翻译</strong></h2><p>对于<strong>少样本语义分割任务</strong>，核心挑战在于如何从有限标注数据中提取类别本质特征。传统方法常受限于语义模糊性和类间相似性，导致前景-背景的像素级分类精度不足。为此，我们提出相关本质特征增强网络 (Relevant Intrinsic Feature Enhancement Network, RiFeNet)。该网络创新性地引入无标注分支训练策略，通过指导模型提取对类内差异具有鲁棒性的本质特征，显著提升了前景实例的语义一致性。值得一提的是，该无标注分支在测试阶段可完全移除，无需额外无标注数据支持且不增加计算负担。</p><p>为增强类间区分度，我们设计了多层次原型生成与交互模块。该模块通过建立全局原型（表征整体类别特征）与局部原型（捕捉细节特征）之间的多粒度互补关系，有效提升相似类别的可区分性。实验表明，RiFeNet 在 PASCAL-5i 和 COCO 基准测试中，无论是定性可视化结果还是定量评估指标，均超越了当前最先进的语义分割方法。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><p><strong>语义分割</strong>是计算机视觉领域的基础且关键任务，在医疗图像理解、工业缺陷检测等众多视觉任务中应用广泛。随着卷积神经网络和基于Transformer方法的发展，全监督语义分割取得显著成功，但获取像素级标注需大量人力和成本。因此，少样本语义分割范式受到关注，该范式让模型利用少量标注数据学习分割，再迁移到查询输入进行测试。</p><p> 然而，以往少样本语义分割方法存在语义模糊和类间相似性问题，影响分割效果。对于前景对象，同一类不同实例存在语义模糊，类内差异会导致查询图像出现语义错误；在区分前景和背景方面，类间相似性使像素级二分类困难，不同类但纹理相似的对象同时出现时，前景和背景的局部特征易混淆。 为解决这些问题，本文提出相关内在特征增强网络（RiFeNet），旨在提高少样本任务中前景分割性能，增强前景语义一致性，扩大前景和背景的类间差异。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><strong>语义分割</strong>：自全卷积网络（FCN）将语义分割转化为像素级分类后，编码器 - 解码器架构被广泛应用，近期研究聚焦于多尺度特征融合、注意力模块插入和上下文先验等。受视觉变压器启发，相关方法在语义分割任务中表现良好，但难以应对稀疏训练数据。</li><li><strong>少样本分割</strong>：主流方法分为原型提取和空间相关两类。空间相关方法虽保留空间结构，但计算复杂度高、参数多；原型学习方法以较低计算成本取得不错效果，如SG - ONE、PFENet等。</li><li><strong>少样本分割中无标签数据利用</strong>：少数研究探索了无标签数据的利用，如PPNet和Soopil的方法，但都需额外无标签数据，与原始少样本任务设置不符</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><p>本文提出了<strong>相关内在特征增强网络（Relevant Intrinsic Feature Enhancement Network，RiFeNet）</strong>，用于解决少样本语义分割任务中存在的语义模糊和类间相似性问题，以下是该模型的详细介绍： </p><ol><li><strong>整体架构</strong>：RiFeNet由三个共享主干网络的分支组成，在传统的支持 - 查询框架基础上增加了一个无标签分支，帮助模型学习保证语义一致性。其前向传播过程包含三个主要模块：   <ul><li><strong>多级原型生成模块</strong>：从支持特征中提取全局原型，从查询分支中提取局部原型，为更好的类间区分提供多粒度证据。    </li><li><strong>多级原型交互模块</strong>：构建不同粒度原型之间的交互，增强特征挖掘能力，以进行准确的识别。   </li><li><ul><li><strong>特征激活模块</strong>：使用n层Transformer编码器进行特征激活，激活包含目标类对象的像素并停用其他像素，提供最终的分割结果。</li></ul></li></ul></li><li><strong>无标签数据特征增强</strong>：引入辅助无标签分支作为有效的数据利用方法，通过对训练样本的子集进行重采样作为无标签数据，并应用相同的分割损失，教导模型避免学习有标签输入的特定样本偏差。无标签分支与查询分支共享参数，使用相互生成的伪标签进行训练。 </li><li><strong>多级原型处理</strong></li></ol><ul><li><strong>全局支持原型生成</strong>：通过对全局特征进行掩码平均池化，从支持特征中提取全局原型，以捕获高维类别级别的类别信息。    </li><li><strong>局部查询原型生成</strong>：从查询分支中额外提取局部原型，为二进制分类提供细粒度信息。使用先验掩码和局部平均池化，并通过1×1卷积和通道注意力机制进行细化。</li><li>-<strong>多级原型交互</strong>：将生成的全局和局部原型扩展到特征图的大小，然后与查询特征和先验掩码连接，经过1×1卷积和激活操作，得到增强的查询特征。</li></ul><ol start="4"><li><strong>特征激活</strong>：使用Transformer编码器对增强的查询特征进行自注意力和交叉注意力处理，输出经过调整大小后传递给分类头，得到最终的逐像素分割结果。</li><li><strong>损失函数</strong>：RiFeNet的损失函数是主损失和自监督损失的加权和。主损失使用DICE损失计算查询输入的预测结果与真实标签之间的差异，自监督损失用于无标签分支的训练，权重β经验性地设置为0.5。 实验结果表明，RiFeNet在PASCAL - 5i和COCO数据集上的表现优于现有方法，证明了其在少样本语义分割任务中的有效性。</li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a><strong>实验（Compared with SOTA）</strong></h2><p>数据集：PASCAL-5${^i}$ 、COCO</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_09-38-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_09-38-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_09-38-26"></p><ul><li><p><strong>PASCAL - 5i数据集</strong>：RiFeNet在大多数实验场景下优于最佳方法。在单样本设置下比CyCTR高约3.5%，五样本设置下高约2%。与现有最先进的DCAMA相比，使用ResNet50骨干时，单样本设置下高出2.5%；使用ResNet101时，高出2.7%。单样本设置下增益更大，原因是随着有标签图像增加，无标签数据与有标签图像的比例从2降至0.4，无标签分支的积极影响减小。</p></li><li><p><strong>COCO数据集</strong>：在该数据集的复杂场景下，RiFeNet在单样本设置的几乎所有分割中仍比当前最佳的DCAMA高出0.8%。定性结果也证明了RiFeNet的有效性，在支持集和查询集中前景对象姿态、外观和拍摄角度差异较大的情况下，RiFeNet在保持前景语义一致性方面有显著改进，在处理前景与背景相似性问题上表现更好。</p></li></ul><h2 id="实验（Ablation-Experiments）"><a href="#实验（Ablation-Experiments）" class="headerlink" title="实验（Ablation Experiments）"></a><strong>实验（Ablation Experiments）</strong></h2><ul><li><strong>关键组件有效性</strong>：在单样本设置下，使用无标签分支或多级别原型交互均可使性能提升约2%，两者结合时，RiFeNet在基线基础上提高3.1%。</li><li><strong>多级别原型设计选择</strong>：实验证明了为无标签分支添加引导等操作的合理性和可靠性。</li><li><strong>无标签分支设计选择</strong>：无引导查询原型的无标签分支性能比基线更差，增加基线的训练迭代次数对性能影响不大，证明该方法的有效性源于学习到的判别性和语义特征，而非数据的多次采样。</li><li><strong>不同超参数</strong>：在单样本元训练过程中，无标签图像数量设置为2时效果最佳。初始时，随着无标签图像数量增加，模型分割效果提升；数量继续增加，准确率反而下降，原因是无标签增强效果过强会使特征挖掘注意力转向无标签分支，干扰查询预测，导致特征模糊。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h2><p>在少样本分割任务中，传统方法存在<strong>语义模糊和类间相似性问题</strong>。为此，作者提出了<strong>相关内在特征增强网络（RiFeNet）</strong>。该网络引入无标签分支，在不增加额外数据的情况下，约束前景语义一致性，提高了前景的类内泛化能力。同时，提出多级原型生成与交互模块，进一步增强了背景和前景的区分度。 实验表明，RiFeNet在PASCAL - 5i和COCO基准测试中超越了现有技术水平，定性结果也证明了其有效性。消融实验显示，无标签增强和多级原型策略共同作用时，RiFeNet性能提升显著。综上，RiFeNet是一种有效的少样本语义分割模型。 </p>]]></content>
      
      
      <categories>
          
          <category> 少样本语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Feature Enhancement Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation</title>
      <link href="/post/scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation/"/>
      <url>/post/scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation/</url>
      
        <content type="html"><![CDATA[<p><strong>Hohai University, Nanjing, China</strong></p><p><strong>RMIT University, Melbourne, Australia</strong></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><p><strong>Scribble-supervised semantic segmentation</strong> presents a cost-effective training method that utilizes annotations generated through scribbling. It is valued in attaining high performance while minimizing annotation costs, which has made it highly regarded among researchers. Scribble supervision propagates information from labeled pixels to the surrounding unlabeled pixels, enabling semantic segmentation for the entire image. However, existing methods often ignore the features of classified pixels during feature<br>propagation. To address these limitations, this paper proposes a prototype-based feature augmentation method that leverages feature prototypes to augment scribble supervision. Experimental results demonstrate that our approach achieves state-of-the-art performance on the PASCAL VOC 2012 dataset in scribble-supervised semantic segmentation tasks. The code is available at<br><a href="https://github.com/TranquilChan/PFA">https://github.com/TranquilChan/PFA</a>.</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a><strong>翻译</strong></h2><p>**涂鸦监督语义分割（Scribble-supervised semantic segmentation）**提出了一种经济高效的训练方法，通过使用涂鸦生成的标注进行模型训练。该方法因能在显著降低标注成本的同时实现高性能表现，因此备受研究人员推崇。其核心原理是通过将已标注像素的信息传递至相邻未标注区域，从而完成整幅图像的语义分割。然而，我们发现现有方法在特征传递过程中普遍忽视已分类像素的特征特性。针对这一局限性，本文提出基于原型（prototype）的特征增强方法，通过挖掘特征原型（feature prototypes）的统计特性来强化涂鸦监督效果。实验表明，我们的方法在 PASCAL VOC 2012 数据集的涂鸦监督语义分割任务中达到了当前最佳水平，相关代码已开源：<a href="https://github.com/TranbilChan/PFA%E3%80%82">https://github.com/TranbilChan/PFA。</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><p><strong>标注成本问题</strong>：深度学习技术推动了深度神经网络在图像分割的发展，但是，对于标注像素级别的样本需要大量的人力和财力，并且其标注过程也非常繁琐。因此，研究者越来越关注利用涂鸦标签进行监督学习的方法。<strong>涂鸦标签属于弱监督学习</strong>，相比像素级标注，能显著减少标注工作量、提高效率，且比点、边界框和图像级标签提供更多关键语义信息。</p><p><strong>现有方法的局限性</strong>：现有涂鸦监督语义分割方法主要依赖<strong>正则化损失、一致性损失、伪建议、辅助任务和标签扩散</strong>等，但这些方法存在一定缺陷。例如，正则化方法常忽略利用高层语义信息，一致性损失未在类别层面提供直接监督，伪标签方法耗时，辅助任务会引入额外数据和预测误差，标签扩散主要依赖局部信息，且许多方法忽略了正确分类像素特征在指导边界区域像素分类中的作用。 基于以上背景，作者提出基于原型的特征增强方法，以解决现有方法的不足，提高涂鸦监督语义分割的性能。 </p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><p><strong>标注方式</strong>：图像语义分割任务训练通常需大量高质量标注样本，像素级标注耗时耗力，因此弱监督学习方法受关注，如使用涂鸦、点、边界框和图像级标签等。其中，涂鸦监督能提供更多关键语义信息，表现更优。</p></li><li><p><strong>现有方法</strong>：现有涂鸦监督语义分割方法主要依赖正则化损失、一致性损失、伪建议、辅助任务和标签扩散等，但这些方法存在一定局限性。</p></li><li><p><strong>原型方法</strong>：特征原型在计算机视觉任务中用于增强模型识别能力，部分方法在弱监督语义分割中探索了原型的使用，但未充分发挥其特征增强和引导作用。</p></li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-29-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-29-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_19-29-04"></p><ul><li><strong>特征提取</strong>：使用基于Mix Transformer的编码器（Segformer中的MiT-B1）提取初始特征图。</li><li><strong>初始预测</strong>：将特征图输入解码器生成语义分割预测图，通过部分交叉熵损失（partial cross-entropy loss），利用涂鸦标签进行监督，细化预测结果。</li><li><strong>原型提取与更新</strong>：从初始预测图的高置信区域中提取对应特征向量，通过加权平均形成局部原型。在训练迭代中，局部原型动态更新全局原型。</li><li><strong>特征增强</strong>：使用局部和全局原型通过原型特征增强器对初始特征进行增强。</li><li><strong>一致性监督</strong>：将增强后的特征图再次通过解码器生成增强预测图，使用一致性损失（consistency loss）对初始预测图和增强预测图进行约束。</li></ul><h2 id="实验过程（Compared-with-SOTA）"><a href="#实验过程（Compared-with-SOTA）" class="headerlink" title="实验过程（Compared with SOTA）"></a><strong>实验过程（Compared with SOTA）</strong></h2><p>数据集：<strong>PASCAL-Scribble</strong></p><ul><li>选择<strong>MiT-B1</strong>作为骨干网络，与现有方法在<strong>PASCAL VOC 2012</strong>验证集上进行比较。</li><li>与当前最先进的方法TEL相比，尽管MiT-B1骨干网络在全监督数据集上的性能稍弱，但该方法的mIoU仍提高了0.6%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-33-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-33-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-11_19-33-24"></p><h2 id="实验过程（Ablation-Experiments）"><a href="#实验过程（Ablation-Experiments）" class="headerlink" title="实验过程（Ablation Experiments）"></a><strong>实验过程（Ablation Experiments）</strong></h2><ul><li><strong>各组件有效性</strong>：以仅使用部分交叉熵损失作为基线，对<strong>局部原型增强</strong>和<strong>全局原型增强</strong>方法进行消融实验。结果表明，同时使用两种原型增强时性能最佳，mIoU比基线提高了10.4%。</li><li><strong>原型设置</strong>：实验发现，当每个类别的全局原型数量增加到约5时，mIoU的增加趋于饱和；在原型提取时，k百分比为8%时方法性能较好。</li><li><strong>骨干网络影响</strong>：研究了不同骨干网络对方法的影响，发现基于Transformer的骨干网络在效率和性能上限方面表现更优。使用MiT - B5时，mIoU达到81.5%，显著超过现有方法。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h2><p>作者提出了一种基于原型的特征增强方法用于涂鸦监督语义分割，得出以下结论：</p><ul><li><p>从<strong>涂鸦监督</strong>初始结果的置信部分提取原型，利用这些原型增强初始特征，并根据涂鸦监督的具体情况采用不同原型策略，能以正确分类像素的原型引导错误分类像素的分类，提升预测性能。</p></li><li><p>实验结果表明，该方法在PASCAL VOC 2012数据集上达到了最先进的性能，相比当前最优方法TEL，使用稍弱的骨干网络MiT-B1仍使mIoU提高了0.6%。</p></li><li><p><strong>未来计划将此方法应用于其他任务，以挖掘其巨大潜力和应用价值 （下一个创新点）</strong></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 涂鸦监督语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semantic Segmentation </tag>
            
            <tag> Feature Augmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation</title>
      <link href="/post/cross-domain-few-shot-semantic-segmentation-via-doubly-matching-transformation/"/>
      <url>/post/cross-domain-few-shot-semantic-segmentation-via-doubly-matching-transformation/</url>
      
        <content type="html"><![CDATA[<p>Nanjing University of Aeronautics and Astronautics 、State Key Laboratory of Integrated Services Networks, Xidian University</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><blockquote><p>Cross-Domain Few-shot Semantic Segmentation (CD-FSS) aims to train generalized models that can segment classes from different domains with a few labeled images. Previous works have proven the effectiveness of feature transformation in ad-<br>dressing CD-FSS. However, they completely rely on support images for feature transformation, and repeatedly utilizing a few support images for each class may easily lead to overfitting and overlooking intra-class appearance differences. In this paper,<br>we propose a Doubly Matching Transformation-based Network (DMTNet) to solve the above issue. Instead of completely relying on support images, we propose Self-Matching Transformation (SMT) to construct query-specific transformation matri-<br>ces based on query images themselves to transform domain-specific query features into domain-agnostic ones. Calculating query-specific transformation matrices can prevent overfitting, especially for the meta-testing stage where only one or several images are used as support images to segment hundreds or thousands of images. After obtaining domain-agnostic features, we exploit a Dual Hypercorrelation Construction (DHC) module to explore the hypercorrelations between the query im-<br>age with the foreground and background of the support image, based on which foreground and background prediction maps are generated and supervised, respectively, to enhance the segmentation result. In addition, we propose a Test-time Self-Finetuning (TSF) strategy to more accurately self-tune the query prediction in unseen domains. Extensive experiments on four popular datasets show that DMTNet achieves superior performance over state-of-the-art approaches. Code is available at<br><a href="https://github.com/ChenJiayi68/DMTNet">https://github.com/ChenJiayi68/DMTNet</a>.</p></blockquote><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a><strong>翻译</strong></h2><blockquote><p>**跨域少样本语义分割（CD-FSS）**旨在训练能够以少量标注图像对不同领域进行分割的通用模型。以往的研究已经证明了特征转换在解决CD-FSS问题中的有效性。然而，这些方法完全依赖于支持图像进行特征转换，而重复利用少量支持图像来处理每个类别，容易导致过拟合，并忽视类内外观的差异。为了解决上述问题，本文提出了一种基于双重匹配转换的网络（DMTNet）。我们并不完全依赖支持图像，而是提出了自匹配转换（SMT），通过查询图像自身构建特定的转换矩阵，将领域特定的查询特征转换为领域无关的特征。计算查询特定的转换矩阵有助于防止过拟合，特别是在元测试阶段，此时仅使用一张或几张图像作为支持图像来对数百或数千张图像进行分割。在获得领域无关特征后，我们利用双重超相关构建（DHC）模块，探索查询图像与支持图像前景和背景之间的超相关性，基于此生成前景和背景的预测图并进行监督，从而增强分割结果。此外，我们还提出了一种测试时自我微调（TSF）策略，以更准确地在未知领域自我调整查询预测。在四个流行数据集上的大量实验表明，DMTNet在性能上优于现有的最先进方法。相关代码可在 <a href="https://github.com/ChenJiayi68/DMTNet">https://github.com/ChenJiayi68/DMTNet</a> 获取。</p></blockquote><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><blockquote><p><strong>语义分割</strong>近年来依赖大规模标注数据集取得快速发展，但实际场景中收集大量训练数据耗时且成本高。少样本语义分割（FSS）应运而生，旨在用少量标注支持图像实现查询图像的准确分割，常采用元学习，但在实际应用中，源数据集和目标数据集存在较大领域差距，导致FSS模型对未见领域的泛化能力较差。 为解决FSS模型在跨领域场景下性能显著下降的问题，跨领域少样本语义分割（CD - FSS）被提出。现有主要的CD - FSS方法PATNet通过将特定领域特征转换为领域无关特征来消除领域差距，但在元测试阶段仅基于少量支持图像的转换矩阵为大量查询图像生成领域无关特征，易导致过拟合。此外，多数现有CD - FSS方法在分割过程中只关注前景目标区域，忽略背景区域。 基于上述问题，本文提出一种基于<strong>双重匹配变换的网络（DMTNet）</strong>，以解决特征变换过度依赖支持图像、类内外观差异以及信息利用不充分等关键问题。 </p></blockquote><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><strong>少样本语义分割（FSS）</strong>：现有方法分为基于度量和基于关系两类。前者将支持图像表示为类原型，通过非参数测量工具分割查询图像；后者构建支持 - 查询对的密集对应关系。但在源域和目标域差距大时，性能会下降。</li><li><strong>跨域语义分割</strong>：分为域<strong>自适应语义分割（DASS）<strong>和</strong>域泛化语义分割（DGSS）</strong>。DASS通过联合使用源域和目标域数据训练模型；DGSS通过归一化和白化（NW）、域随机化（DR）等方法缩小域差距。</li><li><strong>跨域少样本语义分割（CD - FSS）</strong>：近期提出了一些方法，如PixDA、RTD、PATNet等，旨在解决少样本和域差距问题。</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-05-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-05-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-05-44"></p><p><strong>本文提出了一种基于双重匹配变换的网络（Doubly Matching Transformation-based Network，DMTNet）用于跨域少样本语义分割（Cross-Domain Few-shot Semantic Segmentation，CD-FSS）</strong></p><ol><li><p><strong>自匹配变换模块（Self-Matching Transformation，SMT）</strong></p><ul><li><p><strong>相似性自匹配</strong>：通过查询特征与支持图像的前景和背景原型之间的基于相似性的自匹配，为查询图像生成粗略的分割掩码。将支持特征划分为多个局部特征，通过测量支持局部原型与查询全局特征之间的相似性，生成更细粒度的预测查询掩码，并使用二元交叉熵（BCE）损失函数进行监督。</p></li><li><p><strong>自适应特征变换</strong>：为支持和查询特征分别构建专门的变换矩阵，确保在自适应变换过程中前景对象的不变性。通过求解线性方程得到变换矩阵，同时提出整合支持图像的广义逆来优化查询图像的广义逆。</p></li></ul></li><li><p><strong>双超相关构建模块（Dual Hypercorrelation Construction，DHC）</strong>：探索查询特征与支持图像的前景和背景特征在无域特征空间中的密集相关性。分别基于支持前景特征和背景特征与查询特征构建4D相关张量，将得到的密集相关图输入到4D卷积金字塔编码器和2D卷积金字塔解码器中，生成预测的查询前景掩码和背景掩码，并使用BCE损失函数进行训练监督。</p></li><li><p><strong>测试时自微调策略（Test-time Self-Finetuning，TSF）</strong>：在元测试阶段，通过尝试预测支持图像的真实掩码来微调网络，使模型学习目标域的风格信息，从而为查询图像生成更准确的掩码。只微调编码器的少数参数，避免对支持图像过拟合。</p></li></ol><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a><strong>实验（Compared with SOTA）</strong></h2><blockquote><p>数据集：PASCAL VOC 2012、ISIC2018、Chest X-ray、 Deepglobe、 and FSS-1000</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-10-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-10-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-10-33"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-14-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-14-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-14-22"></p><p>与迁移学习、少样本语义分割和跨域少样本语义分割的几种先进方法进行比较，<strong>DMTNet在四个数据集的平均结果上表现优异</strong>，在1 - shot设置下达到**59.74%<strong>的平均IoU，在5 - shot设置下达到</strong>66.01%**的平均IoU。与最先进的PATNet相比，在1 - shot和5 - shot设置下分别提高了3.68%和4.02%。</p><h2 id="实验（Ablation-Study）"><a href="#实验（Ablation-Study）" class="headerlink" title="实验（Ablation Study）"></a><strong>实验（Ablation Study）</strong></h2><p>验证了SMT、DHC和TSF三个关键模块的有效性，使用所有三个模块时模型性能最佳，移除任何一个模块都会导致平均性能下降。同时，通过实验确定了TSF策略中微调编码器层能取得最佳性能。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-12-18"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-35.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-35.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-10_10-12-35"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h2><p><strong>作者提出用于跨域少样本语义分割的DMTNet，并得出以下结论：</strong></p><ol><li><strong>DMTNet利用SMT模块基于自身原型为支持和查询图像计算变换矩阵，将特定领域特征自适应转换为通用特征，避免过度依赖支持图像导致过拟合。</strong> </li><li><strong>DHC模块在通用特征空间中探索查询图像与支持图像前景和背景的双重超相关性，生成并监督前景和背景预测掩码，提升分割效果。</strong> </li><li><strong>在元测试阶段，TSF策略微调少量参数，使模型学习目标域风格信息，进一步提高分割性能。</strong></li><li><strong>大量实验表明，DMTNet在四个具有不同领域差距的数据集上有效，达到了当前最优性能。</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> 少样本语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Few-Shot Semantic Segmentation </tag>
            
            <tag> Transformation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prompt-and-Transfer Dynamic Class-Aware Enhancement for Few-Shot Segmentation</title>
      <link href="/post/prompt-and-transfer-dynamic-class-aware-enhancement-for-few-shot-segmentation/"/>
      <url>/post/prompt-and-transfer-dynamic-class-aware-enhancement-for-few-shot-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="中科院"><a href="#中科院" class="headerlink" title="中科院"></a>中科院</h2><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a><strong>摘要：</strong></h2><blockquote><p>For more efficient generalization to unseen domains(classes), most Few-shot Segmentation (FSS) would directly exploit pretrained encoders and only fine-tune the decoder, especially in the current era of large models. However, such fixed feature<br>encoders tend to be class-agnostic, inevitably activating objects that are irrelevant to the target class. In contrast, humans can<br>effortlessly focus on specific objects in the line of sight. This paper mimics the visual perception pattern ofhumanbeings and proposes a novel and powerful prompt-driven scheme, called “Prompt and Transfer” (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task. Three key points are elaborated to enhance the prompting: 1) Cross-modal linguistic information is introduced to initialize prompts for each task. 2) Semantic Prompt Transfer (SPT) that precisely transfers the class-specific semantics within the images to prompts. 3) Part Mask Generator(PMG)thatworks in conjunction withSPT to adaptively generate different but complementary part prompts for different individuals. Surprisingly, PAT achieves competitive performance on 4 different tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new state-of-the-arts on 11 benchmarks.</p></blockquote><h2 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a><strong>翻译：</strong></h2><blockquote><p>为了更高效地在未知领域（类别）中进行泛化，大多数少样本分割（FSS）方法通常会直接使用预训练的编码器，并仅微调解码器，尤其是在当前大模型时代。然而，这种固定的编码器通常是类别无关的，往往会错误地激活与目标类别无关的物体。与此不同，人类可以轻松聚焦于视线中的特定物体。本文模仿人类的视觉感知方式，提出了一种新颖且高效的基于提示的方案——“提示与迁移”（PAT）。该方案构建了一种动态的类别感知提示机制，能够调整编码器专注于当前任务中的目标类别（感兴趣的物体）。为了增强提示效果，本文重点介绍了三项关键技术：1）引入跨模态的语言信息来初始化每个任务的提示。2）语义提示迁移（SPT），通过精确地将图像中的类别特定语义迁移到提示中，提升模型的识别能力。3）部分掩码生成器（PMG），与SPT协同工作，为不同个体生成不同但互补的部分提示。令人惊讶的是，PAT在四个任务中表现优异，包括标准FSS、跨域FSS（如计算机视觉、医学、遥感领域）、弱标签FSS和零-shot分割，并在11个基准测试中设立了新的技术标准。</p></blockquote><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><blockquote><p>本文聚焦于**少样本分割（Few-shot Segmentation，FSS）**领域，其研究背景主要源于当前FSS方法存在的局限性以及人类视觉感知模式带来的启示： </p><ol><li><strong>数据驱动方法的局限</strong>：深度学习在计算机视觉任务中取得显著进展，但这些数据驱动的技术在标注数据不足时表现不佳，半监督学习也难以很好地泛化到未见类别。因此，少样本学习（FSL）应运而生，旨在利用少量标注样本快速泛化到未见领域。 </li><li><strong>现有FSS方法的问题</strong>：为了更有效地泛化到未见类别，大多数FSS方法直接使用预训练编码器，仅微调解码器。然而，这种固定参数的特征编码器往往对类别不敏感，会激活与目标类别无关的对象，增加后续解码器分割新类别的负担，且这一问题未得到实质性解决。 </li><li><strong>人类视觉感知的启示</strong>：人类能够以独特的视觉感知模式选择性地关注视线中的关键对象。受此启发，作者认为理想的FSS特征编码器应具有类别感知能力，能够针对不同任务激活相应的类别对象。因此，本文提出了一种基于提示学习的“Prompt and Transfer”（PAT）方法，以动态驱动编码器关注特定对象，实现类别感知增强。</li></ol></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-30-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-30-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-30-17"></p><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a><strong>研究现状</strong></h2><ul><li><strong>少样本学习（FSL）</strong>：多数方法遵循元学习范式，可分为优化和度量两类，部分方法引入文本信息用于分类。</li><li><strong>少样本分割（FSS）</strong>：主要有原型匹配、特征融合和像素匹配三种方法，多数采用预训练编码器并微调解码器。部分研究开始探索适用于FSS的特征编码器。</li><li><strong>提示学习</strong>：源于自然语言处理，计算机视觉领域尝试引入可学习参数激活语义知识，部分工作探索了与少样本学习的结合</li></ul><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a><strong>提出的模型</strong></h2><ol><li>提出“Prompt and Transfer”（PAT）动态类别感知提示范式，模仿人类视觉感知模式，动态驱动编码器关注特定对象，解决固定编码器类别无关问题。 </li><li>构建三个关键增强点：引入跨模态语言信息初始化提示；设计语义提示转移（SPT）精确转移语义；构建部分掩码生成器（PMG）挖掘细粒度语义提示。 </li><li>在多个任务和基准上取得新的最优性能，且可扩展到跨领域、弱标签和零样本分割等场景。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-34-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-34-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-34-33"></p><h2 id="实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）"><a href="#实验（compared-with-the-state-of-the-art-models-and-ablation-experiments）" class="headerlink" title="实验（compared with the state-of-the-art models and ablation experiments）"></a><strong>实验（compared with the state-of-the-art models and ablation experiments）</strong></h2><ol><li><strong>Comparison with the State-of-the-Arts</strong></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-36-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-36-33"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-50.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-05_19-36-50.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-05_19-36-50"></p><ol start="2"><li><h2 id="ablation-experiments"><a href="#ablation-experiments" class="headerlink" title="ablation experiments"></a><strong>ablation experiments</strong></h2></li></ol><blockquote><ol><li><strong>组件分析</strong>：</li></ol><p>   ​         <strong>引入前景提示（FG）</strong>：与基线相比，在PASCAL - 5i和COCO - 20i数据集上分别实现了0.96%和1.61%的平均交并比（mIoU）提升，证明了动态类别感知提示范式对少样本分割（FSS）的有效性。    </p><p>   ​            <strong>结合语义提示转移（SPT）</strong>：在PASCAL - 5i和COCO - 20i数据集上分别实现了2.27%和3.44%的mIoU提升，表明从支持和查询图像中提取特定类别的线索能显著增强类别感知能力，使编码器更精准地聚焦于目标对象。    </p><p>   ​          <strong>结合部分掩码生成器（PMG）</strong>：在PASCAL - 5i和COCO - 20i数据集上分别实现了3.38%和4.78%的mIoU提升，说明挖掘细粒度的部分语义能进一步发挥提示的作用，实现更精确的分割。    </p><p>   ​        <strong>引入背景提示（BG）并结合SPT</strong>：在PASCAL - 5i和COCO - 20i数据集上分别实现了4.36%和5.91%的mIoU提升，体现了背景语义对分割的重要性。 </p><ol start="2"><li><p><strong>提示初始化消融实验</strong>：    - <strong>随机初始化提示</strong>：在没有SPT和PMG持续增强其类别感知能力的情况下，使用随机初始化的提示来调整编码器不会带来性能提升。    - <strong>引入额外类别语义</strong>：将额外的类别语义作为初始提示可以调整编码器，使其初步定位目标，提高分割精度。    - <strong>语言信息的优势</strong>：语言信息比支持平均令牌更具类别代表性，使用CLIP提取的语言信息作为初始提示具有最优的分割结果。 </p></li><li><p><strong>提示增强消融实验</strong>：    - <strong>部分掩码生成器（PMG）</strong>：        - <strong>定性评估</strong>：可视化结果显示，PMG可以将目标对象清晰地划分为不同的互补部分区域，证明其能够自适应地生成不同的部分掩码。        - <strong>部分掩码数量影响</strong>：当部分掩码数量（Np）从1增加到8时，mIoU精度随之增加；继续增加数量，mIoU精度反而下降，过多的部分掩码可能导致目标对象无法清晰划分，产生冗余和噪声。    - <strong>语义提示转移（SPT）</strong>：        - <strong>支持和查询语义的重要性</strong>：仅将支持图像或查询图像的目标语义转移到提示中会导致不同程度的性能下降，说明两者对于查询图像的分割都至关重要。        - <strong>高斯抑制的作用</strong>：SPT中的高斯抑制通过调整注意力分布，使特定区域的全局语义更好地聚合到提示中，从而提高分割精度。    - <strong>提示增强次数</strong>：在变压器编码器的最后L个块中进行提示增强，更多的语义迁移次数通常能带来更高的分割精度，但综合效率和性能考虑，选择3次（L &#x3D; 3）较为合适。</p></li><li><p><strong>骨干网络设置消融实验</strong>：    - <strong>不同骨干网络的性能</strong>：使用DeiT - B&#x2F;16骨干网络在所有设置下具有最佳的分割精度，DeiT - S&#x2F;16次之，较小的ViT - S&#x2F;16或DeiT - T&#x2F;16虽然分割性能较差，但推理速度具有竞争力。    - <strong>块数量的影响</strong>：不同骨干网络中，块的数量并非越多越好，例如两种ViT变体在10个块时效果最佳，三种DeiT变体在11个块时效果最佳。 </p></li><li><p><strong>使用一致编码器的比较</strong>：在使用一致的特征编码器设置下，PAT取得了最佳的分割性能；盲目使用变压器提取特征可能无法带来预期的性能提升；引入额外的语言信息有助于生成更强大的提示，以生成类别感知特征。</p></li><li><p><strong>PAT与其他FSS方法的结合</strong>：使用更复杂的解码器不一定能优于简单的相似度计算；配备PAT提出的动态类别感知编码器后，其他FSS方法的性能有显著提升，表明该编码器能灵活地为不同的新类别生成类别感知特征，并且与基于解码器的方法具有良好的兼容性。</p></li></ol></blockquote><h2 id="研究结论"><a href="#研究结论" class="headerlink" title="研究结论"></a><strong>研究结论</strong></h2><blockquote><p>作者在摒弃以往冻结编码器以泛化到未见类别的小样本分割（FSS）做法后，提出了一种新颖的动态类别感知提示范式（PAT）。该范式模仿人类视觉感知模式，用于调整编码器以聚焦不同FSS任务中的特定对象。实验结果表明，PAT在三个流行的FSS基准测试中创造了新的最优性能。令人惊喜的是，当将其扩展到<strong>跨领域、弱标签甚至零样本</strong>等更现实的场景时，也取得了令人满意的结果。作者希望这项工作能为小样本场景下的编码器设计提供新视角，并激发未来相关研究聚焦于此。 </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 少样本语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Prompt-and-Transfer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prompting Multi-Modal Image Segmentation with Semantic Grouping</title>
      <link href="/post/prompting-multi-modal-image-segmentation-with-semantic-grouping/"/>
      <url>/post/prompting-multi-modal-image-segmentation-with-semantic-grouping/</url>
      
        <content type="html"><![CDATA[<h2 id="University-of-Chinese-Academy-of-Sciences"><a href="#University-of-Chinese-Academy-of-Sciences" class="headerlink" title="University of Chinese Academy of Sciences"></a>University of Chinese Academy of Sciences</h2><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a><strong>摘要：</strong></h2><p>Multi-modal image segmentation is one of the core issues in computer vision. The main challenge lies in integrating common information between modalities while retaining specific patterns for each modality. Existing methods typically perform full fine-tuning on RGB-based pre-trained parameters to inherit the powerful representation of the foundation model. Although effective, such paradigm is not optimal due to weak transferability and scarce downstream data. Inspired by the recent success of prompt learning in language models, we propose the Grouping Prompt Tuning Framework(GoPT), which introduces explicit semantic grouping to learn modal-related prompts, adapting the frozen pre-trained foundation model to various downstream multi-modal segmentation tasks. Specifically, a class-aware uni-modal prompter is designed to balance intra- and inter-modal semantic propaga-<br>tion by grouping modality-specific class tokens, thereby improving the adaptability of spatial information. Furthermore,<br>an alignment-induced cross-modal prompter is introduced to aggregate class-aware representations and share prompt parameters among different modalities to assist in modeling common statistics. Extensive experiments show the superiority of our GoPT, which achieves SOTA performance on various downstream multi-modal image segmentation tasks by training only &lt; 1% model parameters.</p><h2 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a><strong>翻译：</strong></h2><p>多模态图像分割技术是计算机视觉领域的关键挑战。这项技术的核心难点在于如何有效融合不同模态（如图像、红外等）的共性特征，同时保留各模态独有的特征模式。当前主流方法主要通过对基于可见光（RGB）预训练模型进行全局微调，以继承基础模型的强大特征提取能力。但这种方法存在明显局限：一方面模型可迁移性较弱，另一方面下游任务标注数据往往匮乏。受大语言模型中提示学习方法取得突破的启发，我们研发了分组提示调优框架（GoPT），通过语义分组机制学习模态专属提示，使冻结的预训练模型能灵活适配多种多模态分割任务。该框架包含两大创新模块：首先是<strong>类感知单模态提示器</strong>，通过聚类同类模态特征，在保留模态内独特空间信息的同时，促进跨模态语义对齐；其次是<strong>对齐引导的跨模态提示器</strong>，通过共享提示参数聚合不同模态的类特征表示，有效捕捉多模态数据的共有统计规律。实验数据显示，GoPT 仅需微调模型不足 1% 的参数，就在多个多模态图像分割基准任务中刷新了最高性能记录，展现出显著优势。</p><h2 id="研究背景："><a href="#研究背景：" class="headerlink" title="研究背景："></a><strong>研究背景：</strong></h2><ol><li><strong>多模态融合的重要性</strong>：语义分割旨在为场景中每个像素分配语义类别，随着传感器技术发展，多模态融合用于分割成为图像解释核心问题。深度学习推动下，深度多模态融合展现出比单模态分割更显著的优势。 </li><li><strong>现存方法的挑战</strong>：现有多模态分割方法主要分为基于对齐和基于聚合的融合，但面临诸多挑战。一方面，不同成像机制的模态存在异质差距，基于对齐的融合因信息交换弱，常提供无效的跨模态融合；另一方面，不同模态有效信息不同，基于聚合的融合易忽略模态内传播，导致模态间知识共享和模态内信息处理失衡。 </li><li><strong>全微调方法的局限</strong>：多模态方法常采用基于RGB的预训练分割器，全微调虽有效，但效率低、参数存储负担大，且因样本标注有限，无法充分利用预训练模型知识获得通用表示。</li></ol><h2 id="研究现状："><a href="#研究现状：" class="headerlink" title="研究现状："></a><strong>研究现状：</strong></h2><ul><li><strong>多模态图像分割主流方法</strong>：以深度多模态融合为主，旨在利用多数据源增强细粒度细节和像素级语义，主要分为基于对齐和基于聚合的融合方法。前者通过条件损失对齐子网络嵌入，后者运用特定算子组合多模态子网络。</li><li><strong>模型训练方式</strong>：因缺乏大规模多模态训练集，现有方法通常先加载基于RGB的预训练模型参数，再在特定下游任务数据集上微调。</li><li><strong>视觉提示学习应用</strong>：提示调优作为新范式，在自然语言处理中表现出色，近期也开始应用于视觉任务。</li></ul><h2 id="提出的模型："><a href="#提出的模型：" class="headerlink" title="提出的模型："></a><strong>提出的模型：</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_14-56-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_14-56-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-06_14-56-04"></p><p>本文提出了<strong>分组提示调优框架（Grouping Prompt Tuning Framework，GoPT）</strong>，用于<strong>多模态图像分割</strong>任务，以下是该模型的详细介绍：</p><ol><li><strong>核心思想</strong>：引入显式语义分组机制到提示学习中，通过冻结预训练的基础模型，仅微调少量视觉提示参数，使模型适应各种下游多模态分割任务，以解决现有方法在整合模态间信息和保留各模态特定模式方面的挑战。 </li><li><strong>整体架构</strong>    <ul><li><strong>输入处理</strong>：将RGB图像和辅助模态图像输入到补丁嵌入层，生成对应的RGB标记和辅助模态标记。    </li><li><strong>提示生成</strong>：把标记送入分组提示器，生成特定于模态的提示。    </li><li><strong>特征融合</strong>：将学习到的提示作为残差添加到原始RGB流中，再输入到基础模型的下一层。</li></ul></li><li><strong>主要组件</strong>    <ul><li><strong>类感知单模态提示器（Class-Aware Uni-Modal Prompter，CUP）</strong>：通过引入特定于模态的类标记，对辅助模态的视觉概念进行分层渐进分组，平衡模态内和模态间的语义传播，提高空间信息的适应性。    </li><li><strong>对齐诱导跨模态提示器（Alignment-Induced Cross-Modal Prompter，ACP）</strong>：根据显式分组的语义相似性，聚合辅助模态的类感知表示，将其他数据源的关键模式整合到RGB流中，生成新的跨模态对齐诱导提示，辅助建模模态公共统计信息。</li></ul></li><li><strong>优化策略</strong>：使用基于RGB的预训练基础模型的参数初始化多模态分割模型，在提示调优过程中，仅更新分组提示器和分割头的梯度值，以少量提示参数促进模型快速收敛，并有效继承预训练基础模型的先验知识。 </li><li><strong>实验验证</strong>：在多个下游多模态图像分割任务（RGB - D、RGB - T、RGB - SAR分割）上进行了广泛实验，结果表明GoPT仅训练不到1%的模型参数，就能在各项指标上取得优于现有方法的性能，展现出高效性和优越性。</li></ol><h2 id="实验过程（与SOTA方法的对比）："><a href="#实验过程（与SOTA方法的对比）：" class="headerlink" title="实验过程（与SOTA方法的对比）："></a><strong>实验过程（与SOTA方法的对比）：</strong></h2><p>数据集：NYUDv2、SUN RGB-D、MFNet、PST900、WHU-OS</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-02-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-02-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-06_15-02-29"></p><h2 id="实验过程（消融实验）："><a href="#实验过程（消融实验）：" class="headerlink" title="实验过程（消融实验）："></a><strong>实验过程（消融实验）：</strong></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-05-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-05-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-06_15-05-05"></p><p>通过消融实验，证明了GoPT已经达到了SOTA的标准</p><ul><li>Effectiveness of Prompter Structure  （<strong>table 4，row 7 and row 8</strong>）</li><li>Impact of Multi-Modal Information   （<strong>table 4</strong>）</li><li>Number of Grouping Prompter          <strong>（Figure 6）</strong></li><li>Hard vs. Soft Assignment                <strong>（Figure 7）</strong></li></ul><p><strong>结论：</strong></p><p>作者提出了用于<strong>多模态图像分割</strong>的参数高效视觉调优框架GoPT，通过在提示学习中引入显式语义分组，使冻结的预训练基础模型适应各种下游多模态分割任务。具体而言，设计了类感知单模态提示器（CUP），通过对特定模态的类令牌进行分组，平衡了模态内和模态间的语义传播；引入了对齐诱导的跨模态提示器（ACP），聚合类感知表示并辅助建模公共统计信息。大量下游任务实验表明，GoPT在准确性和效率上达到了最佳平衡，仅训练不到1%的模型参数，就在多个下游多模态图像分割任务中取得了SOTA性能，证明了该框架的优越性和泛化性。 </p>]]></content>
      
      
      <categories>
          
          <category> 多模态图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semantic Grouping </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Disentangle then Parse Night-time Semantic Segmentation with Illumination Disentanglement</title>
      <link href="/post/disentangle-then-parse-night-time-semantic-segmentation-with-illumination-disentanglement/"/>
      <url>/post/disentangle-then-parse-night-time-semantic-segmentation-with-illumination-disentanglement/</url>
      
        <content type="html"><![CDATA[<p>​University of Science and Technology of China               Shanghai AI Laboratory</p><p><strong>摘要：</strong></p><p>Most prior semantic segmentation methods have been developed for day-time scenes, while typically underperforming in night-time scenes due to insufficient and complicated lighting conditions. In this work, we tackle this challenge by proposing a novel night-time semantic segmentation paradigm, i.e., disentangle then parse (DTP). DTP explicitly disentangles night-time images into light-invariant reflectance and light-specific illumination components and then recognizes semantics based on their adaptive fusion. Concretely, the proposed DTP comprises two key components: 1) Instead of processing lighting-entangled features as in prior works, our Semantic-Oriented Disentanglement (SOD) framework enables the extraction of re-<br>flectance component without being impeded by lighting, allowing the network to consistently recognize the semantics under cover of varying and complicated lighting conditions. 2) Based on the observation that the illumination component can serve as a cue for some semantically confused regions, we further introduce an Illumination-Aware Parser (IAParser) to explicitly learn the correlation between semantics and lighting, and aggregate the illumination features to yield more precise predictions. Extensive experiments on the night-time segmentation task with various settings demonstrate that DTP significantly outperforms state-of-the-art methods. Furthermore, with negligible additional parameters, DTP can be directly used to benefit existing day-time methods for night-time segmentation. Code and dataset are available at <a href="https://github.com/w1oves/DTP.git">https://github.com/w1oves/DTP.git</a>.</p><p><strong>翻译：</strong></p><p>大多数现有的语义分割方法都是为白天场景设计的，因此在夜间场景中往往表现不佳，主要是因为夜间的光照条件复杂且不足。为了解决这个问题，本文提出了一种全新的夜间语义分割方法——“先解耦再解析”（DTP）。DTP方法首先将夜间图像分解为不受光照影响的<strong>反射成分</strong>和与光照相关的<strong>光照成分</strong>，然后通过自适应融合这两部分信息来进行语义识别。具体来说，DTP包含两个关键技术：1）与以往方法将光照信息与特征混合的做法不同，我们提出的“语义导向解耦”（SOD）框架能够在不受光照影响的情况下提取反射成分，这样可以帮助网络在复杂多变的光照条件下持续准确地识别语义。2）通过观察到光照成分可以作为一些语义模糊区域的线索，我们引入了“光照感知解析器”（IAParser），该模块能够学习语义与光照之间的关系，并聚合光照特征，从而提高预测的精度。在各种夜间分割任务的实验中，DTP显著超越了现有的先进方法。而且，DTP几乎不增加额外的参数，能够直接用于现有的白天分割方法，提升其在夜间的分割效果。相关代码和数据集可以在<a href="https://github.com/w1oves/DTP.git%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/w1oves/DTP.git下载。</a></p><p><strong>研究背景：</strong></p><blockquote><p>大多现存的语义分割方法都是基于白天场景开发的，这些场景光照充足且均匀。然而在实际应用中，视觉系统近一半时间需在光照不足且复杂的夜间环境下工作，现有白天方法在夜间性能会下降。此前采用无监督域适应技术将白天知识迁移到夜间，但因夜间缺乏对应标签，分割性能提升有限。虽有NightCity等大规模夜间数据集及相关方法提出，一定程度上改善了夜间场景表现，但它们通常基于光照纠缠的表示进行场景解析，不适合夜间复杂的光照条件。</p></blockquote><blockquote><p>夜间场景光照强度低且人工光源复杂，导致物体外观随光照变化，使光不变反射率和光特定光照之间的纠缠加剧，难以提取用于语义分割的判别特征。</p></blockquote><p><strong>研究现状：</strong></p><ol><li><strong>语义分割</strong>：基于全卷积网络（FCN）结合编码器 - 解码器架构的方法成为主流，如DeepLab系列引入空洞空间金字塔池化（ASPP），还有基于自注意力机制和Transformer的网络被应用，但大多聚焦白天场景。</li><li><strong>夜间语义分割</strong>：早期因缺乏大规模标注数据集，采用无监督域适应技术将白天知识迁移到夜间；近期有基于NightCity数据集的方法，如EGNet、NightLab等，但这些方法未明确估计光照对语义的影响，而是学习内容和光照的纠缠表示。</li><li><strong>深度表示解耦</strong>：此前探索了多种图像表示解纠缠方法，如在GAN框架中学习跨域不变表示。</li></ol><p><strong>创新点：</strong></p><ol><li>提出“先分离再解析”（DTP）的夜间语义分割范式，可提升现有日间方法在夜间的性能。</li><li>设计语义导向解纠缠框架（SOD），借助语义约束分离图像，使网络在不同光照下提取一致特征。 </li><li>引入光照感知解析器（IAParser），利用光照组件作为线索，实现更精准预测。 </li><li>细化NightCity数据集，提出NightCity-fine，为夜间分割提供更可靠基准。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-03_15-54-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-03_15-54-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-03_15-54-06"></p><p>实验：</p><ol><li><strong>数据集</strong></li></ol><ul><li><p><strong>NightCity - fine</strong>：原始的NightCity是最大的夜间语义分割数据集，但存在标注错误。作者提出了NightCity - fine，对训练集和验证集中不合理的标注进行了仔细修改，共修正了2554个标签图。    </p></li><li><p><strong>Cityscapes</strong>：这是一个自动驾驶数据集，在白天场景下从50个不同城市采集，包含2975张训练图像和500张验证图像，具有19个语义类别，图像分辨率为2048x1024。</p></li><li><p><strong>BDD100K</strong>：使用其子集BDD100K - night进行补充实验，该子集包含314张夜间训练图像和31张验证图像，其互补数据集为BDD100K - day。 </p></li><li><p><strong>与SOTA方法的对比实验</strong></p></li></ul><p>使用的数据集：<strong>NightCity, NightCity-fine,  and BDD100K-night</strong></p><ul><li><strong>消融实验</strong></li></ul><p><strong>结论：</strong></p><p>作者提出了一种新颖的夜间语义分割范式——Disentangle then Parse（DTP），并得出以下结论： </p><ol><li><strong>方法优势</strong>：提出语义导向的解纠缠（SOD）框架，使分割不受复杂光照干扰；引入光照感知解析器（IAParser），利用光照中的语义线索实现更精确预测。DTP可作为即插即用范式，以极少额外参数助力现有方法提升性能。</li><li><strong>数据集贡献</strong>：对最大的夜间分割数据集NightCity进行细化，提出NightCity - fine，用于更有效的训练和验证评估。</li><li><strong>性能验证</strong>：大量实验表明，DTP显著优于现有技术，与NightCity - fine一起为夜间分割提供了更优的基准。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 夜间语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semantic Segmentation </tag>
            
            <tag> Illumination Disentanglement </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SED A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation</title>
      <link href="/post/sed-a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/"/>
      <url>/post/sed-a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="天津大学，重庆大学等"><a href="#天津大学，重庆大学等" class="headerlink" title="天津大学，重庆大学等"></a><strong>天津大学，重庆大学等</strong></h2><blockquote><p>摘要：Open-vocabulary semantic segmentation strives to distinguish pixels into different semantic groups from an open<br>set of categories. Most existing methods explore utilizing pre-trained vision-language models, in which the key is to<br>adapt the image-level model for pixel-level segmentation task. In this paper, we propose a simple encoder-decoder,<br>named SED, for open-vocabulary semantic segmentation, which comprises a hierarchical encoder-based cost map generation and a gradual fusion decoder with category early rejection. The hierarchical encoder-based cost map generation employs hierarchical backbone, instead ofplain transformer, to predict pixel-level image-text cost map.<br>Compared to plain transformer, hierarchical backbone better captures local spatial information and has linear computational complexity with respect to input size. Our gradual fusion decoder employs a top-down structure to com-<br>bine cost map and the feature maps of different backbone levels for segmentation. To accelerate inference speed, we<br>introduce a category early rejection scheme in the decoder that rejects many no-existing categories at the early layer<br>ofdecoder, resulting in at most 4.7 times acceleration without accuracy degradation. Experiments are performed on<br>multiple open-vocabulary semantic segmentation datasets, which demonstrates the efficacy ofour SED method. When<br>using ConvNeXt-B, our SED method achieves mIoU score of 31.6% on ADE20K with 150 categories at 82 millisecond (ms) per image on a single A6000. Our source code is available at <a href="https://github.com/xb534/SED">https://github.com/xb534/SED</a></p></blockquote><blockquote><p>翻译：开放词汇语义分割旨在将像素划分到一个开放类别集中的不同语义组。大多数现有的方法尝试使用预训练的视觉-语言模型，关键是将图像级的模型适应为像素级的分割任务。本文提出了一种简单的编码器-解码器架构，称为SED，用于开放词汇语义分割。SED包含两部分：基于层次编码器的成本图生成和逐渐融合的解码器，并且具有类别早期拒绝的功能。基于层次编码器的成本图生成使用层次化的骨干网络，而不是传统的Transformer，来预测像素级的图像-文本成本图。与普通的Transformer相比，层次化骨干网络能够更好地捕捉局部空间信息，并且计算复杂度与输入的大小成线性关系。我们的逐渐融合解码器采用自上而下的结构，将成本图与不同层级骨干网络的特征图进行融合，完成分割任务。为了提高推理速度，我们在解码器中引入了类别早期拒绝方案，通过在解码器的早期层次中剔除不存在的类别，从而在不牺牲精度的情况下，实现最多4.7倍的加速。我们在多个开放词汇语义分割数据集上进行了实验，验证了SED方法的有效性。当使用ConvNeXt-B时，SED方法在ADE20K数据集上，针对150个类别的mIoU得分为31.6%，每张图像的推理时间为82毫秒（ms），运行于单个A6000显卡。我们的源代码可以通过<a href="https://github.com/xb534/SED%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/xb534/SED下载。</a></p></blockquote><p><strong>研究背景：</strong> <strong>传统的方法只能分割训练集的种类，不能识别出来在训练集中没有的未知场景</strong>，同时两阶段和单阶段的方法都存在不足。两阶段的框架存在不足：计算效率低，没有充分利用上下文信息；单阶段的框架存在不足：对于低分辨率的输入，主干网络对空间信息变得不敏感，即使加入额外的网络来提供空间信息，也会增加计算资源，分割种类的增加也会增加计算资源。</p><p><strong>研究现状：</strong></p><ul><li><strong>语义分割方法</strong>：传统语义分割方法主要有基于FCN和基于Transformer的方法。前者通过融合深浅层特征、利用空间金字塔网络或注意力模块等提取上下文信息；后者将Transformer用作骨干网络或分割解码器。</li><li><strong>视觉-语言模型</strong>：早期基于预训练的视觉和语言模型开发，后如CLIP从大规模图像-文本对数据中学习视觉特征，ALIGN从噪声图像-文本数据集中学习，在零样本任务上表现出色。</li><li><strong>开放词汇语义分割</strong>：早期通过学习特征映射对齐视觉和文本特征，CLIP成功后，出现基于两阶段和单阶段框架的方法。两阶段先生成掩码提案再分类，单阶段直接扩展视觉-语言模型进行分割。</li></ul><p><strong>研究方法：</strong></p><ul><li><p><strong>Hierarchical Encoder-based Cost Map（基于分层编码器的代价图）</strong></p></li><li><p><strong>Gradual Fusion Decoder（渐进式融合编码器）</strong></p></li><li><p><strong>Category Early Rejection（类别早期拒接）</strong></p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-53-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-53-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_08-53-28"></p><p><strong>实验设定（对比实验+消融实验）</strong></p><blockquote><p>训练集：COCO-Stuff 的训练集，包含大约 118k 密集标注的 171 类目标。<br>测试集：跨数据集测试。<br>ADE20K，包含 20K training 和 2K validation &#x3D;&gt; A-150 和 A-847。<br>PASCAL VOC，包含 1.5k training 和 1.5k validation &#x3D;&gt; PAS-20。<br>PASCAL-Context 来自原始的 PASCAL VOC 数据集 &#x3D;&gt; PC-59 和 PC-459。</p></blockquote><blockquote><p>模型设定：<br>基于 ConvNeXt-B&#x2F;L 视觉编码器形式的预训练 CLIP。<br>类别模板数量 P PP 同 CAT-Seg 一致，均为 80。<br>文本编码器冻结，只训练图像编码器和解码器。</p></blockquote><blockquote><p>GPU: 4xA6000<br>图像编码器学习率多乘以一个 0.01 倍的因子。<br>共 80k 次迭代。<br>训练时剪裁图像 768 × 768 大小，测试时直接放缩图像到 768 × 768大小。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-08.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-08.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_08-59-08"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-43.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_08-59-43.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_08-59-43"></p><p><strong>实验结果：本文的SED方法都表现出较好的效果，缩短了推理时间</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_09-00-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-28_09-00-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-28_09-00-24"></p><p><strong>结论与不足</strong></p><p>作者提出了用于<strong>开放词汇语义分割</strong>的SED方法，得出以下结论：</p><ol><li><strong>方法构成</strong>：SED由基于分层编码器的代价图生成和带有类别早期拒绝的渐进式融合解码器组成。先利用分层编码器生成像素级图像 - 文本代价图，再基于此和分层编码器的不同特征图，用渐进式融合解码器生成高分辨率特征图进行分割。</li><li><strong>速度提升</strong>：在解码器中引入类别早期拒绝方案，能提前拒绝不存在的类别，有效提升推理速度。</li><li><strong>效果验证</strong>：在多个数据集（ADE20K、PASCAL VOC、PASCA-Context）上的实验表明，SED在准确性和速度方面均有效。不过，模型在识别近义词类别时存在困难，未来将探索设计类别注意力策略或使用大规模细粒度数据集来解决该挑战。</li></ol><p><strong>不足：</strong></p><p><strong>类别识别局限</strong>：模型有时难以区分近义词类别，在对语义相近的类别进行分类和分割时存在困难，影响了对复杂场景的理解和处理能力</p>]]></content>
      
      
      <categories>
          
          <category> 开放词汇语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SED </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>High Quality Segmentation for Ultra High-resolution Images</title>
      <link href="/post/high-quality-segmentation-for-ultra-high-resolution-images/"/>
      <url>/post/high-quality-segmentation-for-ultra-high-resolution-images/</url>
      
        <content type="html"><![CDATA[<h1 id="香港中文大学-Adobe-等"><a href="#香港中文大学-Adobe-等" class="headerlink" title="香港中文大学  Adobe 等"></a>香港中文大学  Adobe 等</h1><p>**摘要：**To segment 4K or 6K ultra high-resolution images needs extra computation consideration in image segmentation. Common strategies, such as down-sampling, patch crop- ping, and cascade model, cannot address well the balance issue between accuracy and computation cost. Motivated by the fact that humans distinguish among objects continu- ously from coarse to precise levels, we propose the Contin- uous Refinement Model (CRM) for the ultra high-resolution segmentation refinement task. CRM continuously aligns the feature map with the refinement target and aggregates fea- tures to reconstruct these image details. Besides, our CRM shows its significant generalization ability to fill the resolu- tion gap between low-resolution training images and ultra high-resolution testing ones. We present quantitative per- formance evaluation and visualization to show that our pro- posed method is fast and effective on image segmentation refinement. Code is available at <a href="https://github.com/dvlab-research/Entity/tree/main/CRM">https://github.com/dvlab-research/Entity/tree/main/CRM</a>.</p><p><strong>翻译:</strong> 对4K或6K超高分辨率图像进行分割时，需要额外的计算资源。常见的策略，如下采样、图像裁剪和级联模型，往往难以很好地平衡准确性和计算成本。基于人类从粗略到精细地逐步区分物体的方式，我们提出了“连续细化模型”（CRM）来进行超高分辨率图像的分割细化任务。CRM通过不断地对齐特征图和细化目标，逐步聚合特征，重建图像细节。更重要的是，CRM展现出了很强的泛化能力，能够有效弥补低分辨率训练图像与超高分辨率测试图像之间的分辨率差距。我们通过定量的性能评估和可视化结果，展示了该方法在图像分割细化上的高效性和快速性。相关代码可以在<a href="https://github.com/dvlab-research/Entity/tree/main/CRM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dvlab-research/Entity/tree/main/CRM找到。</a></p><p><strong>研究背景</strong></p><p>随着相机和显示设备的快速发展，图像分辨率越来越高，4K和6K分辨率变得常见，这在人像照片后期处理、工业缺陷检测、医学诊断等领域带来了新机遇。然而，超高分辨率图像也给经典图像分割方法带来了挑战：</p><ol><li><strong>计算成本高</strong>：大量的输入像素在计算上代价高昂，且对GPU内存需求大。 </li><li><strong>细节重建难</strong>：大多数现有方法通过插值对最终预测进行4到8倍上采样，无法在输出掩码上构建细粒度细节。 以往的分割细化方法，如针对1K - 2K分辨率图像的方法，存在图形模型依赖低级别颜色边界、基于传播的方法面临计算和内存限制、大模型易过拟合而浅细化网络细化能力有限等问题。而处理超高分辨率细化的级联解码器方法，虽能取得较好性能，但在推理时需要下采样和裁剪补丁，增加了成本、丢失了细节并破坏了全局上下文。 因此，为解决超高分辨率图像分割中精度与计算成本的平衡问题，作者提出了连续细化模型（CRM），以实现高效、精确的图像分割细化。</li></ol><p><strong>研究现状</strong></p><ol><li><strong>语义分割</strong>：FCN 引入深度卷积网络，PSPNet、DeepLab 系列等方法不断发展，输出步长多设为 4×或 8×，但直接插值预测结果存在边缘锯齿和细节缺失问题。</li><li><strong>分割细化</strong>：针对 1K 分辨率图像的细化技术能提升分割质量，但存在图形模型依赖低层次颜色边界、传播方法有计算和内存限制、模型易过拟合或细化能力有限等问题。对于 4K - 6K 超高清图像，级联解码器方法能取得较好效果，但结构复杂。</li><li><strong>隐式函数表示</strong>：在神经网络中用于表示对象或场景，如 NeRF、PixelNerf 等，其多视图一致性和光滑性有利于分割。</li></ol><p><strong>研究方法</strong></p><ol><li>提出<strong>连续细化模型（CRM）</strong>，引入<strong>隐函数</strong>，利用连续位置信息和特征对齐，有效降低计算成本，重建更多细节。 </li><li>CRM采用多<strong>分辨率推理策略</strong>，适用于<strong>低分辨率训练和超高分辨率测试</strong>，总推理时间不到CascadePSP的一半。 </li><li>实验表明，CRM在超高分辨率图像上取得最佳分割结果，还能提升现有全景分割模型性能，无需微调。</li></ol><p><strong>实验步骤</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-02_14-24-58.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-02_14-24-58.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-03-02_14-24-58"></p><ol><li><strong>数据集和对比方法选择</strong>    - <strong>训练数据集</strong>：遵循CascadePSP的设置，将MSRA - 10K、DUT - OMRON、EC - SSD和FSS - 1000合并为训练数据集，包含36,572张具有超过1000个语义类别的图像。    - <strong>测试数据集</strong>：使用CascadePSP提出的高分辨率图像分割数据集BIG（图像分辨率范围2K - 6K）进行超高清图像评估；还在重新标注的PASCAL VOC 2012数据集上进行评估；为证明模型的通用性，将CRM作为全景分割和实体分割的扩展进行评估。    - <strong>对比方法</strong>：选择CascadePSP作为超高清图像的主要对比方法；选择MGMatting作为掩码引导抠图方法，Segfix作为高分辨率分割细化方法；选择PanopticFCN和Entity Segmentor作为全景和实体分割的基准方法。 </li><li><strong>实现细节</strong>    - <strong>模型实现</strong>：使用PyTorch实现模型，使用去掉conv5 x的ResNet - 50作为编码器Eθ。    - <strong>训练设置</strong>：使用Adam优化器，学习率为2.25×10⁻⁴，在22,500和37,500步时将学习率降至十分之一，总步数45,000步。训练输入是从原始图像及其对应的扰动掩码中裁剪的224×224的图像块，扰动掩码是在真实掩码上随机扰动得到，随机IoU阈值在0.8 - 1.0之间。    - <strong>评估设置</strong>：在实验中从连续范围中选择4个缩放比例进行细化，CRM的总推理时间仍不到CascadePSP的一半。</li><li>定量结果评估**    - <strong>对比实验</strong>：在BIG数据集上对比CRM、CascadePSP、Segfix和MGMatting的性能，包括交并比（IoU）、平均边界准确率（mBA）、全景质量（PQ）和平均精度（AP）等指标。结果表明CRM性能更好，在高分辨率图像上运行速度更快，且FLOPs和参数更少。    - <strong>扩展实验</strong>：在全景分割和实体分割实验中，将CRM添加到PanopticFCN和EntitySeg后，它们的分割性能得到增强；在重新标注的Pascal VOC 2012数据集上，CRM比Segfix表现更好，与CascadePSP的IoU相当，但更注重细节</li><li><strong>定性结果展示</strong>    - <strong>可视化对比</strong>：展示CascadePSP、Segfix和CRM的细化结果对比，CRM的细化结果包含更多细节，仅使用语义分割标注训练就能生成类似抠图的结果，并且能更好地重建粗掩码中缺失的部分。    - <strong>应用示例</strong>：展示将CRM应用于全景分割的可视化结果，掩码细节和整体分割效果都有显著改善。</li><li><strong>消融实验</strong>    - <strong>CRM和推理分辨率</strong>：分析CRM和隐式函数对不同分辨率下性能的影响，CRM在低分辨率下能细化出较好的通用掩码，随着分辨率增加，能生成更多细节，mBA提高。    - <strong>CAM和隐式函数</strong>：验证CAM和隐式函数都是CRM不可或缺的部分，二者协同作用能提升性能。    - <strong>推理连续性的影响</strong>：性能随着采样的缩放比例数量增加而提升，更多的缩放比例意味着推理分辨率的连续性更好，有助于提高性能直至收敛。</li></ol><p><strong>结论：</strong></p><p>作者提出了用于超高清图像分割细化的连续细化模型（CRM），并得出以下结论：</p><ol><li><strong>性能优势</strong>：CRM 能连续对齐特征图与细化目标，有助于聚合特征以重建高分辨率掩码上的细节，在性能和速度方面表现出色，实验表明其在超高清图像上的分割效果最佳，还能提升现有全景分割模型的性能。</li><li><strong>泛化能力</strong>：CRM 具有显著的泛化潜力，可处理低分辨率训练和超高清测试之间的分辨率差距，即使从低分辨率细化到高分辨率，总推理时间也不到 CascadePSP 的一半。 </li><li><strong>未来展望</strong>：目前采用“低分辨率训练和超高清测试”的配置，使用超高清图像进行训练和测试仍耗资源，解决该问题将是未来工作方向。</li></ol><p><strong>不足：</strong></p><ol><li><strong>训练资源限制</strong>：目前采用“低分辨率训练和超高分辨率测试”的配置，使用超高分辨率图像进行训练和测试仍面临资源消耗大的问题，这限制了模型在实际应用中对超高分辨率数据的处理能力。</li><li><strong>未来工作待明确</strong>：如使用预训练或低分辨率训练测试。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 超高分辨率图像语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ultra High-resolution Images </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Segment Anything</title>
      <link href="/post/segament-anything/"/>
      <url>/post/segament-anything/</url>
      
        <content type="html"><![CDATA[<h2 id="Meta-AI"><a href="#Meta-AI" class="headerlink" title="Meta AI"></a><strong>Meta AI</strong></h2><p><a href="https://segment-anything.com/">https://segment-anything.com</a></p><blockquote><p>摘要：We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion<br>masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can<br>transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that<br>its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We<br>are releasing the Segment Anything Model (SAM) and cor-responding dataset (SA-1B) of 1B masks and 11M images<br>at segment-anything.com to foster research into foundation models for computer vision</p></blockquote><blockquote><p>翻译：我们推出了Segment Anything (SA)项目：一个全新的图像分割任务、模型和数据集。通过使用我们高效的模型进行数据收集，我们成功构建了迄今为止最大的图像分割数据集，包含超过10亿个分割掩码和1100万张符合许可且尊重隐私的图像。这个模型被特别设计并训练为能够接受简单提示，因此它可以零样本迁移到不同的图像类型和任务。我们在多个任务中测试了该模型，发现它的零样本表现非常优秀——通常与之前的完全监督方法相当，甚至在某些情况下表现更好。我们将在segment-anything.com网站发布Segment Anything Model (SAM)和对应的数据集(SA-1B)，该数据集包括10亿个掩码和1100万张图像，旨在推动计算机视觉领域的基础模型研究。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-28-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-28-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-28-44"></p><p><strong>task, model, dataset, data engine, experiments, responsible AI, release</strong></p><ul><li><p>What <strong>task</strong> will enable zero-shot generalization?</p></li><li><p>What is the corresponding <strong>model</strong> architecture?</p></li><li><p>What <strong>data</strong> can power this task and model?</p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-37-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-37-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-37-11"></p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-39-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-39-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-39-37"  /><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-40-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-40-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_20-40-29"></p><blockquote><p>**研究背景：**本文聚焦于构建图像分割基础模型，其研究背景主要源于自然语言处理（NLP）和计算机视觉领域的发展现状与需求。</p><p>在NLP中，基于大规模网络数据集预训练的大语言模型展现出强大的零样本和少样本泛化能力，通过提示工程可适应多种任务和数据分布，且性能随模型规模、数据集大小和训练计算量的增加而提升。</p><p>计算机视觉领域虽也对基础模型有所探索，如CLIP和ALIGN利用对比学习训练文本和图像编码器实现零样本泛化，但计算机视觉问题广泛，许多问题缺乏充足的训练数据。</p><p>在图像分割方面，尚无网络规模的数据源，且现有方法难以实现强大的泛化能力。因此，本文旨在开发一个可提示的模型，并在广泛的数据集上进行预训练，以解决新数据分布下的一系列下游分割问题。具体通过定义可提示的分割任务、设计相应的模型架构（SAM）以及构建数据引擎收集大规模数据集（SA - 1B）来实现这一目标，从而推动图像分割进入基础模型时代。</p></blockquote><p><strong>研究现状：</strong></p><ul><li><strong>基础模型发展</strong>：大语言模型在自然语言处理（NLP）领域展现出强大的零样本和少样本泛化能力，通过提示工程可适应多种任务和数据分布。计算机视觉领域也在探索基础模型，如CLIP和ALIGN利用对比学习训练文本和图像编码器，实现零样本泛化。</li><li><strong>图像分割任务</strong>：图像分割领域存在多种任务，如交互式分割、边缘检测、实例分割等，但缺乏大规模、多样化的分割数据集，且现有模型在泛化能力和处理模糊提示方面存在不足。</li></ul><p><strong>创新点：</strong></p><ol><li><strong>任务创新</strong>：提出可提示分割任务，能作为预训练目标，通过提示工程实现零样本迁移到下游分割任务。<strong>（Task）</strong></li><li><strong>模型创新</strong>：设计Segment Anything Model（SAM），由图像编码器、提示编码器和掩码解码器组成，支持灵活提示、实时计算且能处理歧义。<strong>（Model）</strong></li><li><strong>数据创新</strong>：构建数据引擎收集SA - 1B数据集，含超10亿掩码，数量和质量远超现有数据集，为模型训练提供强大支撑。<strong>（Data）</strong></li></ol><p><strong>最后提出本文的不足：</strong></p><ol><li><strong>细节处理欠佳</strong>：SAM可能会遗漏图像中的精细结构，有时会生成小的、不相连的虚假组件，且生成的边界不如一些计算密集型的“放大”方法清晰。</li><li><strong>特定场景表现弱</strong>：在提供多个提示点时，专门的交互式分割方法通常会优于SAM，因为SAM更侧重于通用性和广泛适用性，而非高IoU的交互式分割。</li><li><strong>文本到掩码任务待完善</strong>：文本到掩码任务的探索还不够成熟，不够稳健，需要更多的努力来改进。</li><li><strong>特定提示设计困难</strong>：目前尚不清楚如何设计简单的提示来实现语义和全景分割。</li><li><strong>特定领域表现不佳</strong>：在特定领域，一些专门的工具可能会比SAM表现更好。</li></ol><blockquote><p>写作启发：<strong>Promptable Segment（提示词分割</strong>），<strong>Foundation models（基础模型）</strong>，<strong>数据集的创新</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> mata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模板</title>
      <link href="/post/mo-ban/"/>
      <url>/post/mo-ban/</url>
      
        <content type="html"><![CDATA[<p>作者单位</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><h2 id="提出的模型"><a href="#提出的模型" class="headerlink" title="提出的模型"></a>提出的模型</h2><h2 id="实验（Compared-with-SOTA）"><a href="#实验（Compared-with-SOTA）" class="headerlink" title="实验（Compared with SOTA）"></a>实验（Compared with SOTA）</h2><h2 id="实验（Ablation-Experiments）-1st-place-medal"><a href="#实验（Ablation-Experiments）-1st-place-medal" class="headerlink" title="实验（Ablation Experiments）:1st_place_medal:"></a>实验（Ablation Experiments）:1st_place_medal:</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>精读模板</title>
      <link href="/post/jing-du-mo-ban/"/>
      <url>/post/jing-du-mo-ban/</url>
      
        <content type="html"><![CDATA[<p>作者单位</p><table><thead><tr><th>标题</th><th>XXXXXXXXXXXXXXXXXXXXX</th></tr></thead><tbody><tr><td>翻译</td><td>XXXXXXXXXXXXXXXXXXXXXXX</td></tr><tr><td>主题</td><td>XXXXXXXX</td></tr><tr><td>方法</td><td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</td></tr></tbody></table><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><blockquote><p>第一句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第二句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第三句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第四句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第五句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第六句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第七句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第八句：</p></blockquote><blockquote><p>翻译：</p></blockquote><blockquote><p>第九句：</p></blockquote><blockquote><p>翻译：</p></blockquote><h1 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h1><p><strong>研究背景</strong></p><blockquote><p>第一句：随着数字媒体产业的快速发展，由各种资源捕捉到的海量视频数据集正以爆炸式的速度增长。  </p><p>第二句：大多数监控视频只包含有限的重要事件。</p></blockquote><p><strong>提出视频摘要的概念</strong></p><blockquote><p>第三句：用户将视频中的重要事件浓缩转发  </p><p>第四句：提出了视频摘要的概念</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FCN</title>
      <link href="/post/fcn/20250224-fcn/"/>
      <url>/post/fcn/20250224-fcn/</url>
      
        <content type="html"><![CDATA[<h1 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h1><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-51-25.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-51-25.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-51-25"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-53-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-53-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-53-54"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-55-17.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-55-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-55-17"></p><h2 id="FCN基本原理"><a href="#FCN基本原理" class="headerlink" title="FCN基本原理"></a>FCN基本原理</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-03-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-03-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-03-28"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-07-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-07-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-07-54"></p><h2 id="FCN细节"><a href="#FCN细节" class="headerlink" title="FCN细节"></a>FCN细节</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-13-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-13-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-13-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-14-27.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-14-27.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-14-27"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-15-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-15-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-15-24"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-19-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-19-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-19-15"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-20-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-20-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-20-21"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-21-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-21-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-21-01"></p><h2 id="FCN结果"><a href="#FCN结果" class="headerlink" title="FCN结果"></a>FCN结果</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-22-21.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_21-22-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_21-22-21"></p><h1 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h1><h2 id="SegNet的基本原理"><a href="#SegNet的基本原理" class="headerlink" title="SegNet的基本原理"></a>SegNet的基本原理</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-56-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-56-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_08-56-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-57-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_08-57-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_08-57-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-00-18.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-00-18.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-00-18"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-01-08.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-01-08.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-01-08"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-02-54.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-02-54.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-02-54"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-04-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-04-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-04-52"></p><h1 id="UNet"><a href="#UNet" class="headerlink" title="UNet"></a>UNet</h1><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-10-01.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-10-01.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-10-01"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-11-20.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-11-20.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-11-20"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-12-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-12-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-12-11"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-16-06.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_09-16-06.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_09-16-06"></p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FCN模型讲解</title>
      <link href="/post/fcn/20250224-fcn-mo-xing-jiang-jie/"/>
      <url>/post/fcn/20250224-fcn-mo-xing-jiang-jie/</url>
      
        <content type="html"><![CDATA[<figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">class FCN_VGG16(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">    FCN 的 backbone，由 VGG16 修改而来，舍弃最后的全连接层</span></span><br><span class="line"><span class="string">    以池化层为区分，一个池化层到上一个池化层之间的部分认为一个卷积块。</span></span><br><span class="line"><span class="string">    &#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(FCN_VGG16, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            # 第一个卷积块: 输入通道数：3，输出通道数：64，卷积核大小：3<span class="number">*3</span>，步长：1，填充：1</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=3, <span class="attribute">out_channels</span>=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=64, <span class="attribute">out_channels</span>=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第二个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=64, <span class="attribute">out_channels</span>=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=128, <span class="attribute">out_channels</span>=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第三个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=128, <span class="attribute">out_channels</span>=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=256, <span class="attribute">out_channels</span>=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=256, <span class="attribute">out_channels</span>=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第四个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=256, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512,out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">            # 第五个卷积块</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="attribute">in_channels</span>=512, <span class="attribute">out_channels</span>=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),</span><br><span class="line">            nn.ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # 每一层在 features 中的范围，&#123;0，1，2，3，4&#125; 为第一个卷积块，&#123;5，6，7，8，9&#125; 为第二个卷积块<span class="built_in">..</span>.</span><br><span class="line">        self.range = ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31))</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = &#123;&#125;</span><br><span class="line">        # 每一块的输出</span><br><span class="line">        <span class="keyword">for</span> idx, (start, end) <span class="keyword">in</span> enumerate(self.range):</span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> range(start, end):</span><br><span class="line">                input = self.features[layer](input)</span><br><span class="line">            output[<span class="string">&quot;x%d&quot;</span> % (idx + 1)] = input</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def <span class="built_in">test_vgg</span>():</span><br><span class="line">    # Backbone 的测试函数</span><br><span class="line">    input_x = torch.<span class="built_in">randn</span>((<span class="number">1</span>,<span class="number">3</span>,<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line">    vgg = <span class="built_in">FCN_VGG16</span>()</span><br><span class="line">    output_y = <span class="built_in">vgg</span>(input_x)</span><br><span class="line"></span><br><span class="line">    for key in output_y:</span><br><span class="line">        <span class="built_in">print</span>(output_y[key].<span class="built_in">size</span>())</span><br><span class="line"></span><br><span class="line"><span class="built_in">test_vgg</span>()</span><br></pre></td></tr></table></figure><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_10-19-29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_10-19-29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-25_10-19-29"></p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语义分割基础</title>
      <link href="/post/fcn/20250224-tu-xiang-fen-ge-ji-chu/"/>
      <url>/post/fcn/20250224-tu-xiang-fen-ge-ji-chu/</url>
      
        <content type="html"><![CDATA[<h1 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h1><h2 id="1-1-什么是图像分割"><a href="#1-1-什么是图像分割" class="headerlink" title="1.1 什么是图像分割"></a>1.1 什么是图像分割</h2><p>预测目标的轮廓</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-24-33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-24-33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-24-33"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-25-10.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-25-10.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-25-10"></p><h2 id="1-2-图像分割的应用场景"><a href="#1-2-图像分割的应用场景" class="headerlink" title="1.2 图像分割的应用场景"></a>1.2 图像分割的应用场景</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-26-34.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-26-34.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-26-34"></p><p>医学图像分割：医学影像，CT照片等</p><h2 id="1-3-图像分割的前景和背景"><a href="#1-3-图像分割的前景和背景" class="headerlink" title="1.3 图像分割的前景和背景"></a>1.3 图像分割的前景和背景</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-28-51.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-28-51.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-28-51"></p><blockquote><p>things和stuff</p></blockquote><h2 id="1-4-图像分割的三个层次"><a href="#1-4-图像分割的三个层次" class="headerlink" title="1.4 图像分割的三个层次"></a>1.4 图像分割的三个层次</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-31-28.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-31-28.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-31-28"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-33-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-33-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-33-52"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-35-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-35-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-35-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-37-03.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-37-03.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-37-03"></p><h1 id="2-经典数据集"><a href="#2-经典数据集" class="headerlink" title="2.经典数据集"></a>2.经典数据集</h1><h2 id="2-1-PASCAL数据集"><a href="#2-1-PASCAL数据集" class="headerlink" title="2.1 PASCAL数据集"></a>2.1 PASCAL数据集</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-43-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-43-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-43-44"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-22.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-22.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-44-22"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-48.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-44-48.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-44-48"></p><h2 id="2-1Cityscape-用于自动驾驶场景"><a href="#2-1Cityscape-用于自动驾驶场景" class="headerlink" title="2.1Cityscape(用于自动驾驶场景)"></a>2.1Cityscape(用于自动驾驶场景)</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-47-03.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-47-03.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-47-03"></p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-48-10.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-48-10.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-48-10" style="zoom:150%;" /><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-49-14.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-49-14.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-49-14"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-50-40.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-50-40.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-50-40"></p><h2 id="2-3-COCO数据集"><a href="#2-3-COCO数据集" class="headerlink" title="2.3 COCO数据集"></a>2.3 COCO数据集</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-53-15.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-53-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-53-15"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-54-04.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-54-04.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-54-04"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-55-37.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-55-37.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-55-37"></p><h1 id="3-评估指标和优化目标"><a href="#3-评估指标和优化目标" class="headerlink" title="3. 评估指标和优化目标"></a>3. 评估指标和优化目标</h1><h2 id="3-1-语义分割评估指标"><a href="#3-1-语义分割评估指标" class="headerlink" title="3.1 语义分割评估指标"></a>3.1 语义分割评估指标</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-57-42.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-57-42.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-57-42"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-59-26.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_19-59-26.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_19-59-26"></p><h2 id="3-2-语义分割常用优化目标"><a href="#3-2-语义分割常用优化目标" class="headerlink" title="3.2 语义分割常用优化目标"></a>3.2 语义分割常用优化目标</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-04-23.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-04-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-04-23"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-06-05.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-06-05.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-06-05"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-07-51.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-07-51.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-07-51"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-09-59.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-09-59.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-09-59"></p><h1 id="4-上采样"><a href="#4-上采样" class="headerlink" title="4. 上采样"></a>4. 上采样</h1><h2 id="4-1-图像分割网络的两个模块"><a href="#4-1-图像分割网络的两个模块" class="headerlink" title="4.1 图像分割网络的两个模块"></a>4.1 图像分割网络的两个模块</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-13-52.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-13-52.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-13-52"></p><h2 id="4-2-上采样实现方法–插值法"><a href="#4-2-上采样实现方法–插值法" class="headerlink" title="4.2 上采样实现方法–插值法"></a>4.2 上采样实现方法–插值法</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-16-32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-16-32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-16-32"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-18-13.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-18-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-18-13"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-26-45.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-26-45.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-26-45"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-27-49.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-27-49.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-27-49"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-31-39.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-31-39.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-31-39"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-36-24.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-36-24.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-36-24"></p><h2 id="4-3-典型的图像分割网络"><a href="#4-3-典型的图像分割网络" class="headerlink" title="4.3 典型的图像分割网络"></a>4.3 典型的图像分割网络</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-39-19.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_20-39-19.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_20-39-19"></p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语义分割上采样</title>
      <link href="/post/fcn/20250224-yu-yi-fen-ge-shang-cai-yang/"/>
      <url>/post/fcn/20250224-yu-yi-fen-ge-shang-cai-yang/</url>
      
        <content type="html"><![CDATA[<h2 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h2><blockquote><p>可以将较小的 feature 映射回一个较大的 feature map，这样的操作称为上采样，常用的上采样包括转置卷积，反池化，插值等操作。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-54-56.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-54-56.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-54-56"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-55-44.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-55-44.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-55-44"></p><p><strong>小特征图 -&gt; 大特征图</strong></p><p>可学习的上采样：转置卷积</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_15-01-27.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_15-01-27.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_15-01-27"></p><p>转置卷积，也叫反卷积，它并不是正向卷积的完全逆过程，用一句话来解释：</p><blockquote><p>反卷积是一种特殊的正向卷积，先按照一定的比例通过补 0 来扩大输入图像的尺寸，再进行普通的卷积。</p></blockquote><blockquote><p>卷积核大小：kernelsize</p><p>卷积步长：stride</p><p>特征图填充宽度：padding</p></blockquote><p>对于普通的 kernelsize&#x3D;(3,3),strides&#x3D;(2,2) 的卷积，其过程如下图所示</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides.gif" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides.gif" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="padding_strides"></p><p>kernelsize&#x3D;(3,3),strides&#x3D;(1,1) 情况下的反卷积则体现为：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides_transposed.gif" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides_transposed.gif" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="padding_strides_transposed"></p><p>我们可以发现，对于反卷积而言，补 0 主要是通过输入边缘的 padding 和输入内部插 0 实现。</p><p>具体的说，padding 的层数为 $kernelsize−stride$，而对于输入的内部插 0，其插入的 0 的数量为$stride−1$。</p><p><strong>接口与参数说明</strong></p><p><strong>torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride&#x3D;1, padding&#x3D;0, output_padding&#x3D;0, groups&#x3D;1, bias&#x3D;True, dilation&#x3D;1)</strong></p><ol><li><strong>in_channels</strong>(int) – 输入信号的通道数</li><li><strong>out_channels</strong> (int) – 卷积产生的通道数</li><li><strong>kerner_size</strong> (int or tuple) - 卷积核的大小</li><li><strong>stride</strong> (int or tuple,optional) - 卷积步长，即要将输入扩大的倍数。</li><li><strong>padding</strong> (int or tuple, optional) - 输入的每一条边补充0的层数，高宽都增加2*padding</li><li><strong>output_padding</strong> (int or tuple, optional) - 输出边补充0的层数，高宽都增加padding</li><li><strong>groups</strong> (int, optional) – 从输入通道到输出通道的阻塞连接数</li><li><strong>bias</strong> (bool, optional) - 如果bias&#x3D;True，添加偏置</li><li><strong>dilation</strong> (int or tuple, optional) – 卷积核元素之间的间距</li></ol><p>输出尺寸的计算公式为:<br>$$<br>H_{out}&#x3D;(H_{in}-1)<em>stride[0]-2</em>padding[0]+kernelsize[0]+outputpadding[0]\W_{out}&#x3D;(W_{in}-1)<em>stride[1]-2</em>padding[1]+kernelsize[1]+outputpadding[1]<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语义分割概述</title>
      <link href="/post/fcn/20250224-yu-yi-fen-ge-gai-shu/"/>
      <url>/post/fcn/20250224-yu-yi-fen-ge-gai-shu/</url>
      
        <content type="html"><![CDATA[<h2 id="1-什么是语义分割"><a href="#1-什么是语义分割" class="headerlink" title="1.什么是语义分割"></a>1.什么是语义分割</h2><blockquote><p>所谓的分割，就是从像素层面上对图像进行描述，即某一个像素属于哪一类物体</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-03-50.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-03-50.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-03-50"></p><p>分类：<strong>图片级</strong>，比如区分一张图片是猫还是狗</p><p>检测：<strong>区域级</strong>，比如检测一个区域是猫还是狗</p><p>分割：<strong>像素级</strong>，比如区分一个像素是猫还是狗</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-09-11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-09-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-09-11"></p><p><strong>语义分割</strong>：也称为全像素语义分割，它是将每个像素分类为属于对象类的过程。</p><p><strong>实例分割</strong>：是语义分割或全像素语义分割的子类型，它将每个像素分类为属于对象类以及该类的实体 ID。<br>简单的说，语义分割只需要对像素进行分类就行了，而实例分割不仅要对像素进行分类，同一类的不同物体也要进行分割。</p><p><strong>全景分割</strong>： 是语义分割和实例分割的结合，即要对所有目标都检测出来，又要区分出同个类别中的不同实例。</p><h2 id="2-模型的输入和输出"><a href="#2-模型的输入和输出" class="headerlink" title="2.模型的输入和输出"></a>2.模型的输入和输出</h2><blockquote><p>我们先要对模型有一个大的了解。模型的输入是什么？很显然，是一张张的图片；那么输出是什么呢？</p></blockquote><p>我们假设模型的分类数量为 <code>n</code>，输入的图像大小为$W\times H$的 <code>RGB</code> 三通道的图像，那么实际上模型的输出为$W\times D \times n$，这要我们就可以把每一个像素的预测看成是一个分类任务，回顾一下之前的全连接网络的分类模型，对于 <code>n</code> 分类模型，输出节点数为 <code>n</code>。</p><p>所以，在语义分割而言，可以看成是$W\times H$个分类任务，每个像素的分类类别均为 <code>n</code>，所以得到的输出 <code>shape</code> 为$W\times D \times n$。事实上，输出也可能会比实际输入更大或者更小一些，因为网络的设计未必那么完美，这时候需要对边缘进行裁切或者补零</p><h2 id="3-常见的分割模型"><a href="#3-常见的分割模型" class="headerlink" title="3.常见的分割模型"></a>3.常见的分割模型</h2><p>语义分割模型：FCN，RetinaNet，RefineNet，Deeplab 等等</p><p>实例分割模型：Mask RCNN，DeepMask等等</p><h2 id="4-语义分割的思路"><a href="#4-语义分割的思路" class="headerlink" title="4.语义分割的思路"></a>4.语义分割的思路</h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-23-53.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-23-53.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="Snipaste_2025-02-24_14-23-53"></p><p>滑动窗口的思路可以概括如下：</p><ol><li>将图像分割成一个个小的区域；</li><li>遍历一些小区域；</li><li>对这些小区域做分类。</li></ol><p>但是这个思想一个很重要的问题就是效率低，重叠区域的特征反复计算</p><h2 id="5-评价指标"><a href="#5-评价指标" class="headerlink" title="5.评价指标"></a>5.评价指标</h2><p>一般来说，分割的指标主要有两个：像素分类精度（<code>accuracy</code>）和区域的交并比（<code>IoU</code>），下面给出具体的计算公式。</p><p>说明：</p><ul><li><p>$n_{ij}$表示像素点属于分类$i$被预测为分类$j$的像素点数量，</p></li><li><p>$t_i&#x3D;\sum_{j}n_{ij}$表示标签中所有分类为$i$的像素点数量</p></li><li><p>总共有$n_{cl}$个不同的分类。</p></li><li><p>像素分类准确率 (<code>pixel accurarcy</code>)</p></li></ul><p>$$<br>pacc&#x3D;\frac{\sum_{i}n_{ii}}{\sum_{i}t_{i}}<br>$$</p><p>该指标表示所有像素中分类正确的比例</p><ul><li><p>平均准确率 (<code>mean accuracy</code>)：<br>$$<br>macc&#x3D;\frac1{n_{c1}}\sum_{i}\frac{n_{ii}}{t_{i}}<br>$$<br>计算每一类分类正确的像素点数和该类的所有像素点数的比例然后求平均</p></li><li><p>平均交并比 (<code>mean IoU</code>)：<br>$$<br>mIoU&#x3D;\frac{1}{n_{cl}}\sum_{i}\frac{n_{ii}}{(t_{i}+\sum_{j}n_{ji}-n_{ii})}<br>$$<br>其中 IoU 的概念与目标检测中的 IoU 概念一致，表示两个区域的交并比，只不过相比较目标检测的矩形框，在分割任务中，区域为不规则的物体边界。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习环境配置2——windows下的torch=1.2.0环境配置</title>
      <link href="/post/windows-xia-de-torch-1.2.0-huan-jing-pei-zhi/"/>
      <url>/post/windows-xia-de-torch-1.2.0-huan-jing-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Anaconda安装"><a href="#一、Anaconda安装" class="headerlink" title="一、Anaconda安装"></a>一、Anaconda安装</h3><p><strong>Anaconda的安装主要是为了方便环境管理，可以同时在一个电脑上安装多种环境，不同环境放置不同框架：pytorch、tensorflow、keras可以在不同的环境下安装，只需要使用conda create –n创建新环境即可。</strong></p><h4 id="1、Anaconda的下载"><a href="#1、Anaconda的下载" class="headerlink" title="1、Anaconda的下载"></a>1、Anaconda的下载</h4><p>同学们可以选择安装新版Anaconda和旧版的Anaconda，安装步骤没有什么区别。</p><p><strong>旧版本anaconda的下载：</strong><br><strong>新版本的Anaconda没有VSCODE，如果大家为了安装VSCODE方便可以直接安装旧版的Anaconda，百度网盘连接如下。也可以装新版然后分开装VSCODE。</strong><br>链接: <a href="https://pan.baidu.com/s/12tW0Oad_Tqn7jNs8RNkvFA">https://pan.baidu.com/s/12tW0Oad_Tqn7jNs8RNkvFA</a> 提取码: i83n</p><p><strong>新版本anaconda的下载：</strong><br>如果想要安装最新的Anaconda，首先登录Anaconda的官网：<a href="https://www.anaconda.com/distribution/">https://www.anaconda.com/distribution/</a>。直接下载对应安装包就可以。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ccda457f3e2c14fa490e5dee510e15ff.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ccda457f3e2c14fa490e5dee510e15ff.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/79729ea1f6363089a7b848e2bbb41119.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/79729ea1f6363089a7b848e2bbb41119.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>一般是下载64位的，下载完成后打开。</p><h4 id="2、Anaconda的安装"><a href="#2、Anaconda的安装" class="headerlink" title="2、Anaconda的安装"></a>2、Anaconda的安装</h4><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b04e1b9b3c820f4212c77e872f721ff0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b04e1b9b3c820f4212c77e872f721ff0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>选择安装的位置，可以不安装在C盘。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cf41baaf1550d7d707c56da7997bf467.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cf41baaf1550d7d707c56da7997bf467.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>我选择了Add Anaconda to my PATH environment variable，这样会自动将anaconda装到系统的环境变量中，配置会更加方便一些。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/34a7c27d1eb9256186f88e6e610ffbd5.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/34a7c27d1eb9256186f88e6e610ffbd5.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>等待安装完之后，Anaconda的安装就结束了。</p><h3 id="二、Cudnn和CUDA的下载和安装"><a href="#二、Cudnn和CUDA的下载和安装" class="headerlink" title="二、Cudnn和CUDA的下载和安装"></a>二、Cudnn和CUDA的下载和安装</h3><p><strong>我这里使用的是torch&#x3D;1.2.0，官方推荐的Cuda版本是10.0，因此会用到cuda10.0，与cuda10.0对应的cudnn是7.4.1。</strong></p><h4 id="1、Cudnn和CUDA的下载"><a href="#1、Cudnn和CUDA的下载" class="headerlink" title="1、Cudnn和CUDA的下载"></a>1、Cudnn和CUDA的下载</h4><p><strong>网盘下载：</strong><br>链接: <a href="https://pan.baidu.com/s/1znYSRDtLNFLufAuItOeoyQ">https://pan.baidu.com/s/1znYSRDtLNFLufAuItOeoyQ</a><br>提取码: 8ggr</p><p><strong>官网下载：</strong><br>cuda10.0官网的地址是：<br><a href="https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal">cuda10.0官网地址</a><br>cudnn官网的地址是：需要大家进去后寻找7.4.1.5。<br><a href="https://developer.nvidia.com/cudnn">cudnn官网地址</a></p><p>下载完之后得到这两个文件。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/829e732c6e6228e02d96c3b7bd115d9b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/829e732c6e6228e02d96c3b7bd115d9b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8d273ca827020e1e079a78743bd000c5.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8d273ca827020e1e079a78743bd000c5.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h4 id="2、Cudnn和CUDA的安装"><a href="#2、Cudnn和CUDA的安装" class="headerlink" title="2、Cudnn和CUDA的安装"></a>2、Cudnn和CUDA的安装</h4><p>下载好之后可以打开exe文件进行安装。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c1fa30103f2316fc350436a8815d54e0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c1fa30103f2316fc350436a8815d54e0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>这里选择自定义。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bf16ce7629339969e0830a1630bd182.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bf16ce7629339969e0830a1630bd182.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="不"><br>然后直接点下一步就行了。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/d833eb6e1fa90ca9f621eb1072fe25aa.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/d833eb6e1fa90ca9f621eb1072fe25aa.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>安装完后在C盘这个位置可以找到根目录。<br>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0<br>然后大家把Cudnn的内容进行解压。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0ae6fdd762c2435ef118a642b341d4ba.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0ae6fdd762c2435ef118a642b341d4ba.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>把这里面的内容直接复制到C盘的根目录下就可以了。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a88d013177374fcfecfec1e3865e3c5e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a88d013177374fcfecfec1e3865e3c5e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h3 id="三、配置torch环境"><a href="#三、配置torch环境" class="headerlink" title="三、配置torch环境"></a>三、配置torch环境</h3><h4 id="1、pytorch环境的创建与激活"><a href="#1、pytorch环境的创建与激活" class="headerlink" title="1、pytorch环境的创建与激活"></a>1、pytorch环境的创建与激活</h4><p>Win+R启动cmd，在命令提示符内输入以下命令：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create –n pytorch python=3.6</span><br></pre></td></tr></table></figure><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate pytorch</span><br></pre></td></tr></table></figure><p>这里一共存在两条指令：<br>前面一条指令用于创建一个名为pytorch的环境，该环境的python版本为3.6。<br>后面一条指令用于激活一个名为pytorch的环境。</p><h4 id="2、pytorch库的安装"><a href="#2、pytorch库的安装" class="headerlink" title="2、pytorch库的安装"></a>2、pytorch库的安装</h4><p>由于我们所有的操作都要在对应环境中进行，所以在进行库的安装前需要先激活环境。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate pytorch </span><br></pre></td></tr></table></figure><p>此时cmd窗口的样子为：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e67dbf5cfc4f4125fedbffcb3bd85b77.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e67dbf5cfc4f4125fedbffcb3bd85b77.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h5 id="a、官方推荐安装方法（推荐）"><a href="#a、官方推荐安装方法（推荐）" class="headerlink" title="a、官方推荐安装方法（推荐）"></a>a、官方推荐安装方法（推荐）</h5><p>打开pytorch的官方安装方法：<br><a href="https://pytorch.org/get-started/previous-versions/">https://pytorch.org/get-started/previous-versions/</a><br>官网推荐的安装代码如下，我使用的是Cuda10的版本，不太懂为什么要写3个&#x3D;才能正确定位，两个&#x3D;会定位到cuda92的whl：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># CUDA 10.0</span><br><span class="line">pip install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure><p>这是pytorch官方提供的指令，用于安装torch和torchvision。</p><h5 id="b、先下载whl后安装"><a href="#b、先下载whl后安装" class="headerlink" title="b、先下载whl后安装"></a>b、先下载whl后安装</h5><p>需要注意的是，直接这样安装似乎特别慢，因此我们可以进入如下网址:<br><a href="https://download.pytorch.org/whl/torch_stable.html">https://download.pytorch.org/whl/torch_stable.html</a><br>找到自己需要的轮子下载。下载的时候使用迅雷下载就行了，速度还是比较快的！<br><img src="https://i-blog.csdnimg.cn/blog_migrate/08b8a756b9d7d214ce81f10bb5b73758.png#pic_center" class="lazyload placeholder" data-srcset="https://i-blog.csdnimg.cn/blog_migrate/08b8a756b9d7d214ce81f10bb5b73758.png#pic_center" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"  /><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/737ffc86979d1e7ceda0d98b5ddcef41.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/737ffc86979d1e7ceda0d98b5ddcef41.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>下载完成后找到安装路径：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1eae8e7fae0ea98cc5559e6287059451.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1eae8e7fae0ea98cc5559e6287059451.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>在cmd定位过来后利用文件全名进行安装就行了！<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/32c32f513e9be037243f885cd6f4ef11.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/32c32f513e9be037243f885cd6f4ef11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>这里我也传一个百度网盘的版本。<br>链接: <a href="https://pan.baidu.com/s/14-QVk7Kb_CVwaVZxVPIgtw">https://pan.baidu.com/s/14-QVk7Kb_CVwaVZxVPIgtw</a><br>提取码: rg2e<br><strong>全部安装完成之后重启电脑。</strong></p><h4 id="3、其它依赖库的安装"><a href="#3、其它依赖库的安装" class="headerlink" title="3、其它依赖库的安装"></a>3、其它依赖库的安装</h4><p>但如果想要跑深度学习模型，还有一些其它的依赖库需要安装。具体如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scipy==1.2.1</span><br><span class="line">numpy==1.17.0</span><br><span class="line">matplotlib==3.1.2</span><br><span class="line">opencv_python==4.1.2.30</span><br><span class="line">torch==1.2.0</span><br><span class="line">torchvision==0.4.0</span><br><span class="line">tqdm==4.60.0</span><br><span class="line">Pillow==8.2.0</span><br><span class="line">h5py==2.10.0</span><br></pre></td></tr></table></figure><p>如果想要更便捷的安装可以在桌面或者其它地方创建一个requirements.txt文件，复制上述内容到txt文件中。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/7f76e6ad79f6bed1e2f4676b627354d3.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/7f76e6ad79f6bed1e2f4676b627354d3.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>使用如下指令安装即可。<strong>下述指令中，requirements.txt前方的路径是我将文件放在桌面的路径，各位同学根据自己的电脑修改。</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -r C:\Users\33232\Desktop\requirements.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="4、安装较慢请注意换源"><a href="#4、安装较慢请注意换源" class="headerlink" title="4、安装较慢请注意换源"></a>4、安装较慢请注意换源</h4><p>需要注意的是，如果在pip中下载安装比较慢可以换个源，可以到用户文件夹下，创建一个pip文件夹，然后在pip文件夹里创建一个txt文件。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/28006284902c6a57318e718daccee1a8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/28006284902c6a57318e718daccee1a8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>修改txt文件的内容，并且把后缀改成ini</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = http://pypi.mirrors.ustc.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">use-mirrors =true</span><br><span class="line">mirrors =http://pypi.mirrors.ustc.edu.cn/simple/</span><br><span class="line">trusted-host =pypi.mirrors.ustc.edu.cn</span><br></pre></td></tr></table></figure><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/783a72953baad1fd9de83303701cbaf8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/783a72953baad1fd9de83303701cbaf8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8bd41332dee625e1c6e182608acb9a29.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8bd41332dee625e1c6e182608acb9a29.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br><strong>全部安装完成之后重启电脑。</strong></p><h3 id="四、安装VSCODE"><a href="#四、安装VSCODE" class="headerlink" title="四、安装VSCODE"></a>四、安装<a href="https://so.csdn.net/so/search?q=VSCODE&spm=1001.2101.3001.7020">VSCODE</a></h3><p><strong>我个人喜欢VSCODE，所以就安装它啦。其它的编辑软件也可以，个人喜好罢了。</strong></p><h4 id="1、下载安装包安装（推荐）"><a href="#1、下载安装包安装（推荐）" class="headerlink" title="1、下载安装包安装（推荐）"></a>1、下载安装包安装（推荐）</h4><p><strong>最新版本的Anaconda没有VSCODE因此可以直接百度VSCODE进行安装。</strong></p><h5 id="a、VSCODE的下载"><a href="#a、VSCODE的下载" class="headerlink" title="a、VSCODE的下载"></a>a、VSCODE的下载</h5><p>直接加载VSCODE的官网<a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a>，点击Download for Windows即可下载。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f69abbceb271a9dc5d12142e76df4ebc.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f69abbceb271a9dc5d12142e76df4ebc.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h5 id="b、VSCODE的安装"><a href="#b、VSCODE的安装" class="headerlink" title="b、VSCODE的安装"></a>b、VSCODE的安装</h5><p>首先同意协议，点一下步。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a80d57bcc63ffc394fb5b59aed099347.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/a80d57bcc63ffc394fb5b59aed099347.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>其他里面的几个勾要打起来，因为这样就可以右键文件夹用VSCODE打开，非常方便。下一步。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4156c82700455d0ab65ea5bb8f68eeb3.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4156c82700455d0ab65ea5bb8f68eeb3.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>继续下一步安装即可。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6523e522b685fc8512cacaedfb1934d8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6523e522b685fc8512cacaedfb1934d8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p><strong>安装完成后在左下角更改自己的环境就行了。</strong><br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4a6e8c3ce2dec68836338fdcc57a0dc1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4a6e8c3ce2dec68836338fdcc57a0dc1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><h4 id="2、anaconda上安装"><a href="#2、anaconda上安装" class="headerlink" title="2、anaconda上安装"></a>2、anaconda上安装</h4><p>打开anaconda，切换环境。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/361dd496e006335bd418e8b03e91354e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/361dd496e006335bd418e8b03e91354e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"><br>安装VSCODE，安装完就可以launch一下了，之后就可以把VScode固定到任务栏上，方便打开。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f781c276062dded3ab0d4c9f37aef3bf.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f781c276062dded3ab0d4c9f37aef3bf.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> CUDA </tag>
            
            <tag> cudnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vscode配置latex环境</title>
      <link href="/post/latex/visual-studio-code/"/>
      <url>/post/latex/visual-studio-code/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>LaTeX</strong> 作为一种强大的排版系统，对于理工科，特别是公式比较多的数学专业（秃头专业），其重要性自不必多说，不在本文探讨范围之内。</p><p>而选择一个比较好的编译器是很重要的，至少对笔者而言是如此。笔者前期使用的是<strong>TeXstudio</strong>进行文档的编译的，但是其编译速度比较慢，并且页面不是很美观。最让人头疼的是，当公式比较长的时候，使用的括号就比较多，但<strong>Texstudio</strong>的代码高亮功能实在是…（它对于括号根本就没有高亮，头秃）</p><p>而<strong>Visual Studio Code</strong>呢？话不多说，直接上图！<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5b90636b6875472ff796fe988e98826e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5b90636b6875472ff796fe988e98826e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="展示图"></p><p>可以看到，它不仅能够对代码高亮，不同级别括号用不同颜色标注了，颜值也很高。且 vscode 最突出的特点就是其强大的插件功能，每个使用者都能够根据自己的需求和想法下载相应的插件，从而将之配置为高度个性化的编辑器。可以这么说，每个使用者的 vscode 都不一样，为其专属定制编辑器。</p><p>笔者配置了好久，找了很多资料，很多博主也只是贴上了配置代码，没有详细的介绍说明。为了让更多人能够有一个比较清晰的了解，以此可以随时对自己的配置代码进行更改，故笔者写下了此文。希望能够对大家有所帮助。</p><p>注 1： 本文使用图片均为笔者自身编辑器截图或笔者朋友的编辑器截图（经过对方同意），且所有引用在文中或文末注明了来源，其余均为原创内容（代码不算哈哈哈）。</p><p>注 2： 若您的 vscode 页面和笔者所用图片中展示的页面有略微不同，均为笔者所安装的其余插件以及其余设置所致，并不影响本文中所说的所有配置，您无需担心，只需按照图片中所指向图标进行配置即可。</p><p>注 3： 文末有完整的个人配置代码（有的地方需要更改路径，有具体说明）。</p><h2 id="1-TeX-Live-下载与安装"><a href="#1-TeX-Live-下载与安装" class="headerlink" title="1 TeX Live 下载与安装"></a>1 TeX Live 下载与安装</h2><p>笔者选用的 Tex 系统是 TeX Live ，如果您想了解 TeX Live 和 MiKTeX 的区别，可以查看此篇文章：<a href="https://www.cnblogs.com/liuliang1999/p/12656706.html">https://www.cnblogs.com/liuliang1999/p/12656706.html</a></p><p>接下来是 TeX Live 的<strong>下载与安装说明</strong>：</p><p>① 通过网址 ：<a href="http://tug.org/texlive/acquire-iso.html">http://tug.org/texlive/acquire-iso.html</a> 进入 ISO 下载页面，点击图示红框圈画位置进入随机的镜像网站。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b238c6ecae7799a007459ab0cacddfdc.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b238c6ecae7799a007459ab0cacddfdc.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="下载网址"><br>② 可以看到的是，笔者进入了清华大学镜像网站，点击红框圈画链接进行 TeX Live 下载。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4cea74b28cf59bd59bd28cb03bdcb6e4.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4cea74b28cf59bd59bd28cb03bdcb6e4.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="下载"></p><p>③ 如果<strong>下载速度过慢</strong>，可以返回前一页面，进行重新点击，随机进入另一镜像网站进行下载尝试，直到下载速度在您的可接受范围内即可。或者在前一页面，点击 <strong>“mirror list”</strong> 进入镜像列表</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2a9a2b2fa4447587774d4f2f4eb1c9fb.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2a9a2b2fa4447587774d4f2f4eb1c9fb.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="mirror list"></p><p>然后手动选择某一镜像网站进行下载：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/45f4adf32411091223ef279f96fda7ed.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/45f4adf32411091223ef279f96fda7ed.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="镜像列表选择"></p><p>④ 找到下载好的压缩包，右键，在打开方式中选择**“Windows 资源管理器”**打开</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/db2bbda5fb583717e095af7edf5465a6.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/db2bbda5fb583717e095af7edf5465a6.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="资源管理器打开"></p><p>⑤ 找到 <strong>“install-tl-windows”</strong> 文件，为了后面不必要的麻烦，右键<strong>以管理员身份运行</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b666c4f9426f2fdf0a4a69bc86413ada.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b666c4f9426f2fdf0a4a69bc86413ada.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="以管理员身份运行"></p><p>⑥ 会先出现下图，无需理会，等会儿会消失</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/94aa279e1d0f6a59c34d95f4e85e0666.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/94aa279e1d0f6a59c34d95f4e85e0666.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="出现狮子"></p><p>⑦ **基本更改：**出现下图后，需要进行路径的更改；由于 TeX Live 自带的 TeXworks 不怎么好用，并且此文主要将 vscode 作为 LaTeX 的编辑器，故而取消 <strong>安装 TeXworks 前端</strong>的选项，再点击安装</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5382a34df2373e3c3febc4172ad4af1d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5382a34df2373e3c3febc4172ad4af1d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="修改路径"></p><p>⑧ <strong>个性化安装：</strong> 如果您需要个性化程度高的话，那么可以点击上图左下角的 <strong>Advancde</strong> ，根据您的需要进行相应的更改，但<strong>建议</strong>在不明白各个选项的作用时，不要对其进行修改，以免后期使用产生奇怪的问题。要注意的是，<strong>Adjust searchpath</strong> 这个选项一定要选中，将之添加到环境变量，否则后期手动添加比较麻烦；<br>而对于我们大部分人来说，只需要更改下图框选出的部分即可，也就是上图所完成的功能，再点击<strong>安装</strong>即可</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/64e11c5a3bfce6dae1620833aa528df6.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/64e11c5a3bfce6dae1620833aa528df6.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="advanced"></p><p>⑨ **进行安装：**接着就会出现下图，具体的安装指标已在下图标明，可根据其数字来判断安装所需时间。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/22a9567ff88b850c6de3365fd1bd570e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/22a9567ff88b850c6de3365fd1bd570e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="安装ing"></p><p>当上面标示的时间安装完之后，会出现一些配置文件的安装运行写入，进行等待即可，几分钟左右：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4bd5a634521b8a18328976fa6884d688.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4bd5a634521b8a18328976fa6884d688.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="安装后续"></p><p>当出现下图所示弹窗时，说明安装完毕，点击关闭即可。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/dd26e23808c26c27614803e4fbca04ca.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/dd26e23808c26c27614803e4fbca04ca.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="欢迎进入"></p><p>⑩ <strong>检查安装是否正常：</strong> 按win + R 打开<strong>运行</strong>，输入<code>cmd</code>，打开命令行窗口；然后输入命令<code>xelatex -v</code> ，如下图</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4042a6e9ec2aa56e6b1442fa1d4c96a0.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/4042a6e9ec2aa56e6b1442fa1d4c96a0.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="安装检查"></p><p>如上图所示，若输出了一些版本信息，则安装正常。</p><h2 id="2-vscode下载与安装"><a href="#2-vscode下载与安装" class="headerlink" title="2 vscode下载与安装"></a>2 vscode下载与安装</h2><p>官网下载： <a href="https://code.visualstudio.com/">Click here to download Visual Studio Code</a>.</p><p>点进去之后就可以进行下载了。具体安装过程与常见的软件安装过程一致，这里就不作赘述。笔者只对几个要点进行提及：</p><p>① 记得<strong>修改安装路径</strong></p><p>② 根据个人想法可以选择是否在开始菜单文件夹创建 vscode 的快捷方式</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b52103381640e671ae6421fd329a600e.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b52103381640e671ae6421fd329a600e.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p>③ 一定要选上”添加到PATH”这个选项，能省很多麻烦。其余如图所示，自行选择。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c721d489a01c6fde012b9777eaae64ea.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c721d489a01c6fde012b9777eaae64ea.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="在这里插入图片描述"></p><p>安装好之后，打开 vscode，应如下图页面所示：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/56e8910ef21f541e6a699bb00f0a4f57.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/56e8910ef21f541e6a699bb00f0a4f57.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="启动页面"></p><h2 id="3-中文语言环境配置"><a href="#3-中文语言环境配置" class="headerlink" title="3 中文语言环境配置"></a>3 中文语言环境配置</h2><p>vscode的中文环境需要下载插件来进行支持。如下图所示：</p><p>① 点击拓展图标，打开拓展；</p><p>② 输入”Chinese”，选择第一个Chinese (Simplified) Language Pack for Visual Studio Code插件；</p><p>③ 点击”install”进行安装，等待安装完成；</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e74e7680283789d9bec131c5f69234d7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e74e7680283789d9bec131c5f69234d7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="下载中文插件"></p><p>④ 点击页面右下角跳出窗口中的”Restart now”，进行 vscode 重启。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/02a0f5f7d9a509bab220dfc4fd2b26b9.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/02a0f5f7d9a509bab220dfc4fd2b26b9.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="vscode重启"></p><p>⑤ 完成中文环境配置，显示如下：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57e75ca8310629f3fc4e2daf27cc8610.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57e75ca8310629f3fc4e2daf27cc8610.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="中文环境启动页面"></p><h2 id="4-LaTeX的支持插件-LaTeX-Workshop安装"><a href="#4-LaTeX的支持插件-LaTeX-Workshop安装" class="headerlink" title="4 LaTeX的支持插件 LaTeX Workshop安装"></a>4 LaTeX的支持插件 LaTeX Workshop安装</h2><p>① 点击拓展图标，打开拓展；</p><p>② 输入”latex workshop”，选择第一个LaTeX Workshop插件；</p><p>③ 点击”install”进行安装，等待安装完成；</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/254d1f24e8a90b349a77958b5c192d4d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/254d1f24e8a90b349a77958b5c192d4d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="latex workshop安装"></p><p>④ 若在安装完该插件之后在 vscode 页面右下角跳出如下弹窗，无需在意，只是提醒该插件已经更新到了8.11.1版本。若您想要了解新版本增加的功能，可以点击”Change log”进行查看；若不想了解，点击 “Disable this message” 即可。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c96cdbe7e01da3ff326da9df306918d1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/c96cdbe7e01da3ff326da9df306918d1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="弹窗"></p><h2 id="5-打开LaTeX环境设置页面"><a href="#5-打开LaTeX环境设置页面" class="headerlink" title="5 打开LaTeX环境设置页面"></a>5 打开LaTeX环境设置页面</h2><p>① 点击设置图标</p><p>② 点击设置</p><p>③ 转到 UI 设置页面</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2ee6e759bdd67de85e8380d332db1a1a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2ee6e759bdd67de85e8380d332db1a1a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="打开设置"></p><p>④ 点击下图 1 处打开 json 文件，进入代码设置页面</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b8d742a52940e90d31777f7efd9943f1.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/b8d742a52940e90d31777f7efd9943f1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="json设置"></p><p>注 4 ： UI 设置页面和代码设置页面均为设置页面，其功能是一样的。不同的是，UI 设置页面交互能力较强，但一些设置需要去寻找，比较麻烦；而代码设置页面虽然相对 UI 而言不那么直观，但却可以对自己想要的功能直接进行代码编写，且代码设置可以直接克隆别人的代码到自己的编辑器中，从而直接完成相应设置，比较便捷。</p><p>注 5 ： 可以直接按Ctrl + ，进入设置页面。</p><h2 id="6-LaTeX环境的代码配置"><a href="#6-LaTeX环境的代码配置" class="headerlink" title="6 LaTeX环境的代码配置"></a>6 LaTeX环境的代码配置</h2><h3 id="6-1-LaTeX配置代码展示"><a href="#6-1-LaTeX配置代码展示" class="headerlink" title="6.1 LaTeX配置代码展示"></a>6.1 LaTeX配置代码展示</h3><p>先给出效果图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43a5acbf6be4a170f4a9dbdbbcff2209.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43a5acbf6be4a170f4a9dbdbbcff2209.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="LaTeX代码配置"></p><p>LaTeX 配置代码如下（不包含外部 pdf 查看器设置）：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;latex-workshop.latex.autoBuild.run&quot;: &quot;never&quot;,</span><br><span class="line">    &quot;latex-workshop.showContextMenu&quot;: true,</span><br><span class="line">    &quot;latex-workshop.intellisense.package.enabled&quot;: true,</span><br><span class="line">    &quot;latex-workshop.message.error.show&quot;: false,</span><br><span class="line">    &quot;latex-workshop.message.warning.show&quot;: false,</span><br><span class="line">    &quot;latex-workshop.latex.tools&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;-pdf&quot;,</span><br><span class="line">                &quot;-outdir=%OUTDIR%&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.latex.recipes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;XeLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;PDFLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;BibTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;bibtex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;LaTeXmk&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;latexmk&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex -&gt; bibtex -&gt; xelatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.latex.clean.fileTypes&quot;: [</span><br><span class="line">        &quot;*.aux&quot;,</span><br><span class="line">        &quot;*.bbl&quot;,</span><br><span class="line">        &quot;*.blg&quot;,</span><br><span class="line">        &quot;*.idx&quot;,</span><br><span class="line">        &quot;*.ind&quot;,</span><br><span class="line">        &quot;*.lof&quot;,</span><br><span class="line">        &quot;*.lot&quot;,</span><br><span class="line">        &quot;*.out&quot;,</span><br><span class="line">        &quot;*.toc&quot;,</span><br><span class="line">        &quot;*.acn&quot;,</span><br><span class="line">        &quot;*.acr&quot;,</span><br><span class="line">        &quot;*.alg&quot;,</span><br><span class="line">        &quot;*.glg&quot;,</span><br><span class="line">        &quot;*.glo&quot;,</span><br><span class="line">        &quot;*.gls&quot;,</span><br><span class="line">        &quot;*.ist&quot;,</span><br><span class="line">        &quot;*.fls&quot;,</span><br><span class="line">        &quot;*.log&quot;,</span><br><span class="line">        &quot;*.fdb_latexmk&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.latex.autoClean.run&quot;: &quot;onFailed&quot;,</span><br><span class="line">    &quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;,</span><br><span class="line">    &quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注 6 ： 若您不想要配置外部查看器以及了解内部查看和外部查看之间切换操作，可以直接复制上述代码至 json 文件中，即可完成 LaTeX 的配置，从而可以对 LaTeX 代码进行编译。</p><p>注 7 ： 根据 json 文件编写规则，每个代码语句（除了代码块儿最后一句）都需要加上英文状态下的<code>,</code>，否则就会报错；而每个代码块儿的最后一句是不需要加上<code>,</code>的。从上文整个代码块儿可以看出此规则。</p><p>如果您日后需要在上述代码之后再添加其他代码，请记得在最后一句</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;</span><br></pre></td></tr></table></figure><p>后添加上<code>,</code>，即变为</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;,</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>其中的<code>......</code>为您添加的其余代码。</p><p><strong>切记！</strong></p><h3 id="6-2-LaTeX配置代码解读"><a href="#6-2-LaTeX配置代码解读" class="headerlink" title="6.2 LaTeX配置代码解读"></a>6.2 LaTeX配置代码解读</h3><p>如果您对此不感兴趣，可以跳过该小节。下面进行代码<strong>注释解读</strong>：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.autoBuild.run&quot;: &quot;never&quot;</span><br></pre></td></tr></table></figure><p>设置何时使用默认的(第一个)编译链自动构建 LaTeX 项目，即什么时候自动进行代码的编译。有三个选项：</p><ol><li><strong>onFileChange</strong>：在检测任何依赖项中的文件更改(甚至被其他应用程序修改)时构建项目，即当检测到代码被更改时就自动编译tex文件；</li><li><strong>onSave</strong> : 当代码被保存时自动编译文件；</li><li><strong>never</strong>: 从不自动编译，即需编写者手动编译文档</li></ol><p>此项笔者设置为<strong>never</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.showContextMenu&quot;: true</span><br></pre></td></tr></table></figure><p>启用上下文LaTeX菜单。此菜单默认状态下停用，即变量设置为<strong>false</strong>，因为它可以通过新的 LaTeX 标记使用（新的 LaTeX 标记能够编译文档，将在下文提及）。只需将此变量设置为<strong>true</strong>即可恢复菜单。即此命令设置是否将编译文档的选项出现在鼠标右键的菜单中。</p><p>下图展示两者区别，左边为设置<strong>false</strong>情况，右边为设置<strong>true</strong>情况。可以看到的是，设置为<strong>true</strong>时，菜单中多了两个选项，其中多出来的第一个选项为进行tex文件的编译，而第二个选项为进行正向同步，即从代码定位到编译出来的 pdf 文件相应位置，下文会进行提及。<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/80cef33dcac33ac0e86c82c101461f3b.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/80cef33dcac33ac0e86c82c101461f3b.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="无"><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bfedcbe76bb0f0ee3a27db3f6e4d538.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bfedcbe76bb0f0ee3a27db3f6e4d538.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="有"><br>笔者觉得菜单多了此选项较方便，故此项笔者设置为<strong>true</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.intellisense.package.enabled&quot;: true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>设置为<strong>true</strong>，则该拓展能够从使用的宏包中自动提取命令和环境，从而补全正在编写的代码。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.message.error.show&quot;  : false,</span><br><span class="line">&quot;latex-workshop.message.warning.show&quot;: false</span><br></pre></td></tr></table></figure><p>这两个命令是设置当文档编译错误时是否弹出显示出错和警告的弹窗。因为这些错误和警告信息能够从终端中获取，且弹窗弹出比较烦人，故而笔者设置均设置为<strong>false</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.tools&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;-pdf&quot;,</span><br><span class="line">                &quot;-outdir=%OUTDIR%&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>这些代码是定义在下文 recipes 编译链中被使用的编译命令，此处为默认配置，不需要进行更改。其中的<code>name</code>为这些命令的标签，用作下文 recipes 的引用；而<code>command</code>为在该拓展中的编译方式。</p><p>可以更改的代码为，将编译方式: pdflatex 、 xelatex 和 latexmk 中的<code>%DOCFILE</code>更改为<code>%DOC</code>。<code>%DOCFILE</code>表明编译器访问没有扩展名的根文件名，而<code>%DOC</code>表明编译器访问的是没有扩展名的根文件完整路径。这就意味着，使用<code>%DOCFILE</code>可以将文件所在路径设置为中文，但笔者不建议这么做，因为毕竟涉及到代码，当其余编译器引用时该 tex 文件仍需要根文件完整路径，且需要为英文路径。笔者此处设置为<code>%DOCFILE</code>仅是因为之前使用 TeXstudio，导致路径已经是中文了。</p><p>更多详情可以访问 github 中 LaTeX-Workshop 的 Wiki: <a href="https://github.com/James-Yu/LaTeX-Workshop/wiki/Compile#placeholders">Click here for more details about this.</a></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.recipes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;XeLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;PDFLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;BibTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;bibtex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;LaTeXmk&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;latexmk&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex -&gt; bibtex -&gt; xelatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>此串代码是对编译链进行定义，其中<code>name</code>是标签，也就是出现在工具栏中的链名称；<code>tool</code>是<code>name</code>标签所对应的编译顺序，其内部编译命令来自上文<code>latex-workshop.latex.recipes</code>中内容。</p><p>定义完成后，能够在 vscode 编译器中能够看到的编译顺序，具体看下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/bfcdea57459bf5f1687f3a4c548e868a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/bfcdea57459bf5f1687f3a4c548e868a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="编译链"></p><p>可以看到的是，在编译链中定义的命令出现在了vscode右侧的工具栏中。</p><p>注 8 ： <strong>PDFLaTeX</strong> 编译模式与 <strong>XeLaTeX</strong> 区别如下：</p><blockquote><ol><li><p>PDFLaTeX 使用的是TeX的标准字体，所以生成PDF时，会将所有的非 TeX 标准字体进行替换，其生成的 PDF 文件默认嵌入所有字体；而使用 XeLaTeX 编译，如果说论文中有很多图片或者其他元素没有嵌入字体的话，生成的 PDF<br>文件也会有些字体没有嵌入。</p></li><li><p>XeLaTeX 对应的 XeTeX 对字体的支持更好，允许用户使用操作系统字体来代替 TeX 的标准字体，而且对非拉丁字体的支持更好。</p></li><li><p>PDFLaTeX 进行编译的速度比 XeLaTeX 速度快。</p></li></ol></blockquote><p>注 9 ： 编译链的存在是为了更方便编译，因为如果涉及到**.bib**文件，就需要进行多次不同命令的转换编译，比较麻烦，而编译链就解决了这个问题。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.clean.fileTypes&quot;: [</span><br><span class="line">        //&quot;*.aux&quot;,</span><br><span class="line">       // &quot;*.bbl&quot;,</span><br><span class="line">        &quot;*.blg&quot;,</span><br><span class="line">        &quot;*.idx&quot;,</span><br><span class="line">        &quot;*.ind&quot;,</span><br><span class="line">        &quot;*.lof&quot;,</span><br><span class="line">        &quot;*.lot&quot;,</span><br><span class="line">        &quot;*.out&quot;,</span><br><span class="line">        &quot;*.toc&quot;,</span><br><span class="line">        &quot;*.acn&quot;,</span><br><span class="line">        &quot;*.acr&quot;,</span><br><span class="line">        &quot;*.alg&quot;,</span><br><span class="line">        &quot;*.glg&quot;,</span><br><span class="line">        &quot;*.glo&quot;,</span><br><span class="line">        &quot;*.gls&quot;,</span><br><span class="line">        &quot;*.ist&quot;,</span><br><span class="line">        &quot;*.fls&quot;,</span><br><span class="line">        &quot;*.log&quot;,</span><br><span class="line">        &quot;*.fdb_latexmk&quot;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>这串命令则是设置编译完成后要清除掉的辅助文件类型，若无特殊需求，无需进行更改。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.autoClean.run&quot;: &quot;onFailed&quot;</span><br></pre></td></tr></table></figure><p>这条命令是设置什么时候对上文设置的辅助文件进行清除。其变量有：</p><ol><li><strong>onBuilt</strong> : 无论是否编译成功，都选择清除辅助文件；</li><li><strong>onFailed</strong> : 当编译失败时，清除辅助文件；</li><li><strong>never</strong> : 无论何时，都不清除辅助文件。</li></ol><p>由于 tex 文档编译有时需要用到辅助文件，比如编译目录和编译参考文献时，如果使用<code>onBuilt</code>命令，则会导致编译不出完整结果甚至编译失败；</p><p>而有时候将 tex 文件修改后进行编译时，可能会导致 pdf 文件没有正常更新的情况，这个时候可能就是由于辅助文件没有进行及时更新的缘故，需要清除辅助文件了，而<code>never</code>命令做不到这一点；</p><p>故而笔者使用了<code>onFailed</code>，同时解决了上述两个问题。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;</span><br></pre></td></tr></table></figure><p>该命令的作用为设置 vscode 编译 tex 文档时的默认编译链。有两个变量：</p><ol><li><strong>first</strong> : 使用<code>latex-workshop.latex.recipes</code>中的第一条编译链，故而您可以根据自己的需要更改编译链顺序；</li><li><strong>lastUsed</strong> : 使用最近一次编译所用的编译链。</li></ol><p>笔者选择使用<strong>lastUsed</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;</span><br></pre></td></tr></table></figure><p>用于反向同步（即从编译出的 pdf 文件指定位置跳转到 tex 文件中相应代码所在位置）的内部查看器的快捷键绑定。变量有两种：</p><ol><li><strong>ctrl-click</strong> ： 为默认选项，使用Ctrl&#x2F;cmd+鼠标左键单击</li><li><strong>double-click</strong> : 使用鼠标左键双击</li></ol><p>此处笔者使用的为<strong>double-click</strong>。</p><h2 id="7-tex文件编译"><a href="#7-tex文件编译" class="headerlink" title="7 tex文件编译"></a>7 tex文件编译</h2><h3 id="7-1-tex测试文件下载"><a href="#7-1-tex测试文件下载" class="headerlink" title="7.1 tex测试文件下载"></a>7.1 tex测试文件下载</h3><p>为了测试 vscode 功能是否比较完整，笔者编写了一份简单的 tex 文件，以此测试其是否支持中英文，能否编译目录，能否插入图片，能否进行引用，能否编译参考文献（编译bixtex文件）等功能。</p><p>测试所用的 tex 文件可以从 github 下载：<a href="https://github.com/Ali-loner/Ali-loner.github.io">Click here to download the LaTeX testfile for vscode</a></p><p><strong>下载步骤</strong>如图：<br><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6aca9b730b7c4ee2946a88fbe6ac40ad.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6aca9b730b7c4ee2946a88fbe6ac40ad.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="测试文件下载"><br>将之下载后，进行解压。</p><p>注 10 ： 若因网络原因无法连接到github导致无法下载，可以使用自己的tex文件进行测试，或者复制以下代码进行文档的简单编译测试，但其只能测试一部分功能：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[a4paper]&#123;article&#125;</span><br><span class="line">\usepackage[margin=1in]&#123;geometry&#125; % 设置边距，符合Word设定</span><br><span class="line">\usepackage&#123;ctex&#125;</span><br><span class="line">\usepackage&#123;lipsum&#125;</span><br><span class="line">\title&#123;\heiti\zihao&#123;2&#125; This is a test for vscode&#125;</span><br><span class="line">\author&#123;\songti Ali-loner&#125;</span><br><span class="line">\date&#123;2020.08.02&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">\maketitle</span><br><span class="line">\begin&#123;abstract&#125;</span><br><span class="line">\lipsum[2]</span><br><span class="line">\end&#123;abstract&#125;</span><br><span class="line">\tableofcontents</span><br><span class="line">\section&#123;This is a section&#125;</span><br><span class="line">Hello world! 你好，世界 ！</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><h3 id="7-2-tex-测试文件编译"><a href="#7-2-tex-测试文件编译" class="headerlink" title="7.2 tex 测试文件编译"></a>7.2 tex 测试文件编译</h3><p><strong>① 打开测试文件所在文件夹</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/927b15521ea5f9c20d3ba6c426e098ba.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/927b15521ea5f9c20d3ba6c426e098ba.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="打开文件夹"></p><p><strong>② 点击选中 tex 文件</strong>，进行文件内容查看</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6a28c240c45d77c6972a7e2fb6cd62a8.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6a28c240c45d77c6972a7e2fb6cd62a8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="tex文件查看"></p><p><strong>③ 开始编译文件。</strong> 由于进行测试的文件中涉及参考文献的引用（<strong>.bib</strong>的编译），故而选择<code>xelatex -&gt; bibtex -&gt; xelatex*2</code>编译链。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/306b76bae450c39ddd5a7cea4074c84a.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/306b76bae450c39ddd5a7cea4074c84a.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="开始编译"></p><p>注 11 ： 为了更方便进行编译，可对其设置快捷键，设置快捷键步骤如下：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/24c26cd24b87c3f1e59568b8b8e8bb00.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/24c26cd24b87c3f1e59568b8b8e8bb00.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="快捷键绑定"></p><p>笔者将快捷键设置为Ctrl+Alt+R。</p><p><strong>选中tex文件的代码页面</strong>（若未选中，则无法进行编译），然后按下该快捷键，在编辑器页面上端进行编译链选择，如下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cbaed92ca564f9dbbb4204e0d9c2fee7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/cbaed92ca564f9dbbb4204e0d9c2fee7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="快捷键编译"></p><p><strong>④ 编译成功</strong></p><p>当发现页面下方出现 <strong>√</strong> 符号时，说明编译成功，相反，如果出现 <strong>×</strong> 符号，说明编译失败，就要找失败原因了。</p><p><strong>a.</strong> 左侧工具栏</p><p>当编译成功后，选中 tex 文件中任意的代码，以此来选中 tex 文件，然后进行图示操作。其中侧边栏所展现的就是上文提及的新的 LaTeX 标记。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/04f02e79054c25953a761251a72e8682.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/04f02e79054c25953a761251a72e8682.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="pdf查看"></p><p><strong>b.</strong> 快捷键</p><p>选中 tex 文件中任意的代码，然后按Ctrl+Alt+V，出现编译好的 pdf 页面。该快捷键为默认设置。若您想要更改，可以根据上文进行配置。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/518090c31940a4a61b55f75a31f7c923.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/518090c31940a4a61b55f75a31f7c923.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="编译成功"></p><p>注意到，现在编译的结果为内部查看器查看。</p><p><strong>⑤ 正向同步测试</strong>，即从代码定位到 pdf 页面相应位置。有以下三种方法：</p><p><strong>a.</strong> 使用侧边工具栏</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/20bc13379b5e1d69eecb53c98175e896.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/20bc13379b5e1d69eecb53c98175e896.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="正向同步1"></p><p><strong>b.</strong> 使用右键菜单</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ac759bac5a92854811260f5ea5f56f0f.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ac759bac5a92854811260f5ea5f56f0f.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="正向同步2"><br><strong>c.</strong> 使用快捷键</p><p>选中需要跳转的代码所在行，按Ctrl+Alt+J，右侧就会跳转到相应行。这里的快捷键为默认设置，可自行通过上文方式设置为您想要的快捷键。</p><p><strong>⑥ 反向同步测试</strong>,即从 pdf 页面定位到代码相应位置</p><p>在编译生成的 pdf 上，选中想要跳转行，鼠标左键双击或ctrl+鼠标左键单击，跳转到对应代码。此处快捷键的选择为上文设置，若使用笔者的代码，则为鼠标左键双击。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bc30efe92862befcd35000da3b3eb93.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/1bc30efe92862befcd35000da3b3eb93.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="反向同步"></p><h2 id="8-SumatraPDF-安装设置（可选）"><a href="#8-SumatraPDF-安装设置（可选）" class="headerlink" title="8 SumatraPDF 安装设置（可选）"></a>8 SumatraPDF 安装设置（可选）</h2><p>您可自行选择是否需要设置此部分内容。</p><p>有的时候，由于想要看到 pdf 文件的完整展现效果，使用内置查看器已无法满足需求，这时可以使用外部查看器进行查看。</p><p>外部查看器的优势是能够看到 pdf 文件在查看器中的目录，可以实时进行跳转；且根据笔者使用来看，外部查看器展示出来的 pdf 默认会放大一些，使得字体变大，要更加让人舒服一些。</p><p>笔者选择 <strong>SumatraPDF</strong> 作为外部查看器，该软件的优点在于在具有 pdf 阅读功能的同时很轻量，安装包不到 10MB 大小，且支持双向同步功能。通过调整其与 vscode 的窗口位置，能够在拥有这些优势的同时，达到与内置 pdf 查看具有相同的效果。</p><h3 id="8-1-SumatraPDF下载与安装"><a href="#8-1-SumatraPDF下载与安装" class="headerlink" title="8.1 SumatraPDF下载与安装"></a>8.1 SumatraPDF下载与安装</h3><p>官网下载：<a href="https://www.sumatrapdfreader.org/download-free-pdf-viewer.html">Click here to download SumatraPDF</a></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/46ef16263fbbb2396ceb7029c1963a32.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/46ef16263fbbb2396ceb7029c1963a32.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="sumatraPDF"></p><p>其安装很简单，与通用软件安装过程一致，记得更改安装路径并记住，下文配置需要使用其路径。</p><h3 id="8-2-使用SumatraPDF查看的代码配置"><a href="#8-2-使用SumatraPDF查看的代码配置" class="headerlink" title="8.2 使用SumatraPDF查看的代码配置"></a>8.2 使用SumatraPDF查看的代码配置</h3><h4 id="8-2-1-代码展示"><a href="#8-2-1-代码展示" class="headerlink" title="8.2.1 代码展示"></a>8.2.1 代码展示</h4><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;,</span><br><span class="line">    &quot;latex-workshop.view.pdf.ref.viewer&quot;:&quot;auto&quot;,</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.args&quot;: [</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.args&quot;: [</span><br><span class="line">        &quot;-forward-search&quot;,</span><br><span class="line">        &quot;%TEX%&quot;,</span><br><span class="line">        &quot;%LINE%&quot;,</span><br><span class="line">        &quot;-reuse-instance&quot;,</span><br><span class="line">        &quot;-inverse-search&quot;,</span><br><span class="line">        &quot;code \&quot;F:/Microsoft VS Code/resources/app/out/cli.js\&quot; -r -g \&quot;%f:%l\&quot;&quot;, // 注意修改路径</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此代码仅为展示所用，让您进行查看，为下文解读之用。如需写入到 json 文件内，可直接完整复制文末笔者的个人配置到自己的编译器内。</p><h4 id="8-2-2-代码解读"><a href="#8-2-2-代码解读" class="headerlink" title="8.2.2 代码解读"></a>8.2.2 代码解读</h4><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;</span><br></pre></td></tr></table></figure><p>设置默认的pdf查看器，有三种变量参数：</p><ol><li><strong>tab</strong> : 使用 vscode 内置 pdf 查看器；</li><li><strong>browser</strong> : 使用电脑默认浏览器进行 pdf 查看；</li><li><strong>external</strong> : 使用外部 pdf 查看器查看。</li></ol><p>此处选择 <strong>external</strong> 参数，使用外部查看器。</p><p>注 12 ： 此参数为下文进行pdf内部查看和外部查看进行切换的关键参数。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.ref.viewer&quot;:&quot;auto&quot;</span><br></pre></td></tr></table></figure><p>设置PDF查看器用于在 <strong>\ref</strong> 命令上的[View on PDF]链接，此命令作用于 <strong>\ref</strong> 引用查看。有三个参数变量：</p><ol><li><strong>auto</strong> : 由编辑器根据情况自动设置；</li><li><strong>tabOrBrowser</strong> : 使用vscode内置pdf查看器或使用电脑默认浏览器进行pdf查看；</li><li><strong>external</strong> : 使用外部pdf查看器查看。</li></ol><p>此处设置为<strong>auto</strong>。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.viewer.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;// 注意修改路径</span><br></pre></td></tr></table></figure><p>使用外部查看器时要执行的命令，设置外部查看器启动文件<strong>SumatraPDF.exe</strong>文件所在位置，此处需要您根据自身情况进行路径更改，正常情况下只需更改磁盘盘符即可。</p><p><strong>请注意</strong>中间为 “ &#x2F; “ 而不是” \ “ ，不然会报错。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.viewer.args&quot;: [</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>此代码是设置使用外部查看器时，<code>latex-workshop.view.pdf.external.view .command</code>的参数。<code>%PDF%</code>是用于生成PDF文件的绝对路径的占位符。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.synctex.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot; // 注意修改路径</span><br></pre></td></tr></table></figure><p>此命令是将生成的辅助文件 <strong>.synctex.gz</strong> 转发到外部查看器时要执行的命令,设置其位置参数，您注意更改路径，此路径为 <strong>SumatraPDF.exe</strong> 文件路径。与上文相同。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&quot;latex-workshop.view.pdf.external.synctex.args&quot;: [</span><br><span class="line">        &quot;-forward-search&quot;,</span><br><span class="line">        &quot;%TEX%&quot;,</span><br><span class="line">        &quot;%LINE%&quot;,</span><br><span class="line">        &quot;-reuse-instance&quot;,</span><br><span class="line">        &quot;-inverse-search&quot;,</span><br><span class="line">        &quot;code \&quot;F:/Microsoft VS Code/resources/app/out/cli.js\&quot; -r -g \&quot;%f:%l\&quot;&quot;// 注意修改路径</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>设置当 <strong>.synctex.gz</strong> 文件同步到外部查看器时<code>latex-workshop.view.pdf.external.synctex</code>的参数。<code>%LINE%</code>是行号，<code>%PDF%</code>是生成PDF文件的绝对路径的占位符，<code>%TEX%</code>是当触发syncTeX被触发时，扩展名为 <strong>.tex</strong> 的 LaTeX 文件路径。</p><p>上面代码串中记得进行 <strong>Microsoft VS Code</strong> 路径修改，修改如下图:</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43de3a7d084a4b2b2eae6a259ba24f33.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/43de3a7d084a4b2b2eae6a259ba24f33.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="路径修改"></p><h2 id="9-SumatraPDF-的使用"><a href="#9-SumatraPDF-的使用" class="headerlink" title="9 SumatraPDF 的使用"></a>9 SumatraPDF 的使用</h2><p>将完整代码复制到自己的 json 文件内后，即可使用 SumatraPDF作为自己的 pdf 外部查看器了。以下为具体操作：</p><p>① 点击编辑页面任意位置来选中 tex 文件；<br>② 按Ctrl+Alt+V，打开编译出的 pdf 文件；<br>③ 出现如下图页面。可以看到的是，原本内嵌输出的 pdf 变为了在 SumatraPDF 上查看，且侧面带有书签：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e5003691bd206012a74ecddf6c7bfa82.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/e5003691bd206012a74ecddf6c7bfa82.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="sumatrapdf查看"></p><p>④ 为了出现和内嵌输出具有相同的效果，可以将 vscode 和 SumatraPDF 进行分屏，且根据需要关闭标签，如下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57974febf6f2c77c5b9bb9e22235887d.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/57974febf6f2c77c5b9bb9e22235887d.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="分屏查看"></p><p>⑤ 且同样支持双向同步（正向同步和反向同步），其操作步骤与内嵌输出 pdf 时操作步骤相同，此处就不再赘述。查看效果图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6118b9cb1ea3e22d80a5a04edd5fb6a7.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/6118b9cb1ea3e22d80a5a04edd5fb6a7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="双向同步"></p><h2 id="10-pdf-内部查看与外部查看的切换"><a href="#10-pdf-内部查看与外部查看的切换" class="headerlink" title="10 pdf 内部查看与外部查看的切换"></a>10 pdf 内部查看与外部查看的切换</h2><p>以下展示由外部查看转为内部查看的操作，由内转外操作相同。</p><p>共有两种操作方式：<strong>UI界面设置</strong> 或 <strong>Json界面设置</strong> 。具体见下图：</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8fde188378a7f0d0d15e09941768c364.png" class="lazyload placeholder" data-srcset="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8fde188378a7f0d0d15e09941768c364.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="内外切换"></p><p>您可根据个人适应选择相应的方法。</p><h2 id="11-个人完整配置"><a href="#11-个人完整配置" class="headerlink" title="11 个人完整配置"></a>11 个人完整配置</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> //------------------------------LaTeX 配置----------------------------------</span><br><span class="line">    // 设置是否自动编译</span><br><span class="line">    &quot;latex-workshop.latex.autoBuild.run&quot;:&quot;never&quot;,</span><br><span class="line">    //右键菜单</span><br><span class="line">    &quot;latex-workshop.showContextMenu&quot;:true,</span><br><span class="line">    //从使用的包中自动补全命令和环境</span><br><span class="line">    &quot;latex-workshop.intellisense.package.enabled&quot;: true,</span><br><span class="line">    //编译出错时设置是否弹出气泡设置</span><br><span class="line">    &quot;latex-workshop.message.error.show&quot;: false,</span><br><span class="line">    &quot;latex-workshop.message.warning.show&quot;: false,</span><br><span class="line">    // 编译工具和命令</span><br><span class="line">    &quot;latex-workshop.latex.tools&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;xelatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;pdflatex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;latexmk&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-synctex=1&quot;,</span><br><span class="line">                &quot;-interaction=nonstopmode&quot;,</span><br><span class="line">                &quot;-file-line-error&quot;,</span><br><span class="line">                &quot;-pdf&quot;,</span><br><span class="line">                &quot;-outdir=%OUTDIR%&quot;,</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;bibtex&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;%DOCFILE%&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    // 用于配置编译链</span><br><span class="line">    &quot;latex-workshop.latex.recipes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;XeLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;PDFLaTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;BibTeX&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;bibtex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;LaTeXmk&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;latexmk&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;xelatex -&gt; bibtex -&gt; xelatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;xelatex&quot;,</span><br><span class="line">                &quot;xelatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;,</span><br><span class="line">            &quot;tools&quot;: [</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;bibtex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;,</span><br><span class="line">                &quot;pdflatex&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    //文件清理。此属性必须是字符串数组</span><br><span class="line">    &quot;latex-workshop.latex.clean.fileTypes&quot;: [</span><br><span class="line">        &quot;*.aux&quot;,</span><br><span class="line">        &quot;*.bbl&quot;,</span><br><span class="line">        &quot;*.blg&quot;,</span><br><span class="line">        &quot;*.idx&quot;,</span><br><span class="line">        &quot;*.ind&quot;,</span><br><span class="line">        &quot;*.lof&quot;,</span><br><span class="line">        &quot;*.lot&quot;,</span><br><span class="line">        &quot;*.out&quot;,</span><br><span class="line">        &quot;*.toc&quot;,</span><br><span class="line">        &quot;*.acn&quot;,</span><br><span class="line">        &quot;*.acr&quot;,</span><br><span class="line">        &quot;*.alg&quot;,</span><br><span class="line">        &quot;*.glg&quot;,</span><br><span class="line">        &quot;*.glo&quot;,</span><br><span class="line">        &quot;*.gls&quot;,</span><br><span class="line">        &quot;*.ist&quot;,</span><br><span class="line">        &quot;*.fls&quot;,</span><br><span class="line">        &quot;*.log&quot;,</span><br><span class="line">        &quot;*.fdb_latexmk&quot;</span><br><span class="line">    ],</span><br><span class="line">    //设置为onFaild 在构建失败后清除辅助文件</span><br><span class="line">    &quot;latex-workshop.latex.autoClean.run&quot;: &quot;onFailed&quot;,</span><br><span class="line">    // 使用上次的recipe编译组合</span><br><span class="line">    &quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;,</span><br><span class="line">    // 用于反向同步的内部查看器的键绑定。ctrl/cmd +点击(默认)或双击</span><br><span class="line">    &quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //使用 SumatraPDF 预览编译好的PDF文件</span><br><span class="line">    // 设置VScode内部查看生成的pdf文件</span><br><span class="line">    &quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;,</span><br><span class="line">    // PDF查看器用于在\ref上的[View on PDF]链接</span><br><span class="line">    &quot;latex-workshop.view.pdf.ref.viewer&quot;:&quot;auto&quot;,</span><br><span class="line">    // 使用外部查看器时要执行的命令。此功能不受官方支持。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    // 使用外部查看器时，latex-workshop.view.pdf.external.view .command的参数。此功能不受官方支持。%PDF%是用于生成PDF文件的绝对路径的占位符。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.viewer.args&quot;: [</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ],</span><br><span class="line">    // 将synctex转发到外部查看器时要执行的命令。此功能不受官方支持。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.command&quot;: &quot;F:/SumatraPDF/SumatraPDF.exe&quot;, // 注意修改路径</span><br><span class="line">    // latex-workshop.view.pdf.external.synctex的参数。当同步到外部查看器时。%LINE%是行号，%PDF%是生成PDF文件的绝对路径的占位符，%TEX%是触发syncTeX的扩展名为.tex的LaTeX文件路径。</span><br><span class="line">    &quot;latex-workshop.view.pdf.external.synctex.args&quot;: [</span><br><span class="line">        &quot;-forward-search&quot;,</span><br><span class="line">        &quot;%TEX%&quot;,</span><br><span class="line">        &quot;%LINE%&quot;,</span><br><span class="line">        &quot;-reuse-instance&quot;,</span><br><span class="line">        &quot;-inverse-search&quot;,</span><br><span class="line">        &quot;code \&quot;F:/Microsoft VS Code/resources/app/out/cli.js\&quot; -r -g \&quot;%f:%l\&quot;&quot;, // 注意修改路径</span><br><span class="line">        &quot;%PDF%&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>写在最后</strong> ： 笔者也只是一个初学者，文中如果出现错误的地方，欢迎您在评论区批评指正，笔者会虚心接受这些产生错误的地方，争取以后学得更扎实再编写这些文字。</p><p>另：若您感觉此文写得勉强还行，希望您能够不吝点赞，给笔者一点小小的激励，以此来进行更多更好的文字编写。非常感谢！！！</p><p>注：转载自<a href="https://zhuanlan.zhihu.com/p/166523064">https://zhuanlan.zhihu.com/p/166523064</a></p>]]></content>
      
      
      <categories>
          
          <category> latex </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vscode </tag>
            
            <tag> latex </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
